# CIS-5200 Fall 2025 - Lecture 1: Introduction to Machine Learning

## Course Context and Related Courses

Okay, as we used to do when I was, like, in third grade. People remember that? She remembers that, it's good. 

Okay, so there are lots of other courses at Penn. There was one era when I first created this course by breaking apart AI into two pieces, when this was the only machine learning course. Now, there are lots of them on campus. Some of them sit over in Statistics and Wharton. Trust me, they're actually nice people—my best friends are statisticians at Wharton. It's a good place. They have courses.

Also, to be clear, if you want to learn deep learning, we will cover deep learning as a piece of machine learning. This is not a deep learning course. I teach those too, and other people teach them mostly in Electrical Engineering these days. There are lots of good courses if you don't end up here. I will love you just as much if you leave. 

Questions on courses, apart from getting into this one? Any other questions? Good.

## Instructor Introduction

Okay, um... Introduction! Great, I'm Lyle Unger, uh, my home department is Computer and Information Science. These days, I mostly do computer science meets psychology. I build chatbots to teach K-12 education, to teach English as a second language, through coaching and therapy.

Over the decades, I've done lots of stuff. I've worked in computational biology and economics, and I lectured this morning to Executive Wharton, so my interests are pretty eclectic. The thing I love about machine learning is you get to talk to everybody and help them with their problems.

So I spend a lot of time talking to people who are hernia doctors, trying to understand how to improve the hernia procedure, which is sort of gross, but it's a good thing to improve. And what else should I say about myself?

### My Relationship with Mathematics

Um, I love math and I hate math. This is a mathy course. I really don't see the point in most theorem proving. I really, really do see the point in the intuition. I like to think in terms of matrices and norms and distances and kernels. I think these things are actually useful.

Um, I'm hoping that half of you, and I'm afraid that half of you will say the course is too mathy. In which case, you're in the wrong course. We have other ones that are less mathy at Penn, lots of them. Half of you will say it's not mathy enough. That's life. Um, it will be an unhappy compromise. We'll circle around that.

But it is a course that really tries to cover, in some sense, all of machine learning, which is hopelessly broad. We'll talk about it in a second, and it tries to put it on some sort of mathematical foundation. 

Cool! Um, let's do a quick...

## Student Demographics

Who are you? We'll do this two ways. One, just show of hands. Um, Computer Science undergraduates? Lots of them.

Security master's students. You in? Computer Science PhDs? Hey, um, Wharton Statistics people. None today? Usually, at least there's one statistician to keep—oh, there we go, yay! Awesome!

Fun, you'll see lots of stuff like MLE and MAP that you've seen before. Hopefully this will be like, "oh yeah, it makes more sense this different way." And I'll use different language than you will, in terms of what a model is. You can sort of think, "why do these computer scientists use the same word for something different?" It'll be fun.

Um, people in the medical school, bio world, biomedical, yeah, lots of... yay, go GCB! I have an appointment at GCB. Um... Mechanical? Yeah. Yay!

Um, and I missed a whole bunch. My point is: you guys come from really different backgrounds.

## Course Pace and Teaching Style

I'm going to talk... oh yeah, really fast. I watch all my podcasts at 1.5× or 1.75×. I kept, like, I've internalized it. So, I will cover things really quickly.

Half the time, you'll be going, "yeah, linear regression again." Like, okay, step back and think: What am I trying to learn? What's a different way to think about it?

Half of the time, you're going, "whoa, we just covered entropy in, like, 15 minutes, and there's, like, whole books on entropy. I haven't read Cover & Thomas yet, you know?"

Great, go back, read some more, catch up on it. So realize that the course is gonna go at a fairly relentless pace. And you individually will go back and forth, going, "wow, it's slow, wow, it's fast." And I really can't help with that. We'll try and provide secondary resources and things there.

Okay, now, turn to someone next to you, introduce yourself, and... Why are you in this course? What do you hope to learn? Maybe where are you from? Say hello to each other.

Oh, say hello, I'm curious, a very nice person. You can talk to them, too, they're very nice. Say hello! You know each other? Great! Then talk to someone else. You all know... oh, you guys are useless. Okay.

[Student interaction time]

Okay! Okay... coming back in, deep breath, yay!

Okay. Excellent. Okay, you're clearly a very friendly crowd, this is awesome, I love the noise, it's like being in a bar, as I almost... was there. Great, um... Cool!

## Student Motivations

I don't know, what do people want to learn from this course? Why are you here? What would you like? Yep.

**Student:** Foundation, understand current literature.

Yeah, hopefully you will know all the jargon that shows up in the current literature. We're not gonna read current literature in this course. But yes, after this, you should be prepared to read almost anything out there. Yeah.

**Student:** How machines learn.

Hmm... What does that mean?

I used to teach AI, and I would start by saying in the first lecture, "I don't believe in artificial intelligence. There's no such thing—just a set of tools and techniques." But sure, we'll talk about how machines get better at making predictions. We won't talk much about—I also teach cognitive science, how do humans and rats learn—but that's not this course, yeah?

**Student:** Don't want to call APIs anymore, want to know how these things are written.

So we're gonna both, in this course—this course is highly schizophrenic. There'll be me trying to push the math intuition. There'll be some super short quiz questions to check to see you know the jargon of what is a norm. Then there will be some Jupyter notebooks where you write stuff which are not API calls, but in fact scikit-learn and PyTorch, which are darn close to API calls. And then there'll be some things where you have a little bit of math behind it. 

So my hope is that you will magically tie together this notion that when you call a random forest on scikit-learn, that you:
- A) of course, know what the split criterion of entropy is, conditional entropy, and 
- B) probably more importantly, that you understand what the hyperparameters are and how they affect how well it works.

So in that sense, it's super applied, yeah.

## Machine Learning vs. LLMs

**Student:** Works better.

So here's the... So, there is this view of the world that there's machine learning, and then there's large language models and deep learning and cool things. Um, that's not quite right, actually. Um, what there is, is classic machine learning—the first half of the course—which, in terms of software, scikit-learn makes a point of not putting in anything that's deep learning.

This is great for tabular data, things that look like Excel spreadsheets, and currently beats any LLM on those.

And then there's lots of problems where you want to use some form of a large language model or another. All of which... are machine learning.

And we'll see that the LLMs are, in fact, one... and there are a bunch of them. There are diffusion models, and there are attention models, and there are CNN models, so we'll cover a little bit of the large language model world, but very much from a machine learning perspective. And I think it's, in some sense, a false dichotomy to say, you know, "LLMs do this, machine learning does this better than LLMs." I'm going, "wait, wait, LLMs are sort of a kind of machine learning." So they're not quite different. Uh, maybe one more, yeah.

**Student:** What is reconstructive versus straight up mean?

We'll try and understand how they work. Again, most of my goal is to help you understand them in a way that you can sort of understand when and why they work, and what assumptions are behind them.

Cool! Um... Good.

## Course Structure

So structure: Lectures twice a week. Oh, Monday, by the way, is a holiday in America, so I won't be here Monday. Go out hiking, won't miss you there. Um...

I'm also gonna be doing slides, eventually I will hook up and do some little bit of scribbling for the more mathy things. I will use Poll Everywhere to try and do things, we'll get there in a second. There's a wiki and a bunch of other pieces.

There are recitations on Thursdays and Fridays, which is a chance—I love to talk to all of you one-on-one, but it's hard to get some sort of more pieces there. If you go to Canvas, you can sign up. A lot of you have, a lot of you haven't. Um... oh, attendance...

### The Attendance Debate

There's a huge debate, and I hate to pull you into it, but hey, you're here. Um, among the faculty of what the best way to help people learn is.

I know I have to give you a grade, but I'm actually much more interested in your learning. Um, and one version of it is: I give lectures on the chalkboard, and I don't record them. Show up if you want to learn it. If not, it's not my problem.

Most of my life, I've actually given lectures like this and made the Zoom recordings available for anyone who wants to see them. My experience is a lot of people are like, "Yeah, awesome, there's no point coming to class, because I can watch the Zoom recording." And then they don't quite get around to watching the Zoom recording. And then they sit down and do the homework on the assumption that that will actually cover all the material that gets them ready to do the exam, which it doesn't, because the homework is teaching something different than the exams, to a large extent.

So, I'm trying an experiment, which is asking you all to show up. And I will try to be engaging, and you will try to be engaged, and we'll see how it works. 

I know it—I talked to my daughter, who just graduated from college. She's like, "you're gonna require attendance? My computer science course was great, I skipped all the lectures and just went to the online hands-on stuff. I learned so much because the teacher was so slow." 

But okay! Um, maybe. We'll see, there'll be a chance every week for you to give me feedback. I will modify the course. I haven't taught this course since the pandemic—life is different now. I mean, I've taught lots of other courses.

So, we'll see. But at the moment, my hope is to hammer the TAs into doing a really cool job where you're there talking and engaging and meeting people. The recitations will be a chance for you to meet other people to do your final project with. It'll be nice.

Cool! Um, there are office hours, which you may not need, because, hey, if you have a question, you could ask a large language model—it's like GPT-4 level, good at that, but there'll be lots of them. 

Um, Ed, which, if you're new to Penn, is a standard discussion platform that Penn uses. It's not my favorite, but it's fine. It's much more effective for questions than trying to email me or an individual TA, because I have a whole huge staff doing things. 

Uh, there'll be Jupyter Notebooks, there'll be homeworks, yeah.

### The LaTeX and Handwriting Experiment

Oh, the other weird thing about homeworks! How to do them, like, going back and forth. Always in this course, I've required everyone to learn LaTeX, which is the language that machine learning researchers work in. Finally, I sort of decided, hey, that low level is really not super necessary. There is really good code now that you write your equations by hand, it OCRs it straight into LaTeX. That sort of low-level coding is really not that important.

I feel pretty much the same about Python. Almost everything you want to do in Python, I need you to think in vectors, not in scalars. But in terms of the actual syntax of the coding, Gemini is really good.

I tried one of the Jupyter notebooks, I put the questions up, and I forgot to disable Gemini? It answered all the coding questions before I could even read them. Oh, it was very embarrassing. So, um.

We're going to try an experiment which is to have you, for the handwritten homeworks, for the first time in time immemorial, write them by hand. Like with a pen on paper, or a stylus on an iPad—I'm not opposed to modern handwriting. And:
- A) that will discourage you from just pasting into GPT.
- As far as I can tell, last semester, the people who used GPT got better scores on their homework than the people that did it themselves.

So I can say, statistically, that's with GPT-4, or 4.0, maybe o3, I'm not sure. Right? So, already last semester, for answering most of these questions, GPT is better than you are.

Cool, frightening, whatever. Most of you hope to grow up and get jobs someday. I think that understanding this stuff is really critical. And so I'm doing a lot of things that are putting friction into your life. I'm a psychologist. It's sort of annoying to write things out by hand, as opposed to, oh, letting GPT autocomplete it for you. But the friction is gonna help you learn it.

And I'll get feedback, you'll see how you like it. But I think it's worth thinking about—oh, there are exams. In the modern era, of course, as interviewers have discovered, if you do a job interview over Zoom, you can have GPT answer all the questions for you. There are nice things that display it, they're really good. Um, so in person, so midterm and final will be in this room. We'll talk more about that later, but, um, multiple choice, cheat sheet allowed.

### Quizzes as Learning Tools

Cool! And quizzes! Oh, the other thing about quizzes? Most people do quizzes to assess and see how good you are, to give you a grade. Most of the quizzes in this class are for you to self-assess and to help you learn. It turns out that taking a quiz actually is a highly instructive per-minute use of your time.

And so there will be, for each lecture, just 5 short questions. Go through, answer them. Um, most of them will tell you immediately, are you right or wrong? The goal is review and understanding, right? 

So a lot of this stuff is structured to be slightly annoying, or frictionful, in a way that hopefully helps you learn.

Questions on the bureaucracy? Yeah?

**Student:** Do we have...?

This week, there are recitations. We'll be flexible about counting attendance, because there are a couple people who are stuck in China trying to get visas and a couple of annoying—welcome to the modern world, don't let me talk about it.

Um, so... So, yes, there are recitations this week. You should go and sign up, but we'll be super relaxed about, you know, if you have made other plans or can't do it, well, you know, it'll be chill.

**Student:** Yes?

Uh, the recitations are 60 minutes. In fact, they'll be 50 minutes. So yes, 50-minute recitations, um, and there's... they're... a couple on Thursdays, they're mostly on Friday during the day.

Good, good.

## Weekly Schedule

Okay. Um... I just said it. Monday, Wednesday, cover stuff here. Quizzes coming out Wednesday. Thursday, Friday, recitations. Homework due the next week. 

Super short cycle, because I'm hoping you will stay up, keep up with the stuff, do it. It shouldn't be taking huge amounts of time. This course covers an enormous amount of stuff. Most of it fairly simple and straightforward. If you keep up with it, it should be fine.

**If you are spending 15 hours a week on the homework, come see one of the TAs or me. You're spending too much time, something is not going right.**

Right? So, good homework should be things that hopefully help you get exposure. What does the code look like? What does the idea look like? Check it, do this thing differently. It shouldn't be hugely, hugely time consuming. If it is, cycle back. Cool, yes.

Recitations, I just said, go to Canvas. If you can't make it, let your recitation leader know, they'll have contact information. Lots of concepts, yes, yes, yes, yes. I just said all that.

## Course Topics Overview

Um, we're gonna cover a lot of stuff. I won't read through them all, but these are a bunch of the sort of standard buzzwords in machine learning. Um, we're obviously not going to cover all of deep learning or all of anything. But, you know, we'll talk about it. Um.

And you'll see a little bit of... run some code to see what it looks like, think about it. Hopefully you're thinking. If it's something that is busy work and not thinking, well, LLMs will do it anyway, so what's the point?

Uh, we'll also cover a bunch of math, which people mostly have seen or not. Um... How many people know what KL divergence is, just so I get a quick... Ooh, almost all, uh, cool, okay, that'll be fun. MLE versus MAP?

One statistician's gotta raise his hand. Okay, so a lot of stuff that you guys haven't seen, great. Lots of jargon, lots of straightforward, um, concepts that are not super widely known in undergraduate computer science.

We'll do EM algorithm, we'll do reinforcement learning—quick two weeks that could be a whole semester.

Um, good. Everything is on Canvas. If you're registered for the course, you're in Canvas. If you go to Canvas, if you click there, you'll get to Gradescope, you'll get to Ed. Go to everything from Canvas. Homeworks are posted there, it's the central shopping area to find everything.

Great. And then, if you have questions, click from Canvas to Ed, and oh, look, if you wouldn't mind, on Ed, check to see if the question was answered. There's a couple hundred of you, someone probably asked the same question. Someone probably answered it. We'll try and get back to you within 24 hours with questions there.

Um, if they're general questions that everybody should know, like, "why is this equation in the homework wrong?" post them publicly, it's good. If there's something you need to ask more privately, you know, hey, "I think I'm getting COVID, should I come to class?" you can post it privately.

Um, by the way, don't come to class if you have COVID, but yeah, okay. But, you know, a few questions that are private, you can post them privately as... both of those. 

**Super simple thing: Private is not the same as anonymous.** It's okay to post something anonymously, I don't mind that, but to ask, "hey, I have a problem, could you contact me?" if it's anonymous, it's actually anonymous. I can't contact you—I haven't broken the security.

Cool! Um, good.

## Textbooks and Resources

Our bureaucracy, also bureaucracy. There are lots of textbooks, lots of them are open source. If you go to Canvas, there's a bunch of PDFs that have lots of textbooks there. I don't actually think most of you will read any of them. But I will post some particular chapters for background reading. 

In general, for most things, there'll be a little bit of "you should make sure you know these concepts," but mostly it'll be, "Hey, if you didn't follow the lecture really well, here's the chapter you should read to see that." And we'll have also lots of custom readings attached to the pieces.

So they're all out there, they're all good, but I'm not gonna follow a textbook. I probably stole the most from Chris Bishop's book. Um, don't waste—

Okay!

## The Elephant in the Room: AI and Employment

Do the non-Americans know what "the elephant in the room" is? This is a weird American expression, right? It's something that's so big that no one wants to talk about it. You can't miss it. AI-generated, of course. Um, yeah, pretty good, right? It's even better on a darker screen, but yeah. Something so big that it has to be there, but often not discussed. 

Of course, the elephant for all of this...

So, what do I think about... oh, I should have brought the slide. I talked to Wharton Exec Ed course, and I showed a slide from a recent study last week by Erik Brynjolfsson from Stanford, looking at employment by age category in the programming sector, the tech sector, the one where most of you will end up. The GCB guys are separate, it's okay. Um, and what it shows, if you look over the last 8 years:

- The employment of number of people at a company—it goes up if you're 30 and over, it just sort of goes up and is fine. 
- If you're under 30, the employment is going... down. 

As you will have noticed from your friends who are out there getting jobs, it's not a super good job market at the moment.

Um, okay, that's the bad side. What's the good side? 

There still are plenty of jobs, and I think what's happening is changing. I think Penn students are pretty well set up to try and do that, but in some sense, it's being able to take the concepts and use them for something.

The coding—if you're hired as a programmer... I have a programmer who works for me. He doesn't write any code. He specs things out, he does security tests, he does unit tests, he helps to collect the data. There's no coding. It's all done by Gemini. But to understand the concepts and to test it, and to check what's going on—

So I think what you'll see, and hopefully will be able to help you see better, is: Yeah, API calls do everything you want to do, pretty much. You mostly don't have to change the algorithms to do lots of stuff. But on the other hand, if you don't know which API call to make, then there's lots of things that go wrong. And there's lots of choices.

And most things that are API calls in machine learning have, like, 10 or 15 choices of model or hyperparameter. And the AutoML that learns what model to use automatically is still pretty—we'll play with it, maybe the last week.

### How to Use LLMs for Learning

So, my hope is:
- A) that you will all use LLMs.
- B) that you will think really hard: are they helping you learn, or are they hurting you from learning?

And it's a simple process, mostly: **If you think first, and then talk to the LLM, that usually works really well.** Ask it questions: "Why is this?" "Explain this." "Why doesn't this work?" I do it a lot as I prepare this course. I ask it questions and see what it says. I think about whether I like them, whether they're helpful, whether they're right.

Um, on the other hand, if you actually go and put the homework questions into GPT first and then copy it over? Well, if you really do a good copy, you won't notice at all. You can manage to copy without thinking at all. If you handwrite it, like we're trying to enforce, at least you'll have to think about it, but you learn much less.

That make sense? 

One of the TAs said, "What about the final projects? What if GPT gives everybody their ideas?" I'm going, "Yeah, is that a problem?"

Um, I find most of the GPT ideas off the shelf to be pretty boring for final projects. You could probably, with a state-of-the-art LLM customized for it, get a C final project by not thinking very much and doing it in a day with GPT-type thing. They're not doing B-level final projects yet. Maybe it'll come, but I don't see it yet.

Now, a good final project is:
- Something you know something about, so you're able to check to see whether that makes sense or not.
- Something that's sort of interesting
- Something that's not been done 57 times on the web

Oh, right, which is mostly what GPT suggests. Right? 

When GPT first came out, Ethan Mollick, my colleague at Wharton, said, "Hey, make a science project, it's so cool," and it does the IRIS dataset, which is perhaps the most boring dataset ever invented in machine learning. It's like these different flowers—you can classify the flowers as linearly separable, so it's trivial. You can classify the flowers into three classes, and I'm going, "Great, Ethan, it wrote a whole science project." F in my course. No thinking was involved whatsoever. 

So, I want you to be very conscious... oh, in fact, let's take a second.

And, well, again, turn and talk to your neighbors, even your friends now. And... **how do you use LLMs currently?** See if you learn other ways. What do you like about them? What do you not like about them, in particular for things like a machine learning course? 

Make sense as a question? Talk to each other, how you're using it? Which ones do you use?

Engage! Talk!

[Student interaction time]

You gotta include her in the conversation, too. Talk to your neighbor! Include her!

You guys can sit with a laptop, but if you talk to each other, you will learn more, you'll be more engaged, it'll work better. I've discovered lots of people at Penn think they know how to use LLMs, and there are other ways they haven't thought of. So I'm encouraging engagement. Even in the back.

[More interaction time]

Okay.

### LLM Usage Poll

Okay, awesome! So, first, just for me to get a flavor, how many people use one of the... you can answer many things. One of the OpenAI GPT products? Almost everybody. 

Gemini, Google products? Fewer, uh, Anthropic Claude? Lots, actually. 

Um, any of the specialized coding tools? Yeah, pretty widely used here. 

One of the big ones am I missing? Llama, DeepSeek?

DeepSeek, I'm curious. Interesting, okay. Um...

How many are using something that's at least a paid version, like a \$20 a month level? So, yeah, almost everybody...

There's a big difference. Most of them now have free student versions for Gemini and such. And so it's worth, if you're using a non-paid version, I would strongly suggest upgrading to a paid version for free. It's nice to use something, you know—great to go use DeepSeek for free, it's good, but it's not as good at some of these tasks as Gemini, and you're not really seeing it. So it's worth—and for most of them, there are free student accounts. So do it. 

Good. Um... We'll come back and talk about that, maybe in recitation. Good! Um...

## The Three Levels of Computer Science

So to put this in context, okay, obscure point: anybody know who this guy is? Okay, who?

**Student:** Andrej Karpathy.

And he's famous for...?

**Student:** Vibe coding.

Vibe coding would be his... one of the more recent ones. He argues that there are three levels of things in computer science. I think it's not a bad way to think. There's:

1. **The coding level**, which is not this course, but you should all have graduated from that level.
2. **The ML region**, which, of course, requires coding
3. **The AI/LLM version** on top of it

You can see him on YouTube saying this more excitingly than I do, but it's not a bad way of thinking about things, right? So this course is very much in the middle level. We're not talking about AI as a product. We're not gonna do prompt engineering, although you can worry about how to do it. We're going to talk really about the middle part. 

Obviously, each one builds on the other one. These days, to be a computer scientist, it's good to be full-stack in all sorts of stacks, including this one. So we're in the middle there. Um...

### The Importance of Understanding Despite AI Capabilities

I've said this before, but I'm gonna say it again: It's really good at doing almost everything, and so you should be thinking, what's your role in life when the LLM can answer the pieces there? 

I do think, and I really do believe, although I wouldn't teach the course if I didn't, that having that knowledge is important. When I look at my friends who write best-selling books—and there are several people at Penn who have, you know, like, at the million-copy level—they all write the first draft themselves, and the second draft. Then they talk to Claude, which they say works better than Gemini or GPT for getting the right tone, and they then edit it there, and then they send it to their professional editor.

But they spent years writing stuff by hand. And so I fully expect 3 years from now, of course you will use an AI system to do your machine learning. Like, you're gonna do it by hand with an abacus? Right? 

But, on the other hand, it does seem plausible that even 5 years from now, to do any of these things well requires some intuition and understanding of how they work.

Now, it's possible I'm wrong, and Amodei is right—Dario Amodei, the head of Anthropic—and next year, half the entering jobs in the service sector will be automated, and five years from now we'll all be unemployed. In that case, I got nothing to do for you, because we'll all be unemployed.

But I don't think that's the case. These guys have been wrong over and over. Ray Kurzweil: "The singularity is here, we're all gonna be gone"... yeah, maybe.

So these things are really good at some stuff and really bad at others, but you need to understand the intuition to get there. I'm spending a lot of time on this because I'm really concerned about your learning stuff, not you passing the course, which you will if you work.

Cool. Um...

## Grading

So that's the question. Grading!

So, a lot of completion-based grading:
- Do the quizzes
- Show up (hopefully, if you're here, you're engaged, but at least it's an encouragement)
- Show up at recitation
- Do the homeworks

"Completion" means not just "is there a character there," but "is there something that looks like you tried?"

And then there are:
- Midterm
- Final (in class)
- Project

that are actually real grading.

So that's sort of, I think, the best one can do. Other people are doing in-person interviews with everybody, but that's, like, a really slow interview process. And surprisingly, multiple choice questions are really good assessments if they're done well. But that's another psychology question.

Cool! Um... Hello.

## The Exercise Metaphor

Okay, so I think there's an interesting metaphor, which I'm not sure I fully believe, but if you're gonna do machine learning, you actually have to do it. If you have the robot go to the gym for you and do the presses, it probably doesn't make you stronger.

So do think a little bit about that. And I can quote Karpathy again. I'm not sure I believe him, but he says, and hey, he's a smarter guy than I am: **"Learning's not supposed to be fun, it requires effort, you need to be sweating."**

I'm gonna try to make it as easy as possible. I have experiments in psychology that actually show that, hey, if you give good examples and you coach people well, you do learn more effectively without working harder. But, um, but do think as you're using large language models: **Are they helping you exercise and get stronger? Are they answering the questions you need? Or are they just a way to cheat, and they're actually doing the exercise for you?**

In the long run, it's not—whether you get an A- or an A in the course—it's gonna matter. It's gonna be: do you have the skill set?

Cool! Um, one more word.

## Engagement and Note-Taking

The other piece... oh yes, why show up? Engagement, think, ask questions of yourself. I'll try and ask questions.

There's an interesting set of studies in psychology that argue—and I think they're actually convincing—that **if you take notes by hand, you learn and remember more than if you make notes on a computer.**

I sort of feel terrible because I take most of my notes on a computer when I watch videos of Sam Altman. You know who Sam Altman is? This is the OpenAI CEO. He's always taking little notes on a piece of paper and then ripping them and throwing them on the floor for someone to pick up. But lots of very techy guys have figured out that paper notes are good.

Um... lots of laptops here, that's fine. I think I'm not gonna—in the psychology courses, I ban laptops, too, but they're smaller and more focused and more discussion-based. I do like taking notes on a computer. Um, but it's worth thinking about it. Um, so I want you engaged...

## Poll Everywhere Exercise

Let's, first of all, see if the Poll Everywhere thing is working. I'm now going to do a whole bunch of things online, which violates my rule of getting rid of phones and laptops. This is probably just sheer evil, where I'm, like, encouraging you to use the device and check out and not engage.

So there's a tension of: can I get you engaged by asking you questions where you actually have to click, commit to something, which also helps you know where you are?

Um, by the way, one of the interesting things in collecting data is to think about the bias... I'm gonna give you a little bit of time, I'm stalling, um, for people to try and... go there.

### Data Bias and Sampling

**The data is the data you have**, and the machine learning models we build are the machine learning models on what you have.

I can now build a model which says what? That 98% of you, with some error bar, have Poll Everywhere. True? No? Yes?

Definitely 98% of you have Poll Everywhere. Is that what that says? 97%? Okay, 97% of you have Poll Everywhere? Is that... yeah?

**This is conditional on your answering it.** Right?

I always love... I once saw a Wharton student who did a survey asking, you know, "Do you like filling out surveys, or do you think it's horrible?" And it's like, you know, 95% of people liked filling out surveys, and I wasn't sure which was weirder—that, or the 5% who hated it, but still filled it out, right?

So, one of the things that's not taught in machine learning, but I want you to be thinking about, is: **Where is the data coming from? What is it?** 

It's some event space, some sample space of things that are happening. And without a count, which this is annoyingly not showing me, I have no idea how many people are out there. Right? 

So, realize:
- A) Anytime you take a survey, 8-10% of people will just make up crap. All the data is noisy.
- B) Also realize that the more important errors are the stuff you didn't include. Right? What data did you not collect? 

Cool! Um, onward, talk to me. Computer, you love me, no? No? No? 

Oh, right! There's an impossible interaction between Poll Everywhere and my clicker, where if it's actually in Poll Everywhere mode, I can't click. Great. Um...

So I occasionally toyed with the idea, but I haven't done it, of banning phones and laptops. The other version we could do, which is almost happening, is, you know, there could be a background of laptop people and no laptops in the first half, but okay. Um...

As the sample size goes up, you can see that things change. You can see that the first responders are statistically... let's see, 6... are different from the later responders, right? 

**If you take the data in chronological order, and train on the first half, and test on the second half, you get really different results than if you randomly sample them.**

One of the things I want you to be paying attention to: **Never trust a mathematician.** There's always fine print. You know, "if you sample the data IID, and you go to the same distribution later on, then we can prove within something..." right.

Was the data sampled IID? What about the guys who never clicked because they don't have Poll Everywhere? Right? Um...

The first half is not the same as the second half. This hospital's not the same as that hospital. You trained on the Penn Hospital system, you go to the suburbs—people die way more in the Penn hospitals than in the suburban hospitals.

Because our doctors are worse? Why do we kill more people at Penn than they do at Bryn Mawr?

**Student:** Sorry? There's more traffic?

So that kills them, yeah?

There are more specialists here. If you're really sick in Bryn Mawr, they send you to Penn, where they can either save you or kill you, right? So it's...

So again, I want you to be thinking about, sort of, the underlying model. **We're going to mostly study correlation, not causality. And we're gonna mostly assume that everything is IID, but it isn't.** 

Cool. That's sort of interesting. Widely spread. Okay, and I can't use my clicker, because they have Poll Everywhere up. Hey, technology. I don't do hardware. 

Okay, um, good. So, a few things just to think about stuff, um...

So, you will periodically have homework questions...

Okay, we are... there's also an interesting phenomenon: Whatever is the majority vote tends to—it concentrates. People all shift their thing back to the majority vote, because the majority's always right, which, in this case, they are. Um...

## Communication Guidelines

So, in general, you should not be emailing individual people. You're gonna get much better service if you send stuff to Ed, and 5 or 10 of you will email me, and I'll say, "I'd love to help you if you post it to Ed. I or someone will get back to you super fast, or you can wait and sit in my long email queue."

Um, cool. Um...

## Programming Language

We're gonna teach in Python. I used to teach this course in the Dark Ages in MATLAB, which is still popular in certain parts of the world—both in psychology, interesting for the vision lab, and in parts of EE. And in statistics, they mostly teach in R.

[Poll question about which language matters]

Okay, one person got it wrong. **It doesn't matter what language you're coding in.**

I mostly code in Karpathy's favorite language: **English!**

Yeah, he doesn't speak Mandarin very well, nor do I, so we code in English. Right? 

I don't care what language you code in. What I want you to do is to know how to give super precise instructions—I don't care if it's in English or Mandarin or Hindi, or Tamil. They're all fine, they're interchangeable as far as I'm concerned, except I only speak one of them, because I only know European languages. Um, but the point is to understand precisely what you're doing. **The code part is easy!**

That's the base, you already have that. So, sure, we're gonna use Python, because I like Python. As a language, it's clean, it's beautiful. I learned Perl, like, 5 times, and... I've learned Python once. It's sort of nice. And I don't like garbage collection and other things. 

Okay, great. Um, next. 

So, I hope I didn't clear it. Let me... clear responses. We already... we already did this one! LLMs can answer most questions in the course. Yes, you already said that, that's boring. Move on.

Um, where do people use machine learning? Oh, I'm not even gonna do this one. There are so many... You can do it. **Everywhere! Every startup, every company, the whole world.** Good! Um...

## The Three Components of Machine Learning

Now there's gonna be a theme that's gonna show up over and over and over in this course. I'm gonna cover it 5 times. And that is that **every machine learning system can be broken up into three pieces**—moving out of the bureaucracy and into substance.

### 1. Representation

**What's the input? What features do you have?**

Or predictors, if you're a statistician. I'm gonna swap languages a bit, because one of the things I want you guys to do is to be comfortable with what language different people use. So I'm gonna be a little bit inconsistent in my nomenclature, intentionally. Because one of the things I do a lot, and ML people do a lot, is talk to people in different fields.

And if someone says, "How many predictors do you have?" you should be fine, whether it's a predictor or a feature—I don't care. Whether it's an independent variable—they're all the same, right? 

- What are the features? 
- What are the outputs? ML people call them labels. I like "output" better, I don't care. 
- What are the dependent variables? 
- What goes in? What goes out?

Super important. 

### 2. The Model

And then... **what's the model?** 

And one thing we'll see over and over, we'll cover a dozen different models:
- Nearest neighbors
- CNNs
- Decision trees
- Random forests

**Every model has some assumption behind it. Every model is optimal for some problem and not good for other ones.** Right? And in some sense, this has to be the case.

If you were to try and learn anything possible, it's not possible. Right? In some sense, learning is NP-hard. **You can only learn about the world if there's some regularity.**

That make sense? We'll see this in many, many ways, and we're gonna say over and over: **What's the model you're assuming, and how does that map to the problem you care about?**

And again, these are all... You should be going through this: what are all the jargon terms?

- **Inductive bias**: What's the inductive bias of the model? What does it assume about the world?
- Does it assume the world is translationally invariant? 
- Does it assume it's rotationally invariant?
- Does it assume the top and the bottom of the image are the same?

Right? Which of those are more true? Translationally invariant for image recognition? Pretty good. Um, if you flip the top and the bottom, does the image change?

A girl upside down now? I've never seen a whole class of upside-down people before. Maybe in Australia, I don't know. Um...

### 3. The Loss Function

The second piece, after the model, is **the loss function**.

And one thing we'll spend a lot of time on is picking the right loss function. We're going to define 5 or 10 different loss functions, all of which have special names that hide the loss function. 

There are things like Support Vector Machines, which is a codename for a loss function. Do we know the loss function behind an SVM? Good. Okay, we'll get there. Um...

### The Optimization Method

Then there's the third thing: **an optimization method, a search method, a solution method**. How do you find the solution to the model? Mostly it's finding parameters, weights in a model, but it may find all sorts of other things. 

We're also going to talk a lot about:
- **Parameters in the model**: Think of the weights in a linear regression, or the weights in a neural net, or coefficients (if you want to call them that—if you're a statistician, the weights are called coefficients)
- **The hyperparameters**: What model is it? How complex is it? We'll see lots of questions about learning rates.

So we'll see lots of pieces. We'll go over and over for each method.

And many of the old methods were defined as algorithms. When I first learned k-means clustering, it was a clustering algorithm: "Do this, do this, do this, do this," and then it converges to something. Provably converges. Just... something. Um... Provably converges to what? We don't know, that's a different problem.

Now, we redefine it as, "Oh, right, that's a search procedure." It's a model with some sort of inductive bias—we'll get there in the second half of the course—which assumes something about the world. If that thing is true, k-means is great. If that thing is false, it's a terrible thing to do. And it minimizes some loss function, which we'll also look at.

Make sense? 

In... whoop, hello? Okay, good.

## Mathematical Formulation

So, let's try looking at it more precisely. We're gonna use mostly the notation—I will switch notations a bit on you, but I'm gonna try to mostly use a notation that says:

For **representation**, for at least supervised learning (the first third of the course), you've got some $\hat{Y}$—what you're trying to predict. Statisticians love the hats on it, computer scientists get a little sloppy about the hats.

$Y$ is the true label or output. And "label" is a weird word, because it could be a number.

Um, $\hat{Y}$ is some function of $X$ and some weights. Oh, right, which probably should have a hat on them—they're estimated—and no hat if they're the true ones. But we'll be a little bit sloppy, because, hey, we're computer scientists. And when we use $W$ for a neural net, we don't put a $\hat{W}$ on it, but it's an estimate, it's not the reality. 

And you might have a model that says that, hey, it's:

$$\hat{Y} = W^T X$$

which has got a name: **Linear regression**.

Right? So far? So that's a model form of representation. The input is $X$, the output is $Y$.

### Loss Functions

We then have some sort of a **loss function**, which says: How are we gonna compute how good or bad our predictions are? 

Mostly we'll do loss functions where bigger is worse. We'll talk about terms. So most of our loss functions are things where you want it to be as small as possible, which means it's gotta have a bunch of properties. Like, if it could be infinitely negative, you'll never get a solution, so often they're bounded by zero.

And the most popular one is the **$L_2$ loss**, which is the sum of the squares:

$$\text{Loss} = \|Y - \hat{Y}\|_2^2$$

And again, I'm gonna write everything in vectors, because I love vectors—they're so clean and nice. And you should write all your code with vectors, because they're so clean and nice. And pretty code runs faster. 

I don't care about—I do care about speed. I've got literally a billion tweets on my computers here. It takes a long time just to read them all. But...

**Why does pretty code run faster in machine learning?**

**Student:** Sorry? Vectorization.

**Why is vectorization the answer to all problems in machine learning? Why is it so important?**

**Student:** It has to do with zeros and ones, but in some sense, but why does vectorization matter?

And you can, um, come back to it. Yeah, so I think the first thing to note is: You guys in this course are not trying to optimize your code, and if you try to do the low-level CUDA optimization, you're not as good as the guys who wrote PyTorch or Python, or even the scikit-learn guys—they're pretty good. So, that's not your problem. But if you obfuscate what's going on hard enough, you can confuse the compiler. 

So I think there are two things:
- A) **If you use vectors, you'll get stuff right**. And the math—by vectors, I include matrices, or tensors, they're all the same: one vector, two vector, three vector... vector, matrix, tensor. We'll use all those. But all of those:
  - A) are cleaner to understand
  - B) let the computer guys (which is not us for this course) worry about how to throw them on a GPU if it exists

And yes, it'll be on a GPU for most stuff you'll do. But who cares? 

By the way, **if you're running the homeworks on Colab, and it's really, really annoyingly slow, what did you not do?**

**Student:** Request a GPU?

Right? **It really is faster to use the GPU than the CPU.**

And so, I'm just saying this because periodically someone does that, like, "Your homework took me an hour of compute time," and I'm going, "Hmm, were you running on a CPU?" Like, "I don't know!" Okay? 

So, as much as I hate hardware, it really is useful to:
- A) Write in vectors
- B) Make sure when you have that little option at the top—you can pick Python or R (I don't really care, but Python's the right choice)—and you could pick CPU or GPU, but for this course, you mostly want to be on a GPU, especially when you get into the deep learning stuff. It's really gonna be slow if you're not using a GPU.

One of the big, big inventions that made AI—that made LLMs work—was: "Hey, we can run these gaming machines and use them for something not useful, but, you know, different."

Cool.

### $L_2$ Loss Function

Loss function? Um, I will use that notation. Subscript 2 means we are using the $L_2$ norm. We'll talk—oh, next class, which is a week from today—about norms, make sure we know them formally. Um...

Sum of squares:

$$\text{Loss}_{L_2} = \|Y - \hat{Y}\|_2^2 = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2$$

And then some sort of **optimization method**. If it's $L_2$ loss with linear regression, as we will prove in two weeks, it's a closed-form solution. It's easy:

$$(X^T X)^{-1} X^T Y$$

And eventually, we'll talk about how to compute $(X^T X)^{-1}$ really efficiently when we do the second half of the course. So it's closed form. But for an awful lot of machine learning, it's **gradient descent**.

All of large language models... and we'll cover a bunch of gradient descent techniques.

Cool.

## Why $L_2$?

If you took an undergraduate course in statistics or things, or if you do linear regression, you will have used an $L_2$ norm. You minimize the sum of the squares.

**Why is the $L_2$ norm... I use $L_2$ norm sometimes, I use $L_1$ norms, I use $L_0$, which is not really a norm. I use all sorts of... oh, a KL divergence, which we'll cover. I use lots of loss functions. Why is $L_2$ so popular?**

**Student:** Yeah, you want to penalize the outliers more.

Does $L_2$ penalize the outliers as much as possible? Could I do an $L_4$ norm, where I take $Y$ minus each $Y$—each item—raise it to the fourth, and take the fourth root of it? Yeah? 

$L_2$ norm, yes, we'll come back and write all the math behind these. I didn't set up my... I should have set up my laptop thingy there. Right? So if I take:

$$\|Y - \hat{Y}\|_2$$

That's equal to the square root of:

$$(Y_1 - \hat{Y}_1)^2 + (Y_2 - \hat{Y}_2)^2 + (Y_3 - \hat{Y}_3)^2 + \ldots$$

So, positive and negative, so squaring things. 

If I did, instead of this, an $L_1$ norm—one there, that's now $L_1$—I could do:

$$\|Y - \hat{Y}\|_1 = |Y_1 - \hat{Y}_1| + |Y_2 - \hat{Y}_2| + |Y_3 - \hat{Y}_3| + \ldots$$

Does that work? No, the root's gone now. $L_1$ norm is—before I had the whole thing to the half, now I have the whole thing to the one. 

And again, we'll cover all this in detail a week from today. But I want you to start thinking, right? We're gonna cover norms and all these pieces. But I want you to start thinking about **the assumptions behind the model**. 

**Linear regression assumes you're doing an $L_2$ loss.** You're taking the square root of the sum of squares. That's fine! But there's no magic to it. There's a little bit of magic to it, but not much. 

I could do an $L_1$ norm—I take the sum of the absolute values raised to the one. I could take an $L_4$ norm, and take the fourth power of each of these things, take the fourth root of it. Those are all valid norms, and... oh, **they're all optimal for some problem.** Right? 

Every model and every loss function is optimal for something. Sort of a stupid folk theorem, but it's true. 

So why do people love $L_2$ norms? Again, we're going to cover this for 2 weeks, 3 weeks, but I'm trying to start setting up questions that will sort of hopefully percolate. Yeah.

**Student:** It's continuous.

Is the $L_1$—is the absolute value continuous? It's continuous. It's differentiable except at one point. It turns out that one point is not really a big problem. The $L_1$ norm works beautifully. We'll talk... yeah?

**Student:** Yep, it's the Euclidean distance, and it feels natural, sure, but...

**Student:** Yep, I think any small outliers will start, like, affecting huge values to me over a lot of loss, so we will not be able to actually make our... What's that? You know, what's the—

What's the difference? It's a positive distance, you're always gonna... never cancel it out. Uh... Yeah, I mean, it's gotta... **a norm has got to be non-negative. That's a key feature of a norm.** 

I'm gonna leave this here, because this is an intro course. But the point I wanted to make was that if you use a package like linear regression, you're making super strong assumptions. We will both show that this is optimal under a given assumption of Gaussian noise, which we'll derive the math behind. So **if your noise is truly Gaussian**, then $L_2$ is optimal.

Oh, I work with language. **The noise is really not Gaussian in language. No language I've worked with is Gaussian. It's a terrible loss function.** Right? 

And in the old days, when I was your age, people used to use the $L_2$ norm to try and model language. It's like, "Oh my god, how stupid were we?" And now we'll use, well, cross entropy, which is in fact the KL divergence. 

So we'll think about what the assumptions are behind these models, right? And again, my goal is not to answer this today, but to ask questions. 

Cool. Um... Great!

## Google Search Example

So let's take 5 minutes, and the other piece I want to show you is a little bit the challenge of actually doing these. Um, a lot of my students have gone on and worked at Google, making way more money than I do, and doing machine learning there, because they got our PhDs here, and I worked briefly at Google. 

As a typical piece, you put a query in and you type, you know, "show me machine learning books," and you get either some organic results (the main piece) or you get ads. And the ads... whoops, where are the ads? Hello, give me my piece there. Um, the ads either show up across the top or on the right-hand bar, right? So you get the ads there. 

And if you think about a classic machine learning problem, you would like to learn a model that does a good job in returning either:
- Organic search (the main results), or 
- The ad search (the banner ads across the top, or the ads along the side)

Make sense as a problem? **It's an important problem, like billions of dollars important.** And if you could get 1% better, you've paid your salary for longer than you're gonna live. Unless you really, really believe the singularity's here and you'll live forever, but even then, you'll pay it forever, because it's a lot of money, right? 

And so, it looks like a straightforward problem.

### Exercise: Define the ML Problem

So I want you to turn again to your neighbor and tell me: We have 3 things I want to do here. You can either pick organic search or ads, I don't care, pick one of them.

**To be a machine learning problem, it's got to have:**
1. **A set of features that go in** ($X$)
2. **Some sort of a label or $Y$ that goes out** (that can be observed)
3. **Some sort of a model**
4. **Some sort of a loss function**

I wouldn't spend a lot of time on the model, because I will give you that in one word, but think a bit about and talk:
- **What goes into this prediction of what to show you?** 
- **What is the label it's predicting?**
- **What's the loss function?**

Does that question make sense? Okay, it's... I think it's harder than it sounds. 

Take 5 minutes, let's do it. Starting at 10 till, I'll stop you at 5 till.

[Student discussion time]

You've solved it? I think there's more work to be done on thinking what the features are, and the loss function is, like...

So, yep. I was supposed to, uh... Like, are you just saying, like, uh, matching the query into... I'm... Yep, I'm saying there's a machine learning problem which Google faces, which is: you're gonna type a query, they're gonna return and serve some ads. You've just been hired to solve that problem.

**What are the features going in? What's the thing coming out? And what's the loss function?**

Nope. Eventually we'll do a model, but... You solved it? You're stuck? Maybe?

Before you get into tokens, **what's the feature set? What's going in?** Machine learning takes in an $X$, gives you a $Y$ for this case. **What's the $X$? What's the $Y$?**

What does Google know about you?

Yep, so that's all part of the $X$. **And what's the $Y$? What's it trying to predict here?**

Look... Okay, I'm gonna pull things back. 

Okay, whoop, we got one more minute. Yes, okay. Yep. 

Uh, I just want to ask about that question. Is it because that's a quadratic form, so we can solve it in a closed form? 

**Yeah, exactly.** One of the two reasons: that, under Gaussian noise, it's optimal. **If the noise is Gaussian, then that is the right loss function.** So we will prove in a couple of weeks that if you have Gaussian noise in measuring your $Y$, that the $L_2$ loss is the right loss to use. Oh, yeah, it's, uh... wild. Yep. Yep, exactly. So they're both right.

Okay...

### Discussion Results

Awesome! So **what does Google know about you that it can use in the feature set?** What's the input here? Yep.

**Student:** My location.

Does Google know my location? Ish. Google knows the IP address of my server from here. Sometimes it thinks I'm here at Penn. At home, it actually thinks I'm in Harrisburg, because the IP server for my home is Verizon, which somehow tells Google I'm in Harrisburg. So it's got a somewhat... some notion. And if you do a different query—if I do a search here versus San Francisco, do I get different results? Absolutely. Right? So it knows my location a little bit, yep.

**Student:** It knows my prompt.

Super important, right? It takes the prompt, which are a bunch of tokens, which... I don't know the current version, but I'm willing to bet it then encodes them. We'll talk about embedding and mapping tokens to vectors, yep. 

**Student:** It knows my search history.

I've checked—last time I checked, it has... I've made 43,121 queries. It has them all. Yep. 

**Student:** Time of day or time of year.

Yeah, that matters, yep.

**Student:** [Something about YouTube history]

Yeah, I don't know if that's interesting. I don't know if my YouTube history is used in the search or not. The searches—I didn't work in the search division, and even within Google, it's very private, plus it was a long time ago—but yeah, plausibly.

Yep, back.

**Student:** It knows what other people have queried.

Right? So a bunch of things. So we got a bunch of features, which we're gonna make into, at some point, a long vector. Because in the end, **all you get in machine learning is vectors.**

And we'll spend some time later on mapping that query, which is a bunch of characters, into tokens and into vectors, which we'll call **embeddings**. And we'll map all those things into a big vector. 

Okay, that goes in. **What's the output it's predicting?**

Yep.

**Student:** A binary "click or not."

That's a good one. To have that, I need one more thing which you didn't put in the input yet, which is arguably what you want: **a function of me and the webpage** (or the book). You with me? 

Because clicking is a function of two things:
- It's a function of the person and their context
- It's a function of the thing being clicked on

Right? So I need a bunch of features which are features of the thing being clicked on, as well as the person. Make sense?

Now, unfortunately, the whole thing's getting really big, because there are a lot of things I could click on. And those things also have attributes. Right? They have character strings associated with them, they have "how often has someone clicked on them," they have "what time of day," right? So I got a lot of stuff about them, too. But okay. 

So we're gonna have both information—that's one model—of me and the object, and the probability of clicking. That's pretty good. 

**Anything else that shows up that matters for clicking?**

One thing that's super important for clicking or not clicking: yep?

**Student:** How does it visually appear?

So you could take features of the thing that would be rendered. There's one even more important than the color. That's... even... yeah, those are all going in. Whatever text I have, those get embedded and put in.

**Also, the position.** If it's first, second, third, if it's on the second page—you're gone. Right? 

Okay, so all those things matter. And so one loss function you could do is try and predict the probability of clicking. That's cool, and last I knew, that's sort of how organic search works.

### Ads and Revenue Optimization

Ads work somewhat differently. **What loss function are companies like Google trying to minimize or maximize?**

**Student:** Yep, money returned.

They want to maximize money. That's their goal. It's a welcome—America's a capitalist society, most of the world is now. Um, they're trying to maximize money. 

Now, money is not quite the thing here. And in fact, what they don't try to do is to maximize how much money they make by showing you an ad, because they can show you a really offensive ad and make a lot of money, and you run away. And it costs, like, 100 bucks to acquire you. 

So they want to, in some sense, maximize something different, which is what they call **long-term value**, or what we at Wharton call **lifetime value**. It's the **expected discounted cash flow of you in the future**.

Does that make sense? What I really care about is how much money I will make from you over your future time, discounted, because a buck tomorrow is not worth as much as a buck today, because there's inflation, right?

That's what they're actually maximizing. Now, that optimization problem's a little trickier, because as you're clicking, **do I know your lifetime value, or your long-term value?** No, it's in the future! Crap. 

So we're gonna have to do some annoying things where we're often estimating something which is the estimate of the thing we're trying to learn, or some other piece there. And we won't get too far into that, but we'll get a little bit toward the end of the course in reinforcement learning.

But the point I'm going to make is:
- A) **It's a lot of work to decide what the features are and put them in, and easy to miss them.** And a lot of people at Google spend a lot of time doing that, or any company.
- B) **Often the loss function is very non-obvious.**

### Loss Function for Revenue

Now let's take one simple question, which is: Assume you just want to maximize the money you get from this one click. Right? So you click, you do something, and the advertiser pays you 1 cent, half a cent, 4 cents, and you just want to maximize that money. 

**What's the right loss function to use if your return is measured in dollars?** Do you want $L_2$, $L_1$...?

**Student:** Yes.

So I'm gonna have something where $Y$ is measured in dollars, right? I got a predictive model, it predicts how many dollars I will make on that click—one click, right? One observation, one click. And I show it to you. If you don't click, I get nothing—\$0. If you do click, I get whatever someone pays me, say a cent, right? \$0.01. Right? 

And I want a model that predicts as accurately as possible how much money I will make by showing you the ad for anything, right? Whatever you want to do, you're doing a model of some input, some world, and you're trying to predict an output which is measured in dollars.

That make sense? And now I'm asking: **What's the loss function that most people should be using as an obvious starting point?** Is it $L_2$ for dollars? 

**Student:** Yep, so $L_1$ is better.

**Why?** Because you're not measured in squared dollars. In some sense, the return is in dollars. A dollar more, a dollar less—for Google, it's pretty symmetric. They make a half a penny more, half a penny less, it goes out in the wash. So we don't worry about asymmetry yet. 

But for a lot of things, it's not squared dollars you care about. In some sense, you're trying to maximize the number of dollars. You don't want to get paid in... square them out if you make a huge amount once and, oh, well, you never get negative dollars in the world, so I don't need to worry about being under, over a little bit—it's sort of the same.

**Student:** Yep.

**Student:** Yeah, so for some things, if you're actually doing a single scalar, then they're exactly the same! Right? So yes, if it's a simple scalar, then the squared or the $L_1$ look exactly the same, right? If it's the square root of the squared, you get back to the absolute value, and it's exactly the same. So if it's not a vector, oops, it looks exactly the same, who cares? Right?

Good point. Cool! 

Moving on, we're gonna go back to all these. I want to cover a few more concepts. I have 12 more minutes, and...

## Three Kinds of Learning

We're gonna cover **3 kinds of learning**:

### 1. Supervised Learning

**Supervised learning**, which is the one I've been talking about: some $X$ comes in, you put it through a model, it gives you a $Y$ out.

And very popular right now is something that's called **self-supervised** in a large language model. You have a whole bunch of $X$'s, and then you predict the next one, which is not really a separate $Y$. So it's still supervised learning—given a whole bunch of tokens you've had as input, predict the next token.

So, that is still supervised learning in the sense of we're going to use the exact same sort of machinery. But it's not supervised in the sense of a separate $Y$. It's not a separate label. Make sense? 

And a lot of the trick, when I cover deep learning, we spend half the class figuring out clever ways to self-supervise. How do you colorize a picture? Start with a bunch of colored pictures, make them black and white—now you've got your training data. $X$ is the black and white, $Y$ is the color. You've now got a supervised learning problem.

We'll talk about supervised learning.

### 2. Unsupervised Learning

We'll talk about **unsupervised learning** in the second half, where you just have a bunch of $X$'s, or $(X, Y)$ pairs, which are also $X$'s. And all you're doing is clustering them, or dimensionality reducing them, embedding them. 

We used to say "dimensionality reduction," now we say "embedding," which is much more trendy, but it's the same thing. We will take these $X$'s and somehow group them into clusters. We'll learn clustering algorithms and embedding algorithms.

### 3. Reinforcement Learning

And finally, we'll talk about **reinforcement learning**, where you have:
- Some state of the world
- You take some action
- You get a reward

And you use it a whole bunch of times, like playing Go or chess: state, action, state, action, state, action, state, action, reward. 

Or you're stocking an inventory at Amazon, where they now increasingly are firing the OR guys and replacing them with RL guys, because, hey, if you want to try and decide how many products to keep, you're taking an action, you're buying it, the environment works, and you repeat this a bunch of times, and you try and learn an optimal policy. 

We'll cover all that later. So reinforcement learning is not an $X$ and a $Y$, but you take an action in some space, and you get a reward or a penalty, and go to a new state. We'll cover all of those.

So we're gonna cover these three big pieces. We'll cover those. Um...

## Regression and Classification

Okay, how many people... oh, why is that? I didn't clear it out! Okay, I'm reusing things. Um...

So I can just do this with a show of hands, but okay. We're going to talk about **regression**. We're going to do everything sort of twice:
- Once from a **Bayesian model**, which says this is a probability distribution
- Once from an **optimization model** with $L_2$

And we'll mostly show that the two things can be converted back and forth between each other—they're mathematically equivalent. And, oh, we're almost 50-50, okay. Um... good. This is helpful for me. 

And this is gonna be over and over: **Half of you will have seen it, half won't.** The second time is really good. I've seen these things 2 or 3 times, and it really helps to see them more times.

Um, so...

### Two Perspectives on Supervised Learning

You can think of supervised learning as: you get some $X$, and that then gives you some probability distribution over $Y$'s. You can then pick the best one—the maximum likelihood one, the most likely $Y$. Or there are cases where you want something else.

Um, you can view it as a **minimization problem**, where you have some norm that says how distant is the prediction from $Y$. 

And we'll show that these things have a duality: you do one, you do the other, they're gonna be mathematically equivalent! And sometimes the probability one makes clearer what the assumptions are—the inductive bias. Sometimes the optimization makes it clearer. So we'll do both of those.

Um, unsupervised is viewed as having a probability distribution over $X$—there is no $Y$. Or sometimes it's written $P(X, Y)$. Same idea. Um...

## Generative vs. Discriminative Models

So, the other big piece, and here the nomenclature's a little sloppy, is between **generative models** (this is sort of the statistics version of it):

### Generative Models

A **generative model** is one that gives you the full distribution. You estimate the probability of $X$, or you estimate the probability of $X$ and $Y$. It's a joint or a full distribution, and that's often called a generative model.

Um, we'll see some of those. We'll probably do diffusion models—your favorite image generator mostly uses these sorts of models.

### Discriminative Models

Versus **discriminative models**, where most of the classifications, where $Y$ is either yes/no, cancerous/non-cancerous. You have something which takes an $X$ (an image, or a medical record, or both) and gives you out a probability: how likely is it this or that?

That makes sense as a big jargony piece. And again, all these things will show up again.

Um, oop, I'm gonna skip that.

### Different View of Generative/Discriminative

So the other view of these, which is slightly different, and again, we're gonna have to come back and cover this a second time, is: 

**Discriminative models** are ones where you have a label. Is it this or that? You don't get to generate new $X$'s.

Whereas in the second half, the middle of the course, we'll be looking more at large language model sort of pieces, where instead of predicting "pick, is it cancerous or not cancerous?" you generate a distribution over all the possible next words or tokens, or a distribution over all the possible images. And then you sample one, and then repeat the process.

So, we'll see that if you're generating images or generating text, then you're actually sampling from a distribution. I'll try to make that concrete. Um, I'll skip this one and go to the next ones.

## Examples: Generative vs. Discriminative

**Loan applicant.** For decades, machine learning has been used—you apply for a loan, should I give it to you or not? You can think about the loss function, but that's a different one. Generative or discriminative?

**Discriminative.**

**You want to generate a whole bunch of MRI scans of brains.** I don't actually want to share with someone the actual MRI images of people's brains—that's private—but I'd like to have a training set I can share more publicly. Generative or discriminative?

**Generative.** You want to generate novel... generative, right?

**Spam or not.** Discriminative.

**Summarize a document.** Generative—generate novel text, right? Conditional on this.

So, we'll see both of these. Um, we'll be doing first discriminative and more generative ones. Um...

## Parametric vs. Non-parametric Models

The other ontology—big word, you don't need to learn that one—is: some of the models, most of them, will be **parametric**, like linear regression. There'll be parameters or weights that you're learning.

Um, where you have some—we call them $W$, in statistics they call them $\theta$ or $\beta$ in other things. They're all the parameters.

Um, **non-parametric**—we'll do k-nearest neighbors and decision trees here, things that are close.

And I'm gonna call neural nets **semi-parametric**, which is sort of my personal thing, because they have parameters or weights in them, but they're so flexible, they're sort of like non-parametric models. So they're mostly parametric—we'll use gradient descent to find the parameters or weights in them—but they're not technically non-parametric like a decision tree or k-nearest neighbors.

Again, we'll cover all these in great detail, so hang on for future ones.

## Statistics vs. Machine Learning vs. Data Science

Let's see, I got 5 minutes. A little bit of background, bigger picture:

### Statistics

**Statistics** tends to do more modeling of the noise, which we haven't talked about, and less modeling of the accuracy. We don't—in computer science, we do things that have a lot of signal-to-noise. Statisticians do things with lots of noise and not so much signal, much less satisfying. 

And in statistics, there's lots of **hypothesis testing**. Can you tell, give you a p-value to show if this is different? We won't do much of that.

### Machine Learning

We will do more about **predictive accuracy**—minimize the loss—and lots of incredibly complex model forms.

### Data Science

**Data science**, which is not this course, does a lot more:
- Collect the data
- Clean it up
- Interpretation
- Less math

Um... Cool, that's the big thing.

## Wrap-Up

So, to wrap up, you should:
- Go to Canvas
- You should do Homework 0, which should be live now or soon
- Do the quiz

Oh, don't check out, give me, like, 30 more seconds. I've got 2 minutes, I know you gotta go somewhere. 

- Join Ed, which is on Canvas
- Review things as you need

Okay? So turn the real-world problem into ML? Shhh. It's hard. I could end 5 minutes early, but then I gotta do it... Sheesh. 

**You will learn better if you hear the things one more time and think about them.** Think about it:
- Think about the Google problem
- Think about unsupervised, supervised, reinforcement
- Think about parametric

These are jargon terms you should know.

Awesome, see you on Canvas. If you're not in the course, or if you have questions, you can come up and talk to me.

We will try and get everything signed up for recitations!

---

## Post-Lecture Q&A

[The remainder of the transcript contains informal Q&A with students about course enrollment, waitlists, and recitation registration. Key points:]

- Students on the waitlist or instructor review will know their status within 24-48 hours
- There are more students admitted than available slots (room holds 262, recitations support ~240)
- The course is overbooked like airlines/hotels
- Students who are accepted will receive an email
- Final decisions will be made by Friday
- Students should not worry about recitation signup until they're officially in the course

---

**End of Lecture**

---

# CIS-5200 Fall 2025 - Lecture 2: Introduction to Machine Learning: Empirical Risk Minimization and K-Nearest Neighbors

---

## Course Overview

Some features in, some labels out. We're going to build a model, we're gonna do some optimization, we're gonna minimize some loss function. There's nothing qualitative about this course. Discrete is not the same as qualitative. Some things will be continuous, some will be discrete—fine, I don't really care. But it's not the same as qualitative.

---

## Empirical Risk Minimization

Okay, so first concept in terms of setting things up. I want to go through the jargon of empirical risk minimization, which is way more fancy-sounding than it is. But it lets me start to set up some of the notation which we'll be using.

### The Framework

As I will say over and over: you're gonna pick one of a set of models. In supervised learning land here, the models in general are called $F$ for some function. Some of the machine learning folks like to call them hypothesis $H$. If the output $Y$ is binary, I'm going to use $F$ as a function in general.

Most of our models, but not today, will be **parametric**. There'll be some set of parameters, which statisticians call $\theta$, and which computer scientists call $W$ for weight. I'm gonna, as I said, switch the notation back and forth. You shouldn't get too upset by that.

We'll be trying to sometimes be good, and we'll say there's a real $Y$—you observe the label. A person lived or they died, that's the $Y$. And then you've got a $\hat{Y}$, which is an estimate of it. Make sense?

So our goal is to find the best estimate. Well, no. Our goal in this case is to find the best $\beta$ (or $\theta$, or $W$) that gives you the best function $F$, that gives you the best estimate of $\hat{Y}$.

### Loss Functions

The first half of this course, after doing that, is we gotta worry about what "best" means. You gotta pick a functional form and a way of finding it.

We will also always pick some sort of a **loss function**, which I'm gonna also annoyingly change notations on. Sometimes I'll use a little script $\ell$, and sometimes a capital $L$, sometimes an $R$ for risk (but we'll try and use $R$ for reward). 

A loss function takes in:
- An actual value $Y$ (could be discrete or continuous)
- An estimate $\hat{Y}$ from your function (that could be a random forest, decision tree, neural net, whatever)

And it says how bad it is—bigger is worse. Make sense?

Today, I want to sort of go through a classic mathematical notation for describing the loss function. So far, so good?

### Training Set and Empirical Risk

What we're gonna have is always some sort of a **training set** in machine learning—some labeled dataset of $(X, Y)$ pairs. We're going to take those and use those to build our model, mostly estimating the $\theta$s.

The **empirical risk** (a jargon term) is the risk on the training data—the data we're given, the historic data—which is the average loss over the training points. Make sense?

**Student Question:** "What is the purpose of loss function?"

**Professor:** What is the purpose of empirical risk apart from the loss function? The loss function can be applied to any dataset. So, let's think about it. Most of the algorithms in machine learning minimize, to first approximation, the empirical risk. What do I actually want to minimize? That's not specific enough—the loss on what? The loss on last week's data? No, I want the loss on next week's data!

You with me? The empirical risk... this is a problem that if you work at Amazon or Google or wherever you work, you don't have a lot of data from the future. Your data is pretty much all from the past. You can estimate, and you will estimate, how well your model works on the historic data. But there's no money to be made in estimating the historic loss. The money is made on today's data, the future loss. Right?

That's obvious, but we're gonna spend an enormous amount of effort building up some mathematical machinery to optimally minimize not the empirical risk, but the future risk.

### Training vs. Testing

People talk about the **training set** (or the design set in statistics, which I hate), and then you've got the **testing set** (or the validation set, future set, etc., hold-out set). There's some sort of future set of data, right?

So, in general, they give you, "Hey, here's the CSV," or "Go scrape it from another course," like, "Here's your data, build the model." And you don't want to minimize the empirical risk. You want to minimize the **future risk**, which you can't actually check because you don't have the future data yet. Make sense?

I'll talk a lot today about why this is a problem and start to deal with it, and then we'll do a bunch of math about how well you can deal with it later in the course.

---

## Concrete Example: Amazon Product Recommendations

Good. Okay, so let's think of a simple, concrete example. I'm hypothetically hired by Amazon, and I want to predict for a given person and a given product—for a given person and, uh, mosquito repellent (I can put my example)—what's the probability of them buying it if they pitch it to me?

Make sense? They got a lot of data. They've shown a lot of people a lot of products, and people have bought or not bought. So far, so good.

You can measure some empirical risk of how close you are to the probability of buying or not buying. Right, maybe it's a zero-one loss function. Oh, maybe it's actually not a 0-1 loss function, because, hey, they make more money on some things than other things.

You may have noticed Amazon's become very ad-based. It used to be Amazon showed you the products you might like. Now they show you products they might like, and the products that they think they'll make more money on, the products someone is paying them to show to you. It's a complex loss function, but let's be simple about it.

### What is Out-of-Sample?

So now the question is: what is out-of-sample? I've got training data, right? That's the last million people and the last million products. Of course, there's a lot more than a million people and products in Amazon, but just hypothetically.

**Student Question:** "Our training data won't be probabilities, right? We would only know, like, this person did buy this product."

**Professor:** Correct! The probabilities are—in fact, the actual $Y$s are only zeros or ones: bought or didn't buy. And so, what we're doing very often in this course, almost all the models, will produce either a probability for binary classification, or sometimes a score. You order the products and rank things, and then you'll have to do a threshold on the score. Above this score, you will actually then purchase or not purchase.

So, yes, we don't actually observe the probability of buying for any given event. You pitch a person a product, and they buy it, or they don't buy it. The $Y$s are always 0 or 1. But in this model, the $\hat{Y}$ is a probability—it's a number between 0 and 1.

### The Challenge of Distribution Shift

So my question, which is really simple or not, is: what does out-of-sample look like? What does the future look like? Why is that non-trivial?

Because the person might change, the product might change. The person might change their requirements, there might be different people or different products. Right?

The simplest model takes in one of a million people as an indicator function, and one of a million products as an indicator function. If you get a new product, what can you say about it? **Nothing!** You've never seen it before! It's way out of the sample.

Of course, Amazon has probably tens of thousands of products every day (I don't know the exact number). I know a Penn graduate who does nothing other than help Amazon take in all the new products that they're adding each day and map them to—well, what would you map them to? **Embed them** somehow, so you can actually give them to your model.

If you have people embedded and products embedded, that's much nicer. Right? Now there's nothing out of distribution.

### Embeddings and Similarity

**Student Question:** "How does embedding help?"

**Professor:** So, imagine there's a new product that you've never seen before. If you're learning a function that maps from person cross product, for the new product, what can you say about it?

If it's discrete, there's no measure of similarity. You have to have a similarity measure. If this is product number 3472, the first... what, this mug? You've never seen a Penn mug? You got no similarity to it.

We're going to talk a lot today about similarities, or distances, which is one kind of anti-similarity. We'll get to similarities later in this course. How similar a product is... how do you know how to measure the similarity between the same product? I know there's a similarity between vectors. All I got in this course is vectors.

If it's a discrete product, how similar is this product to another product? Either it's the same product or it's different. In the discrete world, similarity is really simple. It's a zero function—either 0 or 1, either it's the same or it's different. If it's different, it's different. Right?

How similar are you and I? Either we're the same or we're different. I think we're different people, so similarity is zero or whatever. We haven't defined similar yet in this course, right?

But note that between people, we could embed us and have a distance metric, and measure similarity in whatever features we want. Oh, but that's an engineering design. You know, similar in height and size, like, I could wear your t-shirt almost. Or, similar in hairstyle, not really. It is much nicer, but, you know, can't do that.

This makes sense? So, one thing you want to think about for all problems is: what is out of sample? What's the training set? What's the test set?

But we're gonna move for the first piece of the course into mathematical land. And in mathematical land, we get a much more formal definition of these things.

---

## True Risk vs. Empirical Risk

Okay, so now there is jargon. If you were typing notes, you could actually write this one out by hand, but hey, you got the slides.

### Definition of True Risk

The **true risk** (not the empirical risk—the empirical risk is on the training set) is:

$$R(F) = \mathbb{E}_{(X,Y) \sim P} [\ell(F(X), Y)]$$

Which is very traditional. This is the expected value over $X$ and $Y$, the features and the labels, drawn from some specified distribution $P$.

You hope that the world gives you features and labels, inputs and outputs, predictors and responses (they're all the same thing). You draw from some distribution, and you're going to draw an infinite number of them from that $P$. Hey, we're in math land—infinities are not a problem in this land. We just keep pulling $X$s and $Y$s.

And we now compute the average loss between our predicted $\hat{Y}$, which is $F(X)$, and the actual $Y$.

Questions? This is a key piece of jargon. All this is jargon.

**Student Question:** "How do you decide on the distribution for this?"

**Professor:** How do you decide on the distribution? Well, that's a good question, and sometimes you're a statistician and you'll, like, write down some form and try to approximate it. And we'll do some approximations, but we're mostly not going to worry too much about specifying the distribution in general.

Things like a neural network can approximate it. And in some sense, what do we have? We've got a bunch of training data. So you've got 10 million examples of purchases—you've got 10 million $(X, Y)$ pairs. Make sense? That's an approximation to the true distribution.

We could approximate that distribution by some function. Make sense? We can say it's Gaussian, we can say it's a neural net (which is very flexible). But we're gonna do some way to try and do this.

### The Key Point

**We never know the true risk.** This is in math land. This assumes you actually know the true distribution, which you don't know. We're only going to have an approximation to it, because we live in—I'm an engineer, I'm not a mathematician. I try to live in the real world where real people actually have data, so I never know the true risk. I wish I did, but we're going to try and approximate it. Make sense?

### Loss Function vs. Risk

**Student Question:** "What's the difference between risk and loss function?"

**Professor:** Risk and loss function are very, very different. A **loss function** is what? It's a mapping from one true label and one predicted label to a number, a scalar. That's a loss function, right?

It could be the squared distance between two vectors, or it could be "is it the right or wrong classification?" Or it could be alternative things. That's a loss function. A loss function maps a prediction and a label to a number.

That's really different from a **risk**, which is: for some set of predictions and labels, what's the average loss?

And my point is, it's really different to ask "what's the loss on the training set?" versus "the true unmeasurable loss if you actually took every person in the world?"

Right? What's the true distribution of people in the world? No problem, there's only, like, 8 billion. Just measure all of them. That's the true risk in that distribution. Oh, but it's very expensive. I mean, no one... Facebook's got, what, 2 billion users? But they're still missing most people. Right?

So, even if you're OpenAI with, you know, a billion active users, you don't know most people. You'll have an approximation to the full distribution. Make sense?

### Predicting the Future, Not the Past

So I'm going to care a lot about things like: what's my loss on next week's sales, or predictions, or the next 100 people who walk in and are tested for cancer? And that's not the same as the last hundred.

One of the things we'll worry about over and over with machine learning: I can do really, really well on predicting whether the last 100 people that I tested for cancer tested positive or negative. As a computer scientist, if you want to make good predictions about the past, **take a database course**. Look up the answers, right?

I'm not interested in predicting the past, particularly the training data, which I know the labels for. I'm interested in predicting some hypothetical future people that walk in.

As a mathematician, I'm going to assume they come from some distribution. And usually in this course, in general, we'll assume they come from the same distribution as the past. Now, I don't know the exact distribution from the past, I just have an approximation. I don't know the future distribution.

By the way, do tomorrow's purchases from Amazon look like today's? We should put in day of week, and, you know, changes and trends in the economy, and all sorts of things. Certainly on Sundays and Mondays, they look really different. And on holidays, oh right.

So, you know, but we're gonna mathematically say, look, this is the true risk that we know how to prove theorems about. And then in the real world, we're going to approximate it.

### Clarifying the Difference

**Student Question:** "I just wanted to, again, like... it would be great if you could re-emphasize what's the difference between true risk and empirical risk."

**Professor:** What's the difference between true risk and empirical risk? This is a good question for the midterm. Someone?

**Student:** "Empirical is the historical data, the training data. And the true risk is on... something I'd like to estimate going forward... all the possible everythings!"

**Professor:** Right! It's the future, but it's not the actual future, because I don't know the future. The actual future is finite. But this is—if you were to keep drawing people and products forever from whatever distribution you have, this is how well you would do.

### Summary of the Problem

So, I hope things are clear at this point that we really have this conundrum, this problem in AI and machine learning, which is: we only have training data from the past. And we'll be able to do a really good job by memorizing it. But that's not very interesting.

We want to predict future data, and we gotta know something about it. If the world were entirely different tomorrow, we're hosed. But in fact, the world is somewhat continuous and somewhat stable in some fashion. And if we do a better and better job of picking the right features, it gets more and more stable.

If I'm predicting electric demand, you look to see not just the weather, but what day of the week it is, what month is it, is it a holiday, right? All sorts of other things go in. And the more stuff you know, the more stable your model is.

Oh, and then it still changes over time. Electricity demand shifts. And as you keep building all those data centers and power centers, it sucks up more stuff, and it changes your model. Okay, but I can fit that, too. I can even put a model of how much they're probably building more stuff into my model.

### Mathematical Formulation

Cool. Good? So, the equation for true risk is:

$$R(F) = \mathbb{E}_{(X,Y) \sim P} [\ell(F(X), Y)]$$

This is the expected value over $X$ and $Y$ drawn from the distribution $P$. So, $P$ is a probability distribution. And I'm going to draw my $X$s and $Y$s from a joint probability distribution.

I'm gonna sample them an infinite number of times inside the math land. For each of those infinite draws of $X$ and $Y$, I'm going to plug $X$ into $F(X)$, plug $Y$ into the loss function, I'm gonna compute my loss function, I'm gonna average those infinite number of samples. Right? Way out in math land.

But it's nice to know what you wish you had before you get there, because we're not going to quite make it there. Good.

---

## Bayes Optimal and Irreducible Error

The other concept that will come up over and over is: what error you can get rid of by good modeling and training, and what error is just irreducible, can't be gotten rid of. And the technical term for that is **Bayes Optimal**.

### Definition

The best possible function you could ever learn, $F^*$ (star for optimal in machine learning), is:

$$F^*(x) = \arg\min_{a \in \mathbb{R}} \mathbb{E}_{Y|X=x}[\ell(a, Y)]$$

Where $a$ is in the set of real numbers. If you assume that $Y$ is a real number, $a$ is our approximation to $\hat{Y}$. For each given $X$, I'm gonna... there's a true $Y$, and you're gonna tell me what you think the $Y$ is, and you could give me some number.

### Why Isn't It Zero?

Now, why is it not zero? Let's do something really easy. Assume that $Y$ is either 0 or 1. Right? And for any given $X$—$X$ is a vector that describes your previous purchase history, whatever embedding you're using—for any given $X$, we have a distribution. You can embed a person, product, embedding. We're gonna get some sort of 0-1 by default.

$a$ is the best number you could pick to minimize the loss. Why is the Bayes optimal not zero?

You with me? It seems you should be able to learn things perfectly.

**Student:** "Maybe a loss function doesn't reach zero at a point."

**Professor:** Uh, loss function will—let me go to a bunch of them, but in general, the loss function in my case, when $Y$ is 0 or 1... either $Y$ and $a$ (or $\hat{Y}$) are both 1 and both 0, in which case loss is 0. Or one is 1 and one is 0, in which case loss is 1.

So, no, loss functions in general, if you get it exactly right, will be zero. We haven't actually defined the loss function yet in more detail, that's more math.

So what is $a$, and what is $\mathbb{R}$? $\mathbb{R}$ is the real numbers. $a$ is a particular $\hat{Y}$, a given label for the $X$.

So, you give me an $X$—right, the description of the people and products—and you get to pick whatever label you want, whatever $\hat{Y}$, which I'm calling $a$ here. Because $a$ is a variable that ranges over, in this case, 0 or 1. Or if you're trying to predict how many dollars it would be, then $a$ would be the number of dollars—that would be the $\hat{Y}$, the estimate. Right? $Y$ is the real label.

And $a$ is your best label for it.

### Stochasticity and Incomplete Features

**Student:** "You could have two different $Y$s for the same $X$."

**Professor:** Right! In that case, no matter how good your function is, you're not gonna get it perfectly.

**Student:** "If our $X$ is not comprehensive enough, if there's some variation in $Y$ that just can't be explained by our current $X$s, then our model can only do so good."

**Professor:** Yes, if your $X$ is not comprehensive enough, and in every dataset I've used—thousands of datasets, probably hundreds, probably thousands—I've never had a complete $X$.

I worked at Google, I had the whole web, all of it cached. In most cases we used the English part. In those days it was a long time ago. I didn't have enough features. I took all the features on the page, I took all the pages that pointed to it and the pages from the page. There still wasn't enough stuff to classify the page. Right?

There are not enough features to describe everything you want. The feature space is big. Mostly in the real world, it's effectively infinitely big. I only took the features—the words on the page, the links to the page, the links from the page—but I could have gone two clicks. I only took some information on people clicking on it. Right?

There's always more information you could buy, more $X$s you could buy.

### Medical Example

Does my doctor know perfectly about me? No, of course not! I've got a fairly decent medical record. I've had it forever. And, you know, they got good medical equipment, but it's missing stuff. You know? He doesn't know how far I biked this morning. He doesn't know what I ate for breakfast. There's a lot of $X$s that will affect... I mean, trying to predict, you know, how long will I live?

I did that for a while—it's sort of a fun game to play. We built a life expectancy calculator. The uncertainty bars were enormous. No matter how much you know about someone, you don't know when they're gonna die. It's very uncertain.

For a given $X$, there's gonna be a distribution of outcomes that might come out. There's no reason you could ever get it perfectly right. There's always some **irreducible error**. And that part we're not going to try to fit. We're gonna try to do as well as we can, not fitting irreducible error.

I'll come way back later in the course and talk about bias-variance tradeoffs and other framings.

---

## Case Study: K-Nearest Neighbors (KNN)

Good. Um... so... alright! That was the empirical risk minimization framework, right? We talked about risk or loss on the training data. We talked about expected loss over hypothetical future data. We talked about the fact that you're never going to get zero loss in the real world, because there's always noise, or equivalently, you don't have all the $X$s you need.

Now, let's look at one case study. I think the simplest, dumbest training algorithm that I know, which works embarrassingly well. And it's a great algorithm which doesn't do many of the things we want to do, but it illustrates the right things.

It's called **K-Nearest Neighbors**, and it's gonna allow me to talk about norms and distances and model complexity and lots of things that we're going to see throughout the whole supervised learning part of the course.

### The Basic Idea

Cool! So... a typical sort of problem is you get a whole bunch of observations which sit in some space. I'm drawing a two-dimensional space. But usually the space is a thousand-dimensional in the medical world, or a 768-dimensional space in some sort of embedding space, or a million-dimensional, right? They're big dimension spaces. We'll get there.

But we'll draw two-dimensional pictures, which is sort of helpful and sort of misleading.

And you got things with labels. So these are either labeled as stop signs or fast food signs. And you could then try to find some way for a future question mark, some future point in the embedding space—the $(x_1, x_2)$ space—is that one a stop sign or a fast food sign? How can you do it?

**Professor:** "Stop sign?"

**Student:** "Fast food?"

**Professor:** "You got a majority for fast food signs. Um, why?"

**Student:** "You're hungry."

**Professor:** "Yeah, okay. You're hungry. We'll get to Bayesian inference, robustness, class priors. Those are the two burgers. It's closer to the two burgers than the one stop sign, yeah."

### Decision Rules

We're gonna try to find decision rules that are formal, as simple as possible. Old-fashioned things will put the optimal, in some sense, line that tries to separate them. If you try to draw a line between them, you can do some sort of a loss function. You could find K-nearest neighbors.

All we're gonna do is pick the $K$ closest points and take the majority there. We'd say $K$ should be an odd number, which makes it easy to have a majority winner. So I'll pick a $K$ of 1 or 3 or 5. You could do 4, but then you have ties.

Okay, but we gotta pick two things to define the KNN algorithm:

1. **$K$** - the model complexity
2. **What does it mean to be similar?**

Right? And we'll cover many similarities in this course, but today we're going to talk about **distances**, which is one sort of anti-similarity, and it's very nicely clean math that's well-defined.

### The Algorithm

Good? So, here's the algorithm. Really simple. In general, if you're doing classification... let's walk through the algorithm here and see.

If you have something that's a real number, it doesn't matter. If you're trying to do a label—is it fast food or stop sign?—if you have an odd number of neighbors, there's always a winner. The odd person wins.

If you've got two stop signs and two hamburgers, what do you do? If there are 5 of them, there's always a winner. So people like odd numbers for classification versions.

**Here's the algorithm. Super simple:**

1. We're going to find the $K$ nearest points
2. Take your target point you're trying to label (you're given $X$)
3. Check all the points in the training data, find the $K$ that are closest
4. Then either take the majority vote (if it's binary), or average them if it's a real number

**That's it! Really simple.**

### Historical Example: Text-to-Speech

Uh, it works sort of embarrassingly well. There's a really antique YouTube video, which I might be able to show with some audio, which is just someone using nearest neighbors to do mappings from text to speech.

In the modern era, these things are all done with deep learning. Hopefully I need to turn on the audio, maybe. These are all done with deep learning, but the old-fashioned ones are surprisingly based on nearest neighbors.

[Video plays with audio]

*"Each phoneme is stored in its own little cell in which it is generating them all around."*

Sorry, lots of feedback. Review decision trees later, which is a different way of dividing things up into pieces. So, decision trees specify ways to break up the space in a different way. We're not covering those today.

But it turns out that very often—[mutes audio]—very often you can do amazingly well just by saying, hey, I've got a whole database of little chunks of "here's 3 letters and here's the sound attached to it." And just look in my database and find the 5 closest, or 7 or 9 closest sounds. Take the majority common sound. And that does embarrassingly well.

### The Essence of Supervised Learning

I'm gonna argue in some weird way that **all supervised learning is averaging**. All we're going to do is average, including deep learning. CNNs are averaging.

And that averaging means something of: what are you averaging over? And in general, you average over things that are close. We have to figure out what "close" means.

And so this thing says that if you take a lot of things... you could actually take each of these little phonemes and sounds, and you can look up the sounds. And English, as you all know, is a horrible language. The mapping from the way it's written to the way it's pronounced is really confusing.

Okay, I guess Chinese is worse, but, you know, I speak, like, German and sensible languages, Spanish. You write it, you can hear it. There's just, like, a trivial mapping there. There are no spelling bees in Spanish.

So, by taking lots of examples, you could just look up the ones that are similar. You can watch the video—it's linked. And you can get things where it actually then reads out the text. It does text-to-speech just by finding nearest neighbors. Cool.

---

## Norms and Distances

Good? Okay, but to do this we need to find the $K$ most similar points. And we're going to start by not talking about similarity, which is not a mathematical term, but about **distance**, which is. And distances are measured mostly using **norms**.

### Definition of a Norm

Okay, somewhere I have a clicker. So... a **norm** is a function that takes in a vector, and it has 3 properties, which are sort of important, I guess:

1. **Homogeneity**: Trivially multiply the vector by a constant. You multiply it by 2, the norm becomes twice as big.
   $$\|c \mathbf{v}\| = |c| \|\mathbf{v}\|$$

2. **Triangle Inequality**: If you have two vectors, $\mathbf{u}$ and $\mathbf{v}$... let me draw a picture (I'm not showing the line because I'm trying to set up my iPad, but I'm not sure it's working). You've got some vector $\mathbf{u}$ and some vector $\mathbf{v}$. If you take $\mathbf{u} + \mathbf{v}$, the norm of $\mathbf{u}$ plus the norm of $\mathbf{v}$ can't be smaller than the norm of the sum.
   $$\|\mathbf{u} + \mathbf{v}\| \leq \|\mathbf{u}\| + \|\mathbf{v}\|$$
   
   Make sense? Connecting them can't be longer than those two things added. The triangle inequality.

3. **Positive Definiteness**: The only time the norm is zero is if $\mathbf{v}$ is $\mathbf{0}$.
   $$\|\mathbf{v}\| = 0 \iff \mathbf{v} = \mathbf{0}$$

These simple rules make a really nice way of saying how big a vector is. A norm is a measure of bigness. It is non-negative. And if you multiply something by 5, it becomes five times as big. If you multiply it by minus 5, it becomes five times as big. Right? We don't care about the sign. Always non-negative.

### Do We Strictly Need a Norm?

Good?

**Student:** "We don't strictly need a norm, right? We just need a metric."

**Professor:** For K-nearest neighbors, you can use any measure of similarity whatsoever. We'll have... many of them. We'll eventually do KL divergence, which is not a distance, and not symmetric, but has a way to measure differences between probability distributions. We're going to use all sorts of things for K-nearest neighbors.

But the standard and simple ones use norms, and I want to get this nomenclature. And sometimes I don't care about math, but having a clean notation makes it so easy to write things and talk about them.

### The $L^p$ Norm

We're going to talk in particular about what's called the **$L^p$ norm**. You can take each element, $x_j$ of a vector (I'm going to try and use $i$ for separate observations, and $j$ for the terms in a vector). Right?

So I'm gonna take my different $x_j$s in the vector. I will take the absolute value raised to the $p$ power. I will then sum them up. I'll take the whole thing to the $1/p$ power.

$$\|\mathbf{x}\|_p = \left(\sum_{j} |x_j|^p\right)^{1/p}$$

The most famous version of this is the **$L^2$ norm**, which we like to call the **Euclidean distance** (or sometimes just "the distance"). But in this course, you don't... I mean, Euclidean distance is perfectly fine, but it's not the only one. I think it's fine, but it's not the only one.

So this is an $L^p$ norm.

### Interactive Examples

Let's see if we can get these clickers to work. Okay, so what's the **$L^1$ norm** of the vector $(1, 2, 3)$?

Right? I'm summing over $j$ of $|x_j|$ to the $p=1$, and then you take the whole thing to the $1/p = 1$.

[Polling students]

Everyone thinks it's... what is the $L^1$ norm of $(1, 2, 3)$?

$1 + 2 + 3 = 6$. Awesome!

Okay, give me my next one. The **$L^2$ norm**?

$1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14$, so $\sqrt{14}$. Awesome!

What about the **$L^{1/2}$ norm**? Can I compute the $(1, 2, 3)$ to the $1/2$ norm?

$$\left(\sqrt{1} + \sqrt{2} + \sqrt{3}\right)^2$$

The square root of 1 plus the square root of 2 plus the square root of 3, all raised to the power of 2. Weird, but I can compute it. It's a number. I think it's none of the above. But it's a number, right?

So far, so good?

### Special Cases: $L^0$ and $L^\infty$

Cool. So... it turns out that only norms of $p \geq 1$ are actually norms. But there's a super cool **pseudo-norm**, which is $L^0$.

So, you take this $L^p$ formula, and take the limit as $p$ goes to 0. You do a half and a third and an eighth and a sixteenth, or whatever. You can take this down to zero, where you raise each $x$ to the 0.

What is $x^0$? **One**, except for one particular number. Except for **zero**. Right? So anything that's non-zero becomes 1. And 0 stays zero.

So, the summation here will be zero if and only if everything is 0. And then the $1/p$, we're going to take it to the $1/0$, which is roughly infinity.

If it's a zero, it stays a zero. If it's a 1, it's a 1.

So, what you're getting here is something that basically, in the limit, says: all that matters is... how many non-zero elements are there?

Actually, let me correct myself here. I think I got it the wrong way around. Let me think about this more carefully.

**$L^0$ norm** counts the number of non-zero elements in the vector.

$$\|\mathbf{x}\|_0 = \text{number of non-zero elements in } \mathbf{x}$$

And **$L^\infty$ norm** gives you the maximum absolute value element:

$$\|\mathbf{x}\|_\infty = \max_j |x_j|$$

So, point is:
- **$L^1$** is super useful: take the sum of the absolute values. We use that a lot.
- **$L^2$** is useful: taking sum of the squares and square root, that's Euclidean distance.
- Sometimes you want the **minimum** or the **maximum**.

The minimum is actually not a norm, because it's not convex and doesn't have the right properties. The maximum is, actually.

But you can write these things as $L^0$ or $L^\infty$. And you can do other ones, but those are sort of the four that people use. There's a nice nomenclature that says, hey, you can do different things.

### Example with $L^0$

Good. Um, I think good. Let me do a clicker again.

Okay, so the **$L^0$ norm** of $(1, 2, 3)$?

[Clears responses]

Think about it a little bit. At $p = 0.01$ (I don't have to do exactly zero). So I've got $1^{0.01} + 2^{0.01} + 3^{0.01}$. I think those are my pieces, yeah.

So, what is this, roughly? Which of these matters? Yeah, that's the question. Is $1^{0.01}$ bigger, or $3^{0.01}$?

$3^{0.01}$ is bigger!

Did we get that right? I think I said it wrong before. That's why we work it out in detail. Right? This is to the $1/100$. One to the half is one. Three to the half is bigger, so the 3 is bigger. So you guys are right—those other things don't matter so much.

Now I take that whole thing to the $1/0.01$, which equals 100. So the correct answer is **3**.

And I said something wrong before. Oops. Right? The $L^0$... wait, actually, I need to reconsider. 

**Correction:** Actually, the **$L^0$ norm counts the number of non-zero elements**. So for $(1, 2, 3)$, it's **3** (three non-zero elements).

The **$L^\infty$ norm** is giving me the largest element, which would be **3** in absolute value.

Does this make sense? The $L^\infty$ norm (limit as $p$ goes to infinity) gives you the biggest element. Sometimes it's nice to have the biggest item in a set.

Okay, not super important, but nice.

Good? Questions, yes.

**Student:** "I've got a vector $(1, 2, 3)$. I took the $L^0$ norm, it gives me... didn't it give you the smallest one?"

**Professor:** That'll be the exact opposite of this. Yeah.

**Student:** "Is the maximum still, like, a norm?"

**Professor:** Is the maximum still... it's not technically a norm. If you look at it, we won't do it, but if you look at it, it doesn't follow all the properties of a norm.

**Student:** "What about the minimum?"

**Professor:** I think the maximum does follow the properties. Any $p \geq 1$ is convex, which we'll define in a second, and that is what you need to have a norm.

---

## From Norms to Distances

Okay, onward. Onward, onward. Okay, but we didn't want a norm. We wanted a **distance**. We're gonna have two points and say how similar they are. In fact, we're not going to measure similarity—we're going to measure how dissimilar they are.

### Definition of Distance

The most common dissimilarity measure is **distance**, and the standard **$L^p$ distance** is the norm of $\mathbf{x} - \mathbf{y}$:

$$d_p(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_p$$

If you think $L^2$, it's **Euclidean distance**. If you think $L^1$, it's **Manhattan distance**, right? Or some version, $L^1$ distance, I will call it. So I always use this nomenclature a lot.

### Properties of Distances

Distances have the same sort of properties as norms:
1. A distance is always **non-negative**
2. Distance is **symmetric**: $d(\mathbf{x}, \mathbf{y}) = d(\mathbf{y}, \mathbf{x})$. If it's not symmetric, it's not a distance (this is a math definition)
3. It's only distance zero if it's the same vector: $d(\mathbf{x}, \mathbf{y}) = 0 \iff \mathbf{x} = \mathbf{y}$
4. There's a **triangle inequality**: $d(\mathbf{x}, \mathbf{z}) \leq d(\mathbf{x}, \mathbf{y}) + d(\mathbf{y}, \mathbf{z})$

So, distances have really nice properties.

**Student:** "Distance and the norm is the same?"

**Professor:** No, they're not quite the same. A **distance** is a function of two vectors. A **norm** is a function of one vector. They have a different signature.

But every norm defines a distance. Right? If you've got a norm $\|\mathbf{x}\|$, you can take $\|\mathbf{x} - \mathbf{y}\|$—that's a distance.

So there are distances using norms, but they're not quite the same. Right? Distance says how big is the difference of two vectors. A norm says how big is a vector.

### Visualizing Different Norms

Cool. That's it. Um, great! If you look at what these things look like, you can draw lines of equal distance (I'm gonna call them circles). These are all points that are equally distant from the origin.

In a **Euclidean norm** ($L^2$), the points that are equally distant lie along a **circle**, which you've probably all seen, like, in high school.

But, I can also do an **$L^1$ norm**, where the lines of equal distance form these sort of little... rotated squares, like diamonds.

Or I could do an **$L^\infty$ norm**, where the distance forms **squares**.

So remember, we're doing... we're moving toward K-nearest neighbors (we'll use these all over the course). But for K-nearest neighbors, if I'm finding the $K$ closest points:
- In an **$L^2$ space**, the $K$ closest points are ones that sit within a **circle**. I'm making my circle bigger and bigger until I get $K$ points.
- For an **$L^1$ norm**, I'm gonna make my **diamonds** bigger and bigger until I get $K$ points.

There are different closest points for a different norm.

**Student:** "Why is the $L^1$ norm shaped like that? What does it mean, like, you know, if we can express it?"

**Professor:** Why is an $L^1$ norm shaped like that? Yeah, think about what an $L^1$ norm... think about a point. I got a point $(x_1, x_2)$. I'm going to say how far is the point away from the origin?

If I'm at the point $(2, 0)$, or at the point $(0, 2)$, or at the point $(1, 1)$... because $2 + 0$ and $0 + 2$ and $1 + 1$ are all the same distance (all equal 2). So those are all equally distant.

But remember that the $L^p$ norms use absolute values. And so it's exactly symmetrical. Minus 2 is the same distance from 0 as 2. So, if you actually do an absolute value norm, an $L^1$ norm, you get straight lines that say all these points are equally far away.

And you guys are mostly used to thinking in Euclidean distance, where the set of points that are equally far away are all in a circle. That's fine if you want to be an $L^2$ person.

But there's nothing magic about $L^2$. $L^1$ is equally nice.

Now, unfortunately, if you look at an $L^1$ optimization problem, the points there are going to look... like this... that's non-convex, which is gonna be annoying to solve. We'll define non-convex in a second.

**Student:** "You said that $L^0$... well, that defines it, but... I think it's the number of non-zero elements."

**Professor:** Yeah, $L^0$ is, in fact, counting the number of elements that are non-zero. Yeah. You're right. So somehow I did things wrong there. Yeah, thank you. I kept thinking this has got to be wrong. I was trying to derive it, whatever.

So, yeah, so... let me write it out slowly, right? So:

$$\|\mathbf{x}\|_0 = \text{number of non-zero elements in the vector}$$

I clearly messed up the derivation. I'm not going to try it again because I got it wrong. But I will give it on the homework and post the solution. Thank you.

Yeah, I memorized results, and somehow my derivation went wrong. Oops.

Yeah, it's not a norm because it breaks the triangle inequality. It's not a norm—it breaks triangle inequality. And if you look at these lines of equal distance, what you'll find is the $L^0$ one is gonna look weird. Because $L^0$'s gonna be exactly 0 if you're at the origin, 1 if you have one non-zero coordinate, 2 if you have two non-zero coordinates, etc. So it's gonna be weird in that way.

---

## Convexity

Okay, so... thank you for the fix. We'll fix this. I'll repair it in the notes afterwards.

So, with K-nearest neighbors, we're going to find the closest points. And one thing when we launch this course is **convexity**.

### Definition

The simple world is: anything that's a **convex problem is trivial to solve** with gradient descent. Non-convex is hard, and you gotta be lucky.

So, the simple point: something is **convex** if any line segment connecting two points on the surface lies entirely inside it. That's convex.

If you can take a point here and a point there and draw a line that's outside of it, it's **concave** or **non-convex**.

And we like things that are convex. Unfortunately, we do neural nets which are literally non-convex, which will be annoying, but that's an approximation.

Make sense? I don't do the polling on this one.

---

## KNN with Different Norms

Okay, so what happens if you do K-nearest neighbors with different norms? You get different decision surfaces.

And so I've drawn here 3 different things, which is $K = 1$. So, K-nearest neighbors with $K = 1$ says: find the closest point. Whatever it is, that's the right label.

This is a world that's got either X's or O's (color-coded, sort of hard to see if you see the O's). And you can see in the decision surface: anything on the purple side is closer to an O, anything on the red side is closer to an X. Make sense?

And you can see in an $L^2$ world, you get nice divisions that are **hyperplanes**. We're sitting in 2D, so a hyperplane is 1D (a line). If you're in 100 dimensions, a hyperplane would be a 99-dimensional plane separating it. Part of the picture.

Okay, you could do it in 3D. A point here, the separating hyperplane is a two-dimensional plane. I can't picture 100 dimensions, but I'm imagining 100 dimensions, right?

So you see that in two dimensions, you get this nice division with $L^2$.

With **$L^\infty$ norm** (which somehow migrated farther down—the middle one's $L^\infty$), what you get is something that looks sort of similar.

The $L^1$ gets a really weird shape where in the $L^1$, there's a whole long arm of the red thing sticking out.

**Student:** "Oh, I got them wrong, yes, you're right. Thank you, thank you. This is, like, just so annoying."

**Professor:** I did these... thank you! I did these through... [adjusts slides] There we go.

I went to Google Slides and it actually messed them up. Thank you! Someone's paying attention.

So the $L^1$ is similar to $L^2$, as it should be. The $L^\infty$ is, in fact, the one with this weird piece where you get this little extra arm of pieces there.

### Weighting Features

So, bottom line is: depending upon how you define distance, things matter.

The other thing to note is all these norms we talked about—in the simple version, **every feature is weighted equally**. In the real world, that's never the case. Well, it's not always the case.

If you're doing pixels, it's not bad. Or pixel embeddings, it's not bad to weight them equally. In an embedding space, equal weighting is usually pretty good.

But if you're actually doing things like medical records, where you have your blood pressure and your height and your weight... mmm! You know, you got a value of either 140 for blood pressure and your 70 kilogram weight. It's not clear they should be equally weighted. Make sense?

So in the real world, you're gonna have to do something to the variables to figure out how to weight them. As long as the weights are non-negative, then things tend to work out pretty well for keeping the norm properties. It's not quite right, and we'll see later that there are matrix operations—you're multiplying by a matrix, and the matrix has to be a positive semi-definite matrix for the weights. We'll get there eventually.

So the piece that I want you to be thinking about is: we're gonna have to have some way to sensibly give a weight to every feature. Does that make sense?

And again, lots of datasets have some values in dollars and some in RMB, and some in euros. And you don't really want to have them all weighted equally to measure how distant it is. We'd like to have some sort of a currency exchange rate weighting or something sensible.

So, K-nearest neighbors works well if you know what it means to be similar. But that's going to be something that's gonna be harder to figure out.

### Adding Points Changes Decision Boundaries

Good. I think, good.

**Student:** "In that diagram there, suppose we add just one new point—the entire decision boundary changes, right, again?"

**Professor:** If I add one point, does the entire decision boundary change? Maybe. Let's see. If I add an O over here... what happens? **Nothing.** Let's see, if I added an X over here... now I've got a whole other red region over here! Right?

So sometimes adding a point has an effect, and sometimes adding a point has no effect. Good question, yeah.

**Student:** "Is there any characteristic to think about them?"

**Professor:** Um... I think the answer is: play with them and try and think about what the decision line looks like.

So if you take the decision boundary between this X and this O, there should be a perpendicular line that goes at 90 degrees between them that separates them. For the $L^2$ norm, and that should go until you hit the next point. Between this point and this point, you get a Voronoi tessellation (the term I haven't defined).

If you look at the $L^1$, now you've got something where there's this weird extra kink here. It's not on that dividing line, because at this point, the other one's distance kicks in.

Um, so I think the answer is: play with it, and try and play with the numbers. You get some sort of intuition. I don't have a better rule than that, yeah.

### Distance Weighting

**Student:** "Just an observation. This doesn't really care about how far you are exactly from your nearest neighbors, right?"

**Professor:** You're pulling me right. Yes, it doesn't... it cares what are the $K$ closest ones. This is both **wonderful and terrible**.

We will later use **kernel methods**, and we will use neural nets like deep neural networks as kernel methods. Those ones are going to be really different, because with K-nearest neighbors, you can have regions that are really dense with lots of points. And now your $K$-nearest neighbors are super close. Or you can have really sparse data where there's, like, nobody in your training data, and the $K$ candidates are really far away.

That would be really, really different from—we haven't done kernels or Gaussians yet, but it'll be really different from the future ones—where you have to be within a certain fixed distance.

Right? So if your data's got some stuff which is really dense and some which is really sparse, K-nearest neighbors is awesome. Because if there's not much data, you have to look a long way away until you find the album closest to Nine Inch Nails. Whereas if you're looking for, you know, the Beatles, which are really dense, right?

So, it's a different assumption, or a different **inductive bias** about the world.

---

## Three Components of Machine Learning Algorithms

Cool. What is my thing. So, I want to come back to what I said last class, which is: **every machine learning algorithm has three components**:

### 1. Representation

In this case, it's not like a regression or a neural net. There's no $\theta$, there's no weights. This is **non-parametric**. Right? There are no parameters in this model.

Is $K$ a parameter? No, we don't call that a parameter. We're going to call it a **hyperparameter**.

We'll see—most of our models will have **parameters** like weights. For regressions, they're coefficients, which are exactly the same thing. Think of linear regression and neural nets—they have weights or coefficients. Those are parameters.

Then there are things that define something about the complexity of the model, the structure of the model. So $K$ is not called a parameter, confusingly. It's called a **hyperparameter**. Right? It's something we'll have to pick.

It's as if it's a parameter, but it's something that characterizes and controls the model complexity. And it can be adjusted. One thing you will do a lot if you're ever a data scientist is you'll have to pick the hyperparameters, which is harder than picking the parameters.

The parameters will be solved mostly by gradient descent, but the hyperparameters will require all sorts of search methods to find. We'll get there.

Um, so it's not parametric. There are no parameters in it.

### 2. Loss Function

The **loss function**... well, it's sort of a weird piece here. But, in some sense, we didn't define a loss function explicitly. All we said was how close things are in the feature space. We never talked about what we meant in the $Y$ space. Make sense?

But if you're doing estimations of a real number, I said we'll take the average. And we'll see in many cases, the average is optimal under an $L^2$ loss, a square error loss. So, that was sort of hidden away inside it.

### 3. Optimization

And in some weird sense, this is also not a traditional machine learning algorithm, because **there's no learning**. You got the training set, the $X$s and $Y$s. There's no parameters to estimate. There's no training. Right? You just got the data.

So this really breaks the whole paradigm. We're gonna spend almost all the first half of the semester with things that have parameters in them, like linear regression. We will then use optimization methods like gradient descent to find the best ones. We will then also do a search over hyperparameters to try to find the best ones. And it could be really different.

So this is a very weird algorithm.

### Loss Function for Hyperparameter Selection

Cool. Yeah.

**Student:** "What are you doing to minimize the loss function?"

**Professor:** If I do the simplest K-nearest neighbors, it's either 1 or 0, majority vote. Remember that normally when I had a parametric model, I said, find the function that minimizes this loss.

Now I'm not really searching to minimize a loss here, right? So where would I minimize a loss? And the answer is, the only thing I'm really picking is a **hyperparameter**, $K$.

Which you can try a bunch of different $K$s and you can see which one did best for predicting some future data. So that one would have a loss function minimized to pick the hyperparameter.

Or I could do a search over different metrics. I've got an $L^1$ metric and an $L^2$ metric. Oh, that's a weird thought, because you can say: which distance metric for similarity gives me the lowest testing error loss?

So I can use the loss function to pick the hyperparameters. But there are no parameters to fit with the loss. There's a slightly odd model. Does that make sense?

It's different from a linear regression or a neural net, where you're adjusting parameters to minimize the loss. Here, the only thing you get to pick is the hyperparameter—the one hyperparameter $K$—and the distance metric.

So, yes, you could pick which $K$ minimizes the testing error loss under some loss. So that one would have a loss function in it.

### Two Different Distance Spaces

**Student:** "Why specifically do we use the $L^2$ norm? Is there a reason for that?"

**Professor:** If you want for calculating distances, or even things that are not distances... but realize that we're using distances for **two different things** here.

One thing is the **distance in the feature space**. How different are the $X$s of two observations? That's for K-nearest neighbors.

The other thing is, **what's the norm of the difference between $Y$ and $\hat{Y}$?** That's a distance in the predictor, the $Y$ space.

So there's two different sets of distances you can talk about, yeah?

**Student:** "Do we have a $Y$ for K-nearest neighbors?"

**Professor:** We always have a $Y$. We got a bunch of $X$s. Maybe there's a picture of a stop sign or a picture of a hamburger. And that's an $X$. It's either the pixels or an embedding of the pixels—that's $X$.

And I've got something labeled. The label is either "fast food" or "stop sign." So those are... there's always a $Y$: fast food or stop sign. Right?

And you're gonna have some sort of a measure of how good you did, which is usually, did you get it right or wrong? In that case, it was just binary—that's all you get, right or wrong, right?

But you classified every point in pixel space to either be a stop sign or a hamburger stand. I got a $\hat{Y}$, which is, "I think it's a stop sign."

So it's not parametric, but that's different. The $\theta$ or the $W$—it's not the same as the variable $Y$. This is supervised learning, man.

### Different Norms for Different Spaces

**Student:** "Do we have to use the same norms for calculating the loss function as well as the distance?"

**Professor:** No, the distance in $X$ space is entirely unrelated to the distance in $Y$ space. They could be entirely different. The $Y$ norm is what you care about. But your boss cares about it.

She wants dollars, or whatever she wants—less time. You know, it's either seconds or dollars. That's the $Y$—that's what you care about. That's one norm which is given by business considerations. Right? The cost of killing someone, or not killing someone, or diagnosing cancer when you shouldn't, or missing it when you shouldn't, right? Those are loss functions on the $Y$.

That's different from the $X$ space, which is: how similar are your medical records? They're very different things.

**Student:** "The embedding really matters for this, right? Like, for the stop sign example, like, say our embedding is not rotation invariant. Then, like, all hell breaks loose, right?"

**Professor:** Yes, this whole thing requires your **feature space to be one that's sort of sensible**, over which the distance makes sense. Right?

And it turns out that very often neural net-based embeddings make sense and have good distances in $L^2$ space. But there's no guarantee they will. That's why you guys have jobs!

Cool.

---

## Picking $K$: Model Complexity

So, no gradient descent here. Great. Um... how do we pick $K$?

Typically, we have our training data, which we don't get to pick—that's what we've got, unless you spend more money. Or you buy Scale, you're Mark Zuckerberg, you buy Scale for a billion dollars, now you got more labels. Get a bunch of people in Kenya, like, labeling stuff for you.

Cool. Um... so you pick some sort of a loss function. What do I care about if I'm predicting a real number $Y$? Do I care about the $L^2$ or $L^1$ norm on that number?

And then, based on that, I'm gonna try a bunch of different—whoops, I'll go back—yeah, I'm gonna try a bunch of different $K$s.

### Example: Linear Regression vs. KNN

So here's a simple, trivial picture, which is one $Y$ and one $X$. And just for pictures, I've drawn a straight line through them. So I've plotted the best linear fit.

And you can see here:
- Here's a nice straight line through data that's almost linear
- Here's a line through something which has sort of an increasing sinusoid
- Here's a line through data with lots of noise

Great, that's linear regression for different problems. So far, so good? Right? Fit the best line.

Now, let's do **1-nearest neighbor** on those same three datasets.

So here's... given a $Y$, which is the nearest neighbor, and if you look really close, you can see little red dots in the data here.

On this first dataset, the nearest neighbor is always the one at that $X$ value, right? And on the second dataset, the nearest neighbor is always the $Y$ down here. And it's really noisy!

As I make $K$ bigger... [advances slides]

If it's the sinusoid, it starts to smooth. Here's the one with noise—if it's noisy, it's really noisy.

**Student:** "Doesn't look great."

**Professor:** You're shaking your head, you don't like that one? Somehow, for some of these, 1-nearest neighbor looks plausible, and for some of them it looks bad.

**Student:** "It's not even just a KNN problem. Like, the $X$ doesn't seem very explanatory in the first place."

**Professor:** This... which... this second one? Third one.

**Student:** "Third one."

**Professor:** The third one you can't do a great job—it's noisy. But we're still gonna do the best we can.

I know a guy who went to work in finance, and he said if he can get a **half a percent increase in accuracy** or whatever metric, that was a **billion dollars**.

I worked at Google. If you could get a half a percent increase in click-through rates, improving click-through rates, that was a **billion dollars**.

There's a lot of things that are incredibly noisy. But if you get some signal, and I'm gonna argue there's not pure noise—look at it. If you look at that line, there is a little bit of a slope. I haven't computed a p-value to see if it's significantly different from zero, but it's plausible to me. We'll have to check it. Maybe there's some signal there.

If not, I should be taking the average. Right? If there's no signal, the best I can do for those things is average all the points. That's linear regression with no features. Right?

My job as a machine learning person is to always give a $\hat{Y}$. And sometimes the best I can do for the $\hat{Y}$ is just average all the $Y$s. That's a limiting case, but hey, that's life. Right?

### Varying $K$

Okay, so I did 1-nearest neighbor. I can do **9-nearest neighbors**.

And now what I get is something that's way smoother. Like, 9-nearest neighbors looks pretty good—it's not that far from a straight line. Should make you happier.

And this 9-nearest neighbor here is... pretty good in the middle, but this is over-smoothing. It's not capturing the variation here. And here, it's doing great in the middle, and it's sort of really missing the edges.

So, depending upon what your data looked like, one $K$ or another may be better. We're gonna have to pick the right hyperparameter to control the complexity of the model.

For 1-nearest neighbor, it's a more complex model. If I make $K = N$ (the number of points), what's it giving me? **The average!** Which is sometimes optimal. Right? Sometimes you can't do better than just taking the average. It's a bummer, but sometimes that's all you can do.

---

## High Dimensions and Curse of Dimensionality

Okay, so I'm gonna skip these slides and leave them for you to think about. I'm gonna try and move to wrap up.

**High dimensions have weird properties**, which we'll come back and talk about at some point in the future. But if you think about a 100-dimensional cube, then it turns out that... it's really weird. It's hard to have the intuition of high dimensions.

But it turns out that in high dimensions, **almost every point is equally close to every other one**. It's not like two dimensions.

Um, I don't have time to do it now, but it's a fun math assignment—maybe we'll do it in recitation. And I am going too fast, because we're not going to have to do pieces on it. We'll come back to it later, because I want to wrap up and make sure you get out on time.

---

## Summary

So at this point, you should know:
- **K-nearest neighbors**, with $K$ controlling complexity
- Picking the **metric** is important
- What a **norm** and a **distance** are
- What **convex** or **non-convex** means

And that will just wrap up. I will get you out on time, a couple minutes early to let you get out.

---

## Attendance and Logistics

And we'll do a quick poll on attendance... oh, we're doing pretty well today, that's good.

Um, we'll skip the questions. And hopefully we'll have one last poll.

And then I've just... let me go back. Let's do this one for attendance. So, what you want to be doing is using your Penn email to log in to Poll Everywhere. And that should then register that you are actually you. If you're not signed in, or if you're signed in using your Gmail, I can't find you.

Cool. So, last thing: **recitations are happening**. You should all be signed up. You should all be on Ed and communicating. If you have questions, post on Ed. The TAs and I will get back quickly.

Have a great weekend, I'll see you next week!

---

## Post-Lecture Q&A

[Various student questions and informal discussions about course logistics, waitlist, and specific concepts]

**Student:** "For people on the waitlist, what's gonna happen?"

**Professor:** I will look today to see if there are any open slots. I hope to get the data from the first class. Don't worry too much. Yes, I understand that not everybody will get in.

[Discussion continues about various topics including the Bayes optimal definition, weighting features, and L-norms]

**Student:** "So you're basically picking an $a$ as a $Y$?"

**Professor:** Yes, you're picking the $a$ that gives you the lowest loss for that given $X$ you're seeing. Pick the $a$—that minimizes the expected loss.

[More technical discussions about the mathematics and implementation details]

---

**End of Lecture**

---

# CIS-5200 Fall 2025 - Lecture 3: Gradient Descent

## Introduction to Loss Functions

We begin by discussing different ways to measure error. The square root of $\frac{1}{n}$ times the sum of squares gives us the average, which is also not a norm. So we're going to mostly prove things in the squared error space, which is nice for doing math.

But when we report things, we'll mostly look at something that's much more like the square root of the sum of the squares.

Does that make sense?

You know, we want to minimize losses in dollars, yen, or seconds, but it's going to turn out to be just cleaner to do the math for things if we have squared dollars.

Right? So I'm going to say the loss function in this case is something like, for example:

$$L(\theta) = \sum_{i=1}^{n} \left(f_\theta(x_i) - y_i\right)^2$$

This is the summation over $i$ equals 1 to $n$ of our data points, of the difference between our function (which I put a little subscript $\theta$ to remind us it depends on $\theta$) of $x$, minus the actual response or output $y$. And we're in the $L_2$ space today.

Good? And that difference is called a **residual**.

**Jargon:** Residual is the difference between the prediction and the actual value.

Good? Good, good.

## Visualizing the Loss Function

Awesome! Now, let's do this in one dimension first. What we're going to have in general is some feature $x$.

Which will be really big—thousands of dimensions. But the $x$ here is what? The $x$ here is $\theta$.

Right? We're going to look at how the loss varies as a function of $\theta$.

Right? So it's a funny $x$ and $y$. The $x$ is the $\theta$, the weights.

Right? And this could be a very big $x$.

Right? By the end of the course, we'll have millions or hundreds of millions of weights in our neural nets. It's just a big vector.

Everything's just a big vector. We have a big vector of weights, and we'll have a loss function. For the moment, it'll be scalar. Eventually we'll do fancier ones where the output could be a vector, and we'd like to find the minimum loss.

Well, on the training set or some loss function, eventually on a test set, but for the moment it's empirical risk.

And there'll be some sort of a function which describes how the loss varies as you change the weights.

Makes sense? I'm going to draw one dimension because I'm good at drawing one dimension for $\theta$, but it's really very big.

Questions?

Good? Good!

## The Gradient Descent Algorithm

So now we have a super easy thing, which is: we start with some initialization, maybe a random number. We're at some actual $\theta$ here.

And we want to move toward lower loss.

And the way to move to lower loss is super easy:
1. We fit a tangent line
2. We find the slope there
3. And we're going to walk down that slope

Trying to go to a lower loss.

### Estimating the Slope

There are two ways to estimate the slope.

Right? And in some sense, we have what is the loss of $\theta$ equal to the loss at some $\theta^*$ (theta star, where we are at the moment):

$$L(\theta) = L(\theta^*) + \nabla_\theta L(\theta)|_{\theta^*} \cdot (\theta - \theta^*) + \text{higher order terms}$$

Plus a gradient! This is $\frac{\partial L}{\partial \theta}$ of $L(\theta)$ at $\theta^*$, plus a squared term. We do a Taylor series expansion—that's very abstract.

What's $\nabla_\theta$? It's $\frac{dL}{d\theta}$. Make sense?

If you can do it analytically, that's nice. If not, you can always do a numerical version.

### Numerical Gradient Approximation

So, most of the software allows you to put in some $L$, some $f$ of $\theta$. We can take a little $\nabla\theta$, it's going to be multiplied times some change in $\theta$. If we do a little change in $\theta$, we're going to change state a little bit.

We take the change in $L$ here minus the $L$ there, the $\theta$ here minus the $\theta$ there, and we'll get the difference:

$$\frac{\partial L}{\partial \theta} \approx \frac{L(\theta + \delta) - L(\theta)}{\delta}$$

Machine learning people like to use $h$ instead of $\hat{y}$ because $h$ is the hypothesis, as opposed to $\hat{y}$, which is the function of $\theta$—they're exactly the same.

But they're not the same as $y$.

Make sense? $y$ is the thing you measure. That's the actual—how many dollars you spent. $\hat{y}$ or $h$ are functions, estimates.

So, we can say we'd like to take the gradient of the sum of squared errors, the loss. Take $h$ at $\theta$ plus $\delta$, a little change, a small amount to $\theta$, take $h$ at $\theta + \delta$, take $h$ at $\theta$, divide that by $\delta$—that's the slope.

People good?

In practice, this is probably not quite the smartest way to do it if you're actually a numerical methods person. Mathematicians love this, then let $\delta$ go to zero.

I'm an applied guy, so my $\delta$ is never zero. Right? On a real computer, $\delta$ is, you know, $10^{-6}$ or something.

### Sum of Squared Errors (SSE)

What is SSE? SSE is the **Sum of Squared Errors**.

So, this thing over here is the SSE:

$$\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

That's the sum of squared errors loss.

Right? You can pick whatever loss you want, but for the next half hour, the loss is the sum of the squared errors, as opposed to the $L_2$ norm of $y - \hat{y}$.

### Symmetric Difference Quotient

Bonus question: why might I want something slightly different than what a mathematician uses here?

You can take numerical methods? This one's asymmetric—you've gone one direction. You might be a little happier to add $\delta$ and subtract $\delta$, so you have something symmetric about it:

$$\frac{\partial L}{\partial \theta} \approx \frac{L(\theta + \delta) - L(\theta - \delta)}{2\delta}$$

So this one, if you look at the picture, I'm fitting some sort of a curve. I'm here, I take a little $\delta$ here, I take the delta there. That's great, but maybe I'd be happier taking $\delta$ there and $\delta$ here and fitting something that goes plus $\delta$ on one side and minus $\delta$ on the other.

Not super difficult, and most people use software that does this automatically, so you won't have to worry about it.

But I think the key point is: you can compute the slope, right?

## Multi-Dimensional Gradients

You've got some sort of a slope here, you take the value here, you take the value there, you've got a distance $\delta$. You take $L$ at $\theta + \delta$ minus $L$ at $\theta$, divide by $\delta$, pick $\delta$ small—that gives you the slope.

But wait, though: this is one dimension.

If we're in a thousand dimensions, what does this delta look like?

We're going to have to do this a thousand times: add a little bit of $x_1$, see the slope; add a little bit of $x_2$, see the slope; $x_3$... For each of the thousand dimensions, we're going to have to know how it goes down in each of those thousand dimensions.

It's a little annoying, but hey, it's linear in the number of dimensions. It's not hopeless.

And so then you can say, okay, if I change $x_1$ by a little bit, $\epsilon$, how much do I reduce or increase the loss? If I change $x_2$ by a little bit...

I'm going to have:

$$x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_p \end{bmatrix}$$

We'll usually go out to $x_p$. In this course, I mostly use $p$ because the statisticians love $p$ for predictor! If you're a real mathematician, then you would do $x_d$ for number of dimensions. But mostly not $x_n$.

### The Gradient Vector

Okay. So if you want a general version of this: we've got a scalar loss, which is a function of a vector $\theta$.

Right? Which is 1,000 or a million-dimensional. We'll have to find the slope in each of these little directions. And now we want to go downhill in all the directions.

So far, so good? So we start with some $\theta$.

We are then going to say:

$$\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)$$

We take the theta we're on and subtract off some learning rate $\eta$ (eta) times the gradient with respect to $\theta$ of the loss of $\theta$.

## Learning Rate

How big a step do I want to take?

If you take too small a step, you walk down slowly, slowly, slowly, slowly, slowly.

If you take too big a step, you end up way over here!

So, the trick is to go fast enough that you're moving toward the downhill, and not so fast that you bounce up the far side.

Oh, except it's much uglier because you're in a very big, high-dimensional space, so there's lots of sides.

Sound good? So we'll spend, like, a half hour figuring out how to take a good step size.

Questions? People are very quiet—either it's clear or not.

**Question:** A little bit louder, there's, like, air conditioning. The loss... SSE is the loss function I picked, so I'm doing a concrete loss function, which is the SSE for this particular case. We'll plug in a thousand different loss functions, but today, the loss function is the sum of squared errors.

So, I've written SSE as: well, there it is. SSE is defined as the sum over all training points of the function of $x$, $\hat{y}$ minus the $y$ quantity squared. That is the definition of the sum of squared errors:

$$\text{SSE} = \sum_{i=1}^{n} (\hat{y}_i - y_i)^2$$

Right? And it's an $L_2$-ish sort of loss, but it's not a norm because there's no square root.

The square root makes it nicer in some ways and has nice math properties. The sum of squared errors makes it easier for us to actually take the derivative and do the math.

And so people like to play with sum of squared errors. It's monotonic: if you make the norm bigger, SSE gets bigger. If you make SSE bigger, norm gets bigger. So they're equivalent in terms of maximizing one versus the other.

### Higher Dimensional Tangent Spaces

**Question:** If you're in a higher dimension, are the changes additive?

Right, so the thing to notice is if you're in a high dimension—let's sort of sit in a two-dimensional piece in three dimensions—at any given point, the tangent, if you're in three dimensions, there'll be a two-dimensional surface.

If you're in $p$ dimensions, the tangent is a $(p-1)$-dimensional surface going... is that right? That is right, right?

But the direction you're going down—so there's a tangent which is $(p-1)$-dimensional, and then along that, there's a direction which is the steepest one, which we're going to move on, which is a vector, which is a one-dimensional steepest one, unless it's degenerate and flat.

And I think the answer is: to approximate these $p$ different gradients (because you have a gradient in each of the spaces), you can actually numerically just do them one by one.

Note that we're living here in mathematical land where the $\delta$ you're adding, or $\epsilon$, is arbitrarily small. So we're doing a local linearization.

Right? The tangent is a local linearization.

Pretty much by definition, if you're locally linear, then you can actually empirically do each of them one by one.

So the generalization, if you're really in math land, it's:

$$\frac{\partial L}{\partial \theta} = \lim_{\delta \to 0} \frac{L(\theta + \delta) - L(\theta)}{\delta}$$

And you can do that in all of the $p$ dimensions separately. And it all works out because it's linear—it's locally linear. So the answer is yes.

### What is Theta?

**Question:** The definition for theta. What is theta? What's theta?

Yes! It's the parameters in the model.

Right? We're back in parametric space. We'll be in parametric space for most of this course. And so, $\theta$ are the parameters of your model.

And again, you should all have seen linear regression. Think of the weights in a linear regression, or think of neural net weights.

## Learning Rate Selection

And now we need the $\eta$, which is the learning rate. So the gradient tells you what direction to go, but we've got to decide how fast to go down, or how far to go.

And so we're going to now spend a half hour going through a bunch of algorithms to pick $\eta$.

Right? Because we need to decide. And again, the point is: if $\eta$ is too small, you'll get there eventually, but you'll run out of compute time, and you'll get bored, and your boss will get pissed at you.

Or if $\eta$ is too big, it'll bounce around. Because if you have something that looks like this: if you take the slope here and you take a big step, you're going to go all the way over down here for the new $\theta$—the new $\theta$'s here.

If you now take the gradient here and you take a big step from the gradient, now you're up here—you got worse!

And if you do this for a living ever, for example with artificial neural nets, you will find that you spend an unreasonable amount of time looking at and trying to tune a bunch of hyperparameters, of which the most important one will be learning rate.

And you will find that if you plot the error as you keep iterating, you hope the error goes down, mostly. In general, it goes wiggly, wiggly, wiggly, wiggly, wiggly, and sometimes it goes wiggly, wiggly, wiggly, wiggly-wiggly and gets bigger and is unstable. And that's super annoying.

And if you're running on 1,000 GPUs, it becomes very expensive.

So we worry a lot about getting the learning rate right. For the first half of the course, we're in statistics land with linear regression and logistic regression—who cares? But later on with neural nets, you really do care about being very efficient about this.

So the neural net people obsess about efficient training.

### Deriving the Update Rule

**Question:** How do we come up with what $\eta$ should be?

Not sure I heard—so how do we get this equation?

This equation is basically (and it'll be in the readings worked out in detail) a linear Taylor series expansion, which says that we have the loss at the value:

$$L(\theta) = L(\theta^*) + \nabla_\theta L(\theta^*) \cdot (\theta - \theta^*)$$

plus the gradient of the loss times the change (you can't even read it here, there's a $\Delta\theta$), plus something which looks like a squared term.

This equation—$\theta$ is replaced by (gets) the old $\theta$ minus a learning rate times the gradient:

$$\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)$$

is exactly the same as this equation right here.

Right? So what do we say? It's the slope, right? $\nabla_\theta L$ is the slope.

If you're in $p$ dimensions, that's a $p$-dimensional vector. $\theta$ is a $p$-dimensional vector.

Think of linear—at least for linear regression—where I have if you have:

$$y = \theta^T x$$

I think that's the simplest one. That's a $p$-dimensional vector.

I can take a loss function (we'll derive it in a second), but I can take the derivative of the loss. The sum of squared errors will get a slope, and we're going to update $\theta$ with a learning rate times the gradient of the loss function with respect to the parameters.

### Why Does This Improve Theta?

How do we know that's going to improve $\theta$?

Locally, if you take a small enough step, it's guaranteed if things are continuous.

We're going to assume our loss functions are in general continuous and differentiable. And what I'm saying is, think of it intuitively: if you're here and you take a step going down, if you take a small step, things will get better.

If you keep doing it, things will keep getting better, as long as you're going down.

The magic of gradient descent—and there's lots of theorems—if you're always going down, you'll keep going down.

### Finite Differences vs. Analytic Derivatives

**Question:** [About approximation errors]

Yeah, so there are always approximation errors. The first one is: if you have the option, don't do a finite difference.

So when we go forward, we'll derive this for an awful lot of the models we'll use. You can do an analytic derivative—don't use the finite difference.

And if you're using PyTorch and a neural net, some nice person has figured out the whole derivatives for you—use an analytic derivative if you can. Don't use the finite difference.

So that's the first answer.

The second answer is: if you do this empirically, you're going to be a little bit careful to try and figure out how big or small to make the $\delta$. It mostly works out better than you would think.

But in general, given the choice between analytic versus numerical, do analytic! Don't do a finite difference.

Right? That works, but you want the limit as $\delta$ goes to zero.

Cool. Good.

## Analytic Gradient Derivation

Onward. So what did I just say? I said instead of actually doing this finite difference, it would be much nicer if I could actually just calculate the derivative analytically, which is super easy because:

What do I have? I have my loss function, my sum of squared errors.

We have a loss function:

$$L(\theta) = \text{SSE} = \sum_{i=1}^{n} r_i(\theta)^2$$

Which is the summation where $i$ goes from 1 to $n$ of the residual squared.

But what I want is:

$$\frac{d}{d\theta} L(\theta)$$

Which is $\frac{d}{d\theta}$ of this.

What is the residual? The residual looks like... Well, I've written it there. We'll write it this way. We'll write it as:

$$\sum_{i=1}^{n} \frac{d}{d\theta}(r_i^2)$$

That is:

$$\frac{dr_i}{d\theta} \cdot 2 \cdot r_i$$

for the residual. That's not very hard math.

Now I need $\frac{dr_i}{d\theta}$.

$r_i$ is... which way did I write it around? I wrote $r$ this time as being $y - \hat{y}$.

If $r$ is a linear... if $r$ is, let's write $r$:

$$r_i = \hat{y}_i - y_i = \theta^T x_i - y_i$$

There we go: $\hat{y}$ minus $y$.

Now, $\frac{dr_i}{d\theta}$ is $x_i$.

Cool! So this piece here is $x_i$.

And I have something that says:

$$\frac{dL}{d\theta} = \sum_{i=1}^{n} 2 r_i x_i$$

The $\frac{dL}{d\theta}$ is equal to the summation of all the points of that.

Cool. It's also written up top in something legible, because, hey, LaTeX is much easier to read than my handwriting!

Does this make sense?

Now, if we have a fancier function that's not linear regression, then the whole thing's going to be messier, but it's the same idea. We'll plug in something that's other than $x$ here; we'll get something different.

Now, we don't need to worry about doing approximations.

Cool. Good?

## Key Points Summary

Maybe. Okay. So, the big points are:

You always can compute the gradient. You can usually do it analytically. Usually somebody has done it for you. If we do logistic regression in a week or two, we'll do neural nets—somebody will have found the gradient for you.

And the one thing you're going to have to do is pick the learning rate $\eta$.

And if you take $\eta$ too small, you're just trickling downstairs. If you take $\eta$ too big, it wiggles and wobbles.

There's all sorts of math. One year ahead, the TAs tried to derive and do some nice Jupyter notebooks to derive and show that the optimal learning rate from the theory and show that that works in the notebook. They spent 6 hours and failed.

So there's a lot of theory, some of which says that the optimal learning rate is to scale $\eta$ with $\frac{1}{\text{number of iterations}}$. Some say you should scale $\frac{1}{\sqrt{\text{number of iterations}}}$.

I'm more of an engineer than a theory guy.

And so, in practice, the answer is you're going to try and empirically keep changing and have algorithms that adjust the learning rate $\eta$.

### Iterations and Epochs

**Question:** By iterations?

Each iteration—great, new jargon. Each iteration is: we're going to be at a $\theta$, we'll compute the derivative, we'll take one step and get a new $\theta$, then we'll repeat the process.

You could call it an epoch in a different world. Yes.

The dumbest way that people did when I was your age was to say, hey, let's just each time we take a step, take a little bit bigger one, and if it starts going worse, make it half as big.

But hey, that's a very old solution. There are much better methods in the last decade.

And let's look at some of them.

## Convexity

Cool. Okay, before we go any farther, though, I should note something.

For the first part of the course, we're going to deal mostly with loss functions $L(\theta)$ that are **convex**.

The convex ones are really nice. What do I mean by convex here?

I mean that if I draw my loss function (I'll do it in 1D): if you're at some point here and go to another point here, my line is always above the two points on the surface.

And you can picture in 3D, it looks like a cup.

So, a lot of problems, like linear and logistic regression, are convex and life is beautiful. When we get to neural nets, the things will look more like [non-convex surface].

Except in high dimension, it's terrible. They have, like, Grand Canyons crossing each other in steep little narrow pieces.

Now, if you're in something that's non-convex: if you do a gradient descent, if you take a big step, you go down here, which may be good. If you take a small step and are super cautious, you end up here—a local minimum.

Right? So for convex solutions, we'll be able to guarantee that if we do something sensible, we'll find the best answer. There will be a best answer. There's a bottom.

If things are nice and convex. If they're non-convex, often there's not any single local minimum.

And it turns out that, for lots of reasons we'll get to much later, neural nets with gradient descent work magically well.

And for neural nets, we actually won't want to find the absolute lowest point, which will turn out to be overfitting, but we haven't covered overfitting yet.

So, for the moment, let's just sort of assume things are convex.

As long as things are convex, we're going to be able to walk down, and as long as you keep moving down, you'll get there. And you want to try and get there as quickly as possible.

Yeah, there's time at the end where we'll say more about different levels of convexity.

Good? Good.

### Which Function is Convex?

**Question:** [Inaudible - about which function is convex]

When I say the function is convex, what function am I talking about here?

The loss function as a function of $\theta$, right? And realize that all these functions are sort of functions of different things.

The loss function, really, is a function of $\theta$ and of $x$ and of $y$. Right?

So, the loss function at some level is:

$$L = L(f_\theta(x), y)$$

Right? So the loss function is a function of $\theta$, it's a function of $x$ and $y$. In machine learning, we're going to fluidly go back and forth between taking derivatives with respect to $x$ if we care about $x$, and $\theta$ if we care about $\theta$.

But today, we're worrying about $\theta$ because we're trying to pick the best $\theta$.

Cool! Onward.

## Stochastic Gradient Descent

So, in practice, sometimes we have really big datasets.

Right? Think of Elon or Sam or any of your best friends, Zuck—they've got big datasets.

And this idea that says you're going to compute a sum of squares by summing over all trillion training examples—it takes a long time to go through trillion examples. You don't want to do them all at once and then learn at the end.

And so a nice approximation (which is actually not what any of those guys use—we'll get there in a second) is to take **one observation at a time**.

One $(x, y)$ pair.

As I take one $(x, y)$ pair, instead of looking at the total loss summed over all the residuals, I'm going to take one $x$, predict one $\hat{y}$, and I will then update by moving $\theta$—not doing the summation $i$ goes from 1 to $n$, but just using one observation.

Now, this is super noisy. It is **stochastic**.

I used a fancy term, which means that each time you see an $x$ and a $y$, you're going to take a little step in the gradient direction.

And you're going to bounce all over.

If you're doing a good job, then in expectation (which is a fancy version for "on average"), you'll be going downhill.

But you'll be bouncing a lot. You'll go in the right direction sometimes, the wrong direction sometimes, but as long as you're going down more than up, you're doing a gradient descent.

And this is called **stochastic gradient descent**.

You could also think of it as an **online method**—you could get an $x$ and a $y$, make the update and throw them away—or a **streaming method**, depending on which community you come from. They're all the same idea.

Which is, rather than trying to take all of your $x$'s and $y$'s and do an average over all of them (right? There's an average here, right? This has got the averaging—there's no $\frac{1}{n}$ because it's sum of squared errors, but it's effectively an average), we could, in fact, just do them one at a time.

Questions?

This turns out to be remarkably useful.

### Learning Curves

**Question:** Won't you be wandering around?

What people often like to do is to plot the loss as a function of the steps, or iterations, or epochs—sort of a learning curve: how do I learn over time as I get more and more training?

I'm going to start with some initial random $\theta^*$.

I'm going to have some loss.

And if you're doing an actual gradient descent and things are working, as you then update once—add in your minus $\eta$ times the slope—it should go down. And you do another step, it should go down again. And you hope that things go down, down, down.

You don't ever expect to get things to zero because there is some irreducible loss. There's always noise. There's a Bayes optimal—the best you can possibly do—so some asymptotic that you can't do better than, because life's not perfect.

But if you're actually doing steps with full gradient descent, if you're descending and actually moving down, that means the loss goes down as you keep updating $\theta$.

Make sense?

If you're doing stochastic gradient descent, now, instead of doing one step here, I'm doing a thousand little iterations, and sometimes it'll go up and sometimes it'll go down.

And you'll, if you're lucky, see something that looks sort of like that, where as you're doing one little—take one $(x, y)$ pair, take one tiny step.

If you have a million observations, we can do a millionth of a step, a little tiny step! It's going to sometimes make the overall loss on everything go up and sometimes make it go down.

Right? It makes it go down for the particular $(x, y)$ pair you had, but that may or may not work for the overall piece here. Remember, this is the whole thing.

Right? This is, for example, the sum of the squared errors—it's something which is all the data points.

### Training Set vs. Test Set

**Question:** [About training vs. test]

Yeah, so a key point is: so far, everything we're talking about is on the training set.

So we're talking today about empirical risk—the training set risk. We're going to have to worry next week about the actual expected risk over some future dataset.

Right? So we want the test set risk, really.

But what I'm talking about today is only empirical risk.

And we're going to have to modify our loss function. We actually don't want to minimize empirical risk. We don't want to minimize the risk on the training set.

We want to approximate the risk on future data.

And therefore, we're going to have to use a different loss function, which will lead us to MAP instead of MLE. But we're not anywhere near there.

So yes, on average, if you're doing these... there's a bunch of ways to formalize this, but I don't want to get into the math.

### SGD Convergence

So, if we think about stochastic gradient descent: if you have regular gradient descent, if you have a million points, you're computing a million residuals and summing them, and then a million more, and then a million more, and then a million more. And maybe after you've done it 20 times, you've gotten up to 20 million.

If you're doing them one by one, you're going to do a million iterations here, and then a million more. Hopefully by 2 million, you've got there. You don't have to do the 10 million.

So in general, it saves you a lot of time. You're updating as you go through each one, rather than waiting to the end of the million.

In the large scheme of things, if you have the learning rate reasonable, they will converge to the same thing.

Certainly, if it's convex, things are non-convex—it's really hard to say what things converge to. You almost can't prove anything about non-convexity. But if it's convex, then in fact, things will converge to the minimum if you take a reasonable step size—not too, too small and not too big—and you'll converge to the same thing, but you'll converge with fewer calculations if you're updating after every single thing.

## Mini-Batch Gradient Descent

**Question:** Is there something where it's neither all nor one?

Funny you should ask!

Yes, in fact, if you look at what Zuck and Elon and all of my friends—I don't know any of these guys—Sam. What they do is everybody in the deep learning world does something called a **mini-batch**.

Which is, instead of updating with one point or with a trillion points, you say: pick a small set—100 points, more likely 64 points or 128 points, for reasons that may make sense later in the course.

You pick a finite set of $K$ points. For those $K$ points, you compute the derivative and you average the slopes for those $K$ points and add them in.

And to give spoilers for the deep learning world: what you want to do is pick enough of them that you fill up your memory on your GPU.

And if you've got too few to fill up the GPU, you're wasting GPU cycles. And Jensen Huang is happy because you'll buy more NVIDIA chips. And if you have too many to fit there, it's going to be really slow because your GPU's going to be waiting to get the other stuff there.

So everything in machine learning is vectors.

And under the vectors are GPUs or TPUs or computers—chips that are designed to do vector operations.

And so, again, for the first half of the course with small data like you have in the hospital—who cares, except for the images?

I have work with people doing breast cancer, and they're really spending months running a calculation. It turns out we have a lot of images of breasts.

And, yeah, it's not as fun as it sounds. And you're trying to look at the different tissue densities in them to predict who's at risk of getting cancer, and you want to do embeddings into nice spaces and build models, preferably ones you could explain to the doctors.

That's really hard, and it turns out to be very expensive.

And so the thing you do is you say, hey, I take my observation of an image, I embed it in a way we'll do later in this class, you then feed that into your fancy model (a neural net, of course), and then you pick a mini-batch size that's just big enough to fill up your GPU chip—and not too big and not too small.

And again, as long as you're taking a reasonable step size, everything works.

Good, good, good.

## Different Error Metrics

**Question:** [About different error notations]

Ah, yes, so I switched notations. There are going to be three different widely used errors:

### Sum of Squared Errors (SSE)

$$\text{SSE} = \sum_{i=1}^{n} r_i^2$$

Which is a summation, $i$ equals 1 to $n$, of the residual squared.

### Mean Squared Error (MSE)

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} r_i^2 = \frac{1}{n} \text{SSE}$$

Which is $\frac{1}{n}$ times the sum of squared errors.

### Root Mean Squared Error (RMSE)

$$\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} r_i^2}$$

Which is the square root of the mean squared error.

### L2 Norm

Oh, and then I guess, finally, there's the $L_2$ norm, which is none of the above.

Right? The root mean squared error looks nice—it measures things in dollars. Root mean squared error measures things in dollars, but it's nicer to do the math on SSE because otherwise you have all these square roots and things floating around that just make stuff ugly.

But they're monotonic. If you minimize SSE with respect to $\theta$, that's the same as minimizing the mean squared error or the root mean squared error, because you can put whatever you want and it doesn't really matter.

And I think the other piece to note is that the SSE is nice for doing math if you've got everything.

But once you start doing mini-batch, now, the error on the mini-batch depends on the size of the batch, and that's sort of annoying.

So, in many ways, it's nicer to think of the error per point.

If I give you 10 times as many points, SSE says the error is 10 times as big, which turns out nice for the math but not very nice for a human.

10 times as many points means the error's 10 times as high? I want the error per point.

Right? So as a human, I like root-mean-squared error because that's error per point in dollars or minutes. But as a mathematician, I like sum of squared errors.

For the mini-batch, I tend to use the mean squared error, because now if I change the mini-batch size—I double $K$ from 32 to 64—I don't change the mean squared error, but I will change the sum of squared errors!

So, we're going to swap back and forth rather sloppily between each of these, and just as you read papers, just be a little bit careful which error they're using.

### Importance for Regularization

They're all monotonic; it doesn't matter for optimization. But if you're actually an engineer and you're computing what is the error, if you're changing the mini-batch size (again, from 32 to 64), it's annoying to have error become twice as big or $\sqrt{2}$ times as big.

So, conceptually, they're all the same; they're all monotonic; who cares? And therefore, we're going to swap shamelessly back and forth between them.

But in about a week, we're going to spend a lot of time worrying about overfitting.

This is all empirical risk. This is the training risk. We're going to add a penalty on.

Every one of you at some point in your life will mess up adding the penalty, because whether you're penalizing the sum of squared errors or the mean squared errors, it's a different penalty.

Right? The penalty is $n$ times bigger with the sum of squared errors than it is with the mean squared error.

And so, at the moment, everything is sort of who cares—scale invariant. But pretty soon, we'll be doing cases where we're putting penalties in, and now the penalties need to be sensibly scaled, so you've got to keep track of whether it's $n$ or not there.

So I'm going to swap shamelessly back and forth between them and trust you guys to keep up.

But if you get them on a homework or an exam, or a real-world case, pay attention to which one it is you want to be using.

Obviously, if $n$'s a million, SSE is a million times bigger than MSE! Which means if you add a penalty, the penalty should be, oh, a million times bigger.

So the theory later on is going to have us actually be very careful about these.

Cool! Good.

## Ill-Conditioning and Momentum

Now, there is one big problem. My TA said I shouldn't show you this slide because it's too ugly, but I'm going to show it to you anyway, because hey...

Which is that very often, you have a curve which is shaped like a folded piece of paper.

And it looks sort of like this that you're trying to minimize.

There's one direction that's very, very flat, and one direction that's very, very steep.

This is called, in the fancy terms, **ill-conditioned**.

And I'm not going to go through the slides at the end (which you don't have to worry about) where you can compute the eigenvectors of the Hessian, which is super cool and nice math. But for our purposes, if you're not careful, there's one direction where you're going down slowly, and one that's very, very steep.

And if you try and do a gradient descent, it's easy to wiggle back and forth, up and down and up and down on the walls. You're going zing, zing, zing!

Which is great for a video game where you're trying to climb the walls, but otherwise it's not helpful.

And so one thing that we worry a lot about is how to make the matrix more well-conditioned and fix it.

### Feature Scaling

One thing that's super common in deep learning is: take all of your $x$'s and rescale them so they all lie between 0 and 1.

Now, that's maybe sensible if things are very, very different. If you've got some things that are measured in kilograms and some that are measured in years—right, like your weight and your age—you can take the ones that... I work with a lot of these.

So, one way to try and make the things better is to rescale them.

There are a bunch of other ways that try and fix this sort of problem, but it is going to be something that we're going to worry about.

On the other hand, with pixels, you probably don't want to rescale them. The ones that are really small on the image of your head—that's the background.

Right? There's a bunch of it which is not your breast, which is still on the picture—it's all black. So rescaling works sometimes, doesn't work other times. We'll figure that sort of thing out more.

### Momentum Method

So, one of the ways that's a great hack that's widely used to deal with this ill-conditioning—these sort of deep valleys—is something called **momentum**.

And the idea of momentum is, if you think of it as like a marble rolling down the hill, instead of rolling just down and zipping, zipping, zipping up and down the sides, what you can say is: hey, if I took one step in a direction, when I take the next gradient, don't just go in the next gradient direction—take some constant times how much I changed before and add it in. Keep going in the same direction you were going.

And sort of in mathy terms, what that's going to look like is that when I try and figure out—oh, I have a clean board!

Cool.

When I think of how I want to make a change (and here I've called $v$—they like to call it velocity for the change):

So, I'm going to write these in reverse order. My new $\theta$, to get the old $\theta$... $\theta_{t+1}$:

$$\theta_{t+1} = \theta_t - \eta v_t$$

minus $\eta$ times what would have been the gradient.

And now, instead of the gradient, we're going to use some $v$, which will look sort of like the gradient, where the $v$ is equal to something plus the gradient $\nabla_\theta L$. And the something here is going to be a constant $\rho$ times the preceding $v$.

Right? So the new $\theta$ is going to be equal to... the learning rate—I'm going to add on, subtract off the learning rate times the change. The change is equal to some constant times how much I changed before plus the gradient:

$$v_t = \rho v_{t-1} + \nabla_\theta L(\theta_t)$$

$$\theta_{t+1} = \theta_t - \eta v_t$$

So this says: don't just go in the direction you were going—I say, don't just go in the new gradient; add a piece of the old one.

Super easy. Great hack.

Has beautiful math, which I won't cover, again tying back to approximating the Hessian, the second derivative.

But the idea says that if you do this, you can then get something... let's see if I have a nice slide with this. Oh, I don't. Bummer.

But empirically, what happens is instead of wiggling back and forth, this helps you average it out and smooth it, and it makes you go, on average, smoother downhill rather than wobbling as much.

Now, I've got, unfortunately, two parameters. One is the learning rate, which I had before, and the other is this momentum term—two hyperparameters.

But that's life.

I can adjust those, or find which ones worked empirically in the past for similar problems, and that helps.

For simple problems like logistic regression, who cares? We don't need this. For deep learning, this will turn out to be super useful.

### Momentum Questions

**Question:** [About the gradient and velocity]

The gradient descent is always the same. We still have this—these are all vectors, right?

This is $\theta_{t+1}$ equals $\theta_t$... oh, $t$, that might be my typo.

The ones on the slide... the slides are correct.

$$\theta_{t+1} = \theta_t - \eta v_t$$

minus $\eta$ times the velocity, the change.

The velocity is:

$$v_t = \nabla_\theta L(\theta_t) + \rho v_{t-1}$$

The gradient (same gradient we had before) plus a constant times the preceding velocity.

Again, view my things here as my talking. If you can read the slides, it's actually a little bit hard if you're in the front row, but the slides actually have the right equations.

Does that make sense?

Thank you—that was wrong here.

**Question:** How does this relate to ill-conditioning? How does this relate to the steep, curvy thing?

Yeah, the way this relates is that if you're going downhill on the curvy thing, you're going to mostly be worrying about going down the steep part and not a lot about doing the shallow part at the bottom.

And you'd like to not bounce up the curve, down the curve, up the curve, down the curve.

And so when you add in the momentum, what it does is it'll help you go less bouncy up and down the sides of the walls, and it'll help you average out that fast-changing, bouncy part, and help you go more smoothly down the smooth part.

**Question:** The velocity... should it be between 0 and 1?

No, the velocity... first of all, is the velocity a scalar or a vector?

This constant here is the learning rate. This constant here—oh, non-negative, yes. This is a positive constant.

I mean, think whether it's less than 1—yeah, it's generally going to be less than 1.

### Second Order Methods

**Question:** [About second-order approximation]

We like to approximate the second order. The problem is, if you're really doing the full everything, what you have is a loss function:

$$L(\theta) = L(\theta^*) + \nabla_\theta L(\theta^*) \cdot (\theta - \theta^*) + \frac{1}{2}(\theta - \theta^*)^T H (\theta - \theta^*) + \ldots$$

You have, at first order, $L$ of $\theta^*$, plus $\nabla_\theta L(\theta^*)$ times some change in $\theta$, some little piece, plus some quadratic term which looks like a Hessian, which is the second derivative.

And the problem with a second derivative? I can never afford to take it.

Because the second derivative—the derivative of the loss function, it's a matrix with respect to $\theta$, with respect to $\theta$.

Make sense? So that one is: if I've got a $p$-dimensional $\theta$, the Hessian (the second term) is $p^2$.

That's really big. So in general, in machine learning, unlike in sort of nice, clean math, we can't actually afford to compute the second derivative here because it's a $p \times p$ matrix. And so we're doing some sort of a hack to try and approximate some little piece of it.

And the cheapest, the oldest hack in the books, is just to use this momentum term, which tries to capture something about this term here.

But that's sort of what's going on. And again, there's beautiful math, which I'm happy to point you to, which says that if you put in this momentum, it makes the Hessian less ill-conditioned. It makes the ratio of the biggest to the smallest eigenvalue smaller. It makes things converge better.

**Question:** [About initialization]

Is that... in general, initially we're going to set these things... yeah, to zero.

So initial... we're just going to initialize everything we don't know; we'll set to zero.

**Question:** Should that be a minus?

I don't think so. So I think... I have the wrong sign? That's possible.

No, I think it's right, because I think... think about this term. In some sense, what do I want to do with $\theta$? I want to go downhill, in some sense, on the $\theta$. So the $\eta$ is positive, the learning rate's positive, we're going downhill. And in this case, instead of using the gradient, I'm adding in something which is a positive function of the previous gradient, right?

So this momentum term looks like, roughly, what was the gradient before?

Right? There's a minus sign here.

I think I'm okay.

**Question:** What are the hyperparameters here?

The momentum, the learning rate, and if you really care, $v_0$ when you first start—there's no momentum; I just make that 0, so nobody ever changes it to anything else.

I'm going to cut things off, so... first of all, people can't hear you in the back, so I need to find a microphone or something.

Yeah, let me not answer that, and if there's time later at the end, we'll answer people's questions. I want to cover a couple more things.

## RMSProp (Root Mean Square Propagation)

So, the basic concept of momentum, which dates back to before my time in deep learning—there's a bunch of other variations and things that people do, and the details mostly don't matter, but I think the ideas really do.

Which is to think that often you want to sort of do some sort of a scaling of how big should this gradient be?

And the obvious thing to scale the gradient with would be to divide the gradient by how big it is.

And for lots of things in machine learning, we're going to rescale them. The fancy word is often "normalize." If we do a more fancy version, we're going to rescale them.

So a sensible thing says: hey, if I'm going to add in the gradient, I don't know if this is big or small—it depends on sort of what $\theta$ is. How about I divide it by how big the gradient is?

Now, the gradient's a vector. What's the standard measure of how big it is? $L_2$ norm would be a sensible way to do that.

And, in fact, people almost do that, except the gradient keeps changing.

And so, a better way of measuring how big the gradient is is to do something that's a **moving average**.

That says: I'm going to take something which is, rather than the square root of the sum of the squares of the gradients (which would be an $L_2$ norm), I'm going to take the square of the gradient at each step and take $(1 - \rho)$ (a forgetting factor) times this one, plus $\rho$ times the preceding sum.

It's a moving average, which says: take my current gradient plus, sort of, $(1 - \rho)$ times the preceding one, plus $(1 - \rho)^2$ times the preceding one, plus $(1 - \rho)^3$ times the preceding one. Each one that gets farther gets weighted down.

Make sense?

### Exponential Discounting

So a standard way, and we'll see this over and over, especially if we get to reinforcement learning, is to have a discounting.

Right? So, simple discounting says, you know: what's a dollar worth a year from now? Well, less than a dollar today.

Depending on what Trump and the Fed do, your guess is as good as mine, but certainly, you're going to say that dollar is worth less. And a dollar in 2 years is going to be less—quadratically less—3 years, 4 years. Make sense?

In this case, we're discounting the past, though, not the future.

We're saying we want to weight each of the gradients from our preceding steps, our preceding iterations, and have them go down: $\rho$, $\rho^2$, $\rho^3$, $\rho^4$.

### RMSProp Equations

So, we have here sort of two pieces. Now look at the equations.

One says we're going to update our $\theta$, our parameters, by subtracting $\eta$ times: divide by what looks almost like a norm (the square root of the sum of the squares—you put a little $\epsilon$ here because when you start with zero, you don't want to divide by zero; that's a hack that always comes in here) times the gradient:

$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_t + \epsilon}} \nabla_\theta L(\theta_t)$$

And this sum of the squares is not just the current one plus the preceding squares, but the current one times... sorry, $(1 - \rho)$ times the current one plus $\rho$ times the preceding one—the preceding average:

$$s_t = \rho s_{t-1} + (1 - \rho)(\nabla_\theta L(\theta_t))^2$$

Does this make sense?

**Question:** Why is the $s$ formed the way it is?

What we'd like to do is to have something that is the same size as the squared gradient.

So we're dividing it between $\rho$ (weighting here) and $(1 - \rho)$ times the other one.

So, $\rho$ plus $(1 - \rho)$ equals 1.

So we're matching them. The total amount here is, in fact, equal to the sum of squares. We're adding a piece of the preceding average and a piece of the current one, such that they sum up to 1.

So this looks like a sum of squares, but in fact, it has the property that you will exponentially forget the past.

And the gradients keep changing, right? As we walk down our gradient, they keep changing. I don't want to use the current gradient because it's very noisy.

I don't want to use the average or the sum of all the gradients because, hey, that's doing stuff from way, way back. I started way up here, I walked down, I walked down, I walked down, I walked down.

So I do something where I do a weighted average that tends to forget.

### Why Square the Gradient?

**Question:** Why are we squaring the gradient?

Great question. Why are we squaring the gradient? If I had my choice of how to write this:

$$\theta_{t+1} = \theta_t - \eta \frac{\nabla_\theta L(\theta_t)}{\|\nabla_\theta L(\theta_t)\|}$$

I want to divide by something. What do I want to divide by here? I want to divide by the norm of the gradient:

$$\theta_{t+1} = \theta_t - \eta \frac{\nabla_\theta L(\theta_t)}{\|\nabla_\theta L(\theta_t)\|}$$

So it looks like that. I guess $\frac{1}{\text{something}}$.

What I want to do is some measure of how big the gradient is, right? Just divide by it. I'd like to divide by how big the gradient is.

Right? But how am I going to approximate the gradient?

Well, what does this look like? This looks like the square root of the squares.

So, what we see here is the square root, and here we see the squares.

But what I'm doing is adding back in weighted pieces of the past so that it is sort of a moving average, so that the farther back it goes, the less it counts. But this thing really does look like, right—the clean version is: this is the gradient divided by how big the gradient is, which makes it roughly size 1.

Now, this, of course, is a vector of size (the number of weights). This is a scalar (how big it is). But this rescales it.

We're rescaling the gradient to make it roughly a size 1.

But because the world's so noisy, because each time we take a step, it'll go to a different part, instead of actually using the actual norm, we'll use something that is this sort of funny approximation with a moving average.

**Question:** Why don't we use the actual norm?

Because empirically, it works better to have something that's a moving average that's smooth.

And so, a lot of these have been done by just people trying lots of stuff. I don't have a good math piece there, except...

I think what happens over and over—think about doing mini-batch, where this is widely used. Each mini-batch is incredibly noisy. You only have 50 or 100 observations, so you're much better to approximate the size of the gradient, not with the current one, but by the average over the last thousand of them.

So I think what happens a lot of times when this is used—this really comes out of the mini-batch world. For sure, if it's stochastic gradient, you just have one observation—the gradient's super noisy.

And so it's better to use an averaged, time-weighted average gradient, which is much more stable.

And again, we're really looking here at the neural net world. The neural net world, there's going to be tons of noise in everything, and we'll do lots of averaging to try and smooth things out.

Does that make sense? At least mini-batch should make you happy that the gradient is a crappy 50-point approximation, but at least the normalization is a stable thing that doesn't bounce around like mad.

**Question:** Is this more variable for a loss function than exponential?

Probably not. I mean, I think the answer is these sort of things work pretty much the same for any loss function, so this equation here—note that I didn't write anything that was quadratic or whatever.

We'll use these for the whole neural net. There'll be a huge, massively ugly loss function here—a whole neural net slammed in: CNN or Transformer or whatever. So we're going to put hugely ugly loss functions into here, and the answer is it'll work out pretty nicely.

### Visualizing RMSProp

Cool. So, what happens if you do this? If you try using just a simple gradient descent, and here's lines of equal loss.

Right? So this is a 2D picture of... if people don't use topological maps anymore for walking around, hiking, but each line here is a line of equal height. So this thing—picture this as a long oval basin, where the part here is the bottom, and then there's a... these lines are all equally high, these are equally high, these are the much highest ones, right? So this is like a cup that's all spread out like a saucer.

Like when they have these sort of curved soap dishes, a milk tray—I think the British have these little milk... cute little milk things that they pour the milk out for to put in their tea, I don't know. I'm not British. I drink coffee. But...

As you go down, the gradient, if you do a straight gradient, tends to wiggle, wiggle, wiggle, wiggle, wiggle.

If you then put in something that uses RMSprop or momentum, what you'll see is that it smooths it so it goes much more smoothly down and less bouncing around.

And by averaging out the pieces and normalizing it, you can get things that go where the downhill is not the local greedy downhill (going downhill here and downhill there and downhill there), but in fact takes it smoothly down.

And again, when we get to neural nets, this is going to be essential; otherwise they don't work.

## Adam and Adam-W Optimizers

Cool. So, I don't want to do too much detail, and I'm just going to wave my hands here and come back and talk... I'll talk a little about this.

You start throwing all of these things together. When you do neural nets, you'll use **Adam** or **Adam-W**, and there's been a bunch of papers that say, "Look, I tried 57 different fancy variations that have all these things, and the best one was 20%, 10% better than Adam."

And I'm going, "Great! I could use Adam, because I don't actually care about 10%."

I mean, it may be different if I were doing lots of NVIDIA chips.

But for me, 10%, 20% is close enough, you know. Whereas if you don't do something like this, you're a factor of a thousand or a million. Then I start to worry, because it takes a year instead of, like, a day.

But often what you do is something that's got everything thrown in it. There are a bunch of these, and they're all coded up in advance.

Where you might have something like both momentum and RMSprop, and what you see in things that are almost too small to see—let's see if we can make it bigger.

Almost too small to see—is that we can sort of build all these things together. So we have:

- One piece, which is the momentum, adding a momentum to the gradient
- One piece here which has the RMSprop, which is the squared part built into it

We then update our weights, subtracting a learning rate plus the square root of the normalization constant (that was the one we had before) times something which has the momentum filled in.

And then people, instead of using the simple $m$ and $v$ we had before, add in some little bits of constant decay rates to try and smooth things out and worry about the fact that when you first start, things aren't the same as they are later on.

But they're all just doing these pieces that are the same ones, right? They're normally just either:
- Adding in momentum: keep going in the same direction
- Or they're normalizing

### Per-Parameter Normalization

The final one, which I won't show you the slides for, but on the very end: if you're really fancy, you can take each of your parameters and renormalize by how big their gradients have been.

And it turns out that it's pretty common that you have some of the parameters changing really quickly and some changing slowly. Some have steep gradients, some have shallow gradients. So you can end up normalizing each gradient—each individual parameter. Each of your million parameters gets its own little "here's the squared sum of squared, discounted changes it's had."

There's all sorts of ways that people play with this, but they're all conceptually the same.

## Global vs. Local Optima

**Question:** When do we get a local optimum, and when do we get a global optimum? What guarantees the global optimum?

**Convexity.** If the problem is convex, you will get a global optimum, unless you're really stupid, in which case you bounce off to infinity.

But unless you're dumb, or doing something really bad with a convex problem, you always should converge to the optimum.

For a non-convex problem, in general, there is no guarantee that there exists a single optimum. There could be lots of local optima.

Right? And so, if the world is non-convex, then it looks sort of like [complex non-convex surface].

And there may or may not be more than one optimum, but there often are multiple optima, and at that point, there is no guarantee in general of anything.

I'm overstating slightly, but not very much.

It turns out that a long time ago, everything that was linear could be solved trivially, and everything that was nonlinear couldn't be.

Today, everything that's convex can be solved trivially. Anything that's non-convex, you sort of hope it's going to work.

### Neural Networks and Optimization

**Question:** [About neural networks and optimization]

I'm not sure I know the difference. For a neural net, what you're trying to do is... first of all, you're not going to find the global optimum.

That's intractably hard. You can never afford to find the global optimum, and we'll see later if you did find it, you'd be horrified because you've overfit.

You've minimized the empirical risk, and you've done a terrible job out of sample.

So the answer is: you've got a limited amount of compute time, and you're going to trade off questions of, given your amount of training data and your compute time, how—given I give you, you know, 20 GPU years—do the best job you can afford.

Right? So almost all these systems are compute-limited.

I mean, some of them are getting data-limited, but most of the big neural nets are compute-limited. You just can't afford to find the best solution in the time you have. And certainly when you get to reinforcement learning at the end of the course, that's really expensive, which is what all of the big deep learning systems now use. Super expensive—you're really limited on compute time.

### Small Models vs. Large Models

So, for the first half of the course, you're going to be data-limited. We're going to do simple models like linear and logistic regression, or gradient tree-boosting models.

For all the trivial models where it's small—"small" is everything in the hospital that's not an image—right, you've got 20,000 people in their full medical records, not counting images—that's small. For the small stuff, you're data-limited. Who cares about the compute? You know, spend another thousand bucks and buy a bigger computer, which I'm always telling the guys there.

So, for the first half of the course, we're worried about small models, little bit of data. We'll be super careful about squeezing out, in the statistics realm, all the data possible.

For the second half of the course, we're going to move toward big models, where you're compute-limited, and we're going to really worry about squeezing out the most compute.

Make sense? And I'm sort of getting ahead of myself, because I'm looking ahead toward the gradient descent for deep learning, even though we're going to do a bunch of tiny models. Heck, linear regression is going to be closed form.

And to be efficient, we'll use singular value decomposition and clever methods. We don't need to do any of this stuff for most of the early problems.

But thank you for the chance to expound on it.

Good!

## Coordinate Descent

So that is sort of the core concepts. There's one more sort of weird thing that shows up, which is that...

This was things that were $L_2$ sort of norms. In an $L_1$ world, still convex, still has a global minimum, life works well. But it turns out that when things are not nicely curved—when the second... if it's quadratic, everything works beautifully. If it's quartic, if it's fourth order, it gets too flat at the bottom; it's no longer strongly convex.

If it's an $L_1$ norm—$L_1$ norm looks like absolute value.

Right? It's got a slope, but no second derivative.

Things don't work quite so nicely in terms of converging, even though it's convex.

And it's often useful to have algorithms—these will be built into scikit-learn and things we use. It's useful to have algorithms for $L_1$ that say: do a gradient step in the first feature, $x_1$. Now do a gradient feature in the second feature. Gradient step in the third feature. Do them one by one. Change all $p$ of them and cycle through.

That's a **coordinate-wise gradient descent**.

And in general gradient descent, we'll do other ones like EM later. As long as you're always going downhill, and as long as it's convex, you'll get to the bottom.

So, the takeaway from this is: you don't have to change all the features all at once. You can cycle through the features, changing them one at a time. And as long as you move one gradient step down the first one, down the second one, down the third one, you'll still be moving downhill. With reasonable gradients, you'll end up at the bottom, if it's convex and there's a bottom.

Right? So jargon term is **coordinate descent**—one feature at a time.

And it's nice for the world that's $L_1$, because in $L_1$, you don't have the second derivative.

Right? So your whole momentum thing is breaking down.

Cool. That's too fast to be useful, but useful to know. You'll mostly not see it.

## Summary

So, where are we?

**Gradient descent:** Follow down the slope toward a minimum.

Right? If you gradually go down, you'll always come to **a** minimum. If it's convex, it's **the** minimum.

Mostly we'll use analytic derivatives. Someone else will take the derivative for you, but you can do numerical.

**Step size is everything:** controlling them, and we've seen a bunch of variations, which you should know the jargon on. All these things are explained. The reading has—and I've stolen it with permission from Eric and Servi from last year—the first half of the reading, the first five pages, cover all this stuff cleanly in math. The second five pages have a bunch of other stuff you're not responsible for, but you're welcome to see if you want to see more math and more convergence theorems and more pieces.

And with that piece, I want to go back and... let people, because we have 5 minutes left... if I go back to the piece here...

I do want to get the admission things to work.

So, hopefully things are cleaner now for people who missed last week's... don't worry about any checking of attendance last week. We're going to... it's all experimental; we're debugging the process.

Hopefully you'll get it working today; hopefully this will be smoother.

I apologize for all the things. We'll end 5 minutes early. I'll hang out for questions.

Thank you! See you Wednesday!

---

# CIS 5200: Machine Learning - Lecture 4: MLE and MAP Lecture

## Administrative Matters

That's what I wanted… that's you. Oops.

Okay, welcome, welcome, welcome! Hello back there!

Okay, so I think we finally have the attendance taking working pretty well. I think two people had trouble last time, but it should be fairly straightforward. You go to Canvas, you click on attendance, you either do the barcode or you type in RT4TE, and it magically shows up on Canvas. If you have problems afterwards, grab your TAs or me.

Good. Um, bureaucracy, anything? Are we okay? Recitations are happening, the feedback has been sent to the TAs. Most of them are good; one or two are not great reviews, and we'll work on it. Hopefully they'll all be great in a week. I think we're okay.

Yes, yes. Okay, so where are we?

## Overview: Probabilistic Models and MLE/MAP

What I want to do is to shift gears a bit and talk for the next week or so on MLE, MAP, and probabilistic models. We said that what we're doing is having some sort of a model of the world, some sort of a loss function, and some sort of optimization.

For an awful lot of machine learning, the output of the model is a probability. That will be true of linear regression and logistic regression and many of the deep learning models.

What we want to do is to find, for a given model form and a given set of $X$'s and $Y$'s, the parameters $\theta$ that are most likely, in some sense, to give the $Y$'s.

The loss function we're going to use is going to be a likelihood: How likely is it that the model will produce, for a given input, the output we observed?

### Course Structure for Probabilistic Models

What I'm going to do is cover this probably four times in this course:

1. **Today**: Something totally trivial, just flipping coins, like an intro probability course. We will look at MLE and MAP - Maximum Likelihood Estimation and Maximum A Posteriori estimation - for flipping coins.

2. **Next**: We'll do Gaussians.

3. **Monday**: We'll do linear regression and logistic regression, which are very popular forms.

We're going to show over and over that there's a duality - a one-to-one mapping. For any given probability model, there will be some sort of a loss function. Some probabilities will lead to $L_2$ loss, some probabilities will lead to $L_1$ loss.

Back and forth across the course, we'll choose how we want to pick the model form, what sort of probabilities do we look for, and it's going to tie to something about the distribution of the data.

What we're doing at the end of the day, I hope, after all this math, is: as you look at real data, you'll say, "Hey, this data has some distribution." Some things are Gaussian, some things are not. I look at language mostly - words are really not Gaussian. I've never seen a negative count of a word. Words are either 0 or sometimes there. They have a long tail distribution. The frequent words are really frequent, the rarer words are really rare - even most words are really rare.

So we'll look at a bunch of different distributions, and we'll try and build models that let us do the best job possible on those.

### Logistics Note

I realize this classroom is terribly far away. I'll think about starting 5 minutes later or something, because at the moment it's not working out super well. Let me think about that, I'll post on Ed.

## Coin Flipping Example

Okay, so I'm going to try a different modality today. I'm going to see if I can actually use my iPad instead of the chart there, and we'll see if that works. You can give feedback later if you like that or not.

So let's try and figure out - we're going to flip coins. We're going to end up with something that says:

$$P(X = \text{heads}) = \theta$$

where $\theta \in [0,1]$.

Do I have a coin? I don't have any coins. Imagine I had a coin. I get a heads, get a heads, get a tails.

I now want to estimate for this coin the probability of getting a heads in the future. What's a good estimate?

### Initial Estimate Discussion

Someone says that a good estimate is that I should say $\hat{\theta} = \frac{2}{3}$, because I had two heads out of 3 tosses.

**Question to class**: How many people think that two-thirds is the best estimate I could do for the probability of heads on the next flip?

How many people think it's not?

Oh, most people think it's not! Why is this not a good probability estimate? What's a better one?

**A half?** There's another probability. I could say that $\hat{\theta}$ (if you want to be precise - I don't know what $\theta$ is, it's just a coin) could be a half.

Better estimate? Best possible estimate?

No? Other possibilities?

### The Trade-off

Somehow, we need to do things in some sense. This will prove to be the maximum likelihood estimate when you do all the math - it'll come through, you'll be able to prove that, lo and behold, under a simple model, the best possible estimate that maximizes the likelihood - that is, the $\theta$ that makes it most likely to see the data you've seen in the training set - is this.

But in fact, somehow you might have something where... you can think about that.

**Example**: If I had 99 heads out of 100, now what probability would you do?

0.99? Okay, that's the MLE - you could do 0.99 (90 out of 100). You could do a half. You could do something in between, right?

What we're going to do over and over again - and today's going to be super trivial, just flipping coins - but it's going to turn out when we fit things like deep learning models, fancy models, it's going to be the case there's always a trade-off between something that may look more like a prior.

If I flip no coins - here's a coin, what's the probability of heads? Probably a half. Of course, if I'm teaching a statistics lecture, I can get, actually, a biased coin. Maybe it's not a random coin; I bring one in carefully. One of my colleagues has mastered the trick flip - he can reliably do heads on a real coin. Think of a robot flipping a coin - it gets heads every single time. He cheats when he does.

So we're going to try and prove these, we're going to try and see what's the right way to derive these.

## Maximum Likelihood Estimation (MLE)

### Deriving the Likelihood Function

Let's start by looking at what happens when we wake up the computer. I have charge on my pencil this time.

If we say something like we're going to have two heads and one tail, what I want to do is to write a likelihood.

The likelihood is:
$$P(\theta \mid \text{data})$$

And if the data is, say, heads, heads, tails, then what's the probability?

Well, I can write it either way around. Let's sort of start with: What's the probability of heads, heads, tails, given some $\theta$?

- The probability of seeing a heads is $\theta$
- The probability of seeing two heads is $\theta \times \theta$
- The probability of seeing a tail is $1 - \theta$

So the probability I get looks like:

$$P(\text{HHT} \mid \theta) = \theta^{\text{# heads}} \times (1-\theta)^{\text{# tails}}$$

Make sense?

### Likelihood as a Function of $\theta$

If I have a probability function - and we're going to do this often in this course (I mentioned it, but I'm going to say it again and again) - sometimes we're trying to predict the probability of data, of the $X$ or $Y$. Sometimes we're going to try and think of that in terms of as a function of $\theta$.

We're back like when we were doing optimization last class - we're trying to find the best $\theta$, which means we have to define what "best" means.

But here, we're going to think of this in terms of the $\theta$. So we'll think of this as a **likelihood function**.

Given some data here, for some $\theta$, we have $\theta^2(1-\theta)$. That is the probability under this model of $\theta$ of seeing two heads and a tail.

That makes sense?

### Finding the Maximum Likelihood Estimate

Now, what I'd like to do for a maximum likelihood estimate is say: **What $\theta$ is most likely to have given that likelihood?**

So I'm going to write this more formally as:

$$\mathcal{L}(\theta) = \theta^H (1-\theta)^T$$

where $H$ is the number of heads and $T$ is the number of tails.

What $\theta$ is most likely to generate that observation? How do I find the maximum of a function?

Take the derivative, set it to zero. You want the function to be concave. This is monotonic; it's got a nice solution.

If I take:
$$\frac{d}{d\theta} \mathcal{L}$$

then that's equal to:

$$\frac{d}{d\theta}[\theta^H (1-\theta)^T] = H\theta^{H-1}(1-\theta)^T + \theta^H \cdot T(1-\theta)^{T-1} \cdot (-1)$$

Let's try and factor this out, so I'm going to get:

$$= \left(\frac{H}{\theta} - \frac{T}{1-\theta}\right) \theta^H(1-\theta)^T$$

Um, that's the derivative. I want to set the derivative equal to 0.

Then I want to solve for that equals 0. Fortunately, this stuff all goes away because I'm just multiplying by 0, so I get something that says:

$$(1-\theta) \cdot H = \theta \cdot T$$

And if I derive that:

$$H = \theta \cdot H + \theta \cdot T$$

$$H = \theta(H + T)$$

$$\boxed{\theta = \frac{H}{H+T}}$$

### What We've Accomplished

So what did I do? I did a long, annoying derivation, but I want to remember the important part, which is not the trivial algebra (which you can all do). It's the idea that I said:

1. I wrote down a likelihood function for a given set of observations $X$ (this is a generative model, a full model - we'll call it generative in this course)
2. We have a probability of seeing some data, which is a function of $\theta$
3. We took it and we maximized it

If it were a complicated function, we'd use gradient descent (we'll see that later in the course). For an easy function, we just set it to zero and solve for it. We get the answer - it's awesome, and it's like, "duh," what people said was two-thirds.

So far, so good? Awesome.

## Maximum A Posteriori (MAP) Estimation

### Introducing Priors

Now, though, we want to put in a prior. What we're going to have is some idea of what do we think $\theta$ looks like before we've seen any data.

You think it's probably a half. Or more precisely, you could give some distribution over it. Because "probably a half" is too vague - I could say there's some distribution of possible $\theta$'s.

So I have some prior over $\theta$.

### Bayes' Rule

Then what I'm going to want to do is to convert:

$$P(X \mid \theta)$$

into:

$$P(\theta \mid X)$$

Which will give me what someone called the most important rule of the 19th century - or 20th century: **Bayes' rule**.

Bayes says you have some prior before you see the data. You then multiply it by this likelihood ratio, and that's the likelihood divided by this normalization constant here, and you get a posterior:

$$P(\theta \mid X) = \frac{P(X \mid \theta) P(\theta)}{P(X)}$$

Cool. So we'll do this a lot in machine learning. We're going to have some sort of a prior over our weights, and we'll use that to compute a posterior over it.

So far, so good?

### Understanding the Components

- **$P(X \mid \theta)$**: This is the likelihood of the data, which we maximize - that gave us the maximum likelihood estimate (MLE), the $\theta$ that gives me the biggest likelihood of the $X$.

- **Prior**: But now I'm going to start with a prior. I think coins tend to be even. I might have a direction of the prior and a strength of the prior.

We will talk later in the course about shrinking the estimate toward the prior. So instead of having something that's $\frac{2}{3}$, we're going to shrink (this is a mathematical term, we'll be more precise) - we'll shrink two-thirds toward a half. It'll be closer toward the prior.

Shrinkage could get bigger, but often we'll shrink weights towards zero. In this case, we'll shrink it to half - the uninformed prior.

### MAP Formulation

What we've done is instead of maximizing the likelihood, we're going to maximize the posterior, which will give us the **Maximum A Posteriori estimate** (the **MAP**).

Again, what I want is going to be:

$$\hat{\theta}_{\text{MAP}} = \arg\max_\theta P(\theta \mid X)$$

The biggest one here - if I take the argmax over this, the $\theta$ that makes this the biggest doesn't depend upon the $P(X)$.

That's a normalization constant to make this thing actually be a probability, so it sums to 1 over the things. So when I try and find the maximum, all I care about - instead of equals here, I'm going to say this is proportional to that:

$$P(\theta \mid X) \propto P(X \mid \theta) P(\theta)$$

This normalization constant shows up all over the place. Later we'll talk about partition coefficients, and there's all sorts of cases where you have to have a normalization to make a probability. But it just makes the math ugly. And mostly we just want the biggest value, the argmax, and so we don't care about the normalization constant. We just want to find the $\theta$ that makes this as big as possible.

Cool.

### Implementing the Prior

So now we're going to have $P(X \mid \theta)$, which is exactly the scribbles which you can't read from my writing. And we need to have a prior over $\theta$.

To do that in this world is sort of ugly and requires a binomial, and I'm going to cheat a little bit and say: The easiest way to think of a prior is as if you had seen some data before.

So let's imagine that instead of seeing two heads and one tail, I have seen three heads and two tails. Or, instead of two heads and one tail, I'd seen 52 heads and 51 tails.

That makes sense? I can say it's as if I had seen some stuff even beforehand.

There's a distribution called a **beta distribution**, which captures those (you have to subtract one - it doesn't quite work like that), but it effectively says, "Hey, it's as if I had seen some coin tosses before I did, before I saw them."

### Working with Log Probabilities

If I now go through the analysis - let me find the piece that's attached to today's lecture. There's a lot of reading for today; it's got all this stuff worked out. It's got the MLE, it's got all the math I just worked through, which you can go through and do it. It does the MAP one, which you can go and you can look at it, and it has the exact piece here.

It's worth stopping for a second to look at how they do it.

Instead of doing maximizing the probability of $\theta$ given $X$ (which is the posterior), instead of maximizing those, it's super common in ML to **maximize the log of the probability**.

Now, logs of probabilities start to be really nice, and eventually we'll cover entropy and information theory, and you'll see that log probability is the one mathematically correct way to view probabilities.

But for now, it's just a nice mathematical hack.

### Why Log Probabilities?

If you think about the likelihood, it's the product of a whole bunch of probabilities. If you've got $n$ observations, it's the probability of the first one times the probability of the second times the probability of the third. You multiply all the stuff together.

**Multiplying a million things together that are small is a terrible idea** because it becomes a million times small - exponentially small! So in general, as an engineer, you should say, "Hey, probabilities are bad, work in log probabilities."

Now, the likelihood is, in fact... the **log likelihood** is the log of the product of the probabilities, which is **the sum of the logs**.

Does this make sense?

- If you're an engineer, this avoids underflow errors.
- If you're a mathematician, it turns out the log probabilities (we'll get there in a month) have much nicer mathematical properties than probabilities in many ways.

In some sense, log probability is the right way to think about distances - distances should be in log probability-based, non-probability space.

#### Digression on Distance in Probability Space

Let's digress. The distance from 0.9 to 0.91 - is that the same as the distance from 0 to 0.01?

It is in $L_2$ space. But should it be? Is that the right distance?

What's more important for you: I go from a 90% to a 91% of something happening, or I go from a zero chance to a 1 in 100?

Somehow, 0 to 1 in 100 seems way more important. You got a 90% to a 91% chance of winning the lottery or dying in a car crash - they're either good or bad, but they're pretty much the same. No chance versus 1 in 100? Different! I'm not going to take the 1 in 100 chance of the car crash. I'm sorry, you know, I just don't like cars or dying or whatever.

So in some weird sense, we'll see that 0 to 0.01 is infinitely far away. $\log(0)$ is like minus infinity; $\log(0.01)$ is, okay fine, it's a small negative number.

So that's a long digression, but I want to point out - because we're going to be setting up a bunch of more mathy things - I want the intuition here, which is that mostly people like to work for good reason in log probability space.

**The log is monotonic.** If you minimize or maximize the likelihood - well, maximize likelihood - if you maximize the log likelihood, it's exactly the same. So it says, wonderfully good.

### Bayes' Rule in Log Space

Look what they've done here - they've written something that is a completely obscure version of what? **Bayes' rule!**

$$\log P(\theta \mid X) = \log P(X \mid \theta) + \log P(\theta) - \log P(X)$$

But that's a constant with respect to $\theta$, so I don't care about it because I'm maximizing over $\theta$. And $X$ is not a function of $\theta$.

Cool. Cool? Cool.

### Beta Prior Mathematics

You can do this in terms of betas, which I won't do, but I think you can see the math here if you want - it's all linked. It's all just too much math. Um, cool!

But what it shows is, in fact, that - well, let's sort of think about what the MAP is going to look like.

The MAP, if you work out the beta priors... let's see if they show the results. Show me the data, show me the equation. Oh, not... there's Python code, but it's not so nice to read. I like equations. Um, there we go.

So here's what happens. The probability of $\theta$ given $X$ is proportional to the probability of $X$ given $\theta$ times the probability of $\theta$. That's exactly this equation here. We ignore the normalizer.

And what happens is we get something that looks like:

$$P(\theta \mid X) \propto \theta^X (1-\theta)^{1-X}$$

where $X$ is the heads and $1-X$ is the tails for one observation.

If we have, as if we had seen $\alpha - 1$ heads before and $\beta - 1$ tails before (those are our prior), there are two hyperparameters. The hyperparameters determine the shrinkage - how much you're going to pull it back, and in this case, the direction.

So the hyperparameters $\alpha$ and $\beta$ in this case are sort of - it's as if I'd seen this many heads and this many tails beforehand.

Usually, people do uninformed priors, make $\alpha$ and $\beta$ equal, assume they're equal to 1.

But if you make them really big, you're really sure that this was a 0.5 coin. If you make them 1 (it's $\alpha - 1$, $\beta - 1$), it's in fact exactly as if you'd seen nothing.

### Relationship Between MLE and MAP

The **MLE is a special case of the MAP**. In particular, if you think of:

$$P(\theta \mid X) = P(X \mid \theta) P(\theta)$$

If $P(\theta)$ is basically a constant - if this thing looks sort of like 1, or a constant (because the constant drops down) - then, in fact, the two are the same.

So if your prior is "I haven't seen any heads or any tails," the MLE and MAP are exactly the same.

- If your prior is "I've seen one head and one tail," you're going to shrink it a little toward 0.5.
- If your prior is "I've seen 1,000 heads and a thousand tails," my two heads and one tail have almost no effect. It'll be, you know, 0.5002.

Does this make sense?

Questions?

## Choosing Priors in Practice

People in general, by default, will assume what... I don't know, what would you want for your prior if you were doing... oh, let's think about this in practice.

How should I pick - let's assume heads and tails are equally likely, so let's just pick one parameter. How would you go about picking a good prior?

You could do it - either there's Bayesians and there's frequentists in the world, but it ends up being the same.

How strong... how many heads and tails... how much do you believe it's a half, or how do you find the process?

**Student**: By trying it out multiple times.

Okay, but every - a great sense is "yes, and." So I was thinking about trying it out multiple times. We're going to try something that says, "Hey, we've seen one head and one tail, we've seen two heads and two tails, four heads and four tails, a thousand heads and a thousand tails."

Which one is the best prior?

**Student**: The maximum with the maximum number of tries?

A lot of tries? I mean, it has the most... the strongest prior?

If you make the prior - if you make the prior infinite, no matter how much data you have, you're always going to say $\theta$ is a half. So the limit of infinitely strong prior, infinite shrinkage, is you say, "I don't care what my data is, I'm always..." You got a thousand heads in a row? It's just dumb luck, it's a fair coin.

I don't like that assumption very much. But that's a possible version. Unfortunately, that means there's no machine learning, because the data had no effect.

Yeah, you could do no prior whatsoever. You could say, "I'm going to do a prior that says, hey, I'll just do the MLE."

### Why We Don't Use MLE in Practice

I've got to say, out of the last thousand data science projects I've done, I've pretty much never done the MLE instead of the MAP.

**The MLE, in general, is a bad thing to use.** It's got nice math properties, but in general, it doesn't work.

And in some sense, that's going to be the biggest theme of the next month and a half: **Why does it not work, and how to fix it?**

### The Missing Piece: Training vs. Test Sets

So let's think back to where we are in the course in terms of what's totally missing so far in this lecture that we've talked about in the last week.

The fundamental structure of real machine learning problems? **Weights, training set and test set.**

This is all empirical likelihood. This is all finding the $\theta$ that does the best job of fitting the historical data. The MLE is, in some sense, definitionally, provably, the best you could do for this given model form on the training data.

And the reason I never use it: **I don't actually care about fitting the training data. I want to fit the test data!**

So, in some sense, you could be a Bayesian and say, "I know about the world, I'm pretty sure that things are equal heads and tails, and I'm pretty sure it's a half, so I'll use 100 of each of them." Which is sort of fine.

But in general, what we'll do in this course is say, "Hey, there's a hyperparameter - how strongly do you believe that this thing is an even coin toss?"

And then we're going to:
1. Pick the hyperparameter
2. Find the MAP on the training data
3. Hope that that does a good job of fitting the test data, the validation data, the future data

### Regularization Perspective

And in some sense, what we're saying is - and in some sense, it's nicer if you use the product. If you do it in the log space, think of this as:

$$\log P(\text{data} \mid \theta) + \log P(\theta)$$

The log of the probability of the data (the training data) given $\theta$, plus the log of what will be a **regularization penalty**.

It's saying, "Hey, the empirical loss is not the one I care about. I want something that will do a good job of estimating the actual loss on the future training set."

And that piece I add in here is a penalty - a regularization constant, a smoothing constant, a shrinkage constant. We'll see what all these things mean when we get to regression, which says, "Hey, doing this gives me a better job of identifying what's likely to be the case on the future unseen test data."

I want some expectation over future coin flips, rather than just this actual training set.

Isn't that cool? And again, we'll see this 3-4 times.

### Parameters vs. Hyperparameters

**Question**: So, let's be clear, where's the parameter? That's easy - there's only one parameter model, that's $\theta$.

Where is the hyperparameter?

The hyperparameter is somewhere - there should be a subscript with beta or something. There's some other hyperparameter.

If I take beta as a distribution (it's overloaded in the thing) - there's a **beta distribution** which has two hyperparameters, $\alpha$ and $\beta$ in them. That's super confusing because it's two different uses of the word "beta."

But basically, if you think about the number of heads and the number of tails I've effectively seen, those are the hyperparameters. We decide the $\alpha$ and $\beta$. So the $\alpha$ and $\beta$ - this prior here depends upon $\alpha$ and $\beta$.

**Question**: Aren't we assuming that we know the future? How are we going to pick in the real world - how are we going to pick the best hyperparameters, $\alpha$ and $\beta$?

Shouldn't that be parameters the machine can learn?

First of all, I really, really want you all to be clear on the **parameter versus a hyperparameter**.

We can learn both of them, but they're learned in different ways:

1. **Parameters**: We're learning the parameter either by solving a closed-form solution or by doing gradient descent.

2. **Hyperparameters**: Then we will pick the best hyperparameters (which we don't usually call learning, but if you wish, you could call that "learn the best hyperparameters") by what? By solving the problem on the training set with hyperparameters, and then checking on a test set to see how well it works.

And then I could do a gradient descent - you went two heads and two tails, you now go four heads and four tails, find the number (which here is closed form), and then check it on your test set and see if it got better.

### The Key Point About MLE

So the key point here is that **MLE is mathematically beautiful, it's optimal** - and that's a funny word you should always ask. Don't trust mathematicians. "Optimal in what sense?"

But optimal in the sense that it's the $\theta$ most likely to have produced, given the model form we used, the actual data that we saw in the training set.

And the problem with mathematicians is they don't say "in the training set," they say "in $X$," which is correct but slightly misleading, only in the sense that I actually don't care about the training set - I care about the test set.

## Understanding the Notation

**Question**: What does that $X$ stand for?

What is the $X$ here?

So $X$ here is 1 if it's heads for a single observation; $X$ is 1 if it's heads, zero if it's tails.

People - I like to... a formal statistician would say you have, you know, $X$ is either heads or tails. And if you have some sequence $X$, it's some sequence: heads, heads, tails.

Or, if you will, there's a probability that $X$ equals heads.

Computer scientists often like not calling things heads or tails; they just use 1 or 0. Everything's a 1 or 0. I don't love it because it's not very type-safe, but it's perfectly fine to say that the answers are always 1 or 0. $X$ is either 1 or 0.

But you can see here that the probability that $X$ is 1 is $\theta$; the probability that $X$ is 0 is $1 - \theta$.

### The Clever Encoding Trick

Power it with $X$, so what are the... So it's $\theta^X$. Ah! Well, again, what is $\theta$?

Think of:
$$\theta^X (1-\theta)^{1-X}$$

If $X$ is 1:
- This piece here, if $X$ is 1, this is $\theta^1 (1-\theta)^0 = \theta$

If $X$ is 0:
- This is $\theta^0 (1-\theta)^1 = 1-\theta$

So note this clever way that you can put a Boolean logic... and we'll do this a bit later with neural nets. We want to do something which has... everything's a formula in machine learning. We don't get any logic.

But there's a logic statement there encoded in that. That's an IF statement:
- IF $X$ equals 1, it's $\theta$
- IF $X$ is 0, it's $1 - \theta$

### Why Formulas Over Logic?

**Why do machine learning people prefer the equation over the logic?**

So we can optimize it, because we're going to want to take the derivative.

All of our optimization pretty much involves either taking the derivative and setting it to zero, or you take the derivative and do gradient descent. We need derivatives.

And one of the things that's stunningly amazing about modern large language models is they can capture all sorts of things that are discrete - like language, sequences of words - and you can optimize them by putting them into an equation like that (but a little bit messier, with a billion parameters), and you can optimize it with gradient descent.

Who'd have thought? And no one realized this and really quite believed it 30 years ago. Who'd have thought that you can use gradient descent to solve a logic problem?

And if you will, it's a **relaxation of a logic problem**, which is to say, in fact, $X$ is either 1 or 0. Either the word is there or not there.

But I can nonetheless take gradients with respect to this $X$ or this $\theta$, even though I'm thinking behind the scenes that it really is either a head or a tail.

### Discrete World, Continuous Models

So note this sort of funny piece which is very common. The real world often is discrete. Either you get a head or you get a tail - you don't get two-thirds of a head.

But nonetheless, most of our machine learning models are parametric models that will give as output a probability of heads, which will then have to threshold to make a decision and call it a head or not head.

So the functions we're optimizing all are, we hope, differentiable. They all have gradients.

Some will have closed form for today and Monday; then pretty much they'll all just be gradients. But they all have this notion that says that you can compute a gradient in them, and that makes life really nice.

### Output vs. Decision

And it's worth distinguishing between:
- The **output of the model** (the probability of a heads, or the probability of a distribution over $\theta$s)
- Something I'm actually going to make a **decision rule** on (is that heads or not? What $\theta$ will I use when I actually go to the future to make my predictions?)

So it's worth thinking again as a Bayesian - one thinks there's a distribution over $\theta$s, and you're uncertain about it. You've seen some $X$'s. There's some distribution over $\theta$s - your belief in how likely any given $\theta$ is.

But at the end of the day, in machine learning, we're not going to do a distribution; we're going to pick one of them, and that's the one we're going to use.

And again, think of something that's a billion-dimensional parameter. It's too expensive to sample a billion dimensions. We're going to do gradient descent, find the best one, and the best one will mostly be a MAP one because we'll use some prior, and the prior will shrink stuff.

For regression and neural nets, the shrinkage is mostly towards zero. So we're going to try and make the weights be smaller than they should be with an MLE to avoid the overfitting, to avoid... we'll talk about that more, but to do a better job on future ones. In this case, we're shrinking towards one-half.

You can shrink toward whatever you want, but you want to think about what does it make sense to shrink it towards.

**Question**: Yeah?

**MAP is Maximum A Posteriori.** Let's see if it's somewhere here on the slide.

So, they didn't write it out. There's... oh, they just wrote MAP - that's not useful at all. Um, somewhere in this whole document they should say what they stand for. They're not going to tell me. I'm going to have to write it out in my bad handwriting. Um, that's super annoying.

Okay, I will write it out if I have decent chalk.

So, MAP is:

**Maximum A Posteriori**

I don't care if you know what it stands for. What I want to know - what I want you to do is... the important thing is:

- **MLE** is the likelihood on the data without a prior
- The **a posteriori** means posterior, after you've seen the data

So, sorry?

Future examples - but technically, that's not what it does, so we'll use it in that fashion, but we haven't gotten there. At the moment, all we're saying is: in MAP, you have a prior over the $\theta$.

So it's "a posteriori" in the sense you're maximizing the posterior rather than the likelihood. So that in the jargon is super important. There's a:
- **Likelihood** - this is a likelihood function
- **Posterior distribution** over $\theta$
- **Prior**

Those jargons are super important, the Bayesian ones. And then the MLE and MAP.

Thank you. Yeah?

### Beta Distribution

**Question about the definition of the beta distribution.**

You don't really care. Um, the answer is: magic, don't worry about it, it's ugly, who cares?

I think the important part I want - and we'll work mostly with things that are simple like Gaussians - what I want you to take away from it is:

**The beta distribution is as if you had seen a certain number of heads** ($\alpha - 1$) **and a certain number of tails** ($\beta - 1$). So it's a distribution that looks like it, and it works out it's the **conjugate prior** to the Bernoulli. This is a Bernoulli, which makes the math work out beautifully.

So again, I don't want to get too much into - statisticians love mucking with distributions. Mostly what I want to be doing is focusing on: Hey, often we're going to have some sort of a prior, and the beta happens to be the prior that makes everything work out beautifully in the math.

Yeah?

### How Do We Know the MLE?

**Question**: How do we know the MLE?

The MLE does not require the prior.

I derived the MLE - let's see if it still shows up on my piece here. So let's go back and look at my... no, it's gone to sleep. Wake up! Wake up, wake up.

How do I know the MLE? I wrote down the likelihood. That's the likelihood. Oh, let's get a color sheet.

I wrote down the likelihood. I took the derivative with respect to $\theta$, I set it equal to zero, I solved.

**In real problems, you're always going to have to assume some functional form for the likelihood.** This is a trivial one. In other cases, this could be whatever functional form you want. We'll cover 5 or 10 different functional forms for the likelihood.

If you're fancy in the modern era, you say, "Just make it a neural net," but which, you know, a CNN neural net or attention neural net. So you're going to have to write down some functional form of $\theta$ - and again, $\theta$ in general is a big vector.

But you're going to write down - this is why you're going to get paid the big bucks - you're the engineer. You pick an appropriate model with the right inductive bias, a likelihood function that's complicated enough and not too complicated, that fits with the nature of your world. It'll be different for images than it will be for sounds.

And given that likelihood function, then what you will do is mostly use PyTorch to compute the gradient, do gradient descent, driving it down - not to zero, but as small as you get until you get to some local optimum (because, hey, you'll be non-convex).

But that's going to be the process over and over again. Oh, except you're not going to find the MLE for most problems, because for a lot of problems, you'll - particularly the first half of the course - you're going to say, "Wait, my model is too complicated, my data's too simple, I need to have a prior."

I've only seen two heads and one tail. That's not enough to have a reliable estimate. I'd better do some sort of a shrinkage.

Good? Thank you.

## Gaussians: MLE and MAP

Good, um, cool. Let's do the same thing over Gaussians.

### Univariate Gaussian

So let's start with a simple univariate Gaussian. I'm going to have some probability of $X$ - well, and some $\theta$. We can do probability of $X$ given $\theta$, or $\theta$ and $X$, whichever I want to do it.

How likely to see some $X$ with a simple Gaussian? Let's just look at what a Gaussian looks like.

Here's a Gaussian. Here's $X$, here's $P$. There'll be some $\mu$, which is the center. There'll be some $\sigma$, which is the standard deviation, which is how wide it is.

And that's equal to:

$$P(X \mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(X-\mu)^2}{2\sigma^2}}$$

Oh, I'm off the thing there. Whoop! Ha! Thank you.

And somewhere - oh, right! I can even show you how to cheat. If you actually just go over and go to GPT and say, "Hey, show me the derivation of estimating MLE and MAP of a Gaussian," it thinks for 38 seconds, which is surprisingly slow.

And then it says, "Hey, assume you have data IID from a normal mean $\sigma^2$. Let $\bar{X}$ be the average of $X$. Let $S$ be this other thing there," but we don't care about that yet.

We're going to first write the log likelihood. Oh, but it sort of goes all too fast and doesn't actually say where all those things came from. So this is like entirely skipped the point I wanted to show you.

### Log Likelihood for Gaussian

So let's go back and look at this. What I have here is a probability of $X$ for a given $\theta$. Make sense?

Probability of $X$... oh, no, we've lost it. You've lost it; you can't see it.

What we have here is:

$$P(X) = \text{constant} \cdot e^{-\frac{(X-\mu)^2}{2\sigma^2}}$$

The probability of $X$ is equal to a constant (which depends upon $\sigma$) times $e$ to the minus $(X - \mu)^2$ over $2\sigma^2$.

So far, so good? That's a probability, or if you will, it's a likelihood. We think of it as a function of $\theta$.

Make sense?

Now to make the math easier, we're going to, in general, compute the **log of the probability**, the log of $P$ - or if you will, the log of the likelihood of $\theta$.

Because $P$ is - you can think of it as a function which gives you $X$ given $\theta$, or you can think of this function gives you $\theta$ given $X$. That property is a function, mathematically, of both $X$ and $\theta$. Neither one of them is more real than the other one.

Make sense?

And we tend to write them mostly as $P(X \mid \theta)$, but I can write it as $P(\theta \mid X)$.

So what's the log of that?

$$\log P(X \mid \mu, \sigma) = \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \log\left(e^{-\frac{(X-\mu)^2}{2\sigma^2}}\right)$$

That's the log of $\frac{1}{\sqrt{2\pi\sigma}}$ plus the log of the exponent, which is wonderful - we love exponential family things.

The log of the exponent cancels out, so it's not plus, it's:

$$= -\log(\sqrt{2\pi\sigma^2}) - \frac{(X-\mu)^2}{2\sigma^2}$$

We good? So that is the **log likelihood**, which is what we're going to maximize. We could have maximized the likelihood, but it's the same maximum.

And this turns out to be wonderfully a **convex function**! So it's going to have a nice solution. In fact, it's a simple function - a quadratic function.

### Finding the MLE for Mean

So far, so good. Now, how do you want to maximize this function?

Take... if we're going to find the... what's the $\theta$... oh, I switched notation. I was using $\theta$ here, and now I've got $\mu$ and $\sigma$.

Oops. So in some sense, $\theta$ is $\mu$ and $\sigma$. $\theta$ is the vector of parameters; $\mu$ and $\sigma$ are the actual parameters - the center and the width of the Gaussian.

Good?

And now, let's to make life really easy, assume we know $\sigma$, which makes the math super easy, and just find the best $\mu$ by "best" - I mean the MLE estimate.

And the MLE estimate is going to be:

Take:
$$\frac{d}{d\mu} \log \mathcal{L}$$

of all of this stuff.

But what's $\frac{d}{d\mu}$ of $\frac{1}{\sqrt{2\pi\sigma}}$? It doesn't depend upon $\mu$.

$\frac{d}{d\mu}$ of this part is:

$$-\frac{2(X-\mu)(-1)}{2\sigma^2}$$

Well, over $\sigma^2$, but I don't really care. And there's another minus sign in front of that.

And I'm going to set that equal to zero. Solve for it - it says $X = \mu$!

### Single Observation

So what? If I have only one observation, I did a single $X$. Let's do it again in a second with $n$ of them.

If I do a single $X$, what's the best estimate of the center of the Gaussian? **It's the $X$.**

Oops.

### Multiple Observations

Okay, now let's do something more interesting. Let's do a likelihood where we have something that has not just one of these, but has 10 of them. That's going to be:

$$\sum_{i=1}^{n}$$

If you go back up here, the probability of a whole bunch of $X$'s will be the product:

$$P(X_1, X_2, \ldots, X_n \mid \mu, \sigma) = \prod_{i=1}^{n} P(X_i \mid \mu, \sigma)$$

Make sense? You're going to see $n$ different observations, you have the probability of each one. They're IID - everything is IID in this course, pretty much. We assume they're IID.

We generate a bunch of $X$'s from the same Gaussian. The probability of them is the product over all of them of each of them individually.

When I take the logs - oh, it's so lovely! Instead of multiplying all these probabilities, we sum up all their log probabilities, which is just so beautiful and simple.

### Deriving the Mean Estimator

And what we get out then is that the... let's see where we have:

$$\frac{d}{d\mu} \log \mathcal{L} = \frac{d}{d\mu} \sum_{i=1}^{n} \left[-\log(\sqrt{2\pi\sigma^2}) - \frac{(X_i-\mu)^2}{2\sigma^2}\right]$$

And I'll end up here with - instead of this piece here - and hey, I don't actually care about the 2 or the $\sigma^2$ because they're constants. They don't affect the result. It's just:

$$\sum_{i=1}^{n} (X_i - \mu) = 0$$

The summation $\sum_{i=1}^{n} X_i$ is equal to the summation $\sum_{i=1}^{n} \mu$, which is equal to $n \times \mu$.

Or if you take the $n$ over here, put the $n$ over there:

$$\mu = \frac{1}{n}\sum_{i=1}^{n} X_i$$

### What We Just Proved

What did I just prove? I just proved something that's sort of obvious, just as obvious as for the coins.

**The MLE estimate of the mean of a Gaussian is: average the $X$'s.**

Okay, that was a lot of work for something that was sort of obvious. But the nice thing is that you can do the exact same trick for things that are more complicated.

Let me pause and take questions. I'm talking a lot today. Yeah?

### Understanding the Probability Notation

**Question**: It's $P$ of...

So I think the thing that we're... to realize about these weird notations of the probability of $X$ and $\theta$ - it's the same formula when you think about either:
- I know $\theta$, I know $\mu$ and $\sigma$, tell me the probability distribution of $X$

Or you can think about:
- Hey, I know $X$, tell me the probability distribution of $\mu$ and $\theta$

Which is a weird way of thinking about it. We're used to thinking, "Given some parameters, tell me how likely the data is."

But today, and Monday, and the coming Monday, we're thinking of this the other way around, which is: **I got a bunch of data, and for that data, I will have some distribution of how likely each of the parameters were that generated it.**

Because remember, it's MLE. I want to tell... tell me what $\mu$ - what center of this Gaussian is most likely to have generated the data I gave?

So I'm sort of swapping back and forth between mostly talking about probability of $X$ given $\theta$, or on Monday we'll talk about probability of $Y$ given $X$ and $\theta$ (that'll be linear regression or logistic regression).

But I'm also saying, "Hey, I can take that same equation that gives my $Y$'s given $X$ (in this case just my $X$'s) and say that tells me about the $\theta$s, because what I want - I want the $\theta$ most likely to have given the $X$."

Well, not quite - that's the MLE. Now we'll have to do MAP still, but we're good; we've got plenty of time.

Yeah?

### Vector-Valued Parameters

**Question**: Yes, if $\theta$ is a vector, which it is in almost all the real world - and even here, it's a vector. I cheated because it's really $\mu$ and $\sigma$. So here, $\theta$'s a two-dimensional vector.

If $\theta$ is a vector, then I'm going to take the derivative with respect to $\mu$ and the derivative with respect to $\sigma$. Or if it's a million-dimensional neural net, I'll take the derivative with respect to all million dimensions.

And if I'm lucky, there's a closed-form solution. And if I'm not lucky, I just do gradient descent.

Exactly right.

## Multivariate Gaussians

Cool. Let's do it once more. Um, we're going to be doing... is doing things that are multivariate. Oh, no, no, come back here! Give me a new page.

So let's think about what this looks like if, in fact, instead of a univariate Gaussian with a single $X$, $X$ is a vector.

And now, what do things look like there?

### Multivariate Likelihood

In that case, my likelihood - let me just go ahead and write likelihood of $\theta$. Oh, I got a new color, how fun.

So what does this look like? I'm going to get something that is going to look like some constant that's a function of $\sigma$ (that's the normalization that I want to skip), which is something that looks like:

$$\mathcal{L}(\theta) \propto e^{-\frac{1}{2}(X-\mu)^T \Sigma^{-1} (X-\mu)}$$

So what do we have before? We had something that was $X - \mu$. There we go.

Let's fix this. Whoop! Write this as:

$$(X - \mu)^T \Sigma^{-1} (X - \mu)$$

$\Sigma$ is the **covariance matrix** now - how much does each of the features co-vary with each other?

Now we've got something that's yet a fancier equation, which says that instead of just taking $X - \mu$...

### Special Case: Independent Features

If $\Sigma$ were the identity matrix - if every one of the features is independent - what does this look like if $\Sigma$ is $I$?

We get:
$$e^{-\frac{1}{2}(X-\mu)^T I (X-\mu)}$$

Which is going to look (and there's a factor of 2 there which I'm skipping - oh, minus, there's always a minus sign somewhere):

This is going to look something that looks like:
$$e^{-\frac{1}{2}\|X-\mu\|^2}$$

So the trivial version says, "Hey, if all the features are independent" (which they never are in the real world - if you measure a bunch of things about someone's medical record or about their purchase history or whatever, there's lots of correlations among the features - we'll have to deal with that), but if they were independent, you could just do a standard thing that says:

**How far away - how likely is it that I see this $X$?**

And it depends really on $e$ to the minus: **How far away is this $X$ from the center of the cluster?**

Make sense?

### Correlated Features

If, in fact, they're correlated (which they always are), I need some sort of a weighting. And it turns out the covariance matrix has beautiful properties (which I guess we haven't defined yet) - it's positive semi-definite, which means that its norm is non-negative, which means that the whole thing is still a norm.

So if instead of taking $(X - \mu)$ vectors squared (which is a standard norm), if we weight them all with something which is a nicely behaved matrix (which $\Sigma$ is - positive semi-definite), then it's still a norm, and the whole thing has nice properties, and it still gives you a distance. It's just a distance that says:

**If two things are highly correlated with each other, they don't count so much.**

Now, think about that for a second. If you've got a bunch of features that are all measuring the same thing - if you've got 13 different measures of cost or 13 measures of weight - you don't like the length of the vector to count 13 times as high.

Does that make sense? **Redundant information should be discounted** - shouldn't count as much if it's redundant.

And what the inverse covariance matrix (and we'll come back and look at all these things, and you'll get some more reviews in the recitation) - the covariance matrix says how do the features correlate? The **inverse covariance matrix downweights things that are redundant** and gives you a nicer measure of distance.

And because it's a covariance matrix found by basically an $X^T X$, it's got nice properties of being - we haven't talked about matrix norms, but it's like a norm. It's positive semi-definite; it's always going to be well-behaved.

### The Loss Function

So what do we have? We have something that says that we have a loss function. Whoop! A loss function which is proportional - well, for each observation, so it should be the product:

$$\mathcal{L}(\mu) = \prod_{i=1}^{n} \left[\text{normalization} \cdot e^{-\frac{1}{2}(X_i-\mu)^T \Sigma^{-1} (X_i-\mu)}\right]$$

There's going to be some normalization constant to make it a probability that depends upon $\Sigma$ (which I don't want to do).

And now I can maximize this likelihood by taking $\frac{d}{d\theta}$ (here $\theta$ is $\mu$). I can take $\frac{d}{d\mu}$, but before I take $\frac{d}{d\mu}$, I want to take the log of it.

### Log Likelihood

Instead of having the likelihood, let's have the **log of the likelihood of $\mu$**, which is equal to - instead of the product, the log is the sum:

$$\log \mathcal{L}(\mu) = \sum_{i=1}^{n} \log\left[e^{-\frac{1}{2}(X_i-\mu)^T \Sigma^{-1} (X_i-\mu)}\right]$$

The log of $e$ is... cancels. Minus:

$$= -\sum_{i=1}^{n} \frac{1}{2}(X_i-\mu)^T \Sigma^{-1} (X_i-\mu)$$

Oh, it's just $(X_i - \mu)$ squared (in the multivariate sense).

### Connection to $L_2$ Norm

So the log likelihood is **the $L_2$ norm of the difference between each of your points and the center of the Gaussian.**

So this is something we're going to see again and again, particularly on Monday, in regression, which is that:

**If you have a Gaussian distribution and maximize the likelihood, you get something that gives you an $L_2$-style loss.**

And it works both ways. If you're doing linear regression, we'll see that you're assuming there's a Gaussian noise distribution, and that's where the $L_2$ loss comes from.

**If things are non-Gaussian, you don't get $L_2$**, and you'll choose the wrong loss to use.

So people like $L_2$ because many things are Gaussian and because it works easily, but it's not always the best.

### The Result

So what we have is a log likelihood which just ends up being nice and clean, which is going to be basically a sum of squares.

You can then do - and I'll skip the math, but I will post it for the readings. If you do the math, it basically says: maximize that, take the derivative. It looks just like we did before; it's this quadratic function with a sum in it. It multiplies out, you solve it, you're going to find, not surprisingly:

**The MLE for $\mu$ is the average of the $X$'s** (if $\Sigma$ was identity).

If you've got a $\Sigma$ there, then the whole thing looks a little messier.

Um, cool. We'll look at that again Monday.

## MAP for Gaussians

I guess one more piece. Yeah, let's do one more piece. Here's questions? Okay.

Way too much talking today. I'll try and talk less on Monday. We'll go back and do Poll Everywhere questions and things.

So last new concept here - or the same concept we've done - this is the likelihood, the MLE, right?

**MLE says**: Take $\frac{d}{d\mu}$ of this, set it equal to 0.

We can do a **MAP**. The MAP says I want to have some prior over $\mu$.

### Gaussian Prior

A good prior is one that makes the math easy. And a simple prior says that my probability of $\mu$ - my prior over my parameter - is just proportional to:

$$P(\mu) \propto e^{-\frac{\|\mu\|^2}{2\gamma}}$$

Or if it's a vector, it would be the $L_2$ norm of $\mu$ squared: $e$ to the minus $\mu$ squared over some constant. I'll call it $\gamma$.

So let me try and clean this up just a tiny bit - it's super hard to read.

### What is a Prior?

So what's a prior? It says: **How likely am I to see a given $\mu$?**

Well, it's got to be a probability, so if you integrate over the whole thing, it's got to come to 1, so I have to have a normalization constant which will look like $\frac{1}{\sqrt{2\pi\gamma}}$, but I don't care about that.

$$P(\mu) = \frac{1}{\sqrt{2\pi\gamma}} e^{-\frac{\mu^2}{2\gamma}}$$

The constant is a **hyperparameter** that says: **How strongly do I believe that?**

### Visualizing Prior Strength

Let's sort of look at what things might look like. Let's just do one dimension. So let's have some $X$. I could have some idea of... well, and $\mu$ is going to be the same thing.

If I have some $\mu$, and I think $\mu$ looks like [narrow peak] - I'm really sure where $\mu$ is.

If I think $\mu$ looks like [wide spread] - looks like they should have the same... they should both integrate to 1. So maybe it should look way lower. Whoop! Looks like that.

If I have $\mu$ here, this is a **weak prior, a diffuse prior** - I really don't know where $\mu$ is.

If I have $\mu$ looking like this [narrow peak], whoop! Still a Gaussian, still have maybe been set in the same place - this is a **strong prior**.

### Hyperparameter $\gamma$

And the more clear I am... and no, it's controlled by $\gamma$. We can see $\gamma$ up there.

If $\gamma$ goes to infinity, strong or weak prior?

**Strong prior? Week prior?**

Okay, we've got one vote for strong, one vote for weak.

$\gamma \to \infty$ is a **weak prior**.

Think about what things look like. If $\gamma$ gets really big, what is $\gamma$? $\gamma$ is roughly a measure of how wide this thing is.

If $\gamma$'s big, it's really wide. I have very... or diffuse. I have no idea. $\gamma$ goes to infinity - I got no priors, just smeared out over everything. At that point, I know nothing about the world. If $\gamma$ gets really big, I know nothing; I have no prior. Doesn't matter what $\mu$ is.

**If $\gamma$ goes to zero**, the prior is really strong. I'm really sure the point is that the $\mu$ is going to be right here.

So this is a hyperparameter which we can pick depending on how strongly you think you know the best version.

In most of our problems, we'll just pick $\mu = 0$ because if you're arbitrary, that's an arbitrary number. And we'll say, "Hey, we'll pick the strength of our prior based on checking on a separate test set." We'll get there.

### Posterior Formulation

But the point that I want to make also on this is that if I make the prior be a Gaussian, then if we think about where we were going with this, what do we have here?

We had - here's our central piece here. We said that the **posterior is the likelihood times the prior.**

Nice. If the likelihood is written in terms of exponential (like it's up there), and if the prior is also a Gaussian of the same exponential, if you multiply the two exponentials, you add the stuff in the exponent.

Or if you will, if you have a Gaussian and a Gaussian, you take the log of them - it gives you the log of this plus the log of that. The log of the exponents go away, and you just add the stuff.

So if you have a Gaussian, the **conjugate prior is a Gaussian** - the math is super clean.

And all that happens if you do the Gaussian is that if you have this prior that says:

$$P(\mu) \propto e^{-\frac{\mu^2}{2\gamma}}$$

then all that does is it just adds something in.

### MAP Formula

So if you look at what happens (showing the math here so it's a little cleaner): If you have a prior that says that my $\mu$ (the center of my Gaussian) is Gaussian with some $\mu_0$ (that's a hyperparameter) and some variance $\sigma_0^2$ (that's a hyperparameter), then that will be distributed exactly like this.

So $\mu$ is Gaussian with mean $\mu_0$ and variance $\sigma_0^2$.

Then what happens is that just multiplies beautifully onto your likelihood, and you'll get a posterior that says, "Hey, the MAP is going to be equal to what?"

It's going to be equal to something that looks like the average of the $X_i$ (what we had from MLE), except now weighted by the $\sigma^2$ (the noise).

So we had some other... whoa! Remember, there's $\mu$ here, but I'm also going to have - if I did my MLE, I'll get an MLE that will also give some sort of a:

$$P_{\text{MLE}}(X)$$

This is my probability, my prior of $\mu$. I'm going to take these two things and multiply them, and I will get some sort of a **posterior**, which will be somewhere in between.

I'll get a posterior:
$$P_{\text{MAP}}$$

My prior on $X$ - if I didn't have any prior on $\mu$ - my MLE of $X$, and this now gets shrunk toward the MAP.

And apparently if you look at the $\mu$:
- There's the $\mu_{\text{MLE}}$
- There's the $\mu_0$
- In between will be the $\hat{\mu}_{\text{MAP}}$

### The Weighted Average

And what is that $\mu$ (or $\hat{\mu}$ to be nice) MAP look like?

$$\hat{\mu}_{\text{MAP}} = \frac{\frac{1}{\sigma_0^2}\mu_0 + \frac{n}{\sigma^2}\bar{X}}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}$$

It's a **weighted average** of:
- My prior $\mu_0$
- The MLE $\bar{X}$

How much is the weighting? The weighting depends on two things:
1. **How noisy is the data?** ($\sigma^2$)
2. **How strong is my prior?** (Is the $\mu$ really... I know where it is? Or it's already called $\gamma$ before, or I called it $\sigma_0$. Is it really tight or not?)

### Limiting Cases

And so if you think about it, if you were to set $\mu_0$... well, let's think of what it is.

**If $\sigma_0$ goes to infinity**:
- That was $\gamma$ going to infinity
- If $\sigma_0$ goes to infinity, what does the MAP look like?
- **It's the MLE**

$\sigma_0$ goes to infinity → this term is 0, this term is just $\frac{1}{n} \sum X_i$ (it's just the average).

So if your prior is arbitrarily weak or diffuse, $\sigma_0^2$ goes to infinity, you get the MLE back.

**Okay, let's do the other side. If $\sigma_0$ goes to 0**:

And by the way, note this process - I'm always thinking for these equations: if it goes to 0, if it goes to infinity, what happens? Gives a nice intuition.

If $\sigma_0$ goes to zero, what's the effect of my data?

This becomes negligible if $\sigma_0$ goes to 0, and this is just $\mu_0$.

$\frac{1}{\sigma_0^2}$ over $\frac{1}{\sigma_0^2}$ times $\mu_0$.

If $\sigma_0$ goes to zero, **my prior's infinitely strong**. I've shrunk things all the way back.

### Asymptotic Behavior

Let's see, how are we doing on time? Oh, we're doing great!

Let's think about one more limit. Statisticians like to live in the asymptotic limit where $n$ goes to infinity.

This is a great limit - **Asymptopia**, where $n$ becomes infinite.

I've got to say, I've worked at very big datasets. I worked at Google; I had the whole internet. I've never seen an observation that's anywhere close to size infinity for $n$.

Okay, so this is the limit that is nice to think about in theory, and people say, "I want an asymptotically consistent estimator." We'll get there.

**I don't want an asymptotically consistent estimator** because I'm not living in the asymptote of infinite $n$. I'm living in a finite $n$, which is why I don't use MLE.

But let's nonetheless think about it. If I were a mathematician, if $n$ goes to infinity, what happens to the effect of... what's the effect of the prior?

**How does this term scale with $n$?**

It's linear in $n$. So if this goes bigger and this one's fixed, this term will dominate; the sum dominates.

**How does this term scale on $n$?**

Linearly. If $n$ goes to infinity, so in the limit of $n$ going to infinity, **the prior washes out** - becomes vanishingly small.

Asymptotically (and "asymptotically" in this course usually means $n$ goes to infinity), this **converges to the MLE**.

It's a nice, unbiased estimator in the limit.

### Why I Don't Care About Asymptotics

I don't really care about that. I mostly talk about it a lot because I occasionally get pushed back from statisticians saying, "Why is your thing not performing optimally for $n$ going to infinity?"

And my answer is always: **Why do I care?**

I don't have infinite data. That's a nice limit, but I don't care so much about it. So this is almost a defensive thing.

But it is nice to think about it.

### Practical Considerations

And here's what's going to matter. As you train up your machine learning models, you're going to have to think about, effectively, **how strong is your prior?**

You're going to pick something like a $\sigma_0$, and how big your $\sigma_0$ should be depends upon, well, really two things here:
1. $\sigma$ - how noisy is the data?
2. $n$ - how many observations?

**Great midterm question**: If I give you twice as many observations, and I had an optimal $\sigma_0$ before, how should I change $\sigma_0$ if I get twice as many observations?

**Divide it by $\sqrt{2}$**, right? It's going to scale - in some sense, things scale sort of as the square root of number of observations.

If you look at the noise, things live in variance land; they live in squared land.

So is the case, sort of, that if you want to keep things sort of the same here, balancing out, if you have twice as many observations with $n$, you sort of want this prior here... well, but do I? Or do I want to change it at all?

### Should We Change the Prior?

So let's think again about what we're doing here, which is: If we have a belief about how much we think this prior is - how strongly we believe that $\mu$ is the right parameter - if we see twice as many observations, should we change that prior?

**No!** What's going to happen? The prior's going to be sort of, you know, half (or square root of 2) as important.

So as you get more observations, **the prior should have less effect**.

So I don't think I really want to go messing with my prior here. And it's going to depend on exactly how I phrase the problem, and there's 20 ways to phrase it.

But think about what's going on, because again, I really want the intuition with a super dumb problem here, because we're going to do the same thing with fancy linear regressions and nonlinear regressions.

### The Key Intuition

And the intuition is:

**As you get more observations, the prior should become less important.**

You will need less shrinkage. If you've written things this way, you don't have to do anything! It automatically takes care of things - the way I've written it here.

Other times, people write this - instead of it, you can multiply everything through by $n$ or divide everything by $n$'s. You get $\frac{1}{n}$ there, so you get an average. Then all of a sudden, things look a little bit different.

So the bottom line is: **You need to look at the equations and think what's going on here.**

But in general, again, quick summary:

1. **$n$ gets bigger** → The prior will automatically become less important

2. **The data get noisier** → If you don't know what the data is... if you do have $\sigma$ and you know it, you can build it in. If you don't, then we're going to see it ends up being fudged.

### Ratio of Noise to Prior

And often we'll have a $\sigma$ - if you multiply everything by $\sigma^2$, you'll see there's a $\frac{\sigma^2}{\sigma_0^2}$.

**All that matters here is the ratio** of how noisy the data is to how strong your prior is.

That makes sense? I don't need two separate numbers. They capture the same thing.

So we'll see these like 5 times, so don't worry too much if it's still confusing.

## Final Questions

And let me take one more round of questions. Let me also go back and bring up the... check-in piece here so you can see...

GPT really is super good. If you're confused with things, ask it questions. Sometimes it skips a bunch of steps. For all this stuff, it's quite good about explaining it; it walks you through the math. It shows you the MAP.

If you don't understand something, so you have a tutor here. It's a really nice tutor.

Um, so let me find this... yes! Um, questions? Yeah?

### Separating Noise and Number of Observations

**Question**: No, they're solved separately, so...

So one thing that matters is how many observations. Another thing that matters is how noisy the world is.

And in some sense, when you're picking the strength of the prior, it's going to depend upon how noisy the world is.

If you know everything... but we'll see all this in practice. If you know everything, you just plug in the noise in the world, you plug in your prior based on previous data, you plug in your $n$, you got everything. It's not a problem.

**In practice**, what we'll see in a couple weeks is that we have to **empirically choose these things**. And we like to have as few things - as few hyperparameters to choose as possible - and so we're going to try and think in terms of **ratios**.

Sometimes you get to choose $n$. How much do you want to spend? I almost always choose $n$, almost always. If I get to do the experiment, but it still costs me more to do more. Or sometimes I'm given it - some doctor runs all the experiments and says, "Hey, here's my data."

The noise, I don't usually get to choose. I'm not a roboticist; I can't buy a sensor that's twice as good.

But I could tell you: **If the sensor were twice as good, how many fewer observations would I need?**

### Where We're Going

So where we're moving toward is this notion that says that as you build these models, it'll be way more complicated than this, but the same concepts are going to be completely the same, which is:

**We're always going to worry about**:
1. How many observations
2. How noisy the data is

And we're going to adjust, in some sense, our prior hyperparameter to empirically do the best on some held-out dataset.

That makes sense? Like, that's where we're going.

What we'll see eventually is that in the real world, we're always doing an empirical... we're going to do over and over for the first half of the course:

**We're going to try a bunch of hyperparameters, build a bunch of models, test each of them on a held-out set, and pick the best one.**

That's going to happen over and over again.

The theory will guide us a little bit in saying: if I had twice as many, four times as many observations, how will that reduce my error? So we'll have some theory to guide us.

But sadly, **this field is mostly pretty empirical**, in spite of the nice math behind it.

## Wrap-Up

Cool, I'm going to stop 5 minutes early. I think I'll probably start 5 minutes late also to let people get over here, but I'll post that on Ed.

See you all next class!

---

*End of Lecture*

---

# CIS 5200: Machine Learning - Lecture 5: Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) Estimation

## Introduction and Course Logistics

Welcome, welcome! Hopefully the 5-minute delay works a little bit. Oh, I have a little bit of an echo... hate the echo! Don't want the echo... Ha, okay, got you.

### Course Feedback and Structure

Two things, I guess. One, there's still feedback I'm getting from the rest of the class, so I will, if things seem really like background material, say great, go to office hours, read the background. There's lots of good references posted to make sure you can handle the mathematical concepts and annoying details. There's lots of good readings.

I will try a different mix today of Poll Everywhere and slides and scribbling by hand. People seem to like or dislike different ones. Keep the feedback coming.

### Study Strategies and Jargon

The other piece is, in terms of studying, I think the main thing to note is that there's lots of jargon. There's lots of terminology: what's MLE, what's MAP, what's a log likelihood, what's an MAP estimation?

One thing to be doing is to be looking at what are each of these pieces. The TA said maybe you could make the quizzes all be in class. But if you take the quiz each week:
- A, you have to... it's completion-based, just do it
- You don't have to use GPT to do the answer, just do it
- If you take the quiz each week, you will learn stuff better

The peer review happens on Friday or Saturday. The material from the preceding week will help it sink in rather than trying to study the night before. So lots of this course is designed to force you to keep up. Right? Keep up with the quizzes. You cannot go and do all the quizzes the night before the midterm or the final. The goal is to learn it as you go along.

In each case, think about what's the math behind it. I've tried... we'll start doing linear regression this week. I talked to the TAs about what interview questions they've been getting on job interviews. Hopefully this will be relevant, not so much on the MLE MAP, but the regularization, the ridge regression, the elastic net. The scale invariance stuff we're gonna cover all shows up on job interviews, so hopefully it's relevant beyond the course.

### Thinking About Limits

Finally, think about limits. What happens with infinite observations? What happens if the regularization gets smaller or bigger? I want some intuition behind these things as to how they work and when they apply.

Good. Good? Good.

## Today's Topics: MLE and MAP (Third Time)

Okay, so today we're going to do MLE and MAP a third time. And maybe a fourth time. There's readings that have all the stuff in detail. I'll be mostly doing stuff mixed on screen and on the other piece here.

### Two Approaches to Regression

What I'm going to do over and over is talk about what we have. So remember, we always pick a model form.

For this week, the model form is just:

$$\hat{y} = \mathbf{w}^T \mathbf{x}$$

Some of the weights times the features. This will turn out to be the same pretty much for a million other things. We'll transform the features, we'll do nonlinear versions.

And I'm gonna show you two things:

#### 1. The Standard Approach: Prediction Loss

One is the obvious thing that every course covers, which is the way to fit the best weights is to look at the prediction loss:

$$\|\mathbf{y} - \hat{\mathbf{y}}\|$$

in some norm. We'll do $L_2$ norm today. You can do $L_1$ norm or other norms. We'll do lots of variations—we'll do hinge losses.

Right? So the standard, obvious way says: You build a model, the model gives you a $\hat{y}$, it's a function of some weights. You pick the weights to make the errors small.

#### 2. The Generative Model Approach

The other piece, which you will not have seen mostly, which we'll do over and over again, says that we can write a generative model that says you're gonna give it an $\mathbf{x}$, generate a $y$ with some probability distribution, where the probability may be:

$$p(y) \sim \mathbf{w}^T \mathbf{x} + \epsilon$$

We can have $p(y)$ distributed as $\mathbf{w}^T \mathbf{x}$ plus $\epsilon$.

And we can take this approach: we're gonna generate $y$'s from some model—today, a linear model, plus some noise. Today, Gaussian noise, but Wednesday, different noise. Next month, different models.

### Equivalence of the Two Approaches

So we can have these two pieces, and I'm going to show in gory detail that they're exactly equivalent. That if you have Gaussian noise, you get an $L_2$ loss. If you have different noise, you'll get a different loss.

And we'll show that we can do priors, different sorts of priors, and one of the things you will do as data scientists and machine learning people is look at a real problem in the real world and say, hey, what sort of a prior do I think I have?

- Do I think the weights should mostly be zero? (Sparsity)
- Do I think the weights should mostly be small? (Different kind of shrinkage)
- What do I expect?

And you're gonna have to pick that sort of prior, the best prior for your problem, using something about the world, that will then lead you to a different loss function.

### Empirical Risk vs. True Risk

And what we'll see over and over is that we have, first of all, the empirical risk:

$$\|\mathbf{y} - \mathbf{w}^T \mathbf{X}\|$$

on the training set. But that's not what we want to minimize. We want to minimize the loss on the future data—the out-of-sample data, tomorrow's data—which will require picking a good prior.

Cool!

### Linear Regression: Closed-Form Solution

Oh, and the other thing that's cool about linear regression is rather than using gradient descent, we just do a closed-form solution. Take the derivative, set it to zero, find the optimum, so it works beautifully.

Good! Okay.

## Review: Poll Everywhere Questions

But before we get there, you have to do a few Poll Everywhere questions. These are not graded, but at least think about it, or I can just hand-hold. I'll see if this is a good thing to do.

### Question 1: What Does MLE Estimate?

But review! We looked at MLE, and MLE is estimating what?

Got one vote for B and one for A? Okay. Well, I should hide the results. They'll converge. I love it.

**Answer:** Right? MLE says: what parameters $\theta$ (if you're a statistician), or $\mathbf{w}$ (if you're a machine learning person), are most likely to generate the data?

Good. Good?

### Question 2: What Does MAP Maximize?

MAP maximizes what?

Oh, and C is winning! So it's not B, right? We have a prior now. The way we talked about it was C.

**What's the relation between A and C?**

It's proportional, right? So, is A just as good an answer? Yes, I hear one yes.

Let's think of Bayes' rule, right?

$$p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)}$$

Where:
- $p(\theta | D)$ is the posterior (probability of parameters given data)
- $p(D | \theta)$ is the likelihood
- $p(\theta)$ is the prior
- $p(D)$ is the evidence (marginal likelihood)

Right? Bayes' Rule? And mostly, we don't care in this class about $p(D)$. We just say it's proportional, so you can view the two: either A or C were correct answers.

Cool? I hope.

### Follow-up: Are MLE's A and B Equivalent?

Okay. Yes? And so, in case of MLE, are A and B also correct? Let's see, let's go back to... are A and B equivalent?

**No!** They're not equal. There's a factor that differs.

Right? Let's go back and pull up the image. A and B are not the same; they differ by the prior.

Right? There exists exactly one prior: if $p(\theta)$ is a constant (if any parameter is equally likely), which is not sensible in general, since most of our parameters are going to be real numbers, like weights in linear regression.

We're gonna have some sort of a prior in general that the weights are small. We're going to shrink them to avoid overfitting. And so the MLE and MAP are not the same.

Cool. I'm gonna zip forward because the TA said that I spent too much time answering questions last time.

## Two Interpretations of Regression

Okay. So, again, two interpretations of regression:

1. **Linear model approach:** Minimize the squared error
2. **Probabilistic approach:** Do an MLE

I'm gonna come back and say all of these. We'll do an MLE, we'll do a MAP. I'm gonna do this super quickly.

But I want to just point out that there's going to be these dual pieces:

### Probabilistic Interpretation

One that says, write down a probability of some data—like the $y$ given the $\mathbf{x}$, given some weights—and maximize the likelihood: how likely are the $y$'s given the $\mathbf{x}$'s?

### Error Minimization Interpretation

And one that's gonna say that we're going to minimize some error. First, an $L_2$ error, sum of squared errors.

And what we'll show is that the prior on the weights is gonna give us some sort of a penalty—a regularization.

### General Form

And that we're going to overall, in linear regression and in neural nets, and in most models (except for gradient tree boosting), we're going to minimize an empirical risk on the training error with some loss function (usually $L_1$ or $L_2$, but we'll see other ones), plus some penalty, which we're going to learn a lot about, which we'll call $\lambda$, some weight times something that says, I want the weights to be small, the $\mathbf{w}$'s to be small.

And often that is: the sum of squares of the weights is small. That's what we'll do today. We'll get ridge regression.

So we're going to show that these two things are equivalent.

## Geometric Intuition: Fitting a Line

Cool! So let's start with a simple idea. This is stolen from the readings from today, but let's start and draw a picture.

So in the simplest world, we got a bunch of $x$'s and a bunch of $y$'s, and they look like that. We're gonna try and fit the best line through them.

And the intuition is we would like these points to be close to the line. We got a bunch of points, we have $y$ as a function of $x$. $\mathbf{x}$ in general will be 1,000, 10,000 dimensions, but for today—no, for the next half hour—it's just one-dimensional $x$.

We got a bunch of points, and the question is, what's the best line?

### Two Notions of "Close"

And again, the two ways we'll think about it is: one, we want the points to be close, where one version of close is $L_2$ close. But in general, we could have other sort of loss functions, like an $L_1$ close.

Let's think about it for a second. If it's $L_2$ close and I got a point up here, way far and away, how important is it versus that being $L_1$ close?

**It's squared, right?** $L_2$ says if something's really far away, it's really important!

I occasionally see data point sets that look like this, and there's one point that's, like, 4 orders of magnitude out. How? Darned if I know. Somebody typed stuff in wrong, or there was a glitch and a bit was off, and that one point that's way over off the edge? It's quadratically important. Probably a bad idea.

Right? So often I'd like to have something be $L_1$ close, absolute value off. Or maybe even less, who knows, right?

### Connection to Noise Distribution

And what we're going to show today is that's equivalent to saying, what if I had a true world where there was:

$$y = wx + \epsilon$$

That's this piece here, plus noise. If the noise is Gaussian, there'll be some distribution of points around the $y$'s, right? If $y = wx + \epsilon$, if it's Gaussian, it's really unlikely to have something very far away.

Right? What's a Gaussian look like? It looks sort of like... we get a distribution that looks, well, sort of like that. It's likely to be close, it's unlikely to be far away.

Right? And so, getting something that's way out here—right, this is one standard deviation—having something that's five standard deviations is vanishingly rare.

**If the world is Gaussian, it makes sense to use an $L_2$ norm.**

**If the world is not Gaussian, it doesn't.**

And again, we have lots of worlds. Often in computer science, the world is binary. Either somebody purchased it or they didn't. But it's not Gaussian.

If you look at how much money someone makes, I sort of would like to live in a world where it's Gaussian, well, maybe with no negative values. But if you look at it, there's some very long tails. Right? That'd be Elon Musk. Sorry, Ellison, right? There's a bunch of people way out in the tail.

You can be a statistician and take the log of the income and make it more Gaussian. But computer scientists mostly just say, fine, we'll deal with it. Change the loss function.

### Goal: Derive Optimal Weights

Okay, so what we want to do today is to say, hey, can we derive the optimal $\hat{\mathbf{w}}$?

Right? We don't know the true $\mathbf{w}$, ever, except in math land. Find the best one.

And if you look at the picture, it's sort of not the yellow line, but the blue line. Right? It should be the one that looks nicely so that this distance:

$$|wx - y|$$

is as small as possible. Or the squared version of that.

Good!

## Deriving MLE for Linear Regression

So, let's try and do this. Let me try this way.

Great, so what I want to do, I'm going to assume that:

$$y = \mathbf{w}^T \mathbf{x} + \epsilon$$

And we're gonna build a model that says that $y$ is distributed normally with some mean, which will look like $\mathbf{w}^T \mathbf{x}$, and some variance which is $\sigma^2$.

Right? So the $\epsilon$ is mean zero, variance $\sigma^2$. The $y$ is normal with mean $\mathbf{w}^T \mathbf{x}$.

Right, so $\mathbf{w}^T \mathbf{x}$—think of the scalar case, just $wx$—gives me my sort of close to my $\hat{y}$, which we'll be finding.

And we'll draw $y$'s, right? So for any given... we'll take a bunch of $\mathbf{x}$'s. For each $\mathbf{x}$ we observe, which could in the general case be a vector, we'll multiply it by $\mathbf{w}$, add some Gaussian noise, we'll get out a $y$.

Sound good?

### Maximum Likelihood Estimation

Now, what we'd like is to maximize our likelihood. So the likelihood, which we're going to think of as a function of $\mathbf{w}$, the likelihood is:

$$L(\mathbf{w}) = \prod_{i=1}^{n} p(y_i | \mathbf{w}, \mathbf{x}_i)$$

The product over $i$ goes from 1 to $n$, for the $n$ observations, of the probability of seeing the $i$-th actual $y$, given the weight and given the observation $\mathbf{x}_i$.

Right? The probability of each $y$ is given by this Gaussian, and the probability of all of them is just the product.

And I want:

$$\arg\max_{\mathbf{w}} L(\mathbf{w})$$

I want the $\mathbf{w}$ that maximizes the likelihood.

No? Yes? Yes, I get some yeses.

### Plugging in the Gaussian Distribution

So, now we can just go through a bunch of trivial math. We're going to plug in, first of all, for this probability. If it's Gaussian, what does it look like?

This is proportional to:

$$\prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mathbf{w}^T\mathbf{x}_i)^2}{2\sigma^2}\right)$$

The product of $i$ goes from 1 to $n$ of some normalization constant, which I don't care about because it's not a function of the weights, times $e$ to the minus:

$$(y_i - \mathbf{w}^T\mathbf{x}_i)^2$$

And in the simplest version here, in the scalar version, it'll just be this squared over $2\sigma^2$.

In the general regression case, instead of dividing by $\sigma^2$, we'll have an inverse covariance matrix in the middle of it, which we'll do in a second. But I want to do the cleanest version.

So this is just the probability, dropping the normalization constant.

### Taking the Log-Likelihood

Now, to maximize the likelihood, there's one standard trick: **always take the logs of probabilities**.

So that's maximized for the log of that. If I put a log in front of everything:

$$\log L(\mathbf{w}) = \sum_{i=1}^{n} \log p(y_i | \mathbf{w}, \mathbf{x}_i)$$

The log of the product is the summation, $i$ goes from 1 to $n$. The log of the exponent cancels out! I love it. If you're in the exponential family, it's great. We get:

$$\log L(\mathbf{w}) = \sum_{i=1}^{n} -\frac{(y_i - \mathbf{w}^T\mathbf{x}_i)^2}{2\sigma^2} + \text{const}$$

Minus $(y_i - wx_i)^2$ over $2\sigma^2$.

### Converting to Minimization Problem

And what do I care about? I don't care about the constant, which is minus $2\sigma^2$. What I want to do is: what? Maximize the likelihood?

Make sense? By maximizing, right, this is:

$$\arg\max_{\mathbf{w}} \log L(\mathbf{w})$$

But instead of that, I could throw out the minus sign and take the minimum. So this will be:

$$\arg\min_{\mathbf{w}} \sum_{i=1}^{n} (y_i - \mathbf{w}^T\mathbf{x}_i)^2$$

The argmin over $\mathbf{w}$ of the summation $i$ goes from 1 to $n$ of $(y_i - \mathbf{w}^T\mathbf{x}_i)$ squared!

### Key Result

So, what did we just prove?

**We just proved that if the noise comes from a Gaussian and you want to maximize the likelihood, that's equivalent to minimizing the $L_2$ norm squared!**

Yes? MAP? MAP is not gonna be... at this point, it's all MLE. We're gonna have to re-derive the whole thing again for MAP in a second. So this one is purely MLE.

So what I just proved—this actually is a proof—is that if you maximize the likelihood with Gaussian noise, you are, in fact, minimizing the $L_2$ error, the empirical risk $L_2$. There's been no MAP at all here, right? There was no prior yet. We'll get there. This is linear regression. We'll get to ridge in a second, which has a MAP.

## Q&A: Gaussian Noise Assumption

**Question:** Do we always assume the noise is Gaussian?

**Answer:** No! We don't always assume the noise is Gaussian. What I showed was—good question, though—if the noise is Gaussian, then it is the case that we will get out this particular form.

If instead of having Gaussian noise, I had noise that was... if I had a problem where the probability is:

$$p(\epsilon) = e^{-|y - \mathbf{w}^T\mathbf{x}|}$$

That's a different kind of noise. Instead of being $e$ to the minus squared, it's $e$ to the minus absolute value. Then I'd have gotten an absolute value here. I would have gotten an $L_1$ regression.

**So the type of the noise determines the form of the regression.**

If it's an exponential distribution, the probability is $e$ to the something, then the logs cancel out and it's beautiful. If it's not exponential, life is ugly, and we don't want to touch it.

But there is this one-to-one correspondence. Any noise here gives you some sort of a loss function.

Great question.

**Question:** Where did the $\sigma^2$ go?

**Answer:** I dropped the $\sigma^2$. Where did the $\sigma^2$... right, there was a $\sigma^2$ here, where did it go?

It's a constant. I don't care what it is. I want the argmax of the likelihood, which, with the minus sign, gives me the argument of the loss. $\sigma^2$ is just a constant. I don't know what it is, I don't care what it is, as long as it's the same for all points.

Right? But the point is, as long as the noise is constant, then, in fact, it drops out. If the noise is variable, we're gonna have to keep it inside the loss function.

**Question:** Is noise in general constant in the world?

**Answer:** Does it vary as a function of $\mathbf{x}$? If you're estimating someone's income, do you think you'll have a constant noise in estimating it?

No, if you're making \$10,000 a year, you're gonna have a much more accurate estimate than if you're making \$10 billion a year. It's not constant. Yep.

**Question:** What is the noise?

**Answer:** Well, is there a label here? Let's think about... we're not in a discrete world here, we're in a continuous world.

So, what I'm saying is that here's the model: you give me an $\mathbf{x}$, I multiply it by $\mathbf{w}$, that gives me a number, and then I add some noise to that number to get the $y$.

Right, so that's the process.

Now, if I think about trying to estimate something from that, is that labeling noise? Maybe it's noise because you measure the $y$ with some noise. Maybe it's a thermometer? And the thermometer has some error.

Right? Is the error in a thermometer dependent upon the temperature at all? Probably, who knows?

Right? So labeling, if you want, you can call $y$ a label. But people often like the word label in computer science to mean something that's discrete.

**Question:** Why is $y$ Gaussian?

**Answer:** That's correct. Why is $y$ Gaussian? Yep. How is that Gaussian?

So, if you start with something which is $y = \mathbf{w}^T\mathbf{x}$ plus Gaussian noise with mean zero and variance $\sigma^2$—so if you add noise to a constant, you'll get something that's also Gaussian.

I mean, let's drop the word noise for a second. Let's just talk about a Gaussian. Start with $\mathbf{w}^T\mathbf{x}$. That's a number, different number for every $\mathbf{x}$. And we're going to add to that a Gaussian, mean zero, variance $\sigma^2$. We're gonna get something—if we add a constant to a Gaussian, we'll get a new Gaussian that's shifted by that constant, same variance.

So noise is sort of almost a philosophical term. But the Gaussian is a mathematical property.

**Question:** How do we know if the noise looks Gaussian in the real world?

**Answer:** Yeah. How do we know what the noise looks like in the real world? If I actually fit a linear regression, can I estimate what the noise looks like?

What does the noise look like? We had a name for that particular quantity—for the noise? It's the **residual**. Bonus points, right?

So, empirically, what you can do is fit a line, or whatever functional form you want, take the residual and look at the residual! If the residual was, in fact, Gaussian, then you did a fairly decent job. If the residual is all over the map, you probably used the wrong noise function, and you used the wrong regression that you should have used.

And in general, what you find is lots of things in the world have long tails.

Right? If you look at how much money does somebody spend buying an Amazon product, the distribution of costs are not Gaussian. Right? There's very little negative purchases (those are called returns). And there's some very big purchases, but not many of them. Things usually have some weird distribution.

**Question:** Does $L$ here stand for likelihood?

**Answer:** Yeah. $L$ here stands for likelihood, that's correct.

So, we were going to use $L$ sort of both for a loss and for a likelihood. Note that the likelihood here, this is $L$ for likelihood. It is also a loss function. Although, be careful, because the normal error you want to make small, whereas the likelihood you want to make big?

Right? And people are very sloppy. We'll talk about using gradient descent to make the likelihood big, which should be called ascent, not descent. But computer scientists don't seem to get up and down—they're just the same.

**Question:** If you analyze the error and find it's Gaussian, then $L_2$ is a reasonable loss function?

**Answer:** If it's not Gaussian, we'll have to worry about it later, but the obvious first one to try would be to try and look at an $L_1$ loss. The loss function should change.

And we'll see later, also, we're going to choose two things, so come back to me. We're gonna choose the loss function and the penalty, the prior.

## Different Noise Distributions

**Question:** If you put the absolute value instead of squared, what does it look like?

**Answer:** Yeah. If you look right here... look at the derivation. We had... we said that the $y$ was normal before with some piece there, which gave us a probability of $e$ to the minus squared error over $2\sigma^2$. If we put, instead of squared, we put the absolute value, then the math goes right through.

And this absolute value noise, which is no longer Gaussian, would give us an absolute value loss function.

Well, see, next class, what it looks like. But what we'll see is that we're going to be changing—you know, they all have names, which statisticians love, which I don't care much about.

But I think the piece is that we're gonna... well, think what it should look like. The Gaussian says that it's very rare to be out farther.

As we shrink the $p$ from $L_2$ to $L_1$, it's going to be more plausible for it to be farther out. Right? So it's gonna have a longer tail and be less bunched.

If we go the other direction... I don't know, let me... ah, no, I said that wrong. Yeah, it's gonna be... it's gotta be more plausible, right? It's less penalty for things being weird, so it's flatter. I think... yeah, not longer-tailed, less longer-tailed. Hang on... hold that thought for next class.

## Finding the Optimal Weights

Okay, I'm gonna take that as success here. And what we said was that basically, if you do the MLE with Gaussian noise, you get an error which is the $L_2$ loss squared, or the $L_2$, right? Same difference to maximize them.

Okay, let's move on. And note that once you have this, how am I going to minimize it?

So let's get a clean piece here. So what do we want? We want:

$$\min_{\mathbf{w}} \sum_{i=1}^{n} (y_i - w x_i)^2$$

The minimum of the summation, $i$ goes from 1 to $n$, of $(y_i - wx_i)$ squared.

So this is it. This is a convex, quadratic function. Life is easy. To minimize this, we just take:

$$\frac{d}{dw}$$

And then we're gonna set that equal to zero.

If we take $\frac{d}{dw}$, we're going to get:

$$\frac{d}{dw} \sum_{i=1}^{n} (y_i - wx_i)^2 = \sum_{i=1}^{n} 2(y_i - wx_i)(-x_i)$$

2 times the derivative of $y - wx$, which is what? Minus $x_i$ times $(y_i - wx_i)$.

Right? So this is the derivative. You take the derivative of the function with respect to $w$. That's gonna give me 2 times the minus... yes, $\frac{d}{dw}$ is $x$. I think I got that right.

And now, I'm gonna have this is equal to 0.

And what did I do wrong? Oh, I lost the summation somewhere. Summation $i$ equals 1 to $n$. That's good.

I can throw away the 2. Right? 2 doesn't matter. Cross out the 2, because it's a constant, I set it equal to 0.

I can cross out the minus sign, for that matter. But what I'm gonna get, I'm gonna get:

$$\sum_{i=1}^{n} x_i y_i = \sum_{i=1}^{n} w x_i^2$$

Summation of $x_i y_i$ (that's this term) is equal to summation $w x_i$ squared.

And we should have a minus sign, probably, here. Let's see. Nope, that's right.

So, I get:

$$w = \frac{\sum_{i=1}^{n} x_i y_i}{\sum_{i=1}^{n} x_i^2}$$

$w$ equals summation of $x_i y_i$ over summation of $x_i$ squared. There we go.

That's really hard to read. Let's see if we have it written out here somewhere where we can read it. We just go through the math, and there it is! Yes.

So, if you're a nice statistician, instead of calling it $w$, call it $\hat{w}$.

### What Did We Do?

But think about, what did I do? What I did was I wrote down the loss function, I took the derivative of it with respect to the weights, I then solved for it, and I got the minimum, which is just:

$$w = \frac{\mathbf{x}^T \mathbf{y}}{\mathbf{x}^T \mathbf{x}}$$

The weights equal $\mathbf{x}^T \mathbf{y}$ over $\mathbf{x}^T \mathbf{x}$.

## The Bias Term

**Question:** Yeah. Where's the bias term?

**Answer:** So, double E's—some computer scientists like to use the word bias term. And so, some people like to write, instead of $\mathbf{x}$ for a vector, we will write eventually:

$$\mathbf{x} = [1, x_1, x_2, \ldots, x_p]^T$$

A vector $\mathbf{x}$ in general will be a $p$-dimensional vector. Right? $j$ goes from 1 to $p$. So in general, $\mathbf{x}$ will be a vector, $\mathbf{x} \in \mathbb{R}^p$.

And what we will do throughout this course, because everyone does it in all of machine learning, is we'll say, hey, we'll add in one more feature, one more $x$, which is the constant, or, if you will, the bias term. In the good old days of electrical circuits, there was a constantly flowing circuit that was the bias. In deep learning, it's called the bias term. In regression, it's often called a constant.

So, I'm gonna be incredibly... I'm not gonna bother writing the ones, because every time I have a vector of size $p$, I will make an... I usually make it an $x_0$.

Guys, hey, I'm zero-based, so I've got real $x$ going from 1 to $p$. I'm gonna pop on a 0th $x$, and just make it a constant: 1.

### Vector Notation

So, when we do this in vector notation, $\mathbf{x}$—I did a trivial version, right? The scalar version. We do it in a vector version. We're gonna get something which is: the $\mathbf{x}$ will be a whole vector like this.

And now what we'll have here, instead of the summation, this is $i$ goes from 1 to $n$. Instead of $xy$, we're gonna have something which is... what will this look like? This will look like something that looks like:

$$\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

$(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$. So, this looks like $\mathbf{X}^T \mathbf{y}$. Right? The $\mathbf{w}$? This looks like sort of inverse $\mathbf{X}$ squared.

Right? We divided by $\mathbf{X}$ squared, and dividing in a matrix world, which has all worked out beautifully in the linear algebra, looks like $(\mathbf{X}^T \mathbf{X})^{-1}$.

So the top part, this part looks like $\mathbf{X}^T \mathbf{y}$. And so what we'll find is that in the end:

$$\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

$\mathbf{w}$ as a vector, in the matrix world, is $(\mathbf{X}^T \mathbf{X})^{-1}$ times $\mathbf{X}^T \mathbf{y}$.

### When Is This Invertible?

This only works if $(\mathbf{X}^T \mathbf{X})^{-1}$ exists. And when we do matrices, we'll remember that if $\mathbf{X}$ is... well, we want $\mathbf{X}^T \mathbf{X}$ to be non-singular, want to be invertible. We'll worry later about how to do efficient ways to compute this. We use singular value decomposition.

We're gonna throw stuff in a second that makes this invertible, but let's think for a second about that being invertible, since we're here!

**What's the dimension of $\mathbf{X}$?**

$\mathbf{X}$ is $n \times p$ for the big $\mathbf{X}$, right? Let's... where's my room to do this? Let's put it right here.

$\mathbf{X}$ is $n \times p$. It's got $n$ observations, and each row is $p$ features (or $p+1$ if you want to put the constant term in; we'll just call it $p$).

So the observation matrix, as the statisticians call it, or the features, as I call it, or the predictors, is a set of $n \times p$, right? $n$ rows, each with $p$ features in it.

And if we take $\mathbf{X}^T \mathbf{X}$, what dimension is that?

That should be $p \times p$.

### When Is It Singular?

If $p$ is bigger than $n$—if you have more predictors than observations—you're out of luck. It's singular, it's not invertible. We'll have to do MAP.

If $n$ is bigger than $p$, usually, it's invertible, but no guarantees. In the real world, often you will have multiple predictors that measure the same thing or are highly collinear.

You'll have how much it costs, and you'll have the tax on it, which in some jurisdictions is always 3% of the cost. In that case, $\mathbf{X}^T \mathbf{X}$ is singular.

So in the real world, we can't generally compute $(\mathbf{X}^T \mathbf{X})^{-1}$. And we'll have to do some regularization, some way to make it stable.

But if you're in math land, you can say, well, if that's invertible, it's fine, and if not, it'll be a pseudo-inverse. And in a month or so, we'll compute pseudo-inverses and do all the math.

## Summary of MLE

Good! Okay, so where are we? We walked through MLE.

What is this thing wanting me to... I don't want to sign into Quizlet. Go away.

So what did we say? We started out, we said we can write the likelihood. If it's Gaussian, we derived that the loss function is $L_2$ loss, so it's an $L_2$ regression, or ordinary least squares regression.

And we showed that you could then solve that by taking $\frac{d}{dx}$, set it to 0, solve it. And I said, without proof, that if it's, in fact, instead of a scalar $w$ and $x$, if it's a vector $\mathbf{w}$ and vector $\mathbf{x}$, then you can solve it, you'll get:

$$\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

Good. Yes?

**Question:** You solved it for the likelihood, and that gave you exactly the $L_2$ norm?

**Answer:** No, I solved it for the likelihood, and that gave you exactly the $L_2$ norm, not close to. It's a mathematical identity.

And then for the bias term, if you're doing this with a vector, $\mathbf{x}$ will always add the bias or the constant as just one more term in the vector.

We didn't show it before at all, because I did everything before for the trivial case. I did a bait-and-switch. I did this with a single $x$ with no constant term, rather than a vector.

And what's gonna happen, we'll do it in a second if there's time, is that when you have a vector $\mathbf{w}$ and a vector $\mathbf{x}$, then once you have a vector $\mathbf{x}$, you can throw the constant term in.

## MAP Estimation and Ridge Regression

Cool. Second piece. Let's do MAP. So I said MLE.

MLE, we just maximized the probability of $y$ given $\mathbf{x}$ and $\mathbf{w}$? That was what we did before, that was our likelihood.

Now we'll do the same thing with MAP, where we have a prior on the weights.

### Why Do We Need a Prior?

And we're gonna need the prior on the weights, because in many, many problems in the real world with $\mathbf{X}$, there'll be several features that are linearly dependent, that are highly correlated, and so $(\mathbf{X}^T \mathbf{X})^{-1}$ won't exist. There will be no unique MLE.

So the MLE in general doesn't exist, which is super annoying. Right? And it doesn't exist in the precise sense that the solution we did doesn't have a unique solution if there are two columns of $\mathbf{X}$ (or any set of columns of $\mathbf{X}$) that are linearly dependent. Or if you have more features than you have observations.

**Is that possible?** Can I have more features than observations?

Sure. Not so much if you're working at Facebook or Google, but if you're working in the hospital, it's common. I have, for most patients, 10,000 different features. I feel like a typical medical record: your blood pressure, which I may or may not know, lots of diagnoses, the drugs you're on or not on—10,000 is a totally normal number of features for someone in a hospital.

And for lots of the cancers, fewer than 10,000 people died of it. Right? It's got to be a pretty common cancer to kill 10,000 people at HUP, our hospital here.

So it's common in many parts of the world—this first half of the course—to have $n$ smaller than $p$ (or $p$ bigger than $n$). So we're gonna really have to regularize or use MAP.

Okay? So now we need a prior on $\mathbf{w}$.

### Gaussian Prior on Weights

Cool. So, now we just add a prior. Before we had $y$ was normal with mean $\mathbf{w}^T \mathbf{x}$ and variance $\sigma^2$. Now we're gonna add a prior over the weights.

And the standard prior is we're going to assume the weights are Gaussian, mean zero, variance $\gamma^2$:

$$\mathbf{w} \sim \mathcal{N}(0, \gamma^2 I)$$

Now, that's one possible prior. On Wednesday, we're gonna look at a bunch of other priors.

In particular, the sorts of priors people use a lot are either assuming that the weights are small (which this sort of does, right? They're centered around zero with some width), or you're going to assume that a whole bunch of the weights are zero.

And it's very popular both for explainability and for scaling, and for all sorts of things, to assume that lots of the weights are zero. I could have a prior on the number of non-zero weights. And that's going to turn out to look like an $L_0$ penalty, a number of non-zero weights.

So Wednesday, we'll look at different priors which will be more sparse, often. But this one—Gaussian—how likely is something to be 0 under a Gaussian?

Zero! Right? It's arbitrarily rare to be the particular value. But this prior, this $L_2$ prior—remember, before we had an $L_2$ loss function? Right? There's two different pieces here, and I want people not to confuse them, because it's confusing.

**One is, what's the distribution on $y$? What's the distribution of the noise?**

**The other is, what's the prior over $\mathbf{w}$?**

The first one we saw shows up in the loss function. The second one here will be the regularization term, which will show up in a second.

### Deriving the MAP Solution

Cool. So let's pull back my screen. So yes, it's up there, right?

So, what's my likelihood now? But this is not just my likelihood, this is gonna be my MAP. Right? So what I'm going to have is something that is:

$$p(\mathbf{w} | \mathbf{y}, \mathbf{X}) \propto p(\mathbf{y} | \mathbf{w}, \mathbf{X}) p(\mathbf{w})$$

The product over $i$ goes from 1 to the $n$ observations of my likelihood itself—the probability of $y_i$ given $\mathbf{w}$ and $\mathbf{x}$—times now my prior over $\mathbf{w}$.

Cool. Now, I'm gonna again throw out the normalization constants in the probabilities. I don't care. But the probability of $y$ given $\mathbf{x}$ is in fact... let's just keep this:

$$\prod_{i=1}^{n} p(y_i | \mathbf{w}, \mathbf{x}_i) = \prod_{i=1}^{n} \text{const} \cdot \exp\left(-\frac{(y_i - wx_i)^2}{2\sigma^2}\right)$$

$\prod_{i=1}^{n}$, it's going to be a constant times $e$ to the minus $(y_i - wx_i)^2$ over $2\sigma^2$.

I've written it slightly differently here on the screen, but it's almost the same.

So this part here is the likelihood, the exact same one we saw before. Now we multiply by the prior, which is, we're gonna assume that this is equal to:

$$p(\mathbf{w}) = \text{const} \cdot \exp\left(-\frac{w^2}{2\gamma^2}\right)$$

$e$ to the minus $w^2$ over $2\gamma^2$.

So $\sigma^2$ is the noise in the $y$'s. $\gamma^2$ is the measure of the strength of my prior.

**If $\gamma$ goes to infinity, how strong is my prior?**

It's very weak, right? The whole thing is spread out, I know nothing.

**If $\gamma$ goes to zero, very strong.** The weights have to all be zero.

Right? So that's going to control how shrunk things are.

### Taking the Log of the MAP

So now... the nice thing about exponents, but hey, we'll take the log of the whole thing anyway. So before we do anything, instead of maximizing the MAP, let's maximize the log of it.

If I take the log of the whole thing, log of everything, I get the log of the product becomes the sum, $i$ goes from 1 to $n$:

$$\log p(\mathbf{w} | \mathbf{y}, \mathbf{X}) = \sum_{i=1}^{n} -\frac{(y_i - wx_i)^2}{2\sigma^2} - \frac{w^2}{2\gamma^2} + \text{const}$$

I take the log of the exponent, I get minus $(y_i - wx_i)$ quantity squared over $2\sigma^2$, and this other term here looks like it adds in, so it's minus $w^2$ over $2\gamma^2$!

Cool. So that is my log of the MAP.

### Converting to Loss Function Form

I now want to... oh, let's look at that as a loss function. It's beautiful, it's quadratic.

And let's write it a little bit differently so it's clean. Multiply by the minus sign, so instead of the argmax, I'm gonna want the argmin. Right? So what I want is the argmin over $\mathbf{w}$ of something which looks like:

$$\arg\min_{\mathbf{w}} \sum_{i=1}^{n} (y_i - wx_i)^2 + \frac{\sigma^2}{\gamma^2} w^2$$

$(y_i - wx_i)^2$. I'm gonna multiply through by $\sigma^2$. So it's plus $\frac{\sigma^2}{\gamma^2} w^2$.

So what does this look like? It looks like the $L_2$ norm of $(y_i - wx_i)$—this has all got a summation over $i$, right? Summation $i$ equals 1 to $n$.

In fact, let's change the notation, make it a little bit prettier. Let's try and make this whole thing look like what? It looks like:

$$\|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2 + \frac{\sigma^2}{\gamma^2} \|\mathbf{w}\|_2^2$$

$\|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2$. Right? So I've taken the summation, $i$ goes from 1 to $n$, and I put the whole thing into a big vector notation.

So this is summation $i$ goes from 1 to $n$ of $(y_i - wx_i)$. If these were vectors, this would be $\mathbf{w}$ times $\mathbf{x}$. So this looks like $\|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2$. This is the $L_2$ norm squared, that's the error, the loss function.

Plus something else that looks like $\frac{\sigma^2}{\gamma^2}$ times the $L_2$ norm squared of $\mathbf{w}$:

$$+ \frac{\sigma^2}{\gamma^2} \|\mathbf{w}\|_2^2$$

### Two Components: Loss + Regularization

So we now have two pieces:

1. **We have a loss function from before, the empirical risk, the $L_2$ empirical risk, the same as the MLE.**

2. **Plus a second term, which is a regularization term or a shrinkage term, which says that I want to make $\mathbf{w}$ small.**

And if we have a prior over $\mathbf{w}$ which is Gaussian, then small means small in the $L_2$ sense.

If, instead of having a Gaussian prior, I had this weird $e$ to the minus absolute value of $w$ over $2\sigma^2$, I would say make $\mathbf{w}$ small in an $L_1$ sense, which we'll see on Wednesday makes it a little bit more sparse, puts less weight on the things that are far away.

## Why Shrink Weights to Zero?

**Question:** Yeah. Why do we want to make the weights closer to zero?

**Answer:** That's a good question. Why do we want a prior that says that... why are we shrinking the weights to zero? What happens... let's sort of think about what it looks like to make the weights smaller or zero?

Yep, so smaller bias—as we'll talk on Wednesday about the bias and the variance—it'll actually make it biased, but lower variance.

I mean, let's think about something where I have... let's try and... let's think about fitting. Let's fit a model that says:

$$y = c_0 + c_1 x + c_2 x^2 + c_3 x^3$$

Okay, you can't read that. And I got some data. And if I fit a high-order polynomial, it'll fit somehow.

### Example: Overfitting with Polynomials

And if I'm shrinking the weights, well, what happens? Let's do something really extreme. If I have a model like this, I can fit it beautifully!

Make sense? In fact, here I've got 1, 2, 3, 4 parameters, only 3 data points. That can't possibly work. I can fit infinite number of models, so let's put one more point here, and now I fit the data like this.

Right? Fourth-order polynomial, four points, I fit the data perfectly, awesome! I'm so good.

But I get a future point. It's terrible.

So what I want to do, I want to somehow make this smoother or shrink the weights, make them smaller.

If I were to do something more sparse and zero out some of the weights, maybe my best fit would be a straight line! Now I've set $c_2$ and $c_3$ to 0.

So I've zeroed out some of the weights, and I've got something that's not as good a fit in the training set—the empirical risk is higher—but hopefully better in the future.

Maybe that's not the best model, and I want one in between. I could fit one that says, okay, I'd like things to be just sort of smooth.

That ends up, if the weights are smaller, I'm gonna fit the data less well. So it'll try to be more biased, but there'll be less variance. And we haven't really talked about what this means, and that's next class, so I'm not going to cover bias-variance decomposition until Wednesday.

### General Principle: Don't Overfit

But what I do want is this notion that says that in general, if you've got something with lots of noise, you want to fit it less well.

Right? If I have a bunch of data that looks like that, I could try and fit the data as best I can, and with a linear regression, maybe it looks like that.

But maybe I'd be happier shrinking the weights. If I shrink the constant, it'll come closer to the zero here. If I shrink the weight, it'll have lower slope. So the blue line is shrunk.

Right? I shrunk the constant—the intercept—I shrunk the slope. I made the weight smaller.

And note that I, in general, do not want to fit the data as well as I can, because the data are noisy.

And in general, in real machine learning applications, I'm gonna have lots of features. I get as many features as I can afford, and I'm usually limited not by the cost of the features, but by the amount of data—the sample size, right?

And the features can be images from your x-ray or your CAT scan—they're high-dimensional. I'll project them down using a... well, these days, a neural net embedding. But they're still gonna be high-dimensional.

Make sense? I've got your whole history. I've got a lot about you. I've got lots of features which I could use, but if I'm not careful, I'm gonna memorize the data.

So I'm obsessing—one second, I'm obsessing in this course about not overfitting, about not going perfectly through the data. And in high-dimensional predictor space, it's really easy to fit the data. And in fact, that's mostly a terrible idea. And it turns out that shrinking the weights will make it smoother, less accurate on the training data, more accurate on the testing data.

## The Hyperparameter Lambda

**Question:** Yeah. Here we have only one hyperparameter?

**Answer:** That's exactly correct. We have one hyperparameter, which is $\frac{\sigma}{\gamma}$. And you can see it over on the curve there: $\frac{\sigma}{\gamma}$.

And in fact, in general, we'll be calling this thing $\lambda$:

$$\lambda = \frac{\sigma^2}{\gamma^2}$$

So in the machine learning literature, the common variable name is $\lambda$.

Note that $\lambda$, your shrinkage parameter, depends upon the ratio of two things:

1. **How much noise is in the data?** ($\sigma^2$)
2. **How strong your prior is?** ($\gamma^2$)

**Question:** Lambda equals to infinity says it's all noise, and lambda 0 says...?

**Answer:** Yeah. $\lambda = \infty$ says it's all noise, and $\lambda = 0$ says it's perfectly explainable, at least there's no reason to shrink the parameters—use the MLE.

### How to Choose Lambda

In the end, for most engineering, we're gonna have to fit $\lambda$ by cross-validation, which we'll cover on Wednesday. We'll have to empirically find the best $\lambda$.

But it's still nice to understand $\lambda$, because if you can make the noise smaller by buying a better sensor for your robot, you will change $\lambda$.

So I'm trying to derive here some sort of intuition. And $\lambda$ really is some measure on a relative scale of how noisy the data is, or how confident you are of the problem.

So we'll come to how to fit $\lambda$ empirically in the future. It turns out it's hard to get it from theoretical grounds, because you usually don't know what the prior should be unless you've seen similar problems in the past.

But it is still a way to think about things. So $\lambda$ is gonna be our regularization parameter that tells us how much to shrink the data.

### General Formulation

And now what we have is something that says that instead of minimizing the empirical risk, we'll minimize:

$$\text{Empirical Risk} + \lambda \cdot \text{(Size of Weights)}$$

The empirical risk plus some constant times the size of the weights.

And if you have a Gaussian prior, the weight size will be $L_2$ norm squared. And if you have a different prior, you'll get a different regularization on the weights.

## Prior vs. Noise Distribution

**Question:** Yeah. Do we decide the distribution of the prior on the distribution of the noise?

**Answer:** Now, the noise is separate. The noise was the $\sigma^2$. And the prior was the $\gamma^2$. So we got two things that matter.

Right, one is how noisy the data is, and the other is how much you think you should shrink. But in practice, all that matters is the ratio, right? There's no point in picking two numbers—there's only one ratio that shows up at the end of the day.

**Question:** Did we decide the prior was a Gaussian distribution because the noise was Gaussian?

**Answer:** Yes? No? Does the distribution of the noise in the data have to be the same distribution as the prior over the weights?

**No!** The prior over the weights is entirely sold separately from the noise.

Right? The prior is going to be: Do you think that all the weights should be shrunk a bit with the biggest one shrunk the most?

Does the prior mean—we'll talk later on Wednesday—that maybe your prior is that 90% of the weights should be zero? That's a nice prior over the world. Most weights in this model should be zero.

The doctors are going to give me 100 features, but they want a model with 10 at the end of the day. There's only 10 questions they'll ask. And they want to ask the 10 best, most informative questions. Every question costs time or money, which are somewhat exchangeable. So often, we'd like to have a sparse model.

**Sparsity doesn't come from noise in the data. Sparsity comes from a prior over the weights.**

Right? So they're two very, very different distributions, and that's important, because we're going to see much more complicated versions later in this class.

### Choosing Loss Function vs. Prior

But again, on the left is the empirical risk loss, which is one form. And on the right is your prior over the weights, the regularization. **You get to pick both of them. Separately.**

**How do you decide on a loss function?** How do I decide that $(y - wx)$ should be a quadratic loss?

Maybe because your cost is quadratic (which it rarely is—I've never seen a quadratic loss in the world). Or maybe because you think the noise is Gaussian (which noise often is Gaussian—lots of sensors have roughly Gaussian noise).

Right? There's a beautiful mathematical result called the central limit theorem. This says if you have enough different noises all smashed together, the noise is sort of Gaussian.

That said, lots of things are not Gaussian. If the answer is either 1 or 0, you don't have Gaussian noise. Right?

## Residuals and Distance

**Question:** When are we wanting the loss to be the projection of $y$ onto $\mathbf{x}$?

**Answer:** So what we're seeing here is sort of two things combined. One is that we have something where we said, this is linear in $\mathbf{x}$, $w$ times $\mathbf{x}$. So we said, hey, the $\hat{y}$ is linear. Later on, I'll put arbitrary complicated functions there.

The other piece we said is that $y - \hat{y}$ is, in fact, Gaussian. Or $y$ is Gaussian. We said that there's a residual here, right? This is a residual, $y - wx$. We said the residual was Gaussian.

So there are two different assumptions:

1. **One is linearity of the model.**
2. **One is that the residual is Gaussian.**

Either of those can be different, and they'll give different loss functions.

The residual is the closest distance, but... now, the residual is not a distance. The distance is $y - \hat{y}$.

### Different Norms for Residuals

We can, in general, combine those. So this is a summation of $(y - wx)$. Let's go write this down. Sorry, important point.

We have something:

$$\text{residual} = y_i - w^T \mathbf{x}_i$$

What we have here is something that says, I want to make the residual—oh, but not that—the residual vector, which is $n$ of these, I want to make this small.

Right? But **small, in general, could be under some $p$-norm.**

Right? So far, on the side over here, the residual is the sum of the squares. This is an $L_2$ quantity squared residual, right?

We could make the $L_2$ norm of the residual small. There's no reason we couldn't make the $L_1$ norm of residual small.

Right? So this could be the $L_1$ norm. We could have the absolute values of the residuals be small rather than the sum of the squares of the residuals be small.

**They're both plausible notions of a small residual.**

And again, if you take a standard undergraduate course, everybody makes the sum of the squares of the residual small. That's fine, but **there's nothing magic about $L_2$ norm, except that if you have a Gaussian for the noise, then it's optimal.**

Right? But it's fine to have instead of the sum of the squares of the residuals, the sum of the absolute values of the residual. Or some other function. We'll see hinge losses, we'll see all sorts of weird losses.

Good? **That's a super important point.**

## Ridge Regression Solution

Okay, we have 10 minutes, which is perfect.

So this thing is working terribly. So, what we just saw was we did the MAP, and we said that it minimizes the $L_2$ error squared plus $\lambda$ times the $L_2$ squared of the weights. And you should know this is called **ridge regression**.

In the statistics land, it's a particular form of many forms of shrinkage, because it makes the weights smaller. And it's sometimes in machine learning called **Tikhonov regression** (a term which I don't love, because I'm not Russian or something). But that's also another name for it, Tikhonov regression.

We will call it ridge regression.

Cool.

### Closed-Form Solution for Ridge

If you go through and do the math and solve it, what you'll find, and again, it's in the readings worked out in detail, is that:

$$w = \frac{\mathbf{x}^T \mathbf{y}}{\mathbf{x}^T \mathbf{x} + \lambda}$$

The weight is $\mathbf{x}^T \mathbf{y}$ over $(\mathbf{x}^T \mathbf{x} + \lambda)$. The $\lambda$ that showed up in the ridge regression shows up in the model.

And in a multivariate regression, life looks similar.

So... let's see what I want to talk about here. I'm gonna do this super fast. But the exact same MLE and MAP I did before, and it's all worked out in the notes, works exactly the same in a vector world.

### Multivariate Case

And in the vector world, what we have now is that $\mathbf{X}$ is going to be an $n \times p$ matrix. And all the math is going to look the same. And you just have to be super careful about keeping track of the pieces.

And at the end of the day, if you do the MAP estimate in the vector world, what you're gonna get is:

$$\mathbf{w} = (\mathbf{X}^T \mathbf{X} + \lambda I)^{-1} \mathbf{X}^T \mathbf{y}$$

$(\mathbf{X}^T \mathbf{X} + \lambda I)^{-1} \mathbf{X}^T \mathbf{y}$.

And this has the wonderful property that **it's now always invertible.**

So, when I run real data, I never run ordinary least squares, because there's always a risk the $\mathbf{X}^T \mathbf{X}$ won't work, and my computer will fail in some bizarre, obscure way, because it gives a non-invertible inverse.

But if you put a tiny amount of ridge—make $\lambda = 10^{-6}$—hello? The resulting matrix is positive semi-definite. All the... and when you put a $\lambda$ in it, it becomes positive definite. **It's always invertible. There's always a solution.**

You need to pick the best $\lambda$ that doesn't overfit. But at least there's always a solution. So it's just a good idea to run ridge rather than ordinary least squares.

Right? I've said this several times: **I always do MAP, not MLE**, because it's super common that there's something in your $\mathbf{x}$ so that $\mathbf{X}^T \mathbf{X}$ is singular.

## Variations on Loss Functions

Cool. We talked about the constant term.

There are a couple of variations on this that I mentioned, but I'm gonna say them again.

### Ordinary Least Squares (OLS) / $L_2$ Regression

The ordinary least squares, which people call OLS, or $L_2$ regression: We have a probability of $y$ given $\mathbf{w}$ and $\mathbf{x}$ that's:

$$p(y | \mathbf{w}, \mathbf{x}) \propto e^{-\|y - \mathbf{x}^T \mathbf{w}\|_2^2}$$

$e$ to the minus $\|y - \mathbf{x}^T \mathbf{w}\|_2^2$ gives an $L_2$ regression.

### $L_1$ Regression

If instead you had something that was:

$$p(y | \mathbf{w}, \mathbf{x}) \propto e^{-\|y - \hat{y}\|_1}$$

$\|y - \hat{y}\|_1$ (the $L_1$ norm), that's a loss function. It's an $L_1$ regression! Equally good.

**Is it convex?** It's $L_1$ convex. I got some no's. I got some yeses. I got some borderline...

**It is still convex.** We'll come back and review that on Wednesday. But it's still the case that when you have absolute value, it's got one little kink at the bottom, so instead of being nice and round equal lines, you get little diamond-y shaped things. But it's still convex. And so it still has a unique optimum. And life's a little messier. But you can still solve it beautifully.

## Scale Invariance

Okay, last topic. Final 7 minutes.

One thing that matters a lot is, is your model scale invariant? If you measure things in meters versus inches, does it change the result?

And for some things like linear regression, if you make the $\mathbf{x}$'s 100 times bigger, the weight will just be 100 times smaller, and it's exactly the same. So, **some methods are scale invariant**—that's nice.

Other ones, like ridge, are **not scale invariant**, because you're shrinking all the weights. You're taking an $L_2$ norm of the weights. And if you change something from meters, and one is meters and kilometers, one is inches and kilometers, you get different scalings, and the scalings change which ones you shrink how much.

So, many of the methods we use are going to be not scale-invariant. If you rescale the variables, change the units from dollars to cents, you'll get different answers out.

**This, by the way, is a great interview question.**

### Example: Scaling Features

**Question:** Yes, question. Just scaling $\mathbf{x}$?

**Answer:** I'm gonna just scale $\mathbf{x}$. So let's look at that. Let's try and make it more concrete.

So here I have a $y$ and a matrix $\mathbf{X}$, and I take the third column of $\mathbf{X}$, and I multiply everything by 2.

Right? I just make everything in that column twice as big. And again, think of going from dollars to yen, or from inches to kilometers—think of something that's a fixed constant. You'd sort of hope that it wouldn't change the answer.

If you go from inches to meters, and sometimes it does, and sometimes it doesn't.

### $k$-Nearest Neighbors

**Is $k$-nearest neighbors scale invariant?**

I got one no. I got two no's. Why not?

A standard distance metric: if you make something bigger, if something goes... this column goes from 5 to 10, all of a sudden, "next to me" is way more important in driving the distance.

The standard distances we use are not scale invariant. If you change from inches to kilometers, things get way closer in some weird sense of closer (or farther away, depending on how you think about it, right?). But it changes.

So, so many, many methods are not scale invariant, and you need to think about how you're scaling the features when you put them in.

### Linear Regression vs. Ridge

Linear regression, beautifully, is scale invariant, right? If you go from 5 to 10, the $L_2$ norm—the MLE—will just multiply the weight by a half. So, **linear regression is wonderful. It's scale invariant.**

But many, many things are not. In particular, **ridge or anything with an $L_2$ penalty? No longer scale invariant.** Anything with a distance? Not scale invariant.

We will see other methods—some of the tree methods, which we haven't covered yet, gradient tree boosting, and those ones will be scale invariant, which is super nice. It makes them, like, more stable. But it's something to keep in mind as we look through the course.

## Looking Ahead

Okay, we are at the end. We're gonna, next week, talk about how to transform the features. Instead of using an $\mathbf{x}$, we'll maybe take every... no, no, hang on, hang on. I'm gonna end promptly at 10 after, but you don't get to leave until 10 after. Or check out.

Because it's good to see stuff in advance, and then see it again, and I have 2 more minutes by my clock.

So we're gonna transform the features sometimes, and then do a linear regression on the transformed features. We'll learn to use kernels. We will put in link functions instead of a linear function. We'll put a link in.

We will see lots of different loss functions, and lots of different regularization penalties. All of which are going to look very similar to what we saw today.

I won't read through these, but you can go back to the slides that are posted. And we did not cover the slides at the very end—you're not responsible for them—so with that one, do review the material, make sure this is all clear. There are lots of office hours.

Yes, I'll see you all Wednesday. Thank you!

---

# CIS 5200: Machine Learning - Lecture 6: Bias-Variance Tradeoff and Regularization

## Introduction to Training vs Testing Error

But we don't care about the training error, or the empirical risk. We care about the testing error. And if you run almost any problem, what you see is something that looks like this:

At some point, the error starts going up on the test set. You want to be somewhere in between these regimes:

- **Overfitting regime**: Where test error increases
- **Underfitting regime**: Where the model is too simple
- **Optimal place**: The sweet spot between the two

And so over and over and over, we're going to try and trade off how much complexity we have, often by regularizing, sometimes by making more parameters.

### Understanding Overfitting

Overfitting can be viewed as spurious correlations - you are fitting the noise. Let's think about doing that empirically.

Let's draw another chart here. Let's have some $X$ and some $Y$'s, and let's have some data.

I can fit a really simple model:
- $Y = 0$ (simplest model)
- I could fit a more complex model: the best line that goes through the origin
- I can fit a more complex model, one that has an intercept and a slope
- I could fit a more complex model, one that has a quadratic term
- I could fit one that's a 7th order polynomial

The more complex the model, the better it fits the training data.

So far, so good?

## Empirical Model Validation

Today we're going to talk about how to do this empirically and how to do this theoretically.

In general, the simple version, empirically, would be:
- You take half your data and use it to build the model (training set)
- Take half the data and use it to check the model (test set)

This, in some sense, should give you an **honest estimate**. That's a technical, statistical term. "Honest" means you're not cheating by using the training data in the testing.

An honest estimate of how good it is.

### Two Reasons for Testing Error

We do testing error for two different reasons, and it's important to keep them straight, because they're not the same:

1. **Hyperparameter selection**: We want to pick the best hyperparameters. The best hyperparameter is the one that gives you the lowest error on a test set.

2. **Performance estimation**: We want to know how accurate our model is on future data, tomorrow's data. How well you do on a test set is a much better estimate of how well you will do than on a training set.

### The Three-Set Problem

Make sense? Now, one super fine technical annoying point, which you asked me last time:

If you pick a lot of hyperparameters, we'll often have 3 or 4 or 5 hyperparameters. If you pick the best hyperparameters on your test set, you've sort of overfit because you really should have 3 sets if you're really super honest:

1. One to fit your parameters on
2. One to pick the hyperparameters on
3. One after you pick the hyperparameters to see how well you really, really did

Now, often life is big enough that we can just afford to not worry about it. If you're picking a million parameters and three hyperparameters, I don't care so much about overfitting the hyperparameters when I have a million observations. If you have a tiny data set, then you start to worry.

## Cross-Validation

Good, good. Now, there's another piece that's also super common. For a lot of data sets, you don't have enough observations that you want to use half for training and half for testing.

If you only have 100 data points, it feels painful to train on 50 and test on 50.

### K-Fold Cross-Validation

So what we're going to do instead is take our 100 data points and break them up into 10 different random sets.

And we will do **tenfold cross-validation**.

What we're going to do is:
1. Take one of the datasets and hide it
2. Train up a model on the remaining 9 folds
3. Use that to pick the best hyperparameters, or see how well we did
4. Then we're gonna take away this piece, train on 90%, test on this seed, see how we did
5. Do that 10 times

We've done that 10 times, we've built 10 models. Each of them was always tested on data that we did not train on for that model.

We'll take the estimated error to be the average of the errors on the ten folds.

**Question**: How many people have seen Tenfold Cross Validation before?

Yeah, everybody, cool, okay, so I won't say more about it. If you haven't, you should look it up.

### Variations in Cross-Validation

My friends in the medical school often do **five-fold cross-validation**, that's fine. In computer science, you almost always do **tenfold**.

The statisticians also like **leave-one-out cross-validation**. You can do the whole thing with 99 points and test on the 100th one. You can sample with replacement, there's a million versions. I don't care so much which one you do.

But I want you to always, always, always think that what you care about is the **test error, not the training error**. And what we'll see today is that your regularization penalty takes the empirical risk and adds something that should take you up to a pretty good estimate of what the actual error is going to be on the test error.

Cool. People are seeing cross-validation, don't want to dwell on that one there.

---

## Bias-Variance Decomposition

Great, I said all that. Great! Okay, now we get technical.

So, we're gonna do the bias-variance decomposition. 

**Question**: How many people have seen bias-variance decomposition?

Yeah, 3, 4, 5? You can go to sleep if you've seen this.

### Mathematical Framework

Okay, so here is how we're going to frame the problem.

We're going to assume the world has an infinite set of $(X, Y)$ pairs. Again, $X$ can be high-dimensional, but I'll draw one-dimensional.

And we are going to, because we're in math land, infinitely many times, grab $n$ observations from the dataset, and we're gonna fit our model to those $n$ observations.

And we can then fit the parameters in the model, which we will call $\theta$. So this is:

$$\hat{Y} = f(X; \theta)$$

Some function of $X$ with some $\theta$.

We fit the $\hat{Y}$ there, and we can compute some sort of error.

### Model Complexity and Variance

Now, what's gonna happen?

The simpler the model, the less good a job it's gonna do at fitting the data.

If I just do a simple model - best line through the origin - there's a simple model that fits the data. So, if you have a simple model, it will fit the data less well.

But, it's gonna vary less from time to time. What do I mean by time? I grab another set of $n$ points. I fit another model.

I do this an infinite number of times with new samples of the set. And if you fit different data, the more complex the model, the more it's going to vary from data set to data set.

People with me? This is sort of a weird thing to be in math land.

In my world, you get one data set. You train on it, you test on the part you held out.

But if you're a mathematician, you've got infinite data, and you just draw over and over training points.

And you would like for your model to be stable. For any $n$ training points you draw, you'd like to get roughly the same model.

But the most stable model? $\hat{Y} = 0$.

$\hat{Y} = 0$ is incredibly stable. It's not gonna vary from dataset to dataset.

And somehow we're gonna trade off complexity.

So far, so good with the setup.

## Formal Definitions of Bias and Variance

Now, let's start to talk about, formally, what we want to do.

The idea is we're going to talk about the bias and the variance of the model.

And confusingly, we can say it either about:
- The bias and variance of the **parameters** in the model (what I would call $\theta$ as a statistician, or $\beta$ for linear regression)
- The bias and variance of the **prediction** (the $\hat{Y}$)

And if it's linear regression, which we'll look at mostly today, they're just off by a constant.

$$\hat{Y} = X \cdot W$$

The bias of $W$ is proportional to the bias of the $\hat{Y}$'s. The variance of $W$ is proportional to the variance of the $\hat{Y}$'s.

### Bias Definition

So let's define bias.

What is bias in this case? The bias for a parameter, the bias for the estimate of the parameter is:

$$\text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}_D - \theta]$$

And again, if you can't read my handwriting, you can see it on the screen there. This is the expected value, if you do this an infinite number of times over your training data, and you build your model, and you estimate it, the expected value of the difference between:

- $\theta$ trained on some data set (this is subscripted with each of the data sets - my expectation over lots of $(X, Y)$ pairs to build it)
- Minus the expected value of the truth, $\theta$!

Which, by the way, is not very useful for a real person, because no one ever tells you the real parameters in the world.

Make sense? With the $Y$'s, this will make more sense. We can do $\hat{Y}$ on $Y$. We can observe the $Y$'s, but we don't get to observe the $\theta$'s.

So, bias:

$$\text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta$$

We're gonna do a whole bunch of times of $\hat{\theta}$ and average over building all the models.

The expected value of $\theta$ will just be $\theta$! Because there is one vector of $\theta$ that's the true parameters that God made up when she made the world. But she doesn't tell us.

You don't know what $\theta$ is. You can write a simulation and pretend you know, but in the real world, all you get are $Y$'s and $X$'s. Those you get.

And you pick a model form, which gives you a functional form $f(\theta)$. But you never know the true $\theta$.

Nonetheless, we'd like to know: Are we systematically high or low in our estimates of $\theta$?

If we do this a thousand times and estimate with a thousand different data sets and build the model, on average, you would hope that you're neither high nor low. We'd like to be **unbiased**. We'd like that to be zero.

### Variance Definition

We can also compute the variance of $\hat{\theta}$, which is equal to:

$$\text{Var}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2]$$

So what does this say? This says that we build a thousand models, we take the average parameters, and how much, on average, does my $\theta$ for the model vary? We square it in this case, so if it's high half the time and low half the time, it's high half the time and low the other half time, it's unbiased.

But it has a higher variance.

Make sense? So this quantifies: if you fit the model over and over on different data sets, how much does it change by in the parameter space?

Questions?

### Variance Computation Process

The variance is: What we're going to do is an infinite number of times, we're gonna grab a training set $X$ and $Y$, we're gonna estimate a $\hat{\theta}$.

We'll take the average of those infinite number of $\hat{\theta}$'s, and we will say, on average, how much is the squared difference between the particular $\hat{\theta}$ for this model and the average?

If I did pictures, this is gonna look really, really ugly.

I got my $X$, my $Y$. I take a set of points, I fit a model, I get a $\hat{\theta}$. Let's say that's $\hat{\theta}_1$ for the first thing. I pick a different set of points, I fit a model again, that's $\hat{\theta}_2$. I pick another set of points, I fit a model, that's $\hat{\theta}_3$. I've got forever. I average all of those, and I now get some $\bar{\hat{\theta}}$ - the average, expected value of $\hat{\theta}$.

And then I subtract off all the $\theta$'s there, square them, sum them up, divide by the number (which is infinity), and I get the expected variance.

I would like things to be **low variance** and **unbiased**.

But I'm going to find that in the real world, I'm always trading off bias and variance. It's gonna be a no-free lunch sort of situation. This can be annoying in the mathematician's world.

### Model Complexity and Variance

Cool. One last thing before we do that. The more complicated the model, I said this before, the higher the variance is going to tend to be.

The model's got a lot of parameters in it relative to the number of observations, it's going to be very unstable. You'll get higher variance. But you might get lower error.

## Bias-Variance for Predictions

Cool. Let me now just write this same equation again before I get to the bias-variance trade-off.

Instead of talking about $\theta$, I could have the expected value of $\hat{Y}$ minus $Y$.

The $\hat{Y}$ is, of course, some function of $\hat{\theta}$, which you can't see because it's hidden, it's invisible. But I now pick something which is an expectation over what? Over $X$ and $Y$ and the training data, capital $D$.

Which you can't read here, but you can see on the slides - the expectation over a particular $(X, Y)$ in my test set, and the particular data set which I used to train it on.

So I'm going to pull lots of data sets of size $n$. And I'm going to look at the difference, for example, between how far off is... okay, unfortunately, now it's an $h$ instead of an $f$.

And someone correctly, annoyingly pointed out, it would be really nice if the slides and the handwriting on the screen used exactly the same notation. It would be. On the other hand, it would be nice if you were happy to say, I don't care if it's $\theta$ or $\beta$ or $W$, I don't care if it's $h$ or $f$ or $\hat{Y}$.

So yes, I'm gonna be annoying and keep switching labels.

### Bias and Variance for Predictions

So, what do I want to say? I want to say that there is something which I could compute. I could take the expected value of $\hat{Y}$ minus $Y$.

$$\mathbb{E}[\hat{Y} - Y]$$

On average, are my predictions high or low?

And I could also ask the same thing: what's the variance of $\hat{Y}$? Which would be:

$$\text{Var}(\hat{Y}) = \mathbb{E}[(\hat{Y} - \mathbb{E}[\hat{Y}])^2]$$

Which says: what's the variance? For any given... and this is averaged over all $X$'s, $(X, Y)$ pairs now. So, this is a weird expectation. Again, it's over the training sets $D$.

I train it over and over and over again to fit a model, and then I take an expectation over the $X$'s and $Y$'s in the test set.

And that says, on average, over all the points in the test set and over all the different training sets I use, how far away is the expectation of $\hat{Y}$? If I use $\hat{Y}$ on each of the points from the average of the $\hat{Y}$'s on that point.

And over here, you can see that some of the points, their $\hat{Y}$'s are always pretty much the same. Some of the points over here - uh-oh, here's a $\hat{Y}_2$, here's a $\hat{Y}_1$, here's a $\hat{Y}_3$. These $\hat{Y}$'s are bouncing madly away from the actual average $\hat{Y}$.

And the same thing over here, but it's squared. So, in the variance, being high half the time and low half the time doesn't help you.

### Understanding Bias

For bias, if you're a thousand over equally often and a thousand under, you're unbiased!

On the other hand, where might I be biased?

**Question**: What's the big biased algorithm we've used?

We've only done two things, though. We had k-nearest neighbors, we did linear regression (OLS), we did ridge regression.

**Question**: Is KNN biased? Is KNN systematically going to under or overestimate the $Y$'s?

Well, KNN doesn't have any parameters, so you can't tell me it depends on the parameters.

So, let me ask... yeah.

#### Ridge Regression Bias

So let's go... we'll come back to you in a second. So, ridge regression does what? Ridge regression, if I do ridge regression here, what does it do?

**It shrinks the parameters.**

It's an $L_2$ penalty, it's gonna tend to make all of the weights smaller. That means that $\hat{\theta}$ is going to be systematically smaller than it should be - shrunken towards zero. And it's gonna mean that...

**Question**: Does the variance go up or down with Ridge?

Think of the limit as the parameter $\lambda$ goes to infinity, as you have infinite ridge.

**Question**: What's the variance with really strong ridge regression?

**Zero**, right?

In the limit of really strong ridge regression, you've shrunk all the parameters to zero, and you've got something that's really biased. It's always low. It's always got the parameters too small.

But, it's got less variance.

Make sense?

**Question**: Are the bias and variance both less in ridge regression?

**Question**: Is linear regression OLS biased?

It's not biased. **It's unbiased.** In linear regression, you're fitting the $L_2$ loss here, and you're going to be equally likely to over-underestimate either $\theta$ or $\hat{Y}$.

There's math in the slides later. But OLS, ordinary least squares with no shrinkage, is unbiased. It's not going to be systematically high or low in either the $\hat{\theta}$'s (the weights) or in the $\hat{Y}$'s.

Ridge makes it biased, yep?

### Clarifying the Framework

**Student question about populations and testing...**

I would distinguish two things (people couldn't all hear that). One is the empirical world that engineers live in, where you have a training set and a testing set.

The other world is one we're in now, which is the math world, where you have a set of training sets - an infinite number of training sets - and an infinitely big test set.

So in the math world, which is the world we're in for the first half of today's lecture, you have an infinitely large set of training data. But you're only going to pick $n$ of them. And you're gonna keep drawing new models and fitting them.

And then you're going to test it on an infinitely large test data.

Engineers don't have the luxury of infinitely large test data, so we'll have to do some hack like tenfold cross-validation. Or for neural nets, we'll just have training data and test data, it's too expensive.

But I want to be clear that what we're trying to do in the engineering world of a training data and a test data is to approximate the math world where you get to draw infinite numbers of training data, and you have infinite number of $(X, Y)$ pairs for testing.

### KNN Bias

Let's come back to the other question, because we didn't answer it.

**Question**: Is k-nearest neighbors biased or unbiased?

Is it systematically gonna... there are no parameters in it, so we're only talking about $\hat{Y}$. But you take the $Y$, you take the 5 nearest neighbors, and you average them to get the $Y$ value. Is that going to systematically underestimate the true $Y$, or overestimate it? Or neither?

Yeah.

**Clarification**: KNN can either be a classification algorithm, where you have 5 labels of the nearest things and you take the majority vote, or KNN can be a regression-type model where it gives a real number. It's perfectly fine to use k-nearest neighbors to say every $Y$ is a real value, and I'm going to estimate the real value $\hat{Y}$ by taking the average of the five $Y$'s from the five closest points.

So that's still KNN - they're both k-nearest neighbors. It's either classification or, if you will, regression, in the sense of having a $\hat{Y}$. And I'm asking for the regression one, because that's the easy one to think about for this setting.

**Question**: Is it unbiased or not? Is the average a biased or unbiased estimator?

**Unbiased.** Average is unbiased. And linear regression is just a fancy average.

A lot of machine learning is just averaging.

K-Nearest Neighbors chooses how many to average and which ones. What's the other way to pick nearest neighbors than distance? We'll do kernels, but what happens is in all these sort of averaging cases, if you average a set of numbers, you get an unbiased estimator.

#### Effect of K on Variance

Now, the variance will shift.

**Question**: If $K$ gets bigger, how does the variance change? Up or down?

I got one down, two downs, three downs - it's **down**.

If you average a big $K$, if $K$ is equal to $n$, the variance is... well, not quite zero, because for each data set, the average will be slightly different, but way smaller than if you take the one nearest neighbor.

The one nearest neighbor, it's all over the place.

Okay, good. So that's the intuition.

## The Complete Bias-Variance Decomposition

Let's look... I do not understand this thing here. Okay, so what I want to do... I'll clear all this stuff and get a clean slate here.

And what I want to say is, anytime you're trying to estimate your error...

The expected value over $(X, Y)$ pairs from the test set and over training datasets $D$ of the difference between:

$$\mathbb{E}_{X,Y,D}[(h(X; D) - Y)^2]$$

I can call it $\hat{Y}$, or I can call it $h$. I'll try and use the $h$ on the slide to make it the same. The $\hat{Y}$, or the $h$, which is a function of:
- The data point you're on
- The training data $(X, Y)$ pairs you use to estimate the model

Minus the true $Y$, squared, all in expectation.

So this is, on average, if I train up an infinite number of models $h$ with different data sets, how much is my error gonna be in an $L_2$ error sense?

Make sense? Yeah.

### Clarifying the Random Variables

**Student question about Y in the data...**

No, no, this $Y$ here is... remember, this $X$... I dropped a subscript here, sorry, but it's the same thing. This is $Y$ of $X$.

So this says, for the test data, you take a given $X$ in the testing set, you look up the $Y$ for it. This is a lookup function. You take a given training data $D$, you take a test data $X$, you look up the $Y$. You grab a huge training set $D$, you build your model, estimate it, you look up that same $X$, you compute $\hat{Y}$.

You say, how high is $\hat{Y}$ on this test data point $X$ from that true $Y$ we know on that test dataset?

And then we average over the whole test. So this is **test** and this is **train**.

Good? Yeah.

$X$ and $Y$ in the test set, and $D$ in the training set.

The random variables are: There's two sets of random variables here. There's random variables in the training set - there's an $X$ and $Y$ in the training set. And there's random variables $X$ and $Y$ in the test set.

So, if you want to think about it, there's two pairs of $(X, Y)$ pairs. And it's an expectation over both - it's two expectations.

You can write the expectation, let's put it right here. There's the expectation over the $(X, Y)$ in the test set of the expectation over the $(X, Y)$ in the training set.

$$\mathbb{E}_{X,Y \text{ (test)}}[\mathbb{E}_{D \text{ (train)}}[(h(X; D) - Y)^2]]$$

$X$'s and $Y$'s, but they're different variables. And the training set is a set of $n$ of these.

A given training set, so we're gonna grab an infinite number of sets of... so this is $n \times 2$, or $n \times (p+1)$.

The training set is $n$ points with $X$ and $Y$ in them. And we're gonna sample from that random variable to build the model. And then we're gonna sample from an $(X, Y)$ pair in the test set to test it.

I don't know, they are random. They're drawn from some distribution.

Yeah?

#### Cross-Validation Framework

**Student question about averaging between 10 points...**

So if you were to think of this in a tenfold cross-validation sense, what you're doing is sampling the $D$ in this case over just 10 different sets of 90%.

So this $D$ is sampled over not an infinite number of subsets, but sets of 10 sets of 90%. And then the $(X, Y)$ ends up being sampled over each of the ones in a held-out set.

I'm not helping much, but I think they are both random variables, just think of two different ones. One is an $n \times p \times 1$-dimensional... $p+1$-dimensional random variable which is the training set, and this is an $(X, Y)$ pair which is the test set.

Maybe that's the way I think about it.

Okay, I'm gonna push on.

## The Three Components of Error

So, given that I want to know, on average, across an infinite number of training sets and all my test points, what's my squared error? That can always be broken up into 3 pieces.

### Component 1: Variance

The first piece is the expected value over $X$ and the training dataset $D$ of, on average or in expectation, how far away is the $\hat{Y}$ (or the prediction on that $X$) from the average value of that $\hat{Y}$, squared.

$$\mathbb{E}_{X,D}[(\hat{Y}(X; D) - \mathbb{E}_D[\hat{Y}(X; D)])^2]$$

So this average value says, if you train it up over an infinite number of $D$'s... this bar here is an expectation over $D$'s. If you average over all the different training sets, how far is this particular prediction for one data set away? And this is what we call **variance** before. And it's still variance.

So, one piece is: How big is the expected variance?

### Component 2: Bias Squared

One piece is: How big is the expected difference over the points in the training set of the average prediction $\bar{h}(X)$ - that's trained up all these $\hat{Y}$'s, all these models over different data sets - from the average $Y$, quantity squared.

$$\mathbb{E}_X[(\mathbb{E}_D[\hat{Y}(X; D)] - \mathbb{E}[Y|X])^2]$$

The average $Y$. The square here is outside - it's the bias quantity squared.

This one, the variance is the expected value of the square difference.

The bias one looks wrong. It should be the expected value... yeah, that one looks wrong on the slides. Crap.

Let me think if they're different. I think it's much nicer to think of it as the quantity bias squared, and now I'm trying to think whether that's correct or not in terms of the other one.

I will double-check, but I think the slide one is wrong. Oops. Because the idea is we're gonna compute the bias, which is: if I do a lot of training models, make a prediction, average those. If I take all the averages of the $Y$'s at the same $X$...

**Question**: Can I have two different $Y$'s of the same $X$?

**Sure!** There's no reason I can't have two $X$'s that are exactly the same thing, and I've got different $Y$'s on them. I've got two different histories of items you purchased, and the next thing you purchase is different. Happens all the time.

So that's the bias. This term is the **bias squared**.

### Component 3: Irreducible Error

And then the third term, which I can't fit on the screen, is:

$$\mathbb{E}_{X,Y}[(\mathbb{E}[Y|X] - Y)^2]$$

The expected value over the $X$, $Y$, and the training set of the average $Y$ at an $X$ minus that quantity $Y$, squared. This is the **irreducible error** or the **noise**.

If you had the Bayes Optimal Prediction, which looks like $\bar{Y}$, the best prediction you could give at a point for a given $X$, there still may be different $Y$'s.

This part, this noise or irreducible error I like to call it, **there's nothing you can do about it**.

If one $X$... if the $Y$ varies for that $X$, there's no possible way to predict it.

And the answer is, of course, there is a way to predict it: **get a better $X$, get better features, go out and buy some more features.**

But for a given set of features, there's always going to be some noise.

### Summary of Decomposition

So, let me summarize what we have. We said that anytime you have a squared error, you can break it up into 3 parts:

1. **Variance**: How much does the model prediction vary from the average of that model prediction?

2. **Bias Squared**: Is the model prediction systematically high or low from the average of the $Y$ at that point?

3. **Irreducible Error**: Just the sheer irreducible noise error.

And I'm not going to do the proof, but the proof is actually quite simple. The proof says, hey, if you want to actually do it, the trick is you take $h$, you subtract off $\bar{h}$, you add in $\bar{h}$ again (which is a null operation) - minus $\bar{h}$ plus $\bar{h}$ - and you subtract off $Y$.

So this term here, $h - Y$, becomes $(h - \bar{h}) + (\bar{h} - Y)$. Use the sum of the squares to expand it. You get the first term, you get the second term squared, you get 2 times the product, and in expectation, the middle part magically vanishes (which I'm not going to show you, but hey, GPT will explain it to you), and you'll get out the pieces, and the whole thing works out.

Sorry, the math is trivial, but I want you to have the intuition.

## Ordinary Least Squares Properties

Cool! I'm gonna go fast. I can't look at it.

For ordinary least squares, $W$ is **unbiased**. The $\theta$, $\hat{Y}$ is unbiased. They also have variances, which you can compute. I'm not gonna show the math. Again, here, if you want to look at it, but the math is... I will show it to you quickly. The math is really easy.

We showed before that the $\hat{\beta}$, which is what linear regression calls the $\theta$, for linear regression is:

$$\hat{\beta} = (X^T X)^{-1} X^T Y$$

If this inverse exists.

And you can see the expected value of $\hat{\beta}$ is this times... these are constants for the expected value of $Y$. It's equal to... $Y$ is equal to $X$ times $\beta$, so the expected value of $Y$ is $X \beta$.

$$\mathbb{E}[\hat{\beta}] = (X^T X)^{-1} X^T \mathbb{E}[Y] = (X^T X)^{-1} X^T X\beta = \beta$$

So, in some sense, if the world is linear, it's trivial to show that the estimate of $\beta$ is unbiased.

And you can do the same thing for the predictions, which is a constant times it, but it's just math, so I don't care.

### Summary of Error Decomposition

So, again, I'm going to summarize this and move on. The error we have, $Y - \hat{Y}$ squared (this is $\hat{Y}$ was $h$, this is $Y$, I'm trying to map the notations for you):

$$\mathbb{E}[(Y - \hat{Y})^2] = \text{Bias}(\hat{Y})^2 + \text{Var}(\hat{Y}) + \sigma^2$$

The expected value of the squared error is equal to:
- The bias of $\hat{Y}$ squared
- Plus the variance of $\hat{Y}$ over here
- Plus the noise, which for linear regression is the noise, which is of size $\sigma^2$

The other thing to note in all of Mathland: **variances add**. Things add in the squared error space. So the things that add are squares - it's the variance (which is a square term), it's the variance here (which is the square of the standard deviation), and it's the bias squared.

So, squared terms, that's just how the math works, add up in an $L_2$ world.

Yeah.

### Clarifying Y-bar vs Y

**Question about the two Y's in the noise...**

Again, what is $\bar{Y}$? It's the average $Y$ across the $X$'s in the test set. Whereas the $Y$ actually is gonna be varied for each $X$.

And annoyingly, this $Y$ should be $Y(X)$ also. I need to fix these slides. This is ugly, but this $Y$ is $Y$ of $X$.

---

## Part 1 Summary

Okay. In the first part, what did I just cover?

If the model's too complex, you will overfit. You will do better on the training set than the test set. That's generally a bad idea.

We want to control the model complexity, either by:
- Picking the number of parameters, or
- Picking the hyperparameters

We can do leave-one-out cross-validation, but that's usually too expensive in machine learning. We mostly do tenfold cross-validation.

If you do leave one out, you could sample with or without replacement. We don't care about that in this course. 

We use this both for:
1. Setting what are the best hyperparameters (remember the minimum of the testing curve)
2. Saying what's the actual error we expect to see (which is the test error, not the training error)

We said that there's a bias-variance trade-off. We said that OLS is unbiased.

And I said before that I pretty much never use unbiased estimators. Because...

**Question**: Why do I always put some sort of shrinkage in? I use a biased estimator. Why do I not just run the linear regression, which is the MLE, the best estimate?

**Answer**: In the real world, the variables are often correlated. If I have $p$ bigger than $n$, then I'm guaranteed to have a problem. If I have $p$ smaller than $n$, I still could have linear dependence. The variance gets really high in these models, often in the real world. So you have to do some sort of shrinkage to stabilize them.

If $X^T X$ is not invertible, the variance effectively is infinite, which is really bad.

---

# Part 2: Regularization and Penalties

Cool. Second piece. Okay, that was a lot of... we're doing two lectures today. Two for the price of one. 

So, we talked about bias-variance, we need to now worry about how to control it.

## Types of Penalties

The way we control it is by doing some sort of a penalty. And the simplest set of penalties we'll start with are penalties on the weights for regression, which will either be:

- An **$L_2$ penalty**: Sum of the squares of the weights (so it's $L_2$ squared)
- An **$L_1$ penalty**: The absolute value of the weights
- An **$L_0$ penalty**: The number of non-zero weights

All of these are **shrinkage**. They make the weights smaller. Some make some of them zero and some of them not smaller. That's still shrinkage. On average, they're smaller. 

All of them will:
- Give biased models and
- Reduce the variance

And all of them are quite useful.

They're each going to require somewhat different optimization techniques:
- If they're convex, we can often do a closed-form solution for $L_2$
- If they're not convex, like $L_0$, we'll have to do a search

Cool! So, okay, we can skip all that.

## Ridge Regression (L2 Penalty)

Let's start by thinking about our core ridge regression.

I'm not gonna take attendance with this, you're welcome to do it or not, but you should play along.

So we're going to minimize - it's $L_2$ regression, or regression:

$$\min_W \sum_i (Y_i - \hat{Y}_i)^2 + \lambda \|W\|_2^2$$

$$\min_W \sum_i (Y_i - X_i W)^2 + \lambda \sum_j W_j^2$$

We're gonna then add $\lambda$, a penalty, times the norm of $W$ squared.

### What Does Each Term Do?

And in general, this term here is...

Well, first of all, does linear regression minimize bias or variance? Or both?

Linear regression is measuring some sort of... it's trying to minimize what? It's trying to measure this loss function here, the empirical risk.

And minimizing this is, in some sense, going to... what do we have? We had the error was the bias squared plus the variance plus the irreducible error. 

So, ordinary least squares is already minimizing some combination of the bias and the variance.

And as we add in a second term, so the first one really sort of does both of these, I think. If you just do the first term, you're both decreasing bias and variance. Does that make sense?

Let me go back and show... you're minimizing this. That's ordinary least squares. You are minimizing bias squared plus variance. You're trying to minimize the noise, too, but that doesn't matter - it's a constant. Doesn't depend on $\theta$.

So, **ordinary least squares minimizes bias squared plus variance**.

So the answer here is, the first term is both of them.

**Question**: The second term tries, if you make $\lambda$ bigger, it's minimizing more bias or variance?

**Variance**, right?

As you make the second term there... okay, my screen is back to me. I don't know what's going on here. The second term - don't worry about the pool everywhere, worry about the piece here. The second term here says, hey, we were trading off bias squared and variance sort of equally here.

Now we're gonna put more weight on removing the variance, which will mean that the bias will go up.

## Choices of Loss and Penalty Functions

Cool. We have different choices of both:
- The loss function for the regression
- The penalty we put

And again, I'll repeat both of them so it's clear.

### Different Loss Functions

We started by saying that linear regression, last time, assumes that $Y$ is normal with mean $W^T X$ and variance $\sigma^2$, so it's:

$$Y \sim \mathcal{N}(W^T X, \sigma^2)$$

$$p(Y|X,W) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(Y - W^T X)^2}{2\sigma^2}\right)$$

A normalization constant to be a probability times $e$ to the minus $(Y - W^T X)$ squared over $2\sigma^2$.

We said, hey, if we minimize this, we get a squared error. And that's what we call ordinary least squares or $L_2$ regression.

But let me remind you that **there's nothing magic to this form**. I could have said, let's assume that $Y$ is not normal. It's some weird distribution:

$$p(Y|X,W) \propto \exp\left(-\frac{|Y - W^T X|}{2\sigma^2}\right)$$

$e$ to the minus $|Y - W^T X|$ in $L_1$ norm over $2\sigma^2$.

And the math works out all the way exactly the same, and that says minimize:

$$\min_W \sum_i |Y_i - \hat{Y}_i|$$

That's **$L_1$ regression**.

**Question**: Which one is better?

**Depends** - it's always the right answer in this class, right?

I mean, either it depends on what probabilistic model you care about, or it depends upon what your actual loss function is in the world. Do you care about the sum of the squares of the difference of your predictions, or the sum of the absolute values?

Good, this should be all straight review and hopefully super boring.

### Different Penalty Functions

Now, the other piece we can do is we'll take this error here, which I've shown as $L_2$ regression, and we add in $\lambda$ times whatever regularization penalty we want.

And my 3 favorite regularization penalties are $L_2$, $L_1$, and $L_0$.

We can make $f(W)$ be equal to:
- Sum of the squares of the weights: $\sum_j W_j^2$ ($L_2$)
- Sum of the absolute values of the weights: $\sum_j |W_j|$ ($L_1$)
- The number of non-zero weights: $\sum_j \mathbb{1}(W_j \neq 0)$ ($L_0$)

### The L0 Controversy

Now, some people don't like the $L_0$. In particular, scikit-learn, the most common package we use - these guys hate $L_0$.

**Question**: Why do many computer scientists really dislike $L_0$ penalties? What's wrong with the $L_0$ penalty?

**It's not convex!**

That means that trying to find the optimum is probably NP-hard, and it's gonna require search, and you can't use gradient descent, and so it's crappy.

That's one side. That's the classic computer science view: use a convex problem where you can actually get the optimum, otherwise you can't prove that you got anything.

Now, **I'm actually on the other side**.

I like $L_0$. It says, what I care about is having lots of the weights be zero.

In my world, it's called **sparsity**, and sparsity is good. It makes it easier to understand, it means all those features you zeroed out don't have to be collected, which saves me money and time and annoying questions of asking people. I love sparsity.

And, I gotta admit, it's non-convex. I don't ever find the true optimum, but personally, I sort of prefer to **find an approximate solution to the right problem** as opposed to **finding the optimal solution to the wrong problem**.

But, hey, I'm an AI guy, so you could pick either side, and neither one is right or wrong.

But note that there are people that feel so strongly about $L_0$, they don't do it. Oh, statisticians, by the way, often use $L_0$. It's super popular in the statistics world.

Because they are dealing with small data sets and interpretability and explainability, and they like actually finding a minimal number of features that they can then show to their clients.

### Understanding L0

So this $L_0$ says, I'm gonna make you a penalty: $\lambda$ times the number of non-zero features. So every feature that has a weight that's non-zero costs you something.

So the optimum is going to give you a biased model, but lower variance, that's going to zero out a bunch of the features. And for the ones that are not zeroed out, it'll just use OLS.

So, zeroing out features is often a really nice property.

Think again: I give you 10,000 features on your medical record, and I'm trying to predict your cancer. There's probably only 10 of them that really matter, and your doctor probably only wants to see 3 of them, because she's busy.

Show me the three features that matter, don't use more than 10 so I can check them. And if you're gonna throw in 100 features, I don't trust your model, because it could have crap in there.

Real people who make decisions - I work mostly with doctors, but the same thing is true - they want to know exactly what's in your model. And if you're gonna have to do a test, if you're gonna do that spinal tap, you really want to know it's important.

Features often cost money. We think in computer science, I just get an $X$ and a $Y$ in mathematics land. Every $X$ I get, I gotta buy. Every $Y$ I gotta buy is even more expensive. $X$ may be cheap - download a YouTube video. If you're Google, that costs almost nothing, because you own YouTube.

Getting a label? Expensive.

But you might want explainable AI, and explainable AI mostly drives sparsity.

If every pixel is part of your story, I'm a human, I'm not gonna look at every pixel. Show the little piece that mattered. Why did you think that was an example of somebody using illegal drugs?

Cool. Good?

## Properties of Different Penalties

So, super important is you can do all of these:

- **$L_2$ doesn't zero out anything.** They just shrink stuff, the biggest stuff being shrunk the most, because it's quadratic.

- **$L_1$ shrinks everything equally.** And in a second, I'll say that the things that are small enough to be shrunk past zero get shrunk to zero. So $L_1$ gives some sparsity.

- **$L_0$ really gives sparsity.** It says, either shrink it to zero, or don't shrink it at all.

Cool. All of these have nice properties and math behind them.

### Names for Penalized Regression

They all have names. So let's just... let me say the names. 

We already did **$L_2$ penalty**, that's called **Ridge** or **Tikhonov regression**. 

The next name, the **$L_1$ penalized regression**, goes either by the name of **Lasso** (or should we call it lasso?).

And that's still convex, but... the absolute value's got a shape like this - goes down and down and has a discontinuity at zero, you know, this kind of slope. It's continuous, but the derivative is discontinuous.

**$L_0$ doesn't have a name**, but in fact, because it's non-convex, you have to use a search procedure.

And the search procedure that's widely used...

**Question**: Which procedure you'll want, because you're a computer scientist?

We usually call **$L_0$ penalized regression, stepwise regression**.

This is sort of evil because the penalty is not the same as the optimization function. You can do whatever search you want, you can do whatever optimization penalty - they're entirely separate.

So, statisticians are slightly nasty in this. I will still call this stepwise regression, but remember, **that's a search technique**, which we'll cover. **It's not a penalty.**

Make sense? On the other hand, you don't have to use that sort of search technique in a convex problem. So, you emotionally don't ever use stepwise regression search for a ridge (which is closed form) or Lasso (which is convex).

Yeah.

### Why L1 Drives Features to Zero

**Question**: Yes, I will explain in a second why $L_1$ drives the features to zero.

Before I do that, what I want to note is that remember the penalty here came from a prior. It was an MAP with a prior on the distribution of the weights.

And the $L_1$ - it looks just like the absolute value of $W_1$ - that's called, if you care, a **Laplace prior**. Everything has a name in statistics. 

And if you have a prior on $W$ that they're either 0 or not 0, that looks like a **spike and slab**.

### Visualizing the Priors

And if you sort of look at what they look like:

You know what a **Gaussian** looks like? There's a Gaussian...

The **Laplace prior** is sort of more spiky, it looks like this (sharp peak at zero, exponential tails).

The **spike and slab** is really weird. It says, my prior is either it's zero with some probability (that's a Dirac delta at 0), or I got no prior whatsoever, and that blue bar goes forever.

So it's sort of weird. Think of what $L_0$ says.

Either there's no shrinkage (in which case, I have no prior whatsoever), or there's infinite shrinkage (and my prior is really strong at zero). 

So your prior for an $L_0$ penalty is: Either for a certain fraction of the time, a certain percentage, I expect it to be zero, and the rest, I know nothing about. That'll be OLS. 

Does that make sense?

And as an engineer, you're gonna have to pick: what sparsity do you expect?

But the theory for $L_0$ is really good because the prior is precisely: what sparsity do you expect? I expect 90% of the weights to be zero.

And under that, information theory (which we'll cover later) gives you beautiful answers, and life makes sense.

## Understanding Shrinkage

Cool. So, what does this look like in terms of shrinkage? And this comes back to another question. 

Intuitively, and I think it's easy to think about this if you standardize...

### Ridge (L2) Shrinkage

What does Ridge do?

Ridge says, for any weight you have, **shrink it by a constant factor**.

There's a penalty on the square of the weights. And the shrinkage on it says, each weight will end up being shrunk proportionally to how big it is. If the weight's twice as big, then it's shrunk twice as much.

### Lasso (L1) Shrinkage

$L_1$ penalty says take each of the weights, and in their standardized form (so they're all made to be centered at zero of size average 1), **shrink it by a constant amount**.

And if you shrink it by a constant amount, the big ones get constantly smaller. The small ones would, in fact, end up being shrunk beyond zero if they went down by a constant amount.

But if you were to shrink things past zero, it'd be bigger again. Making a weight negative gives a bigger $L_1$ penalty than making it 0.

So the way the $L_1$ penalty works, the absolute value penalty works: **shrink things by a constant amount. If it would make it negative, set it to zero.**

Make sense? It's a minimum of the absolute value and zero. There's no point shrinking it past zero.

Does this make sense? This is sort of a weird one. So, **$L_1$ does drive sparsity**. So that's sort of nice. You can be convex and still encourage sparsity.

### L0 Shrinkage

The $L_0$ penalty says: if it's small in terms of importance (which is sort of if you've actually standardized the weight so that you've changed so you don't have this problem of scale invariance of the kilometers and inches and things being different scales), if it's small in importance, **set it to zero**. If not, **leave it exactly where it was**.

And the strength of your prior for $L_2$ says what factor do I shrink it by?

The strength of your prior for $L_1$ says what constant amount do I shrink it by (which will have some side effects of zeroing some)?

And the prior for $L_0$ says what fraction of the weights do I set to zero?

### Understanding the Lambda Parameter

Yes.

**Student question about the prior showing up in lambda...**

So the prior is really showing up in the $\lambda$. And I think the problem with the $L_2$ and $L_1$ is that the priors we saw was a ratio of two things. It was a ratio of the strength of the prior and the probability, and the amount of noise in the model.

And in that case, I don't have a lot to help you with, except to say that we will fit it empirically by cross-validation.

For the $L_0$, it turns out that the prior... and I'm not going to go into it in this class, has beautiful, nice properties because it is, in fact, the percentage that are zero. They're either zero or not 0. Everything is still unbiased in the final model among the features you have, and there we can do theory, but I'm not going to do it right now.

So that's a non-answer. Except to say, empirically, right?

So, what am I trying to drive at? In some sense, you can argue all this math is hopeless and useless. I can't use theory to tell me how big my $L_2$ should be.

But I'm gonna say, but wait! As an engineer, as a data scientist, you're gonna have some intuition. How sparse should this be? How much do I care about shrinking the parameters? Is that gonna bother me or not bother me? 

The doctors I talk to keep fighting me - they don't want to do an $L_2$, they want an $L_0$. They want the parameters to really count for everything, because they're real and they believe the biology of the parameters. Throw them out, but don't shrink them. So they hate shrinking parameters.

On the other hand, if I'm working at Google, the model's got a billion parameters anyway. Shrink the crap out of them, who cares? Just use $L_2$. Why would you consider something non-convex? We're gonna solve everything with gradient descent.

So on one extreme are the doctors: Give me parameters which are not shrunk. I want to know exactly how much this thing affects your chance of dying of cancer. 

On the other hand is Google: $L_2$ it. It's smooth, it makes it easy. Why am I thinking about it?

**They're both right.** And one of the things you have to worry about is: when is each one right?

### Negative Weights and Shrinkage

Cool.

**Question**: Does $L_1$ go below? So, if the weights are positive...

Remember, is this an absolute value penalty? So we make all the weights smaller in absolute value.

So if a weight was positive, it's shrunk toward zero and becomes smaller. If a weight was negative, a negative weight is shrunk towards zero. A negative weight is shrunk towards zero and becomes bigger in real terms, but smaller in absolute value. 

So negative weights get shrunk also towards zero. They become smaller in absolute value.

And if they would be shrunk past zero to become positive, you leave them at zero.

So when I say shrinkage for probabilities, I'm usually shrinking toward an unbiased 0.5 probability. For a real value weight, I'm always shrinking towards zero.

And shrinking means positive things become smaller, negative things become (if you will) bigger, but smaller in their absolute value.

**Shrinkage means move towards zero.**

Super important. Yeah, we always look at the positive side. I'm a positive psychologist in my psychology world. But the negative things become less negative.

Good? Good.

## Comparing the Penalties

Okay, so let's see if we can clear this and actually think about what's going on here. Nope, give me my piece here.

This really doesn't want to let me do this. I'm just doing so badly on technology here. That's sort of funny, I just cleared them and it didn't do anything. 

Okay, so, but let's think about the problem. I don't care. So, for $L_2$, $L_1$, or $L_0$ penalty:

**Question**: Which one most heavily shrinks the large weights?

**$L_2$!** Awesome, awesome!

**Question**: Which one most strongly sets weights to zero?

**$L_0$!** Right? I see it creeping in the right direction.

**Question**: Which of the norms is scale invariant? Remember scale invariance? If I change from miles to kilometers, it'd be really nice if you didn't change the answer.

Uh, it's creeping around, but which one is it?

**It's $L_0$**, right? $L_2$ and $L_1$ both shrink things based on how big they are.

### Feature Standardization

And engineers very frequently **standardize all the features** before they do shrinkage. If you're doing $L_2$ shrinkage (which you mostly do), very often you're going to standardize the features. Subtract off the mean, divide by the standard deviation, make them all equally important.

**Question**: Is it a good idea to make every feature of the same size?

If I have how many dollars I spend on advertising, and how many dollars I spent on product acquisition, and how many dollars I spent on labor, should I standardize those features?

**Bad idea!** They're all in dollars. I don't want them in standardized dollars.

On the other hand, if I use a ridge regression on actual dollars, I'm shrinking one of those more than the other ones. Bad idea. I'm probably better off using an $L_1$ or an $L_0$.

So **$L_0$ is super cleanly scale invariant**. Everything is either in or out. The in-or-out depends upon how important it is, not on how big it is. How much it affects $\hat{Y}$?

So, $L_0$ is scale invariant. That's a super nice property. It means you don't need to worry about standardizing your features. In some sense, it's as nice as OLS, plus you get feature selection at the cost of having to search.

Bummer. The other ones, $L_1$ and $L_2$, are not scale invariant. It's annoying, and in some sense, $L_2$ is even less scale invariant, because it's quadratically bad. Whereas $L_1$ is only linearly bad, but they're both not scale invariant.

And again, you're gonna have to think for real data sets: is this something where I'm happy with dealing with the non-scale invariance? Do I want to standardize it? 

Some things like pixels, voxels, I never standardize because if it's black, it's black - it's pretty much zero. If it's not...

I don't usually like to standardize dollars. If I'm doing medical records with kilograms and inches and A1C scores, then sure, standardize it, why not? I don't know whether an A1C score is bigger or smaller than a kilogram. They don't make any sense, right? One's a blood sugar measurement, one's a weight.

It's something to think about.

Good? Good.

## Optimization Properties

Okay. Onward.

**Question**: Which ones are convex?

**Ridge and Lasso** are both... $L_2$ can be solved - and ridge can be solved in **closed-form solution**.

And it turns out that Ridge... you can prove that this is strictly positive, that this always has an inverse, so ridge always has a closed-form solution.

**$L_1$ is generally solved with gradient descent** of some form or another, like coordinate-wise gradient descent.

And **$L_0$ requires search!**

## Search Procedures for L0

So the last thing I have to cover is how to do search in these problems.

### Greedy Forward Selection

So, the dumbest search procedure, which I sort of like, is **forward stepwise regression**. You start with nothing in your model. You start with the error, which is:

$$\hat{Y} = 0 \implies \text{Error} = \sum_i Y_i^2$$

I'm gonna be a little bit fast, but yes, the slides are all up in Canvas, and you can see them later.

Now, for each of the features, I've got $p$ features ($p$ for predictor), $j$ goes from 1 to $p$. I try adding one feature to the model. I fit the regression on it.

If the new model (which is... this is linear regression, right? It's $W_j \times X_{ij}$ for all of the $i$ points):

$$\text{Error}_{\text{new}} = \sum_i (Y_i - \hat{Y}_i)^2 + \lambda \times |\text{model}|$$

The error of my new model plus my penalty times the size of the model (that's the number of features in it). If this thing goes down, it's a better model!

If $\lambda$ was zero, it never goes up, and usually goes down. So if $\lambda$ is zero, you're gonna just keep putting stuff in all the time.

But if $\lambda$ is bigger, then it's going to cost you $\lambda$ to add each item to the model. And the cost of one more item is a cost of $\lambda$. That's the cost you're paying of a feature, compared to the reduction of error. 

If the error is reduced by adding that feature more than the cost of the feature, then add the feature in! Otherwise, don't.

So this is a **greedy search**. It's linear in the number of features. You add each feature in, test it, see if it helps, keep going on.

**Question**: Does it matter what order the features are in?

**Does...** Right? It's a greedy search. If you try... if there are two features that are measuring the exact same thing, whichever one comes first gets put in. The second identical one doesn't get put in, because it costs you one more feature, and it gives you no reduction of error.

So if two things are identical, one gets in (whichever one hits first), one of them doesn't. Such is life. 

Yeah.

**Question**: Will it, in general, find the best solution?

**Oh, it won't!** But is it too expensive? Actually...

### Stepwise Regression (Best First)

But in fact, what's... this is... only I use this. What everybody in statistics uses is **stepwise regression search**.

And what they do is a less greedy search, but it's still a little bit greedy.

Start with an empty model. Your error initially is $\hat{Y} = 0$, so the error is just the sum of the squares of the $Y$'s.

And now what we're gonna do is **repeatedly try each of the $p$ features**.

So we're gonna build $p$ different models! Very expensive, but statisticians who work with small data sets, if $p$ is 10, fine, I can afford it. If $p$ is a million, forget it.

But, for small ones, you try each of the $p$ features, you pick the feature out of all the $p$ features that most reduces the error.

And remember, the error is... so here's the error. It is the minimum over all the candidate features of the error of adding that one feature to the model so far:

$$\text{Error}_{\text{best}} = \min_{j} \left[\text{Error}(\text{model} + \text{feature}_j) + \lambda \times k\right]$$

Plus $\lambda$ times... well, $k$, that's the number of items in the feature - number of features in the model. 

So you have an error of here's all the features plus the new candidate one, plus the number of features in the model. If the error goes down for adding any feature (this is the best feature), if adding the best feature reduces the penalized model error (complexity penalized model error added in), accept it. Otherwise, **stop**.

Try all the features. Ah, this one is good! Pop it in the model, that's the best feature. It reduces the error, it increases the complexity. Does the total penalized error go down? Yep!

Okay, try all the features. Using the same one again is stupid, because it's not going to help. Try all the other features. This one is the best remaining one. It reduces the error plus the penalty. Nope, doesn't use it. Okay, we're done. Don't try anymore.

So it's **semi-greedy**. It's gonna cost you almost $p$ regressions for each time you do it. But with a very small number of things in it.

If you're checking a million features and you're only going to put in 10, it's not so bad. 

**Question**: How many regressions, roughly, do I run in stepwise regression?

Stepwise regression, a million features, I'm gonna end up with 10 of them. How many regressions do I run?

**Quadratic in what?** Is it quadratic?

Again, let's look at this thing. It's not running... Total worst case is quadratic.

But stepwise regression is not used in the worst-case world. Again, the standard usage is you've got a million features, and you expect 8 or 10 or 12 of them will come into your model. That's the usage case here.

Okay? And again, if I've got a million features, how many regressions do I run to put the first feature in?

**A million!** Although each is a regression with one feature, so it's super cheap.

Make sense?

Now I put the one feature in, now I check to find the second feature. I'm gonna run a million more regressions, each with two features in them.

Make sense? The original feature plus each of the new candidate ones. A million minus one, if you want to be technical, right? I don't have to rerun the same one again.

But hey, a million is a million. I just want the same number to me, because I'm an engineer.

If I'm gonna put in 10 features, I do this 10 times. I end up running **10 million regressions**.

It's not anywhere near quadratic. Quadratic is a million squared. I can't afford anything quadratic in my world.

But I can afford things that are linear. And running 10 million regressions, where the biggest one is of size 10 (which requires inverting a $10 \times 10$ matrix), I can afford to run 10 million regressions where the biggest one is inverting a $10 \times 10$ matrix.

So I can afford to do stepwise regression.

So, statisticians like stepwise regression. It gives a nice search. It's not as greedy as the purely greedy forward stepwise one, and it ends up empirically giving pretty good results. 

If you're a theory guy, you know it's still greedy, it doesn't give the optimal. The problem is, like everything in life, it's probably NP-hard, but, you know, it works reasonably well.

Cool, okay.

## Selecting Lambda

Time is getting short, I've got like 2 more minutes. So, remember that we're gonna have to pick $\lambda$ by trying a whole bunch of different $\lambda$'s and seeing which one gives the best result.

For $L_0$, which I'm not going to cover, there's beautiful information theory called **minimum description length**.

## Summary

So what do we cover? You should know:

- The three different penalties, what they're called, how convex they are
- You should know about training error versus testing error
- You should know that the idea is that you have the **empirical risk** plus your **regularization penalty**, which should be an approximation to the **test risk** plus the **irreducible error**
- We talked about the bias-variance trade-off in those pieces
- We talked about forward stepwise, stepwise

### ElasticNet

Finally, one last, last piece of jargon. The most popular regression used in any machine learning world is called **ElasticNet**.

That's the go-to that everybody in the world uses for machine learning, along with gradient tree boosting.

An ElasticNet is just:

$$\min_W \sum_i (Y_i - X_i W)^2 + \lambda_1 \sum_j W_j^2 + \lambda_2 \sum_j |W_j|$$

**$L_2$ loss plus an $L_2$ penalty plus an $L_1$ penalty!**

It's still convex, so you can do it in scikit-learn or in cheap time. And it has two hyperparameters, which is annoying, but not so bad. 

And so, most people, their go-to method is ElasticNet. You gotta know that - that's a good interview question on a job.

And you should think that it both makes things better and puts a little bit of sparsity with the $L_1$. And it does that still remaining convex.

Cool, we are... you are here, and I will see you all next week, but you'll have recitations. See you soon!

---

**End of Lecture**

---

# CIS 5200: Machine Learning - Lecture 7: Generalized Linear Models and Basis Functions

## Introduction to Model Transformations

We're going to approach model transformations in two distinct ways. Either we apply one link function on $\mathbf{W}^T \mathbf{X}$, or we use a separate function for each of the $X_j$ components.

I haven't written this explicitly here, but this $\phi_j$ could be applied to $\mathbf{X}$. We'll see examples of both approaches.

**Student Question:** Are we transforming a nonlinear function into a linear function?

**Answer:** Nope. The top one is linear, the bottom two are nonlinear.

### Linearity in Different Contexts

If you look at the generalized linear model, is it linear in $\mathbf{W}$? No.

If you look at the basis transformation, is it linear in $\mathbf{W}$? I heard a no... Yes! The basis transformation says: transform the features, and then—at least for the first half of the course—we'll just use a linear model on the transformed features.

**Student Question:** Is the function nonlinear?

We always have to ask: a function of what? In this course, we switch back and forth between functions of $\mathbf{X}$ and functions of $\mathbf{W}$ (or $\theta$, or $\beta$, whatever notation you prefer).

In terms of the optimization for training, we're worried about linearity in the weights, right? So yes, basis transformation is nonlinear in $\mathbf{X}$ but linear in $\mathbf{W}$.

If I say "is it linear?"—that's an ill-posed question on my part. You have to ask: linear in what? We're almost 50-50 in this course between functions of $\mathbf{X}$ and functions of $\mathbf{W}$.

### Why Care About Linearity in Weights?

**Student Question:** Why do I care about being linear with respect to $\mathbf{W}$? What's magic about something that's linear with respect to $\mathbf{W}$?

**Answer:** Super easy to optimize! It usually has a closed-form optimization. If it's L2 loss, it's got a closed-form solution, and then you can prove theorems about it. So it makes the mathematicians happy because you can prove theorems, and it makes the solving people happy because it's a constant-time solution.

## Generalized Linear Models vs. Linear Models

We'll show different versions of models. Note that the generalized linear model—even though it's called a GLM—is not linear! Right? The $f$ is the nonlinear transformation.

So we're setting up two sets of transformations, and eventually neural nets will slam lots of these together. But for small datasets that show up in hospitals and similar settings where you only have thousands of patients with a particular condition (like cancer), often you need to pick what transformations you want.

### The Surprising Power of Logistic Regression

I've got to say, having now written probably a hundred studies of medical data, by far the most useful single model is... **logistic regression**.

It's so embarrassing because the funders want—we'll do gradient tree boosting in a couple weeks—they want neural nets, they want fancy stuff. And you run it, and... crap, I'm so embarrassed: the logistic regression is just as good and interpretable in a nice, clean way.

### When to Use Simple Models

**Student Question:** When do we still use very basic models? Why would I use a "stupid" model like logistic regression when I can fit a neural net?

**Answer:** That's a great question. There are sort of two answers you care about.

**First consideration: Model Complexity**

How complicated is it to fit the model? Forget the compute time—in terms of number of hyperparameters to choose. Neural nets typically have 3, 4, or 5 hyperparameters plus an architecture, so you have to make a crap load of choices. If you make the right choices, neural nets are pretty good.

But if you only have a tiny dataset of a thousand people with cancer and 100,000 that don't—where there's only 1,000 cancer cases—often the neural nets are really tricky to get to converge to something sensible. You overfit the crap out of them.

If you've got a doctor, you want to give them a logistic regression, which is clean. Or we'll show later in this course how to do embeddings: if we have something like a breast image, we will embed it into a dimensional vector space and then pop that into a logistic regression to predict cancer. It's much, much more stable for estimation.

**Second consideration: Interpretability**

The other problem—and it's always about overfitting—the other piece is that it's nice if it's interpretable.

Doctors and people in high-stakes, low-dimensional settings (well, it's not that low—it might be a 10,000-dimensional feature vector for you, but that's still relatively low-dimensional) want to know: "Tell me the 5 reasons that you think this person is likely to live less than one year, so I should recommend that they get palliative care."

They want simple, explainable models. Now, sometimes you can't do it because the world's just complicated. If you're working at Google or Meta, mostly you're going to use a deep learning model, and who cares about interpretability? But that's a different course, or the second half of this one.

## Course Coverage Overview

So far, we've covered linear regression for the first approach. We're going to cover the other two today.

### The New Model for Basis Transformation

For the basis transformation, it's linear in the weights. If it's an L2 loss, we can solve it with linear regression.

Often what happens is you have some original feature set—which might be an image of a brain, or a vector, whatever—and you're going to transform it in some fashion where you want to learn a good transformation.

We're going to look at one class of transformations today, which are **kernel functions**. We'll look at other transformations in the future, but a lot of what you're doing is taking some sort of vector that describes the world and transforming it into some space which is better behaved and easier to predict with.

### Why Make Linear Things Nonlinear?

**Student Question:** If you have something linear, why would you make it nonlinear?

**Answer:** The simplest version: Let's say we have $X_1, X_2$—a two-dimensional feature space—and we've got four data points. You want to fit a model that says:

$$\hat{y} = c_0 + c_1 X_1 + c_2 X_2$$

What's the linear model that's going to predict these points? This is the famous **XOR (exclusive OR) problem**.

If you try—we'll talk about separating hyperplanes—there's no straight line you can draw that divides these things. There is no linearly separable hyperplane to divide these up. We're going to have to do some transformation of them somehow.

A lot of things in the world are often nonlinear. You can't just get away with taking a linear model.

Similarly, if I have something that's very high-dimensional, like an image of your face, putting that into a linear model is pretty crappy. I want some nonlinear transformation of everybody's face into some vector space where I can now classify who's got glasses or not. This doesn't work really well in pixel space, but works really well in some embedding spaces, which we'll get to again.

### Feature Transformation

**Student:** We are basically just using a different set of features, right?

**Answer:** Yes! We're using a different set of features. We're going to take some $\mathbf{X}$s and transform them into things I'll usually call $\mathbf{Z}$s.

So we say:
$$Z_j = \phi_j(\mathbf{X})$$

We're going to transform the $\mathbf{X}$s to a new feature set. Could be a linear transformation (like PCA), or it'll be a nonlinear function today.

---

## Logistic Regression

Time to move forward and cover each of these in some detail. The first one is: behind linear regression, **logistic regression** is the most widely used regression model in all of machine learning.

### Motivation: Binary Classification

The motivation I want to start with is the notion that we're going to have a bunch of $Y$s now that are either true or false.

If you're a statistician, you're going to call them 1 and 0. We're going to do something different in a second, but let's just start with a simple model.

I've got $X$ here, I've got $Y$ here. Assume that the labels are either 1 or 0. Make sense? The $Y$s are either 1 or 0.

If you're a computer scientist, mostly true or false.

### Can We Use Linear Regression?

**Student Question:** Can I fit a linear regression?

**Answer:** Why can't I fit a linear regression? Well, I *can* fit a linear regression—minimize the L2 norm of the distance. I can fit a linear regression using squared loss.

**Student Question:** Does this have Gaussian noise?

**Answer:** No, of course not! When you've got a binary outcome, the noise is nowhere near Gaussian. But I can fit a linear regression—I sometimes do if I'm really cheap.

What am I going to get out? I'm going to get out some $\hat{y}$, and the $\hat{y}$ we're generally going to call a **score**—a number where "more means more."

### Problems with Direct Linear Regression for Classification

**Is this $\hat{y}$ a probability?**

Sort of? Let's be clear: this prediction might be 1 over here, but the $\hat{y}$ might be, like, 1.7 over there.

I've got a friend from Texas. He saw a talk in the statistics department where some guy had a probability estimate of 1.5. He said, "I'm from Texas, I'm used to things real big, but I ain't never seen a probability 1.7 so big!"

(He was trying to embarrass the speaker—he's sort of a jerk occasionally.)

A probability of 1.7 is highly implausible, since probabilities are supposed to be bounded on $[0, 1]$.

So this gives me a score, but not a probability. I can take the score and threshold it—if I wanted, I could say anything over 0.5 I'll call 1 (or true), anything below 0.5 I'll call 0 (or false).

I could do that, and it's not terrible. But it'd be nice to have a model that makes more sense.

### The Logistic Model

That's going to be a model that gives some sort of a probability. We're going to get something that looks like a nice, smooth curve that's bounded between 0 and 1.

Now *that* is something that looks like a probability. For any $X$, it'll give me a probability that $Y = 1$.

Whether it's optimal or not depends on whether the model's good, but we can fit that sort of model. The way we're going to do that is with some sort of a **link function**.

This function that we have drawn up here is going to be some function where we say:

$$\hat{y} = f(\mathbf{W}^T \mathbf{X})$$

In this case, it's trivial because $X$ is a scalar, but in general, it's a function. So far so good.

Now the question is: what's a nice function? There are many functions you could use, but we'll see that one of them is nicer and widely used.

### Probabilistic Formulation

The other way to look at it: instead of just having a $\hat{y}$, I would like to have something that says:

I'd like to estimate the probability that $Y = 1$ given any $X$:

$$P(Y = 1 | \mathbf{X}) = f(\mathbf{W}^T \mathbf{X})$$

This will be the functional form of the probability distribution.

In linear regression, we said the probability of seeing a given $Y$ is $\mathbf{X}^T \mathbf{W} + \epsilon$, where $\epsilon$ is Gaussian.

Now, for logistic regression, we're going to say the probability of seeing $Y = 1$ given $\mathbf{X}$ is some function (which I'll write in a second) of $\mathbf{W}^T \mathbf{X}$.

A different probability model, which we can again optimize by maximizing the likelihood (or log likelihood) once we have the function written out.

Of course, the property is that:
$$P(Y = 0) + P(Y = 1) = 1$$

If it's binary, right?

**Note on Notation:** I'm being a machine learning person and I'm super sloppy about $Y$s and $\hat{Y}$s. For the first equation, $Y = \mathbf{W}^T \mathbf{X}$—that's $\hat{Y}$, and $\hat{\mathbf{W}}$ for that matter.

Realize that I'm going to be sloppy because machine learning people just drop the hats all over. My statistician friends are like, "It's not the same! $\hat{Y}$ and $Y$ are totally different! $Y$'s real, $\hat{Y}$'s an estimate."

But note that $Y = \mathbf{W}^T \mathbf{X}$ is really $\hat{Y}$—there's no $\epsilon$ there. Welcome to machine learning!

And again, if you know $P(Y = 1)$, you know the probability $P(Y = 0)$—they sum to one.

## The Logistic Function

We're going to use the **logistic function**. That's this nice little S-shaped curve here.

The logistic function is (machine learning people like to use an $h$):

$$h_\theta(\mathbf{X}) = \frac{1}{1 + e^{-\theta^T \mathbf{X}}}$$

Or in our notation with $\mathbf{W}$:

$$\hat{y} = \frac{1}{1 + e^{-\mathbf{W}^T \mathbf{X}}}$$

That's the link function that we will use, and that link function is called **logistic regression**.

### Properties of the Logistic Function

Think about it: if $\theta^T \mathbf{X}$ (the total input, as you'd call it in neural nets) goes to infinity, what does this go to?

$$\lim_{\theta^T \mathbf{X} \to \infty} \frac{1}{1 + e^{-\theta^T \mathbf{X}}} = \frac{1}{1 + e^{-\infty}} = \frac{1}{1 + 0} = 1$$

It goes to 1 as the input gets big.

As the input goes to negative infinity:

$$\lim_{\theta^T \mathbf{X} \to -\infty} \frac{1}{1 + e^{-\theta^T \mathbf{X}}} = \frac{1}{1 + e^{\infty}} = \frac{1}{\infty} = 0$$

When input goes very negative, it goes to zero; when it becomes very positive, it goes to 1.

The size of the slope controls whether it's a steep vertical transition ($e^{-a \cdot X}$ where $a$ is a big number makes it almost straight) or very flat ($e^{-a \cdot X}$ where $a$ is very small makes it very gradual).

We've got something that is guaranteed to give an output between 0 and 1, and is also monotone. It's going to keep everything convex, which will make us happy. It's just a nice functional form.

## A Notation Trick: Using ±1 Labels

What I want to do is a trick that machine learning people love to do. Statisticians and undergraduate computer scientists say: one is true and zero is false.

But that's asymmetric. It's prettier to write things out if you make true be +1 and false be -1.

It's completely arbitrary, but it's symmetrical now, and it just makes the writing of things way prettier. It lets us do a really weird hack which I both love and hate.

### Unified Probability Formula

We're going to say that our model is:

$$P(Y = 1 | \mathbf{X}, \mathbf{W}) = \frac{1}{1 + e^{-\mathbf{W}^T \mathbf{X}}}$$

Now, I'm going to say that $Y$ is either +1 or -1. So I can put, for this case, a $Y$ in here:

$$P(Y = 1 | \mathbf{X}, \mathbf{W}) = \frac{1}{1 + e^{-Y \cdot \mathbf{W}^T \mathbf{X}}}$$

This has no effect whatsoever when $Y = 1$, because multiplying by 1 does nothing.

Now let's look at the other case:

$$P(Y = -1 | \mathbf{X}, \mathbf{W}) = 1 - P(Y = 1) = 1 - \frac{1}{1 + e^{-\mathbf{W}^T \mathbf{X}}}$$

This equals:

$$= \frac{e^{-\mathbf{W}^T \mathbf{X}}}{1 + e^{-\mathbf{W}^T \mathbf{X}}}$$

I can now multiply top and bottom by $e^{\mathbf{W}^T \mathbf{X}}$, and I will get: the top becomes 1, the bottom second term becomes 1, and the first term becomes $e^{\mathbf{W}^T \mathbf{X}}$.

Which I'm going to write as:

$$= \frac{1}{1 + e^{\mathbf{W}^T \mathbf{X}}} = \frac{1}{1 + e^{-(-1) \cdot \mathbf{W}^T \mathbf{X}}} = \frac{1}{1 + e^{-Y \cdot \mathbf{W}^T \mathbf{X}}}$$

where $Y = -1$ here.

Now I've done something very funny: both the top case ($Y = 1$) and the bottom case ($Y = -1$) are written out exactly the same!

I've also done something slightly evil: I put the $Y$ into the function, which is not fair because it should be $Y$ as a function of $\mathbf{X}$. But hey, it's mathematically legit.

So now I can say, **regardless of whether $Y$ is +1 or -1**:

$$P(Y | \mathbf{X}, \mathbf{W}) = \frac{1}{1 + e^{-Y \cdot \mathbf{W}^T \mathbf{X}}}$$

You don't have to do this, but it makes it a clean way to cover both cases with a single expression. You'll see a lot of machine learning label $Y$ as +1 or -1 because it keeps life symmetric for the binary case.

**Student Question:** Is it still centered on zero?

**Answer:** It is, but remember: what does $\mathbf{W}$ have in it? There's a $W_0$ times a constant. In all my machine learning, I'm going to say: if you want something with a constant term in it, just make an $X_0$ which is 1, and a corresponding $W_0$. That's what makes it centered (or not) on zero—it's an adjustable parameter, the bias term $W_0$.

## Odds Ratio

The other thing that people like to talk about—a term you should just know—is the **odds ratio**.

What's the odds ratio?

$$\text{Odds} = \frac{P(Y = 1 | \mathbf{X})}{P(Y = -1 | \mathbf{X})}$$

That's the odds—a technical term. You can compute the odds, or people often compute the **log odds**:

$$\log(\text{Odds}) = \log\left(\frac{P(Y = 1 | \mathbf{X})}{P(Y = -1 | \mathbf{X})}\right)$$

That's a definition—that is the log odds ratio.

The log odds ratio, if you actually plug these two expressions in, comes out to be:

$$\log(\text{Odds}) = \mathbf{W}^T \mathbf{X}$$

So we have a model which says that **the log odds is linear in the $\mathbf{X}$s and linear in the $\mathbf{W}$s**.

People like doctors like this because now I can say: $\mathbf{W}^T \mathbf{X}$ for logistic regression gives me the log odds ratio. It's something that has a meaningful interpretation.

Now, the problem is it's not easy to optimize the log odds directly, so we're going to have to go back to the logistic formula (like the top one there) and optimize that one. But the log odds is a nice way to think about it.

## Maximum Likelihood Estimation for Logistic Regression

Now let's look at this model and do the MLE and MAP on it.

What is the log likelihood? It's the log of the probability of the $Y$s given the $\mathbf{X}$s and the $\mathbf{W}$:

$$\log P(\mathbf{Y} | \mathbf{X}, \mathbf{W}) = \log \prod_{i=1}^{N} P(Y_i | \mathbf{X}_i, \mathbf{W})$$

This is the probability:

$$= \log \prod_{i=1}^{N} \frac{1}{1 + e^{-Y_i \mathbf{W}^T \mathbf{X}_i}}$$

Remember, whether $Y$ is +1 or -1, that's the probability. This makes the math really clean.

We take the log of the product—that becomes the sum of the logs:

$$= \sum_{i=1}^{N} \log \frac{1}{1 + e^{-Y_i \mathbf{W}^T \mathbf{X}_i}}$$

$$= -\sum_{i=1}^{N} \log(1 + e^{-Y_i \mathbf{W}^T \mathbf{X}_i})$$

where I've just used the fact that the log of $\frac{1}{\text{something}}$ is minus the log of that something.

### What Have We Shown?

I've shown here that now we have something that is, in fact, related to the log odds.

Before, for linear regression, this was beautiful—it was clean, it was quadratic. Now it's ugly. Can't help you here, but there's one piece of good news: **it's convex**.

So you can solve it with gradient descent! And you don't have to implement it yourself, because every package in the world has a built-in gradient descent to solve it.

But it's nice, right? We have something that's not a gorgeous closed-form formula, but in fact it's convex, so who cares? And it's pretty easy to run gradient descent on it.

We now have something that says: this is the thing we're going to do. We're going to maximize the log likelihood (or minimize the negative of it), and we can do gradient descent.

**Student Question:** How do we know this is convex?

**Answer:** Take my theorem proof class, I guess! The log is a convex function, it's a monotone function. $1 + e^{-\text{something}}$ is a monotone function, so you've got two monotone functions composed, so the whole thing is monotone. That's the short answer.

The other version—which is not very hard to prove—is you can take any two points, draw a line between them, and you'll show the curve is always underneath it. That's not a very hard proof to do.

## Decision Boundaries and Hyperplanes

Now we can get to one more thing, which is nice math and a way to think about these functions.

As I said, we fit a function which is this logistic function, which gives a probability. But in general, what we want is something that looks like a **decision boundary**.

That's something that says: here's a place on $X$. If I put the line here at 0.5, everything above this I'm going to label as +1, everything below this I'll label as 0 or -1.

I want to know: what's that boundary that separates the yeses from the nos?

### Finding the Decision Boundary

We can just solve for it. That boundary is the point at which the probability it's +1 is equal to the probability it's -1, which will be 0.5.

You really shouldn't be scribbling notes—you should just be looking and thinking. (The slides are up anyway.)

This is just:

$$P(Y = 1 | \mathbf{X}) = P(Y = -1 | \mathbf{X})$$

They're both normalized. We set them equal, we cancel out the denominator, and we get:

$$1 = e^{-\mathbf{W}^T \mathbf{X}}$$

When is $1 = e^{-\mathbf{W}^T \mathbf{X}}$? When it's $e^{0}$!

So:

$$\mathbf{W}^T \mathbf{X} = 0$$

is the definition of the dividing line—the **hyperplane** at which the two labels are equally likely.

**Student Question:** The odds is 1, right? That means the odds are 1?

**Answer:** Yes! When the odds are 1, it's equally likely to be the two classes. I could have done the same derivation that way. Equally good proof.

### Understanding Hyperplanes Geometrically

We need to digress and talk about what a hyperplane means, because I said that we want to have this sort of separation.

What was I trying to say? I want to understand geometrically what's happening with this hyperplane that separates things.

In general, if I do this in two dimensions, what I'm going to have is some points that are labeled +1 and some points that are labeled -1, and some sort of line that separates the ones from the minus ones—where the odds ratio is equal. Make sense?

I'm going to draw things in lower dimensions, but let's think about what things look like.

If this is $X_1$ and $X_2$, I'm in a two-dimensional space, and the separating hyperplane is one-dimensional (a line).

If I'm in three dimensions, what does the hyperplane look like? It's two-dimensional (a plane).

If I'm in $p$ dimensions (I can't draw that), what does this separating hyperplane look like? $p-1$ dimensional.

### Defining a Hyperplane

The way to define something which is a $(p-1)$-dimensional hyperplane—and I hate the fact that in two dimensions, this happens to be a one-dimensional hyperplane—the way to define that is by defining the **orthogonal line**.

What's orthogonal? That means there's a 90-degree angle. Note that if this were a two-dimensional hyperplane, it's still a one-dimensional line that's orthogonal to it.

This hyperplane IS the plane on which:

$$\mathbf{W}^T \mathbf{X} = 0$$

That's the place that's equally likely for the two classes, and that defines it.

There is then a line that pops out which points in the direction $\mathbf{W}$. Because what does $\mathbf{W}^T \mathbf{X} = 0$ mean? You're taking the dot product and saying it's 0.

So the line given by $\mathbf{W}$ gives me the definition of the perpendicular $(p-1)$-dimensional hyperplane that separates the classes.

### Example: Line $Y = X$

Here's a simple example: if we have something where we have $X_1$ and $X_2$ (call it $X$ and $Y$), and the line $Y = X$ goes through here, you could write that as:

$$0 = X - Y$$

You can write that as:

$$\begin{bmatrix} 1 \\ -1 \end{bmatrix}^T \begin{bmatrix} X \\ Y \end{bmatrix} = 0$$

This is the $\mathbf{W}$: $[1, -1]$. This is the $\mathbf{X}$ vector: $[X_1, X_2]$ or $[X, Y]$.

The vector $[1, -1]$ is orthogonal to the hyperplane $Y = X$. That's the exact same thing I did here.

**Student Question:** The decision boundary is the place where the probability that $Y = 1$ is equal to the probability of $Y = -1$?

**Answer:** Yep. And where is $P(Y=1) = P(Y=-1)$? Well, since probabilities range from 0 to 1, it's at 0.5. The probability is 0.5. I didn't write it down when I solved it, but when the probability of -1 equals the probability of +1, that's got to be at 0.5. The two of them sum to 1; if they're equal, they're each 0.5.

Points below that decision boundary will be labeled -1 (or false), points above will be labeled +1 (or true).

I know I'm swapping between 0/1 (which statisticians like) and ±1 (which machine learning people like).

**Student Question:** Why do we plug in $Y$?

**Answer:** We plug in $Y$ only because it's easy to write down the probabilities that way.

If you try and write the probability of each point, some of the points will be +1 and some will be -1. You could write—remember we did the first coin flipping example? We had one term for the heads and one term for the tails.

I've now cheated: instead of having a probability of all the heads and a probability of all the tails, this is the probability of either heads or tails, because the heads are +1s and the tails are -1s, and they each give the correct probability here.

This allows me to not write a separate summation over the heads and over the tails like I did the first time. I've now slammed them all together in one equation because it's one or the other. This is the right formula for the +1s, and this is the right formula for the -1s. It just makes the notation very, very clean, even though it's non-intuitive.

**Student Question:** We know the vector by the hyperplane, which is perfect, but what we haven't discussed is how that split...

**Answer:** We did! I proved that if you solve for $P(Y = 1) = P(Y = -1)$, the solution for logistic regression is $\mathbf{W}^T \mathbf{X} = 0$. That is the definition of "equally likely to be heads and tails."

My claim is that that is, in fact, the definition of a hyperplane. And what I just showed was that that hyperplane is orthogonal to $\mathbf{W}$, which is sort of trivial because $\mathbf{W}$ is orthogonal to $\mathbf{X}$ here in the dot product.

**Student Question:** From the data, how would people draw the hyperplane?

**Answer:** How do I draw the hyperplane? Here it is! The hyperplane separates things right along here. This is $\mathbf{X}$, this is the hyperplane. Above this, it's +1; below it, it's 0 (or -1).

I'm having trouble answering the question clearly. The hyperplane geometrically divides the $p$-dimensional space into two subspaces. One side is -1, one side is +1.

The data points are points in a $p$-dimensional space. Let me just do this in two dimensions: if you've got a bunch of points, I've got some hyperplane that will separate the things into two parts. I label this side +1 and this side -1.

Once I fit it, the points are sitting in $p$-dimensional space. The hyperplane is a $(p-1)$-dimensional plane. It has a one-dimensional orthogonal line.

**Student Question:** Would that point be marked as -1 in this case?

**Answer:** Everything to the left is labeled as -1. Yep, life is wrong, and there are two wrong labels over here. I can't help you—things are often misclassified.

## Summary of Logistic Regression

Once we have this, the interpretation is:

We have a **score**, which is $\mathbf{W}^T \mathbf{X}$.

- If it's positive, we say the label is +1
- If it's negative, we say the label is -1

That's the scoring function that transforms into a label. My hypothesis is now a binary hypothesis, which is precisely this function.

If you have +1 and -1 as your labels, it's:

$$h(\mathbf{X}) = \text{sign}(\mathbf{W}^T \mathbf{X})$$

### How Do You Solve It?

Use **gradient descent**. You update the weights by having a learning rate times the gradient with respect to the loss function.

I'm not going to read it, you're not going to read it—take the derivative and compute it. It's straightforward to compute. I'm going too fast to read it? Yes! Because you don't care. It's just gradient descent.

### Maximum A Posteriori (MAP) Estimation

We can do the same thing as with linear regression: we can have a prior, we can assume the weights are given by a normal distribution:

$$P(\mathbf{W}) = \mathcal{N}(0, \gamma^2 I)$$

If you plug things in—and again, I'm going to go too fast because there's conceptually nothing different from the linear regression MAP—now we're going to have something that says it's the original loss we had before:

$$-\sum_{i=1}^{N} \log(1 + e^{-Y_i \mathbf{W}^T \mathbf{X}_i}) - \frac{1}{2\gamma^2} \mathbf{W}^T \mathbf{W}$$

Just like before with linear regression: if you start with a Gaussian prior over the weights, you'll get an L2 penalty. It's the exact same derivation as before, because you're maximizing the log likelihood of the data plus the log of the probability of the weights.

So I get a penalty that looks just like I had before. Everything's the same, except it's now nonlinear. But it's convex, so I don't care!

## Multi-Class Classification (Softmax)

One quick aside: this is binary classification. If it's three-level class, or a thousand-level class, or a 10,000-level class, you can do pretty much the same analysis.

It looks almost the same if you're doing multi-way classification—like which of the diseases is it, which of the objects is it. It's common to have 1,000 or 10,000 different objects or diseases you might want to classify.

Instead of a binary classification with a single hyperplane that splits the world, in multi-class we'll get a bunch of hyperplanes that divide things up.

### From Binary to Multi-Class

For two classes, we said:

$$P(Y = 1) = \frac{1}{1 + e^{-\theta^T \mathbf{X}}}$$

Which I can rewrite by multiplying by $e^{\theta^T \mathbf{X}}$:

$$P(Y = 1) = \frac{e^{\theta^T \mathbf{X}}}{1 + e^{\theta^T \mathbf{X}}}$$

This form is really symmetric!

For $K$ classes, I can do something really cool. I can say the probability of each class is:

$$P(Y = k | \mathbf{X}) = \frac{e^{\theta_k^T \mathbf{X}}}{\sum_{j=1}^{K} e^{\theta_j^T \mathbf{X}}}$$

### Geometric Interpretation

Let me try to get this in a clear direction. If I have something that splits the world up into three classes, I can say:

- For this class here, there's a vector $\theta_1$ that points to the class 1 region
- For this class over here, there's a vector $\theta_2$ that points to the class 2 region  
- For this class over here, there's a vector $\theta_3$ that points to the class 3 region

For each of the classes, I'm going to have something which is proportional to how likely it is. The more it's in this direction, the more likely it's class 1. The more it's in this direction, the more likely it's class 2. The more it's in this direction, the more likely it's class 3.

The likelihoods are $e^{\theta_k^T \mathbf{X}}$. They now have to be normalized to be a probability, so you sum over all $K$ of the classes.

I've now taken something that was asymmetric (the ±1 formulation) and made it so I don't care how many classes there are—it ends up looking the same. Because for each of the $K$ classes, I have a direction pointing there, and now I just normalize to make it a probability.

### The Softmax Function

We get one more piece of jargon. Everything has a different name in deep learning. Deep learning people like to talk about a **softmax function**.

We're going to be doing mostly classification problems with a bunch of classes, and we're going to try and estimate the probability of each of the classes. Sometimes we only care about the one we get right—we'll worry about different loss functions later.

But it's nice to say that we can take $K$ classes and have something that takes any vector $\mathbf{X}$ in the embedding space and maps it into a probability distribution over $K$ classes:

$$\text{softmax}(\mathbf{z})_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}$$

where $z_k = \theta_k^T \mathbf{X}$.

Make sense? Super widely used in deep learning!

**Student Question:** By using equal probability, are all hyperplanes...?

**Answer:** We've assumed so far that the decision boundaries are hyperplanes because we're in the generalized linear model world.

Now, after midterm, we'll shift to deep learning. And shortly, we'll shift to kernels where we will transform the features. We're going to do that today and Wednesday: we'll say, "Hey, take the features $\mathbf{X}$ and transform them," and it'll be linear in the transformed features.

Later, after break, we'll go and say, "Okay, we'll take each of the transformed features and combine them and transform them and link them into a whole bunch of functions again."

So we're gradually building up more and more complex pieces. But I'm doing it piece by piece, because for simple, small problems, it's super important to choose what things you want:
- Choose what link function you want (of which the logistic link is the most popular)
- Choose what feature transformations you want (of which probably Gaussian kernels we'll cover are the most popular feature transformations)

We'll see lots of link functions, lots of transformations, and lots of loss functions.

But again, I want you to remember this piece that says: here's a magic equation that says, you give me any vector $\mathbf{X}$ that describes your features (or transformed features—they could be in the middle of a neural net). Give me that vector $\mathbf{X}$, and I can transform it into $K$ probabilities.

Those $K$ probabilities can be expressed in terms of $K$ different vectors $\theta$, each of size $p$. That's the softmax function—super useful!

**Student Question:** It's symmetric, right?

**Answer:** Yes! It's totally symmetric. $K$ classes, all $K$ classes are exactly the same. If you want, you could say class 1 is magic and the other ones are all different—do an asymmetric version. I don't like it; it's ugly. It's just much cleaner to treat all $K$ classes symmetrically. You can run either math, but the math here is much nicer if it's symmetric.

**Student Question:** Why are we normalizing in the denominator? What's the point of this denominator?

**Answer:** It normalizes it to make it a probability! If you want a probability, it's got to be normalized; otherwise it's not a probability.

If you don't normalize it when you go to do your computations, you can choose not to normalize it, and then you get an energy or something weird. But neural nets are way more stable if it's a probability, because it's guaranteed to sit between 0 and 1 and not go spinning off to infinity.

Normalizing to lie between 0 and 1 just keeps the whole thing way more stable. And if you're a doctor, it's interpretable: you've got a probability of 0.7 of cancer and a probability of 0.2 that it's just your hemorrhoids acting up. (I don't know—I don't know anything about medicine!)

**Student Question:** You're isolating each hyperplane at a time, and then you're isolating the data...

**Answer:** What this gives me: for any given point $\mathbf{X}$, it will give me out $K$ numbers. Then if I want to, I can pick the maximum—the argmax—which direction has the highest probability.

In general, we're sort of greedy in machine learning. We're going to plug in each prediction (each point we want to predict), compute $K$ numbers, and whatever the biggest probability is, that's the answer.

**Student Question:** What's special about the exponential function?

**Answer:** The exponential function has a couple of properties:
1. It ranges from 0 to infinity, so it covers the whole range
2. It's non-negative, which is nice for a probability
3. It's monotone
4. It's easily differentiable

So it's a very pretty function. Everything ranges from 0 to infinity, so you get the full range of probabilities covered when you normalize it, and it never goes negative. It's just easy to take the derivative.

In some sense, it's not magic-magic, but it's very nice to have something that maps any vector $\mathbf{X}$—in this case to the non-negative numbers. Make sense? And the derivatives are super nice.

It's pure mathematics—otherwise it's not magic, but it's what everybody uses because it's so clean.

---

## Summary: Logistic Regression and Link Functions

We talked about the logistic model, which had a transformation function:

$$h(\mathbf{X}) = \frac{1}{1 + e^{-\mathbf{W}^T \mathbf{X}}}$$

which gives you a probability of $Y = 1$.

We showed that you can do the same thing for $Y = -1$, and that's a trivial symmetry. Or you could do $K$ classes.

We showed that this gives you a boundary that separates (for the binary case) the yeses from the nos, and that's a hyperplane defined by $\mathbf{W}^T \mathbf{X} = 0$.

We saw the softmax function for multi-class classification.

Take a deep breath! We still have almost a half hour, and we've just talked about one link function.

Logistic regression is the popular link function. When you get to neural nets, we'll see a bunch of other link functions. But I could, in theory, put anything I want for $\mathbf{W}^T \mathbf{X}$: I could put exponential, or log, or sine, or any link function I want.

The neural net people—I think today Swish/SiLU is one of the most popular link functions (activation functions). Last year it was ReLU. There's a gazillion link functions that people use, and we'll talk about some more of them. Wednesday we'll talk about hinge loss, which is another kind of link-type function.

There are a million link functions out there, and they keep changing based on compute and whatever. But the important point is: you could put whatever one you want in, preferably one that's monotonic, differentiable, and cheap to compute.

### Key Insight About Link Functions

The other thing to note is that it's sort of like thinking:

$$f^{-1}(\hat{y}) = \mathbf{W}^T \mathbf{X}$$

is linear. But there's no way to optimize $f^{-1}(\hat{y})$ directly. The log odds, for example, you can't optimize directly. But you can optimize the link function itself.

---

## Part 2: Basis Functions and Feature Transformations

Now, instead of putting a link function, we do something totally different.

We're going to say: let's let $\hat{y}$ be a linear function of some **transformed features** plus some noise $\epsilon$ for the $Y$:

$$Y = \sum_{j=0}^{K} W_j \phi_j(\mathbf{X}) + \epsilon$$

to make it linear regression. (The $\hat{y}$ doesn't have the $\epsilon$; the $Y$ would have the $\epsilon$.)

In general, once again, I'm going to pick $\phi_0$ to be trivial—it's the constant term. So $\phi_0$ will tend to be: just take $\mathbf{X}$ and give me the number 1. So I have a constant term, or a bias, if you prefer.

### Types of Basis Functions

You could trivially make each function $\phi$ just pick out one of the features—that gives you linear regression:
- $\phi_1$ gives you $X_1$
- $\phi_2$ gives you $X_2$  
- Up to $\phi_p$, so you get all $p$ features

You can put **polynomials** (which I hate). You could put **Gaussians** (which I love).

### Polynomial Basis Functions (Not Recommended)

Here's a polynomial basis function. For each feature, I could take any $X$ and take it to the $j$-th power. So I could say:

$$\hat{y} = W_0 + W_1 X_1 + W_{11} X_1^2 + W_{111} X_1^3 + W_2 X_2 + W_{22} X_2^2 + W_{12} X_1 X_2 + \ldots$$

I can put whatever I want—these are all transformations of $\mathbf{X}$.

In general, going up to things like cubics is a **bad idea**.

Let's do it in one dimension. I've got my data—there it is. There's $X$, there's $Y$:
- If I fit only the constant term, I get a flat line
- If I fit a linear regression, I get a sloped line  
- If I fit a quadratic regression, I'll get a parabola
- If I fit something with a cubic, I'll get who knows what!

Once you start putting cubics or quartics or quintics, you get things really soon that fit data like this—they do a really good job in-sample, but the extrapolation is really bad.

And we're typically in high dimensions—1,000 or 10,000 dimensions. **Everything's an extrapolation.**

So in general, I don't like polynomial basis functions.

### Gaussian Basis Functions (Recommended)

**Nicer are Gaussian basis functions** (also called radial basis functions).

What I can say is: for each $\mathbf{X}$, I'm going to transform it with some function:

$$\phi_j(\mathbf{X}) = e^{-\frac{\|\mathbf{X} - \mu_j\|^2}{2s^2}}$$

What am I doing? I'm saying: take the distance between $\mathbf{X}$ and some point $\mu_j$, square it, have some scaling function $s$ that looks like a standard deviation, and take $e$ to the minus that.

**Note:** This is not a probability—there's no normalization here. This is a **kernel**.

### What Does a Gaussian Kernel Measure?

It measures **how similar $\mathbf{X}$ is to the point $\mu$**.

Think about it:
- If $\mathbf{X}$ and $\mu$ are the same (if $\mathbf{X}$ is on top of $\mu$), the distance is zero, so the similarity is $e^0 = 1$
- If $\mathbf{X}$ is right there at $\mu$, the similarity is 1
- If $\mathbf{X}$ gets infinitely far away from $\mu$, the similarity goes to 0

So picture it: it's a Gaussian function (a "bump" function). $\mu$ is at the center, and the farther $\mathbf{X}$ is away, the value goes down exponentially quickly. Make sense?

### Using Multiple Centroids

Now what I can do is: I can take any $K$ centroids I want—any $K$ points $\mu_1, \mu_2, \ldots, \mu_K$.

I can take my $N$ initial points $\mathbf{X}$, and I can say: for each of these $N$ points, how similar is it to each of the $K$ centroids?

Now what I've done is a funny nonlinear transformation:
- I started with a $p$-dimensional vector (the matrix $\mathbf{X}$ is $N \times p$)
- I took something of size $p$
- I then said: how similar is that vector (size $p$) to each of $K$ different points?
- I've now transformed it to a different space of dimension $K$

**Is $K$ bigger or smaller than $p$?**

You could make $K$ smaller and reduce the space. Could I make $K$ bigger? Yeah! You can make $K$ as big as you want, but usually not bigger than $N$.

I could say: how similar is each point to every other point?

So you can either:
- **Shrink the space** ($K < p$)
- **Keep it the same** ($K = p$)  
- **Make it bigger** ($K > p$)

There's no constraint—it's your choice.

### Why Would I Want to Make It Bigger?

If it's the case that my world is nonlinear, let me think about this:

**I'm measuring similarity.**

Let's think about this for a diagnosis case:
- I've got a training set  
- I've got a set of labels on people
- Each person's a vector
- I can take $K$ people as prototype vectors (maybe one for each of the different diseases, or whatever)—they're the centroids, or $K$ prototype images

I can say: how similar is this image to each of these $K$ points? How similar is this person to each of these $K$ people?

Then I'm going to get a new vector, which is a vector of $K$ similarities.

**Are people following me?** Because it's super important that you can do this both to:
- **Reduce the dimension** (which you do a lot because your original dimension is too big)
- **Increase the dimension** (to capture nonlinear relationships)

### High-Dimensional Formulation

Let's write this out a little bit more cleanly for the high-dimensional case.

What am I looking at for the Gaussian kernel? I'm saying:

$$\phi_j(\mathbf{X}) = e^{-\frac{\|\mathbf{X} - \mu_j\|^2}{c}}$$

This takes any $\mathbf{X}$, and for each of my different $j$'s (the different centroids), that's $e$ to the minus the distance between $\mathbf{X}$ and $\mu_j$ squared, over some constant $c$.

I can put $2\sigma^2$ or whatever I want for normalization, as long as it's non-negative. But note what I'm saying is: **how close is $\mathbf{X}$ to each of these points?**

Note this trick that I took: this thing is a distance. I took the distance, squared it, scaled it by whatever I want, and took $e$ to the negative of that. This takes the distance to a **similarity**.

**Student Question:** What dimensions?

**Answer:** The question I have here is: $\mathbf{X}$ is dimension $p$, but the number of features I'm using (after transformation) is size $K$ (or some different number). So I'm transforming from one space to another.

Let me think about what that looks like. People often, instead of $2\sigma^2$ here, like to write $c$ for a constant. I don't care, as long as it's non-negative, because I want the negative of a positive number. When you get farther away from something, the kernel gets smaller.

## Example: Fitting with Gaussian Basis Functions

Let's try and work an example. I want to fit $y$ as a function of $X$, and I've got some data that does whatever it wants—a bunch of nonlinear data.

But I want to fit a linear regression to this after transforming in some fashion.

### Step 1: Pick Centers

What I'm going to do now is pick some centers:
- Here's $\mu_1$ (I picked one center)
- Here's $\mu_2$ (another center)
- Here's $\mu_3$ (another center)

(Later we'll talk about how to pick them optimally.)

So I've changed $X$ from a one-dimensional $X$ to a three-dimensional transformed space.

### Step 2: Create Gaussian Functions

I can now say: I'm going to put one Gaussian centered here (looks like a bump), one Gaussian centered here (another bump), and one Gaussian centered here (third bump).

I've got 3 Gaussians: $\phi_1$, $\phi_2$, $\phi_3$.

### Step 3: Transform Each Point

For any $X$, I can say:
- How far is it from $\mu_1$?
- How far is it from $\mu_2$?
- How far is it from $\mu_3$?

Make sense? You can see that this $X$ (pointing to one location) is almost entirely $\mu_1$ (high value from the first Gaussian), and it's pretty much zero from $\mu_2$ or $\mu_3$ because they're Gaussians—the value goes down to zero as you get farther away.

### Step 4: Linear Combination

Now I can say my estimate is going to be:

$$\hat{y} = \sum_{j=1}^{3} W_j \phi_j(\mathbf{X}, \mu_j)$$

where $\phi_j(\mathbf{X}, \mu_j)$ is the $j$-th Gaussian kernel.

Every one of these points is now a three-dimensional transformed vector. For every $X$, I also have a $Y$ (which is the exact same $Y$ I had before).

I can now fit with **linear regression** the weights on these transformed features (I like to call them $\mathbf{Z}$s).

### Step 5: Making Predictions

What I get is something that says: I fit a regression, and now when I go to plug in a new point (pick some point, like here), and ask for a prediction at this point, it's going to be:

$$\hat{y} \approx W_1 \cdot (\text{value from } \mu_1) + W_2 \cdot (\text{value from } \mu_2) + W_3 \cdot (\text{value from } \mu_3)$$

where:
- $W_1$ is probably roughly the average of the $Y$ values near $\mu_1$, times how far the new point is from $\mu_1$
- $W_2$ is roughly the average of the $Y$ values near $\mu_2$, times how far the new point is from $\mu_2$
- $W_3$ is roughly the average of the $Y$ values near $\mu_3$, times how far the new point is from $\mu_3$

The contributions from centers that are far away will be near zero.

### What's Happening?

We're doing **more weight to things that are close, and less to things that are far away**.

**What does this vaguely feel like? What's the closest method we've seen?**

It's sort of like **$k$-Nearest Neighbors**!

But it's not exactly $k$-NN. It's importantly different:
- In $k$-NN, if you're in a very sparse space, your $k$ neighbors are far away
- If you're in a dense space, your $k$ neighbors are close
- Here, I'm using the same distance metric everywhere (controlled by the width parameter)

But it's got some of the same sort of flavor.

### Training and Prediction

**Student Question:** (clarification about the process)

**Answer:** For a given $\mathbf{X}$, I'm going to compute how far it is from each of the three centroids. Then I'm going to train—do that with all the data, use linear regression, fit the weights.

Then I have the weights. Now when I get a new point, I just plug into this formula: I compute the three values (the three kernel evaluations), and sum them weighted by the learned $W_j$ values.

But the intuition is: what's going to happen de facto is that for a new point, it's mostly going to be predicted by other points that are similar to it, because they're the ones that contribute. The ones that are far away don't have a big effect.

So I can take something which is a **linear model in the transformed space**, and that still allows me to make a **very nonlinear prediction** in the original space.

This function can go up and down and up—it's quite nonlinear. I can fit it by a linear regression on the transformed features.

### Gaussian Radial Basis Function Formula

For the screen: for a radial basis function, it's:

$$\phi_j(\mathbf{X}) = e^{-\frac{\|\mathbf{X} - \mu_j\|^2}{c}}$$

## Choosing Hyperparameters

**Student Question (in back):** Are the $\mu$ learnable?

**Answer:** Yeah! We haven't said yet how to pick the $\mu$s or the $c$s. We have to pick $\mu$ and $c$ somehow. I'm going to take a few more minutes for questions, then we're going to come back and talk more about kernels on Wednesday.

### How Many Centers? ($K$)

**Student Question:** All right, $p$ is what?

**Answer:** It's a **hyperparameter**! The bigger it is, the more complex the model, the better it will fit, the more it will overfit. The higher the variance, the lower the bias.

So we've got a hyperparameter: that's $K$ (the number of centers). Welcome to my world!

**How do you find $K$?**

If you're an engineer: try a bunch of values. The more principled approach: **cross-validation**.

### Choosing the Width ($c$)

We've got to pick $K$, $c$, and the $\mu$s. Let's first talk about $c$.

If I make $c$ really big:

$$\phi(\mathbf{X}) = e^{-\frac{\|\mathbf{X} - \mu\|^2}{c}}$$

If $c$ goes big, what do the Gaussians look like? **Really flat!** You've smeared everything out together, and you're going to get one average, which is sort of useless.

If $c$ goes to 0, what do the Gaussians look like? **Spikes!** You've got to be right on top of it.

**Somewhere in the middle is the happy $c$.**

That's determined by **cross-validation**—check to see which $c$ gives you the right amount of smoothing. You can see that a big Gaussian is very smooth, a small one is very spiky, and somewhere in the middle is probably the optimum.

### Choosing the Centers ($\mu$)

So far, that was easy. Now I've got to pick the $\mu$s.

The standard way to pick the $\mu$s is a technique we won't cover for another 6 weeks, which is called **$k$-means clustering**.

The simple answer is that, in general, what people typically do for these is to:
1. Take your $N$ data points
2. Pick your hyperparameter $K$  
3. **Cluster the points into $K$ different groups**
4. **Pick the center of each group** as $\mu_j$

After we cover clustering, we will go back and show that this is, in fact, maximizing some likelihood in Gaussian space. It is minimizing some L2 loss in a certain space, and you can solve the whole thing with an iterative algorithm that will be an EM algorithm or iterative gradient descent. But that's a whole separate weeks of stuff.

For now, the answer is: you can find the $\mu$s. You could try doing gradient descent to find the $\mu$s, but in practice that converges really badly.

It turns out that we will need a whole new class of **alternating optimization methods** to find the $\mu$s, so I'm going to punt on that for a long time.

**So for now:** run $k$-means clustering. You:
- Pick the $K$ (hyperparameter over the number of clusters)
- Hyperparameter-tune the $c$ (how wide they are)
- Use $k$-means (a magic algorithm we'll derive later in this course with an ML/MAP framework) to give us the $\mu$s

That's pretty much the story.

## Expanding vs. Reducing Dimensionality

I'm going to say this once again, because people always somehow—when you go to pick how many clusters—everybody somehow thinks: "I've got to make the space smaller."

**No, you don't!**

Often it's good to **make the space bigger**, because bigger lets you be close to someone who's really weird here, and really weird there, and really weird over there. It lets you fit nonlinear functions.

So it's super nice to be able to actually **expand the feature space** to be bigger, or to contract it.

### Example: Medical Imaging

If I'm doing brain images:
- I might have 100 patients and 100 images  
- What do I want to do? What do radiologists do?
- Take this image, find a patient with a similar image (whatever "similar" means), and that's probably what your diagnosis is
- Or pick the three most similar patients (that's $k$-nearest neighbors)

This approach says the same sort of thing: compute a similarity between the two images (probably in an embedded space from a neural net), find the similarity, and now I can do a function that says:

As a function of how close this image is to these labeled images I have, I can now build a linear regression model that predicts probability of having cancer.

So that's a **shrinkage** (dimensionality reduction), because images are in a big space. Brain images in 3D are big images—think of a brain image, it's $X \times Y \times Z$. They're huge.

So those I want to **reduce dimension**.

But if I've got a small number of features, often I want to **blow up the dimension** to capture nonlinearity.

## Summary: Radial Basis Functions

This is the summary. **Radial Basis Functions** is the brand name for this method.

### The Algorithm:

1. **Cluster** all your training data into $K$ clusters using $k$-means
2. **Put a Gaussian function** at each cluster center
3. **Pick how wide the Gaussians are** (the $c$, or $2\sigma^2$)
4. **Fit a linear regression** on the transformed features

That's it! Once more, now in writing.

## Preview: Kernels and Kernel Regression

What we're not going to cover today, but we're going to come back to on Wednesday:

We're going to come back and talk about how these Gaussian kernels are **kernels**—they're an example of another sort of nice, clean piece of math that shows up in things like CNNs (convolutional neural nets).

Kernels are used in many contexts. There's lots of nice math about kernels. They can also be used for **kernel regression**, which we will cover on Wednesday.

So Wednesday I'll talk formally about:
- What a kernel is
- You'll see a bunch of different kernels
- All this kernel regression material

We'll cover all of that, but the **radial basis function** material is the stuff we've seen now.

I've got 2 more minutes. Let me just say that again: don't worry about the kernel regression slides until next time. We're going to cover all of that.

## Quick Poll

Let me do a quick show of hands:
- How many people think it was **too fast**? (some hands)
- **Too slow**? (few hands)
- **Pretty good**? (most hands)

Oh, we're pretty happy!

I won't ask about the board vs. iPad—the people that are remote might have had issues, but if you came in and missed things, make sure to get signed in. Come downstairs and get into the A+ system for attendance.

I think we're all good. **I will see you on Wednesday to talk about kernels and kernel regression.**

The attendance is always done with the A+ system.

**Student Question:** How do I scan it correctly?

**Answer:** I will bring it up...

(Shows QR code on screen)

There we go! You've missed the code if you haven't scanned yet—it's not too late! These folks are busy scanning.

**Student Question about maximizing likelihood...**

(End of lecture)

---

**Note:** This lecture covered:
1. **Generalized Linear Models** - focusing on logistic regression as a link function
2. **Binary and Multi-class Classification** - including the softmax function
3. **Decision Boundaries and Hyperplanes** - geometric interpretation
4. **Basis Functions** - particularly Gaussian radial basis functions for nonlinear transformations
5. Preview of **Kernel Methods** to be covered in the next lecture

---

# CIS 5200: Machine Learning - Lecture 8: Support Vector Machines and Kernel Methods

## Introduction and Testing

Now, how does that work? Testing, testing?

Ah, awesome, okay, good, good, good.

Cool! Um…

Hello, computer, talk to me.

Okay, so…

## Kernels

### Definition and Formalism

First topic, kernels.

And we've seen, sort of, examples of kernels, but I want to be a little bit more formal. We talked earlier about a distance and a metric.

There's no single definition of similarity, but the most common way that you measure how similar two vectors are uses a kernel.

A kernel is a function, a kernel function, which we'll call $K$, which takes in two vectors of the same size, call them $X_1$ and $X_2$, or $X$ and $Y$—two vectors.

And it returns something that will be positive semi-definite, which we'll get to in a second. We're going to talk much later in this course a lot about matrices, and the same way that numbers can be positive, or zero, or negative, matrices can have eigenvectors which are positive, or zero, or possibly negative. When we talk about norms of matrices, which we'll get to eventually, we'll worry about the eigenvectors, but now we're going to be sort of quick and dirty about it.

### The Gaussian Kernel

And the sort of standard kernel that we will see over and over and over again is the **Gaussian kernel**. We talked about that last class.

The Gaussian kernel takes in two vectors, computes the $L_2$ distance squared:

$$K(X_1, X_2) = e^{-\frac{\|X_1 - X_2\|_2^2}{C}}$$

divided by a constant $C$, a non-negative, a positive constant. And that gives you something that tells you roughly how—well, a measure of how dissimilar the vectors are.

Make sense? Note that you can, if you want, plug in any distance measure you want. Right? The Gaussian is, by definition, $L_2$, but you could do $L_1$, or $L_3$ or something silly if you wanted.

Make sense?

### Properties of Kernels

And why are they nice?

As the distance goes to infinity, the similarity goes to zero. So it's a way to do local weighting of things, and almost all machine learning says, we're going to try and smooth a bunch of local things, and ignore the things far away. We start with k-nearest neighbors, that grabbed the $K$ closest things and smooths them, weighting them all equally.

But arguably what you should do is weight everything—if cost were no object, I mean, imagine you're a mathematician—weight every data point, but just weight them based on how similar they are to the one you're… the point you're looking at.

Makes sense? So kernel's got this nice measure of similarity.

Cool. Um, they show up in radial basis functions, which we saw last time, and I'll quickly review. They show up in kernel regression, which I will cover quickly today, and they show up in support vector machines, as we'll see at the very end of today.

### Formal Definition

Cool. There is a formal mathematical definition, which I'm not gonna walk you through it, I'm not gonna use it extensively. But just to note that it has a formal definition—I know this will be very frustrating today. So it's like, yeah, you can do a bunch of stuff, and if you do enough, you know, it's got a nice math definition.

Um, but what we will do is talk about the way that you actually test something, which I think gives a sort of a more clear intuition.

### Kernel Matrix

And the idea is straightforward. If we take any two pair of points, so we're gonna take $N$ endpoints, we'll take all the pairwise comparisons. That would be $N^2$ comparisons. That will then give us a matrix which is $N \times N$.

Make sense? How similar is every point to every other point?

And that matrix, if it's a kernel, will be **positive semi-definite**.

Which, for our purposes, means the eigenvalues are never negative.

How's it $N$ by $N$? We start with an original matrix $X$ which is $N \times P$ in this course. And if we look at how similar is every point here to every other point, we would take something that is then—well, for a linear kernel, which we'll get to in a second—if we took something that was $X^T$, that'd be $P \times N$. I will get something that's then $N \times N$.

Which is something that has in it, each of this point $i,j$. Right, and this gives you some matrix here, which I'm gonna call $K$, the **kernel matrix**, right, $N \times N$.

And $K_{ij}$ is the similarity of $X_i$ to $X_j$.

Make sense?

So it's gonna be quadratic in the number of points.

And something which is a kernel—anytime you make a kernel matrix for any set of points of $X$, any observations $X$, you'll get a positive semi-definite matrix.

### Understanding Positive Semi-Definite

Does that mean that every point similarity is non-negative?

When $K$ is positive semi-definite (PSD), positive semi-definite, does that mean every thing here is greater than zero?

Yes, no?

Not necessarily, right? A matrix can have negative numbers in it and still have all the eigenvalues be non-negative.

For a matrix to be effectively positive doesn't mean the numbers within it, the elements within it are positive. It means the eigenvalues are positive.

Now, for a Gaussian kernel, are the similarities non-negative?

Sure, it's $e$ to the something. Right? It's $e$ to the minus something positive. So we know the Gaussian kernel is strictly non-negative.

### Linear Kernel

But it turns out that this is a perfectly fine linear kernel:

$$K(X_i, X_j) = X_i^T X_j$$

Just take the dot product. And the dot product works. If you take something $N \times P$ times something $P \times N$, you'll get out a kernel matrix. And it will, in fact, be positive semi-definite, but that doesn't mean that taking the dot product of any two vectors is positive—they could be negative easily.

Right? So…

Can the eigenvalues be zero? Um, we're gonna… yes, I guess—we're not going to worry too much about the zero case, this is sort of an edge case of the zero eigenvalues. And people are a little bit sloppy in the math world about whether positive semi-definite is non-negative or strictly positive. And I'm not going to worry about which definition you choose to use, so it's slightly annoying, I wish the math could… Usually, mathematicians are super precise about things, but I can't quite tell if positive semi-definite means non-negative or always positive, depending on who's talking about it.

Um, so dot product is also a kernel. Sort of the easiest one, just take the dot product of two things.

### Types of Kernels

Um, so the simplest kernel: **dot product**.

The most widely, second most widely—most widely used kernel, the **Gaussian kernel**.

You can also do a **quadratic kernel** if you wanted. I use the Gaussian, the linear a bunch, I never used the quadratic one, but there's lots of other functions you could do that are kernels, and you can combine them and build other kernels. But we're not gonna worry about it.

## Radial Basis Functions (Review)

Um, I want to remind you that we did radial basis functions last class. Questions, by the way, let me stop for a second. Good?

We did radial basis functions last class, and again, the method is:

### RBF Algorithm Steps

1. **Pick the hyperparameter $K$**: How many kernels are you going to use?

2. **Run k-means clustering** (which we'll cover in a month), that then gives you $K$ centroids of clusters of points.

3. **Pick how wide the kernel should be**: Which is another hyperparameter. Right? If they're too narrow, then you're getting one point; if they're too broad, then you've averaged everything.

4. **Set a hypothesis function**: Our $\hat{y}$ being equal to a set of weights times each of these kernels:

$$\hat{y} = \sum_{k=1}^{K} w_k \phi_k(X, \mu_k)$$

Which I've written here subscripted with 1, but $\phi_1$ means the kernel of $X$ and $\mu_1$. How similar is $X$ to this representative $\mu_1$?

Which I mentioned doctors love if it's like a radiology image, because now I say, how similar is your image to this representative patient here, patient 1? How similar is your image to patient 2, right? This is an **example-based learning**.

Um, right? So that's the Gaussian kernel.

5. **Estimate the weights using linear regression** (or logistic regression, if you wanted, but then it's not RBF).

Good, straightforward review.

But note this trick of transforming some initial space $X$ into some new space—right? In a new $K$-dimensional space.

Good, good. Review? Yes.

### Questions on RBF

**Question**: In practice, do we adjust the $\mu$ based on gradient descent?

Well, first off, it's called radial basis functions. This is the name of a given algorithm. And this algorithm doesn't actually use gradient descent at all. This algorithm uses k-means clustering to find the $\mu$. And then it uses linear regression to find the $w$'s. So in the classic radial basis function, if you call it in scikit-learn, there is no gradient descent.

Could you use a different optimization scheme? Sure. Right?

So this is also a model with a bunch of parameters in it, and you can estimate the parameters in this model. What are the parameters? The parameters of this model are all the $\mu$'s. Right? There are $K$ $P$-dimensional vectors $\mu$, and then there's the weights $w$. That's the parameters of the model. You could estimate the parameters in the model using gradient descent.

But we're gonna show later in the course that for things like finding the centroids of clusters, gradient descent is not the best algorithm. And we're gonna use what's called an **alternating expectations method**, or an **EM method**. So we'll see that there are better methods for something that's very… it turns out that estimating centroids of clusters is very nonlinear. And so, standard gradient descent is not the best way to do it. You could do it, but you're gonna get stuck in local minima, so that's not in practice how people do it.

So that's a great looking toward a month in the future where we say, we'd love to cluster things. We can write a loss function, but we're gonna use a different kind of gradient descent to find it. It'll still be a gradient descent, but we'll see that we go back and forth in a couple of different dimensions of your gradient descent. So it's not going to be a simple gradient descent.

Yeah.

**Question**: How do we estimate the weights?

We estimate the weights using linear regression.

Yeah.

**Question**: Well, do we pick… how do we pick $K$?

How do we pick any hyperparameter in the engineering world?

**Cross-validation.**

And will it make it linear? Well, we've assumed a linear approximation to it. You're never really gonna make anything linear. And if you do make it perfectly linear, if you have $N$ observations, you could use $N$ radial basis functions.

Theorists like that. So there's a bunch of theory we're not going to cover that says make every single point in the dataset be a centroid. And that's mathematically very nice. That's sort of like all the information you get is using every single… forget the k-means clustering, just make every single point, make $K$ equal to $N$. And that gives beautiful math, but it's not super practical, it's not super efficient, and if you're not really careful making $C$ big enough, you're gonna overfit.

Yep.

**Question**: When we pick a given $K$, we're saying that for each point, we're measuring how similar it is to each of the $K$ different centroids. And that is the feature we're using?

Right? The feature we're measuring, the feature vector—the old feature vector might be pixels in a radiology space, the new feature vector is how similar are you to these $K$ either real images (if they were actually picked with a k-medoids, we haven't covered that either). If they were $K$ real things, or these $K$ fake images that are averages of images.

Right? So it's a really different way of viewing feature sets. You measure your features as how similar are you to exemplars—to example feature vectors.

Right? So the $\mu$'s are example feature vectors, and I can see how similar they are, and now you do a weighted combination. A nice way of viewing the world.

Cool.

That's all I'm gonna say for the moment on kernels.

## Kernel Regression

Let's look at one usage of them that is pretty rare, but sort of nice for building intuition.

And the idea says, if I want to make a $\hat{y}$, all I need to do is take a—for any $X$, some new $X$—is take a weighted vector over my entire training set of how similar is my target $X$?

Jargon: the target $X$ is the one you're trying to predict, to the $X_i$ in the training point. I get that similarity?

I then use that as a weighting on the $y$, the label for that point. And I have to divide by the summation to make it actually a weight.

$$\hat{y}(X) = \frac{\sum_{i=1}^{N} K(X, X_i) y_i}{\sum_{i=1}^{N} K(X, X_i)}$$

Right, so the weight looks like a probability weighting.

So this regression has **no gradient descent**. It has **no parameters to be estimated**.

You tell me the kernel and the kernel function, and that's all I need.

Yeah.

**Question**: There's no $w$?

Well, what does the $w$… yeah, there's no $w$ here, right? There's no weight here!

So, note that this is an example where there is no optimization. This is a model. It assumes a model form. And in particular, you as an engineer had to tell me what the kernel function is. Could have been Gaussian, it could have been linear, it could have been quadratic. If it's Gaussian, you gotta pick the weight, the width of it. So you picked it, that's it.

**Question**: Could you check the performance for that particular $X$?

Checking the performance? Well, there's no—checking the performance, I would take $\hat{y}$ minus $y$ and compute some loss function, whatever loss function I cared about. So there's no checking the performance here, this is just a simple model form, which has no adjustable parameters in it whatsoever.

Right? It's another **non-parametric method**.

### When to Use Kernel Regression vs. K-Nearest Neighbors

**Question**: When is this better or worse than k-nearest neighbors?

Some data sets in my world—the $X$'s are pretty much uniformly smeared about. Some $X$'s, you got a whole bunch of observations, people, objects, products, whatever, close to each other, and some that are really spread around.

Which one would you pick k-nearest neighbors for? A or B?

B, right? Because if you're way out here, you'd like to have something that's farther away to find enough stuff to average over, you want more stuff.

If I'm using a kernel regression, or any kernelized method, what does the kernel assume?

Similarity is sort of the same, no matter where you are.

Right? So k-nearest neighbor is going to say, oh, I'm gonna do a whole bunch of different models right in here, and then one big model for everybody else in the world. Whereas a kernel says, hey! Over here, and over here, I've got the same kernel width. That's terrible. Whereas over here, the kernel width is always pretty much the same, because those things are spread out equally, so it's fine to have a constant kernel width. And maybe it's not bad to have everything contribute somewhat.

And in practice, if you're cheap, instead of doing all $N$ of them, you could actually… people sometimes combine k-nearest neighbors with kernel regression. So they pick the $K$ closest neighbors, and then do a kernel weighting within the $K$.

Yeah.

**Question**: What is the kernel width?

If we have a linear kernel, there's no width. If we have a Gaussian kernel, there's that number $C$ in there, that hyperparameter.

Right? And remember that the key thing that you have to pick if you're using a Gaussian kernel, is how wide is the kernel?

Right? So that hyperparameter $C$ is the kernel width, and it's going to tell you, is the Gaussian looking like that, or is it looking like that?

Yeah.

**Question**: Doesn't it depend upon the kernel function?

It does, but in general, think about doing a Gaussian kernel—you're almost always using a radially symmetric Gaussian.

Right? That's the way it's written up here. It doesn't have to be isotropic, you could do that, but… and you pick one width.

And note that that width here… whoops, that width here, that $C$, doesn't depend upon where you are in space.

Right? So I've seen lots of code out there for doing kernelized SVMs and kernelized regressions and kernelized—we'll finally, eventually get to neural nets, but we'll see that when we do convolutional neural nets, you're again going to be picking something that looks like a kernel width.

So the problem is pretty much everything out there uses a fixed kernel width.

Right? Or k-nearest neighbor is a fixed distance.

Make sense? And so in some weird sense, I'd like a different distance metric here than there. But that's really hard to pull off. It's just hard to have the distance metric expanding or shrinking, depending where you are in the world.

And so, either you transform the data up front to try and make everything roughly spread out, which is what statisticians do. Or you say, heck, I'm a machine learning guy, we're in super high dimensions, it's hard to transform everything and make things work. I will use k-nearest neighbors sometimes, and a kernelized something other times.

Right, very important point, because it really drives—when you create midterm questions—drives when you're going to use it, when you're not going to use it.

### Kernel Classification

Cool. The other thing you can do is kernel classification, and classification looks pretty much the same way, except now you're doing a majority voting.

Right? Take all $N$ endpoints. Take each label $y$, and remember in machine learning, we like the symmetric $y$ as $+1$ or $-1$. We take a weighted combination of all the $y$'s, weighted by how similar each point $X$ is to the point I'm trying to do. We sum them up.

$$\hat{y}(X) = \text{sign}\left(\sum_{i=1}^{N} K(X, X_i) y_i\right)$$

If the sum is positive, the answer is $+1$. If it's negative, the answer is $-1$.

Right? So this is a weighted averaged voting. Every point of the $N$ endpoints votes, and it votes based on how similar it is.

Again, note sort of the elegance of the representation with the $+1$, $-1$. You could have done $1, 0$ and have it bigger than $0.5$ if you were a statistician, which would be mathematically equivalent, who cares? But this would be sort of a machine learning way of writing it.

Cool! Okay, I've already talked about this.

## Kernel Matrix Example

Um, let's just think through what would we do if we have something… so this is $N \times P$, what is $N$? For every matrix, start by thinking the dimensions, how big is $N$ here?

Three, how big is $P$?

Two. If we're going to compute the kernel matrix, what does the kernel matrix look like?

First of all, what dimension is it?

It's $3 \times 3$.

And let's start filling it out. Let me draw it taller, I could write something bigger: $K = \begin{bmatrix} \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot \end{bmatrix}$

So what's the $1, 1$ position here?

It's… if we use… I didn't tell you which kernel function to use, let's use a linear kernel function so it's easy.

So the linear one is, what is this piece here? This is $K_{11}$.

$K_{11}$ is the kernel function of what?

$X_1$ and $X_1$.

Which is… What is $X_1$?

$[1, 3, 5]$? Or $[1, 2]$?

It's $[1, 2]$, it's the first observation, it's how similar is this observation to itself?

And in the linear world, that's $[1, 2]$ dotted into $[1, 2]$, that's $1 + 4$, that's $5$. Whoop!

Yeah.

Now, let's take the next one. What is $K_{12}$? That's the kernel function of $X_1$, $X_2$.

That is the similarity of $[1, 2]$ with $[3, 4]$.

Right? So that's $1 \times 3$, which is $3$, plus $2 \times 4$, which is $8$, which is $11$.

Good?

I keep doing that. By the end, I get to $K_{33}$, which is gonna be $[5, 6]$ dotted into $[5, 6]$, which is $25 + 36$, which is probably $61$.

I don't care about the numbers, arithmetic. Now I've got a $3 \times 3$ matrix, I can now compute the eigenvalues and see if they're non-negative.

But hey, they should be non-negative. I just said they were.

Yes?

Sorry?

**Question**: Does it mean it's more… does it mean that $[1, 2]$ is more similar to itself than…?

Yeah, yeah, the similarities are sort of weird here, so let's think about this. It's… the similarity is sort of a strange thing. Because what you sort of hope would be that something similar to itself would be exactly one, and something dissimilar to itself would be exactly zero. That'd be a nice similarity.

But that's not the case for kernels, certainly not for a linear kernel. So a kernel's a pretty loose notion of similarity. It doesn't quite meet what you might hope it would do.

Right? Because you're right, somehow $K_{11}$ and $K_{33}$ here are… they're both… they should be the same size, but they're not.

Can't help you there, and let's look at what the Gaussian kernel looks like.

It's:

$$K(X_i, X_j) = e^{-\frac{\|X_i - X_j\|^2}{C}}$$

Right? So that's a Gaussian kernel. If now it's the similarity of something to itself with the Gaussian kernel, that's guaranteed to be $1$.

So the Gaussian kernel is at least nice, that the self-similarity, because a distance has a nice property we covered before: distance of anything to itself is going to be $0$, because that's the distance. So if you use a distance in a Gaussian kernel, then the self-similarity is $1$, which is beautiful, and infinite distance corresponds to zero, so the Gaussian kernel is pretty nice.

The linear kernel is sort of weird. It's got nice mathematical properties, but it doesn't actually do things you might hope it to.

Right? So it's… I can't help you, but it is sort of a sloppy notion of similarity, whereas the Gaussian kernel really sort of comes back down to what you think similarity should mean.

And so I like the Gaussian kernel better. But it's not uncommon to use linear kernels, because it was way cheaper. Not so much anymore, because who cares about the cost of doing an exponential? Compute has gotten really cheap.

So now you probably use the Gaussian one, but people in the old days, like 5 years ago, used linear kernels a lot, just because they were cheaper. But they're still a kernel.

Good question. Cool!

Um, good, so we covered kernels, this sort of weird notion of similarity that makes complete sense for a Gaussian kernel, but is sort of weird. We talked about the kernel matrix being positive semi-definite, and we talked about kernel regression and kernel classification.

## Support Vector Machines (SVM)

And now I want to switch topics and talk about support vector machines. These have a sort of a weird storied history. They came out of kernel machines and large margins. Very ad hoc, and I'm going to teach them in a modern fashion, which is sort of backwards from the way they came out historically.

We'll just start with, what's the loss function? And then we'll talk about its properties.

### Review: Lines and Hyperplanes

So, before we get there, we need to do a bit of a review and digression. So last class, we talked about representing lines and hyperplanes. And support vector machines are used for classification problems, like logistic regression, but not as popular. And again, we're gonna have our decision surface, we'll have a linear hyperplane, a $(P-1)$-dimensional plane that separates the $-1$'s from the $+1$'s. And we'll have a vector $w$ that's orthogonal to that plane.

Yes, we covered that in detail last time, so I'm not going to say anything about it.

### Projection onto the Hyperplane

The other piece which I didn't cover last time, which is worth remembering—you should all review this—is just quickly to say, do I say my pointer works. Ah, look, I have a pointer! That's so cool.

Okay, so we have our hyperplane (green), we've got our $w$ here. Often what we're gonna do is we're gonna take $w$ dotted into $X$, project it onto $w$. So we want to say, if we take $X_1$ and project it onto $w$, we'll get a vector that points in the direction $w$, but has length, which is $w \cdot X$, if you have unit—you have to normalize it if you're doing things right.

So you can project $X$ onto $w$, we can measure—what's that gonna give us? We project $X$ onto $w$, it tells us how far away is $X_1$ from the hyperplane.

Right? So we're going to want for support vector machines, they like to call them large margin. We're gonna care how far is a point away from this hyperplane. Now remember, we gotta define the hyperplane. So we're gonna do our optimization to find $w$.

Once we have $w$, we have the hyperplane, and we're going to do it based on how far away the points are from the hyperplane.

Make sense? This distance of how far is $X_1$ from the green hyperplane, and that is gonna look like—if you drop a vertical, right? Oop, I can't quite do it here. If you drop a vertical line, let's do it on the board…

I'll be talking about this a lot in the next half hour.

We've got some hyperplane. We've got some point $X$, and we want to know how far is $X$ away from the hyperplane?

Make sense? And that means we can take the $w$, which is orthogonal to the plane, and we can drop $X$ onto $w$ to get that distance.

Good?

Okay, so once we have a hyperplane… Yes.

### Making Predictions

**Question**: $w$ is defined in these cases as pointing toward the plus one side?

Right? And you'll see that in a second, because what do we do? We say, hey, take some new $X$ you want to predict it with, compute a score:

$$\text{score} = w^T X$$

Not a probability, we're not in probability land here. We're firmly in loss function land with no probabilities, we've lost our MLE/MAP world, sorry. Then we take the sign of $w^T X$:

$$\hat{y} = \text{sign}(w^T X)$$

If $X$ is on the positive $w$ side, the answer is $+1$. If $X$ is on the negative $w$ side, the answer is $-1$.

Right? Very binary, very machine-learning-y. Not prob… logistic regression, you get a probability out as a score, and you can then take the probability and convert it to a threshold, which decision theorists and statisticians love, because the probability—you can pick wherever split you want. I'm gonna call it cancer only if it's greater than $0.2$ probability.

Right? It's a score that makes sense. We're now in machine learning loss function land, there's no probability. It's either positive or negative.

Right? But for a lot of things, either you buy it or you don't buy it. You don't buy it 70%.

So, for a lot of things, the loss function really is binary.

Good? Good.

## Loss Functions for Binary Classification

Now, we can actually look at support vector machines, first of all, from a good, principled fashion of what's the loss function, and then from what's the geometric interpretation.

So, loss functions in the binary world. This is sort of messy, there's a bunch of these. We're gonna write them in the funny notation, where $y$ is $+1$ or $-1$.

And let's look first at the **black line** (0-1 loss).

If $y$ is the same as the sign of $f(X)$—$f(X)$ is our prediction, our $\hat{y}$—if $y$ is the same sign as $\hat{y}$, the loss is $0$. You got it right.

And if $y$ is the opposite sign from $\hat{y}$, you got it wrong.

Right? So that's a binary loss function. That's very computer science-y, you either got it right, or you got it wrong.

Make sense? And note the beautiful multiplication. If they're both $-1$, it's right. If they're both $+1$, they're right, but if they're opposite sign, they're wrong.

Good? So in some sense, arguably, that's what you might care about, is getting it right. How many did you get right? How many did you get wrong? Make sense?

### Continuous Approximations

But, there's a lot of continuous approximations that are used to that.

So the **red one** is the loss function. People recognize:

$$L = \log(1 + e^{-y \cdot f(X)})$$

That was the log likelihood from **logistic regression**.

So what does logistic regression say? It says, hey, if you got it right, the more right you get it, it gets gradually lower and lower loss. But you still want to keep pushing the loss lower? If you got it wrong, the loss goes up pretty dramatically.

Right? So logistic regression, if you say the answer—the probability is $0.9$ and the answer is $1$—you got a little loss, but you're not bad. If you say the probability is $0.1$, but the answer was $1$, now you've got a bigger penalty.

So that's logistic regression in the red.

We'll see later, like, next class, some things use an **exponential loss**, the blue line. You can have a loss that's:

$$L = e^{-y \cdot \hat{y}}$$

That's slightly more aggressive than logistic regression, but pretty similar.

### The Hinge Loss

Today, we're going to talk about a weird but very nice loss, which is the **hinge loss**.

The hinge loss is:

$$L = \max(0, 1 - y \cdot \hat{y})$$

That's this green line.

So what does it say? If your $\hat{y}$ is bigger than $1$—if you got it really right, more than one (this is sort of annoying as a scaling constant here, not scale invariant)—but if your correctness is bigger than $1$, you got it right, your loss is zero.

Otherwise, your loss goes up linearly.

Now, this is sort of weird. It makes sort of sense if you're really wrong. Right? If your $\hat{y}$ is $-2$, you got a loss of $3$. That sort of makes sense, right? The wronger you are, the farther you are away from the margin, the more the loss. But here's the interesting thing about the hinge loss:

If you get it right a little bit—right? This says you're on the correct side of the decision surface—we're still penalizing you.

I'd like to make a model that says that you should be kept away from the decision surface.

So the hinge loss says that if you're really correct, no loss. If you're really wrong, it goes up linearly, which is way less radical than logistic regression. And if you're a little bit right, you still have a penalty.

And we're going to explore why that makes sense, but that turns out to be a surprisingly nice loss function.

Questions?

### Understanding the Hinge Loss Plot

**Question**: What's the x-axis?

The x-axis is $y \times \hat{y}$, $y$ times your score.

And the y-axis is the loss, is the loss function.

**Question**: How did the max of $0$ and $1 - y \cdot f$ end up there?

If $1 - y \times f$ is less than $0$—if $y \times f$ is bigger than $1$—you have no loss. Your loss can never be negative, right? There's a kink in this function.

Right? It's continuous, but it's not continuously differentiable. If your $y \times f$ is bigger than $1$, your score is correct and bigger than $1$, then your loss is zero.

Otherwise, your loss is basically, how far away are you from one?

People got the geometry in this?

Oh, cool, yes?

### Why Hinge Loss vs. Logistic Regression?

**Question**: Yeah, so logistic regression is a full probabilistic model, and it says this is the probability, and if you say a probability of $0.9$, but the answer was yes, you incur a penalty.

The hinge loss says, hey, if you have a point that's far enough away from the margin and correct, I have no idea what the model is. I don't have a probabilistic model. If I got it convincingly right, strongly right, more than one right? I'm happy with it, zero loss.

Logistic regression is wrong, as is everything in statistics. It assumes a particular model form, which is useful, and it says, hey, when you say $0.9$, try harder, push it toward $1$. If your model form were truly correct, that would be great.

And often, in practice, it is great. I use logistic regression all the time. It's an awesomely useful model. But it does make a particular assumption about the distribution of the points, and this… it's a probability model.

If that probability model is really wrong, because sometimes you get occasional things that give scores that are way out here—in general, let's think about what I care about.

If I'm making decisions for—I did admissions to computer science—the people who are obvious rejects, I don't care if it's probability $0.001$ or probability $0.01$, or probability $0.1$, I should reject them. If it's a clear reject, and I know it's a reject, my score is very negative, the $y$ is negative, I got it correct. It doesn't go in my model at all.

Right? And that's most of the data points.

Right? We reject 90% of the applicants. Half of them I don't even want to think about. I don't want to waste degrees of freedom. There's some person from some obscure university somewhere with crappy grades, I'm fine.

What I care about is learning stuff near the decision surface. And there are some that are awesome.

Right? They come from UC Berkeley with a recommendation letter from someone I know and a great GPA, which is hard to get at Berkeley. Great, so I don't care about them either. The loss is zero. What I care about is the ones that are close to the margin, close to the decision surface. That make sense? Yeah.

**Question**: I'm flattening it for the ones that are correct?

So look at the hinge loss. If it's really correct, if the score is really high, the loss is zero. If I got something really wrong—this candidate who I should have taken that turns out to be great—I'm being penalized basically linearly for that.

So it's not a very strong assumption about the nature of the model.

Right? It's a linear penalty on being wrong, which is much less harsh than a regression, which is sort of more quadratic. Yep.

**Question about groups being too far away?**

We'll get… we'll get back there, yeah. Hold these for a second, we're gonna cycle through, I'm gonna do a few more slides on this, then we'll come back, I'll take questions again.

### Sensitivity to Noise

So again, let me just emphasize which loss function is most sensitive if you've got a world with lots of noise in it? Which cares most about—well, the exponential, for sure, the blue one, if you're wrong, noisy on the wrong, it cares the most.

And actually, if you're off a bit on the score on the ones that are right, you know, maybe the logistic cares as much, right? So the logistic, the red one, the blue one—the hinge loss is the least sensitive of these differentiable loss functions, these continuous loss functions that we use. It's the least sensitive to the noise. Right? And you can do a straight 0-1. That's also really insensitive.

## SVM Loss Function

Okay, so here is SVM in one slide.

So, no, I'm not gonna hold off on the questions, we're going to show a few more slides, we'll come back, we'll talk about SVMs, take questions at the end.

The loss function is written backwards from the way we're used to writing them in all of our regression problems. But it's the same concept. So the first term here is:

$$\frac{1}{2} w^T w$$

That is a **ridge regression penalty**!

Right? That's the norm of $w$ squared. So the first term says, hey, I want to make the weights as small as possible. Right? Ridge.

The second term says that I'm going to have a constant, which is a regularization constant. Oh, crap, but normally, we put the constant in front of the weights in regression, now we put the constant in front of the loss function.

So the regularization is written the opposite way here.

And then we're gonna have that times the sum over the $N$ training points of the loss:

$$L_{SVM} = \frac{1}{2} w^T w + C \sum_{i=1}^{N} \xi_i$$

And the loss is the hinge loss. And the hinge loss, represented by what my Greek friends call xi ($\xi$), and my American friends call squiggle—I sort of like squiggle, I don't know.

Um, the $\xi$ thing is:

$$\xi_i = \max(0, 1 - y_i(w^T X_i + b))$$

Here we put the $b$ in and not actually put the $w_0$, because that's the way people like to write it when they're doing SVMs. But what do we have? We have a hinge loss, right? That's the sum of the $N$ points in the training set, hinge loss, times a constant, plus a ridge penalty.

### Important Note About the Constant $C$

And again, I want to warn you, because when you go to run people's code, the $C$ here is backwards from the normal regression and ridge. In ridge, the bigger the penalty, the more shrinkage. In SVM, the bigger the penalty, the less the shrinkage.

Make sense? But it's mathematically equivalent. You can put the $C$ either place.

$C$ is the inverse of ridge, right? $C$ is the inverse of the weight you would put on the ridge penalty.

Right?

**Question**: Why do we write it this way?

The SVM comes out of a weird historical background of maximizing a margin, which I will cover eventually. And the answer is, it came out of a different line of research than ridge regression. And because of the way they derived it, they like to write it differently.

I can't help it. It's sort of stupid if you were to derive it today. You wouldn't put the $C$ over here in front of the loss. You'd put it in front of the weights. But I can't help you. My goal is to say, this is—if you go for any SVM code, there's lots of code out there, scikit-learn, everything, they all put the $C$ on this side. It's $\frac{1}{\lambda}$. That's all it is.

Right? And there's lots of magic about SVMs, and we'll wave our hands about some of it, but the main point is, it is a hinge loss rather than an $L_2$ loss or an $L_1$ loss, and that is typically done with ridge, but we'll see you can put any penalty you want in.

Yeah.

**Question**: Why do you want a ridge penalty?

Why do I ever want a ridge penalty? Why do I want small weights? What's good about small weights?

Less overfitting. You've got to control overfitting. You've got a hyperparameter $C$, that's gonna control how much overfitting you have. The hinge loss does not prevent you from overfitting. You need to shrink weights to avoid overfitting.

**Question**: Well, hinge loss isn't making it smaller?

Behind every loss function is some assumption. And when we did the probabilistic world, what did we say? We said that the loss function is driven either by what do you care about in the real world, in terms of dollars, or minutes, or whatever, or the loss function is driven by a probabilistic assumption of the noise. So the ridge penalty is assuming something that says that, hey, because I'm in classification land, if you're really right on your score, I don't want that to influence my thinking too much. And if you're really wrong on the score, it'll only be linear.

Because, hey, in a regression land, if you're estimating dollars, you could be off by millions of dollars. If you're predicting $-1$ or $+1$, you can't be off by more than two. The worst you can do is say, one when it should have been minus 1. Even if your score is way, way off! You're still in the end, you got it right, you got it wrong.

So, in this classification world, often we don't want to say, we're really gonna penalize something that's very far off, in terms of the score. And note that in the probability world, logistic world, the score is between $0$ and $1$, so you can't be really off. But in a linear world, the score could be really off.

So we want to sort of say, hey, if the score's way, way off, we don't care too much. And then once again, if the score is a little bit positive, and we're still right, we still want to have a little bit of a penalty.

Cool.

Um, good.

## Geometric Interpretation: Large Margin

The classic interpretation of support vector machines is that they are **large margin machines**.

And the idea is that we have some sort of a linear decision surface in an $X_1, X_2$ space here. And we have triangles and circles for my $+1$'s and $-1$'s. We have a decision hyperplane.

And we're gonna introduce one more concept, which is the **margin**.

The margin is a parallel hyperplane that's a distance one away from the decision hyperplane.

Now, what does that mean, and how does that come from? That comes from our hinge loss!

Hinge loss. Right? So we've got… BOOM! Some data, $X_1$, $X_2$, we have some $+1$ triangles! Some $-1$ circles! We have our separating hyperplane that does that.

### Understanding the Margin

What's the hinge loss going to be? The hinge loss is orthogonal to the separating hyperplane. And it says, hey, if you're a distance one away, and right, you have no cost at all.

If you're between $0$ and $1$ away from the separating hyperplane, and again, this is a $(P-1)$-dimensional plane, then the hinge loss goes up linearly.

If you're right on the hyperplane, you have a loss of $1$. And if you're on the wrong side, farther away, the loss keeps going up further, right? So this is the hinge loss. People see the hinge loss on the plane here.

And it says, hey, again, if you're more than one away, if you're outside the margin—this is a margin size $1$, by construction—if you're more than one away from the hyperplane, and you're correct, the answer is you have no penalty. If you're on the other side of the margin, if you've got a point right here, if I've got a triangle here, is this correct?

It is correct. Does it have a penalty?

Yes! Make sense? It's inside the margin.

**Question**: How do we calculate the distance?

That is the projection of it onto the weight. Which is the distance away. So it's the distance here, but remember, the way you do the distance is you project onto the—this is the weight vector $w$.

Right? Remember, the weight vector defines the hyperplane. And they should all be going through the origin here, so I'm a little bit off.

Yeah.

**Question**: And there's some margin on the other side?

So there's another margin at $-1$. And if I have a circle that's over here, this circle, there's a hinge loss for the other side, which is gonna be ugly to draw. I hate to draw it, but the hinge loss on the other side is going to be $0$ up to the other margin, distance $1$ away, and then it'll go up linearly that way.

Right? So, there's always a hyperplane in the middle, and then there's two parallel hyperplanes that define the hinge at the edge. And so a circle here is still correct but penalized, and as that circle moves farther and farther, it ends up being more and more penalized.

And this one, or that one, which is better for loss, this circle or this circle?

No difference.

Right? They're both outside the margin, they're both perfect.

Right? So the **margin**—that's a technical term for SVM. The margin is the point where the hinge loss transitions from zero to being positive.

Yeah.

**Question**: There's some balancing that occurs?

There's all sorts of magic you can talk about, but what did I do? There's a loss function. Let's go back and look at it.

It's all just optimization. The margin, the SVM guys are full of hocus-pocus and large margins and maximize the margins and support vectors, and we'll get to them in a second.

But I want to emphasize, this is just a simple, straightforward **loss function**. It's convex.

Right? The hinge has got a kink, but it's still monotonic. So it's a convex loss function, it's got a quadratic penalty, it's a perfectly fine thing to optimize by, say, gradient descent.

Right? No closed-form solution, but hey, I don't care.

So, all this other stuff gives you intuition, but at the end of the day, you're solving an optimization problem.

Cool. Um, good. Oh, and there's… these are all linked from Canvas, there's a little Jupyter notebook you can look and see some of these things if you want. But I think that's the piece there.

## Linearly Separable Case

The next piece is sort of what's the intuition behind these? And the intuition is, in a world where things are **linearly separable**—

So a technical term, linearly separable means there exists a hyperplane, such that you get a loss of zero on the training set. Right? Empirical loss on the training set of zero.

Now, personally, I've never seen that, except when you're overfitting. So this is sort of a hypothetical world that some mathematician lives in. My world is always not perfectly separable. I can never get 100% accuracy predicting cancer, non-cancer, unless I'm overfit, which is easy to do. But that means I didn't regularize enough. But look at these.

Solid and light circles. I could fit some line through it, and you could ask, what's the best line? Obviously, the empirical risk can be zero for an infinite number of lines. This is underspecified.

Right? So what's the best of these lines? There's the—is that one better or worse? Depends on the margin. So is it better or worse? That one better, or that one?

A or B?

A feels better, right?

So, you'd sort of hope that A is better. They have the same risk, empirical risk, right? They're both zero error on the training data, but you have the strong intuition that says that that one is going to generalize less well. That'll do less well on the testing data.

Right? And that one also seems really crappy on the testing data, right? Just feels unstable. You get another dot just on the edge over there, it moves…

So, in some sense, we could say, well, we'd like to build—of all these ones we could fit—we want the one which is farthest away from the points that are closest.

And that would be talking about the large margin.

### Support Vectors

So we can define the margin, which is the same margin I just drew on the board below, as being parallel to my separating hyperplane, my decision surface. I have two things, a distance one apart in this regularized piece here, and I'd like one that maximizes the margin.

And in the separable world, the ones that are—at least you can't quite see them, these are—pretend these are right on the margin. These two white dots and the one blue dot over there, assume they're right on the margin. They would be called the **support vectors**. They're the ones that are right exactly on the margin, and you'd like to make the line through the center be the one that maximizes that margin.

And it turns out that, lo and behold, magically, the hinge loss does that. Because what's the hinge loss? The hinge loss says, hey, if you're on the margin or more correct, you get perfect score, and same thing on the other side. If you're on the wrong side of the margin, then you pay a cost. And so it turns out, magically, that the hinge loss will, in fact, maximize the margin.

### Non-Separable Case

The real world is gonna have some white dots over and the blue dots. They'll be penalized linearly. There'll be some white dots within the margins, they'll be penalized linearly. And it all works out really nicely.

So, here's the deal, the hinge loss does maximize the margin. And in the world that it's separable, you'll get something that is easily provable, has these points on the edge.

Cool! Yeah, time for a few variations on these.

Um, yep.

## Questions and Discussion

**Question**: We want to… well, the weird thing is they're trying to make it more than one on either side, and the weird thing is, there's an arbitrary normalization that shows up in the middle here, so I think the way to think about it is you'd like to push it geometrically.

What do you want to do? Geometrically, you want this line—which is the one that matters, the hyperplane that separates, the separating hyperplane—you'd like it to be as far as possible from the nearest points.

And that's gonna make it, sort of by definition, symmetrical, so it'll be right down the middle, so the two margins will be equally spaced.

Many years ago, I used to go through the math. The math is sort of annoying, and there's a normalization constant that comes in, and the normalization constant ends up giving you the regularization term, which causes those $w$'s, which is sort of the scaling.

And it's… I don't find the math helps—it's all over the internet. And I don't find it helps the intuition.

But the answer is, geometrically, it should make sense that I want to push the thing down the middle, so it's symmetrically between the points that are closest, and the one is going to be just determined by, in some sense, how big the weights are due to the regularization.

Yeah, so note that—

### Scale Invariance

**Question**: So is SVM scale invariant, or no?

Scale invariant, not scale invariant?

It's not! It's the loss function, which is of order the number of points, and the size plus $w$, the weights. It's not scale invariant. It depends upon how you rescale things. So SVM people typically standardize their data before they do it, unless it's all already of the same size.

So, be careful about the ones on any of these things in terms of scales. You're probably rescaling stuff anyway.

## Constrained Optimization and Lagrange Multipliers

Okay, um, whoop, where was I? Um, right.

So, now I'm gonna wave my hands at something that would take, like, 2 hours to describe beautifully.

But across all of machine learning, and all of optimization—and I strongly encourage you to take an optimization course in ESE—you can view things as having something you're trying to minimize or maximize. You could try and minimize the size of the weights subject to some constraint, or you could try and minimize the loss function, subject to some constraint on the weights.

And in optimization, there's this awesome technique called **Lagrange multipliers**. I feel really bad I can't cover it. But Lagrange multipliers allow you to take a constrained optimization and change it into a single term with a Lagrange multiplier that ends up looking like our $C$ or our $\lambda$, our weight.

And so, if you want, you can view these things as minimize the weights subject to some loss function, some constraints that every point is at least one away—it's hard to verbalize, but that's at least a constraint you could have. Or you could say something to that effect in some soft constraint.

And often the $C$, this loss, hinge loss, is called a **slack variable** in optimization.

But we're not going to cover that, it's not on the midterm, and it's just cool math.

Um, why is that important? It's important because if you think about the general support vector machine—and here I'm feeling really bad about being non-intuitive—is that, well, first of all, let's look at the generalization. I just generalized it. I did two tricks on you.

### Generalizations

Um, instead of a ridge loss, I could put any other penalty I want. That's right. Any other penalty I want. Right? Ridge is an $L_2$ penalty. But I could put an $L_1$ penalty if I wanted, and support vector machines come with—

I could also make the hinge loss not just be the $L_1$ hinge loss, which is what I had. Right? Remember, hinge is non-negative? It's $0$, and then it goes up? So it's a linear—$0$ or linear, so that's sort of an $L_1$-ish style penalty loss function. I could square it if I wanted, and people sometimes do.

So I can change the SVM to have a different penalty (non-ridge penalty), or I could square the hinge loss. Still quadratic, it's still pretty much the same.

### Dual Formulation (Constraint Version)

Um, the piece that's interesting in the Lagrange multiplier sense is you can either write the loss function as I did:

$$\xi_i = \max(0, 1 - y_i \cdot f(X_i))$$

Or you can say what I want is to say the error—which is what? This is my $\hat{y}$—times $y$, so that's my score, corrected to be in the direction $+1$ or $-1$. I want that score to be bigger than $1 - \xi_i$:

$$y_i \cdot f(X_i) \geq 1 - \xi_i$$

This is a sort of a dual formulation, or an alternate formulation. Instead of saying I want a hinge loss, I'm saying I have some sort of a constraint.

And it turns out that there's a one-to-one mapping—at least in this case, sometimes it's not perfect—but there's a one-to-one mapping between a constraint and the loss function.

If you follow it, awesome. If you don't, it's also okay. Not on the midterm, but I think the point is that often if you—just as you see these things, if you see a constraint, think, okay, what's the constraint version with a single equation that'll look like the thing we're used to using?

## The Dual Space

Okay, last piece. We're going to introduce a new kind of duality, which is also gonna go really fast. I got 10 minutes, but that's what I plan to spend on it.

### Primal vs. Dual Space

And we've worked in this course mostly in what's called the **primal space**, which is in the feature space. The weights are dimension $P$, we have $N$ endpoints, but they are dimension $P$. That's the primal space, that's where linear regression, logistic regression sit.

But it turns out you can compute everything by flipping things around, and instead of working—in regression, what, $\hat{y} = (X^T X)^{-1} X^T y$? It works in the $P \times P$ space. We can do everything in the **dual space** in the observation space, the $N$-dimensional space.

And that is working in the $XX^T$ space.

Right? Somewhere it's up—there it is, it's still on the board. $X$ is $N \times P$, $X^T$ is $P \times N$. That is the **kernel space**!

Right? So the kernel space sits in the observation space, the $N$. It's an $N \times N$ matrix. It says how similar is every point to every other point?

And you can formulate anything. If I had more time, I'd do regression, but in particular, it's common in support vector machines to formulate them in the dual space, the kernel space rather than the primal.

And the math is pretty straightforward, and I'm not gonna go through it, I'm just gonna say, hey, you can basically transpose all the equations, and the math works out beautifully.

### Loss Function in the Dual

And I will sort of write out what the loss function looks like in the dual.

So, before we had a set of weights, $P$ weights. Now we're gonna have $N$ weights $\alpha$. We're gonna have something that tries to maximize—sort of, we were minimizing them before, now we're going to maximize them. We're in the dual, we flip things. We're going to try and maximize these $\alpha$ weights!

And we got a loss function, which now has quadratic in the $\alpha$'s, quadratic in the $y$'s, and has something in it that's gonna be the dual, the kernel function in it:

$$L_{dual} = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(X_i, X_j)$$

Right, the dot product of every point. So what this thing is saying, hey, I've shifted my world, instead of trying to find $P$ weights on individual features, I'm gonna try and find $N$ weights on observations.

And in fact, this is each pair of observations. Right? How similar each point is to every other point. And then, are their two labels the same or different? If they're the same, I'll have a $+1$. If they're different, I'll have a $-1$. And then the weights on those two observations multiplied together.

So this is sort of a weird formulation that sounds bad, because mostly $N$ is bigger than $P$ in a lot of worlds. So it looks sort of bigger. And we're gonna have to force all of the weights to be non-negative, otherwise things don't make any sense for maximizing them, and in fact, we're going to make them less than some constant, which is the regularization amount, which is equivalent to the regularization we had before.

### Kernels in the Dual Space

And in this dual space, two things happen. One that's super cool is this is the linear version, that's a linear kernel.

But hey, I can write $X_i^T X_j$ as $K_{ij}$. I can put any kernel I want. And mostly, I want to put a **Gaussian kernel**.

Right? So instead of saying how similar are points in the linear space, I say how similar in a Gaussian sort of measure, which is much nicer. Right? Goes from $0$ to $1$.

So, in the dual space, it's trivial to pop in a kernel, a Gaussian kernel, and everything looks pretty much the same.

### Sparsity and Support Vectors

The optimization sounds ugly, but in fact, turns out to be quite elegant. And it's elegant for the following reason.

If you think about what gives a zero hinge loss or a non-zero hinge loss, any point that gives a zero hinge loss doesn't affect the loss.

That make sense? I could take—once I know the hyperplane, I could take every point that's outside the margin on the correct side, and delete it, and it has no effect on the solution whatsoever.

Make sense? That means that the $\alpha$ for those points are all zero!

So this is **incredibly sparse**. Only points that are on the wrong side of the margin—right? Either truly wrong or less than one away from the hyperplane—only those are binding constraints, only those are active, only those have non-zero losses.

And so for something that's reasonably close to separable—right? Where most things are correct, outside the margin—almost all the $\alpha$'s will be zero.

And the ones that are right on the borderline, which is the edge of that hinge where the hinge meets—those ones are the **support vectors**, they're right on the margin. And so it turns out this optimization is relatively elegant to do because it's mostly very, very sparse.

So I hope you get the geometry intuition behind the hinge loss, that's the real thing.

Um, lots of the code actually solves the SVM in the dual rather than the primal. Because it's super easy to put a kernel function into the dual. Whereas the kernel function doesn't fit cleanly into the primal.

Right? And again, remember the kernel function. How similar are these two points? That's what I really care about, and that's often a nice way to do things with weightings.

So that shifts my loss function slightly here. Um, and it's fairly easy to solve in the dual. And if it's something with a Gaussian kernel, it's still convex, and life is beautiful.

## Non-Linear Decision Boundaries

Cool! Okay. Um, there we go, let's get one last piece of intuition. If you think—hang on, I'm gonna keep pushing ahead—if you have something that in the primal is not linearly separable, or if you use a linear kernel, it's not linearly separable, so here's an $X_1, X_2$, where we have points that aren't separable beautifully.

If you use a kernel, like a Gaussian kernel, you can make things that weren't linearly separable, now be separable.

So this is close to separable, not quite, but it's almost separable in the kernel space once you transform it. Because most of the yellow ones are close to the yellow ones, most of the black ones are close to other black ones. In the kernel space, you have a much more flexible model.

And so it's common to put in a Gaussian kernel. It gives you a richer model without costing you too much.

### Hyperparameters in Practice

And so it's nice when you go to run it, you'll find that there are two parameters to pick. One is the $C$ I told you about. That's the inverse of what we had before for the $\lambda$, right? That's how important are the weights.

The other is the width of the Gaussian. Oh, the third hyperparameter? Ah, no, that's it. You only get two.

Right? How much—how important are the weights? How much did you shrink them? Or inverse of it? And how big are your Gaussians?

Cool! So, um, uh, yeah, I've got time for two questions, yeah.

## Final Questions

**Question**: The notation is opposite?

That's entirely possible. Where was the notation opposite? Uh, be careful. In the, I, uh… they're two different things that are different. One is the matrix $X$, the matrix $X$ is $N \times P$. And I'm going to multiply that by $X^T$, which is $P \times N$.

One individual $X$ is one row here. Right? And so in some sense, it's… I'm going to take one $X$ from here and another $X$ from here. I'm going to take the dot product of them.

And I'm a little bit sloppy, and people are too, in the vector land. Is it $X_1^T X_3$? Or is it $X_1 X_3^T$? Okay, I should be careful, but in fact, $X$ looks like what? $X$ looks like $1 \times P$. And so what I want is $X$—this is $1 \times P$, and I want to dot that into something which is going to be that transpose, which is $P \times 1$. Yeah, looks like I got it backwards.

So, think about what makes sense. Think about getting dimensions always correct, and I probably stole this from someone else, but in general, uh, this, yeah, this one is… yeah, this is gonna be the kernel, so these are going to be—each of these should be an inner product, right? This is $X_i$ dotted into $X_j$. They should give me a scalar, right? This is $K_{ij}$.

So I apologize if the notation is confusing, which it probably is. But think of this as $X_i$ dotted into $X_j$, inner product, not outer product.

Oh, that one. Yep. Um, this one is correct. Right? This is the notation we use in this class. $X$ is $N \times P$, $X^T$ is $P \times N$. On this slide it is correct. Dual is $N \times N$, primal is $P \times P$.

So this one is correct. The other one, I think, is somewhere between wrong and confusing, but I'll double-check it.

## Conclusion

Cool! We are at time. But we are also done. Whoa, no, no! Forget the Poll Everywhere, we're not doing Poll Everywhere.

Um, so you should know hinge loss in 20 ways, an idea of the hyperplane, the margin, and a little bit of kernels and the support vectors.

See you all for information theory next week! It'll be super exciting!

---

# CIS 5200: Machine Learning - Lecture 9: Information Theory and Machine Learning

## Introduction to Entropy and Information Theory

Information theory provides the mathematical foundation for understanding how to efficiently transmit and compress data through a wire. This was originally developed for telephone companies when they used wires for transmission, and it turns out the same mathematics works beautifully for everything, including machine learning.

### Encoding Sequences

Consider sending a sequence of symbols. What's the most efficient way? How many bits does it take to send each item in the sequence?

We might code symbols as follows:
- A as 00
- B as 01  
- C as 10
- D as 11

Now, we can send a sequence like "B" as 01, and "A" as 00. What about spacing? Do we need to put spacing between these? No! While in English we like putting spacing between characters, you don't have to—it has the same information.

Each of these encodings takes **two bits** to send. More precisely, it takes:

$$-\log_2 P(x)$$

bits to send, where $P(x)$ is the probability of symbol $x$.

### Logarithm Bases

When we write $\log$, what base do people usually mean?

- **In Gaussian world** (like last week): $\log$ means log base $e$ (natural logarithm)
- **In bits world**: We use $\log$ base 2 to get bits rather than "nits" (natural units)

You can do everything in log base $e$ (natural log), which gives you "nits" instead of bits. It's all a constant factor off, but it's cleaner to think of entropy in bits. As computer scientists, we'll go back and forth:
- **Gaussian world**: Always log base $e$, because we get the log of the exponent, which cancels out
- **Bits world**: Log base 2 to get bits

So, $\log_2(1/4) = -2$ bits, since $1/4 = 2^{-2}$.

## Variable-Length Coding

What if instead of equal probabilities ($1/4, 1/4, 1/4, 1/4$), we have:
- A: probability $1/2$
- B: probability $1/4$  
- C: probability $1/8$
- D: probability $1/8$

We won't cover Huffman coding in detail (there's lots of nice theory on how to do these things), but the key idea is:

**How do you code things that are frequent?** Use shorter codes!  
**How do you code things that are less frequent?** Use longer codes!

The specific details of the coding are entirely unimportant for this course. The critical part is $-\log P$.

### Example Variable-Length Code

You could code:
- A with 0 (1 bit)
- B with 10 (2 bits)
- C with 110 (3 bits)  
- D with 111 (3 bits)

Then if we see a sequence like "BA", it's coded as:
- B is 10
- A is 0
- Together: 100

Where's the spacing? We don't need any spacing! This works because "10" never has a "1" following it—the code is **prefix-free**.

The important part is:
- A takes **1 bit** to code
- B takes **2 bits** to code
- C and D each take **3 bits** to code

This is as efficient as you can get! We have:
- $-\log_2(1/2) = 1$ bit
- $-\log_2(1/4) = 2$ bits
- $-\log_2(1/8) = 3$ bits

Claude Shannon proved that this is the best you can conceivably do.

Could we have done C as 11 and D as 110? Shannon tells us that's not possible for efficient decoding. If we tried C as 11, then "CA" would look exactly like "D"—it doesn't work!

## Formal Definition of Entropy

### Entropy Formula

For any random variable $X$, the **entropy** of $X$ (always written with $H$ for entropy—we can't use $E$ because that's for expected value) is:

$$H(X) = -\sum_{j=1}^{M} P_j \log_2 P_j$$

where:
- $M$ is the number of possible values $X$ can take
- $P_j$ is the probability of the $j$-th value
- This is the probability of each outcome times the log of its probability

Huffman coding gives a particular way to generate the bits, but in this course, we don't actually care about how you code them. We're not actually transmitting data—we're using information theory as a theoretical framework.

### Information Theory in Machine Learning

What we're going to do is play a game where you and I both know $X$, and I want to send $Y$ to you as efficiently as possible. The way I'm going to send $Y$ to you efficiently is by sending you a **machine learning model** that will allow you to compute the $Y$s from the $X$s.

I'm not really going to send you the $Y$s (I don't work for AT&T or Bell Labs—I don't care about actually sending stuff). But theoretically, the game we're playing is:
- We both know $X$  
- I'm going to efficiently tell you what the $Y$s are
- That efficient coding is going to be the model
- The best model will be the one that does the best job of telling you what the $Y$s are for the $X$s

This looks like maximum likelihood and log probability, but we're going to frame it slightly differently in terms of information.

### Entropy as Expected Value

The way I think of entropy is as the **expected value of the negative log probability**:

$$H(X) = \mathbb{E}[-\log_2 P(X)]$$

What's an expected value? An expected value is the probability of something times that something:

$$H(X) = \sum_{j=1}^{M} P_j \cdot (-\log_2 P_j)$$

### Example Calculation

If we have four different outcomes with probabilities $1/2, 1/4, 1/8, 1/8$, the entropy is:

$$H(X) = -\frac{1}{2}\log_2\left(\frac{1}{2}\right) - \frac{1}{4}\log_2\left(\frac{1}{4}\right) - \frac{1}{8}\log_2\left(\frac{1}{8}\right) - \frac{1}{8}\log_2\left(\frac{1}{8}\right)$$

What this tells us is: for each possible symbol I might have to send, how many bits does it cost to send it? That's $-\log_2 P$. Times: how likely am I going to have to send it? That's $P$. 

This is the **expected value of the number of bits** needed to encode this distribution.

**Note**: This is the expected value per symbol—per specific value per symbol.

### Continuous vs. Discrete

Remember, $X$ is a random variable that takes on a finite support of values (in our example, four values). If it were a real number, the expectation becomes an integral:

$$H(X) = -\int P(x) \log_2 P(x) \, dx$$

Same concept: in discrete space, you have a summation; in continuous space, you have an integral.

## Connection to Machine Learning Models

Machine learning models are a really efficient way of compressing data. There's a whole beautiful set of theory called **Minimum Description Length (MDL)** that says you optimize two things:
1. How many bits does it take to encode the model?
2. How many bits does it take to encode the residual?

If you minimize the sum of these two, you get the optimal model with minimal overfitting.

We don't want to fit the data perfectly—we don't want to reduce the noise to zero. We want to trade these off. Over and over, we have this notion where you want:

$$\text{Empirical Risk} + \text{Penalty}$$

There's lots of nice math for this. The best math is all information-theoretic.

The important thing is: yes, we want to find an efficient coding. We're going to see there's a bunch of cases where we have different notions of "best," and "best" is going to be the thing that either minimizes or maximizes entropy in different ways.

Many things we deal with in machine learning are probabilities. Most machine learning either produces a number or a probability of an outcome. The probability is almost always the thing you're optimizing, which is an entropy.

### Units and Meaning

The entropy is measured in **bits**. The entropy is the entropy of what? It's the entropy of a **probability distribution**.

We write it as $H(X)$, where $X$ is a random variable whose values are drawn from some distribution. The entropy is the **number of bits per item** in this distribution of $X$.

## High Entropy vs. Low Entropy

- **High entropy** means things are really spread out—you don't know what's going to happen
- **Low entropy** means $X$ is really well-focused

### Extreme Cases

**Low entropy**: Things are contained together—we know where it is. If the Gaussian is really spiky, that's low entropy.

**High entropy**: Things are smeared out.

High entropy means **less knowledge** about the world. Low entropy says you know the answer for sure—you know it's either 1 or 0, you know exactly where $X$ is. High entropy says things are spread out—it could be anywhere, you don't know what it is.

**Entropy is uncertainty.**

### Example: Biased Coin

Consider flipping a coin that always comes up heads. Probability of heads equals 1.

What's the entropy of that distribution?

$$P(X = \text{heads}) = 1, \quad P(X = \text{tails}) = 0$$

The entropy is:

$$H(X) = -1 \cdot \log_2(1) - 0 \cdot \log_2(0)$$

Now, $\log_2(1) = 0$. What about $0 \cdot \log_2(0)$? That's tricky, but you can use L'Hôpital's Rule: if you have $\epsilon \log_2(\epsilon)$ as $\epsilon \to 0$, it goes to 0.

So: $H(X) = -0 - 0 = 0$

If the entropy is zero, there's no uncertainty—it's always heads!

### Example: Fair Coin

For a 50-50 coin toss (a real, honest coin):

$$H(X) = -\frac{1}{2}\log_2\left(\frac{1}{2}\right) - \frac{1}{2}\log_2\left(\frac{1}{2}\right)$$

Since $\log_2(1/2) = -1$:

$$H(X) = -\frac{1}{2}(-1) - \frac{1}{2}(-1) = \frac{1}{2} + \frac{1}{2} = 1 \text{ bit}$$

For something that's Boolean (binary), entropy is bounded at **1 bit**. Think about it: if you have a coin or something with only two outcomes, the most uncertain you could be is probability of a half. That's the highest possible uncertainty for a binary outcome.

I can tell you the result of a coin toss in one bit—I send you a 1 or a 0.

### Example: Four Outcomes

What if we have something with four possible outcomes, each equally likely (probability of $1/4$ for each)?

$$H(X) = -4 \times \frac{1}{4}\log_2\left(\frac{1}{4}\right) = -4 \times \frac{1}{4} \times (-2) = 2 \text{ bits}$$

So the entropy can be bigger than 1—just not for a single coin flip! If there are 4 possible outcomes, there's more information to be sent.

### Bounds on Entropy

**Is entropy upper bounded by 1?** No, entropy can be much higher. Be careful between things that are binary (0, 1) and things that have multiple values or real values. If something takes on 10 values, it usually has higher entropy than something that takes on two values.

**Can entropy be negative?** No! Entropy cannot be below zero. Entropy is **strictly non-negative**.

Why? If you think about $-P \log_2 P$:
- $P$ is bounded between 0 and 1
- Because $P$ is between 0 and 1, $\log_2 P$ is always negative (or zero when $P = 1$)
- Therefore, $-P \log_2 P \geq 0$

Entropy is 0 if and only if you have a probability of 1 for one outcome. Entropy of 0 means you know the answer for sure.

### Example: Unfair Coin

If you have a coin with 3/4 chance of one outcome and 1/4 chance of the other:

$$H(X) = -\frac{3}{4}\log_2\left(\frac{3}{4}\right) - \frac{1}{4}\log_2\left(\frac{1}{4}\right)$$

Sadly, this can't be computed easily in your head because $\log_2(3)$ is not a nice number. But this value is guaranteed to be between 0 and 1—it's more uncertain than knowing for sure it's 1, but less uncertain than a fair coin (probability 1/2).

If you plug it into your calculator, you'll get a number somewhere between 0 and 1.

## Conditional Entropy

Most of machine learning involves computing the probability of $Y$ given $X$. We're going to move toward **conditional entropy**.

### Specific Conditional Entropy

Consider a dataset with some $X$s (features) and some $Y$s (labels). We'll ignore the fact that it's a sample and work with the empirical distribution (like empirical risk, in-sample).

From the data, we can estimate:
- The probability that somebody likes Gladiator (50-50 in our dataset): $P(Y) = 1/2$
- The probability of being a math major and liking Gladiator: $P(\text{major} = \text{math}, Y = \text{yes}) = 1/4$ (by counting)
- Conditional probabilities like $P(Y = \text{yes} \mid \text{major} = \text{history})$

We can compute:
- The entropy of $X$ and the entropy of $Y$
- $Y$ has entropy of 1 (since it's 50-50)
- Entropy of $X$ is bigger because there are more outcomes (more uncertainty)

Now, what's the entropy of $Y$ **for some particular** $X$? 

For example, subset the world to only math majors and compute the entropy of $Y$ for people who are math majors. If among the math majors there's one yes, one no, one no, one yes, then the entropy for math majors is 1.

This is the **specific conditional entropy** $H(Y \mid X = x)$ for that particular value of $X$.

For history majors, what's the entropy of $Y$? If all history majors gave the same answer (say, all "no"), then the entropy is **0**! We know for sure that every history major has the same outcome.

(Again, this is on the training data, which is okay for now.)

### General Conditional Entropy

Now we need to take the expectation over all the $X$s. The specific conditional entropy was for each particular $X$. Now we want the **conditional entropy**, which is the expected number of bits to transmit $Y$ given $X$:

$$H(Y \mid X) = \mathbb{E}_X[H(Y \mid X)]$$

Or equivalently:

$$H(Y \mid X) = \sum_{x} P(x) \cdot H(Y \mid X = x)$$

This is the summation over each outcome $X$ could take (e.g., math, history, computer science) of:
- The probability that $X$ takes on that value (which we count from the data)
- Times the specific conditional entropy of $Y$ given that $X$

Then we take the expected value and multiply them out.

It's the same thing we did before! Before, we had:

$$H(X) = \mathbb{E}[-\log_2 P(X)]$$

which is the expected value over the probability of $-\log_2 P$: how likely am I to see each outcome, multiplied by the $-\log_2$ probability (information content).

Now we're doing the same thing: the expected value over each of the specific entropies.

### Interpretation

The left side, $H(Y \mid X)$, is the entropy of $Y$ given $X$, which is what we're doing in machine learning pretty much always. When we're doing machine learning, we're estimating the probability of $Y$ given $X$.

In general, the most common use is: we want to know $Y$ given $X$. The better our model, the **lower** the entropy of $Y$ given $X$ (or more precisely, we want low $H(Y \mid X)$).

The entropy of $Y$ alone, I don't control—that's given by the dataset. The doctors or Google or whoever gives me the $Y$s. But as a modeler, I get to pick the model that estimates $P(Y \mid X)$, and I would like that to have **low entropy**, which means I actually know what's going on.

We will see that this is the right loss function to use in general.

### The Hypothetical Information Transmission Framework

It's a weird conceptual framework because I'm not really transmitting information in practice. But the way to think of it conceptually is:

Assume that you and I both have all the $X$s. I've got a bunch of $Y$s, and I would like to send the $Y$s to you to some accuracy. (Assume they're discrete for the moment, like discrete classes.)

I could try to send them one by one—that would cost entropy $H(Y)$ per symbol. But if there's a model that tells you a lot about them—if I tell you "here's the model," and given this $X$, you can predict $Y$ really accurately—then once you have the model, I don't need to send all the $Y$s, because you can apply the model to your $X$s.

What really happens, in some sense, is:
1. I give you the model (costs some bits to send)
2. You predict all the $Y$s using the model
3. I send you only the residuals (the ones you got wrong out of the thousand)

The better my model, the fewer errors or residuals I have to send you. The model is saving me from sending each $Y$ one by one.

Think of large numbers—say a million $Y$s with corresponding $X$s. I send you the model, which gets 99% of them right. The model itself is pretty short (shorter is better, less complex is better—also measured in bits). Then I only have to send you the 1% I got wrong (the 10,000 wrong ones)—I send you that information.

It's a very hypothetical world, but what it's quantifying is the accuracy of the model: how much you got wrong, how much uncertainty is left after you have the model.

## Information Gain

Everything so far is just mathematics. We're going to start applying this to actual feature selection, decision trees, and loss metrics for neural nets.

### Defining Information Gain

What we're going to do is learn information about the world. What's the value of learning a new feature $X$? Think of a doctor asking questions: which question is the most informative to ask? The 20 questions game.

This allows us to define **information gain**:

$$\text{IG}(Y \mid X) = H(Y) - H(Y \mid X)$$

The information gain of $Y$ given $X$ is:
- The entropy of $Y$ (how uncertain you are about $Y$ knowing nothing)
- Minus the entropy of $Y$ given $X$ (uncertainty once you see $X$)

Knowing $X$ should reduce the entropy of $Y$. Information gain measures **how much more knowledge** you have, how much **less uncertain** you are (measured in bits).

### Example Calculation

Suppose:
- Entropy of $Y$ is 1 (it's a 50-50 coin toss, knowing nothing about $X$)
- The conditional entropy $H(Y \mid X) = 0.5$ (after knowing $X$, it takes on average half a bit to tell you what $Y$ is)
- Information gain: $1 - 0.5 = 0.5$ bits

By learning $X$, we've gained **half a bit** of information about $Y$. A good $X$ to ask about is one that reduces your uncertainty in $Y$.

## Concrete Examples

### Example 1: Predicting Wealth from Gender

Suppose you want to predict whether somebody is poor or rich (using some threshold like $70,000). You have some entropy over "rich" initially—in this case, it's 0.79 bits (because there are fewer rich people than poor people in the dataset).

Then we could ask: conditional on learning whether someone is male or female, what does that tell us? 

In this particular dataset, men are more likely to be rich than women (unfortunately, life is unfair). We can compute:
- The specific conditional entropy of wealth for women
- The specific conditional entropy of wealth for men
- The information gain

It turns out that if I learn whether you're male or female, the information gain is **0.0366 bits**.

This is a quantitative measure: **how much does learning this feature $X$ tell me about this label $Y$?** And it's nice and cheap to compute.

### Example 2: Predicting Wealth from Age

Instead of something binary, let's use age. In demographics, it's common to measure age not as a real number, but by categories: teens, 20s, 30s, 40s, 50s, etc.—something that takes on maybe 9 different values.

You can ask: how valuable is it to ask someone's age in terms of predicting their wealth?

Not surprisingly, in this dataset:
- People in their teens are poorer
- People in their 40s and 50s are wealthiest  
- Apparently people in their 90s are doing okay too

How valuable is it to ask someone's age in terms of reducing entropy? We can compute the information gain here—it's **0.084 bits**, substantially higher than the 0.036 bits we got from gender.

Is it surprising that age tells you more than gender? Why would you expect it?

**Answer**: The more different labels you have for a variable, in general, the more entropy it has and the more information it contains. If you've got something with 9 outcomes, all things being equal, it's going to have more information content and will reduce your uncertainty more.

I could ask "how many dogs or cats do you have"—I don't know if that tells you anything about wealth (I don't have that data). But in general, if you've got something with 9 outcomes, all things being equal, it will be more informative.

### Example 3: Binary Age Question

We could do the same thing and ask: "Are you under or over 40?" That's obviously a much less informative question than "Are you in your teens, 20s, 30s, 40s, etc."

A binary question is not as good as asking something with 10 categories (on the same data, for sure).

### Example 4: Age in Years

What if I ask "What's your age in years?" (1 through 100). That has a little bit more information—probably not much more than the 10-category version, but a tiny bit more information, especially around transitions like age 60 or 65 where there might be little shifts.

## Statistical Considerations

In general, the probabilities we're working with are all based on the **training data**. We're going to do all these things to minimize an **empirical entropy**.

If we were being rigorous, we'd say: we don't care about minimizing empirical entropy—we care about performance on a test set. When we get to deep learning, we'll do regularization to avoid overfitting.

We're going to:
A. Sort of ignore the uncertainty issues (because we're computer scientists)
B. Regularize heavily

When we get to Wednesday's lecture, we'll do ensemble methods with lots of decision trees, and we'll have a bunch of hyperparameters to control the regularization.

We're going to use the machine learning trick: we don't really worry about uncertainties in machine learning, but we **really, really worry about overfitting**.

Clearly, this metric of optimizing information gain is going to overfit. Today we're going to work in "math land" where we just care about the empirical distribution. But on Wednesday, we'll realize we were being naive and need to worry about hyperparameters to control overfitting.

## Two Main Uses of Information Theory in Machine Learning

### Use 1: Feature Selection and Question Asking

In machine learning, we often assume the $X$s are all fixed—we already have the data. But in many real-world cases, you have to decide **which features to collect**. It costs money to measure things. If you're going to instrument your robot or ask your patients questions, which ones are the most informative?

This is precisely an **information gain question**.

The equivalent problem in machine learning is **feature selection**. Even if you have all the features, you'd rather build a smaller model with fewer of them.

**Why?**
1. **Explainability**: If you have 40 features instead of 1,000, doctors will actually look at 40 features, not 1,000
2. **Overfitting**: The fewer features I use, the less complex the model, and the less danger of overfitting

You might think computation speed matters, but actually, I mostly don't care about computation speed. What I really care about is **overfitting and explainability**.

Why is overfitting an issue? The fewer features I use, the less complex the model, and the less danger of overfitting.

## Decision Trees

We'll now cover decision trees. **Nobody uses decision trees by themselves**, but they are the basis for the incredibly widely used **gradient tree boosting** and other methods we'll cover on Wednesday.

Decision trees go by a bunch of names:
- Decision trees (my preferred term)
- Recursive partition trees
- Algorithm names: ID3, C4.5, CART, etc.

They've all been invented multiple times and are basically the same thing.

### Example: Predicting Political Affiliation

Classically, the game you might play in America is to predict whether someone's a Democrat or Republican. This is a 20-questions game where you ask people questions.

Example decision tree:
1. **Are you Black, Hispanic, or Asian?**
   - If no, continue...
2. **Is religion important in your life?**
   - If no, continue...
3. **Are you straight?**
   - If yes, continue...
4. **Did you attend college?**
   - This is a big one in America now
5. **Are you female?**
   - Big split on gender
6. **Are you 50 or younger?**

You can ask about the same attribute multiple times (not the exact same question, but for a real value like age, knowing you're over 50, you could also ask "Are you older than 60?").

This is a **recursive partition**: we're dividing up the world (or Americans, in this case). We split the world into groups in a way that maximizes information gain on whether you're Democrat or Republican.

At each node, you recursively partition based on the most informative feature until you reach leaf nodes with reliable predictions.

### The Decision Tree Building Process

Let's work through a concrete example. Suppose we have 5 people: some have a disease, some don't. We have symptoms S1, S2, S3.

#### Step 1: Choose the Best First Question

If I ask about S1, what do I get?

|  | S1 = Yes | S1 = No |
|--|----------|---------|
| Disease = Yes | 2 | 1 |
| Disease = No | 0 | 2 |

If I ask about S2:

|  | S2 = Yes | S2 = No |
|--|----------|---------|
| Disease = Yes | 1 | 2 |
| Disease = No | 1 | 1 |

How do I measure which is better? **Entropy!**

I want to find the **conditional entropy** $H(\text{Disease} \mid S1)$ and $H(\text{Disease} \mid S2)$ (and $H(\text{Disease} \mid S3)$). Then I ask whichever question **maximizes information gain**:

$$\text{IG} = H(\text{Disease}) - H(\text{Disease} \mid \text{Symptom})$$

The best question in a decision tree algorithm is the one that gives **maximal information gain** or most reduces the entropy.

Do I want the conditional entropy to be big or small? **Small!** I have some initial $H(\text{Disease})$ (which is messy: $-\frac{2}{5}\log_2(\frac{2}{5}) - \frac{3}{5}\log_2(\frac{3}{5})$), and from that I subtract the conditional entropy. I want the conditional entropy to be as small as possible to maximize information gain.

By inspection, you can see which one has lower entropy—it's the one that's more "pure" (more segregated into distinct classes).

#### Step 2: Build the Tree

Suppose S1 has the lowest entropy. I start my decision tree:

**Do you have S1?**
- **If Yes**: Everyone with S1 has the disease. Done! No more questions needed. (Entropy is 0 for this branch.)
- **If No**: There are 3 people here. What's the best next question?

For the people without S1, let's say S3 does it perfectly:

**Do you have S3?**
- **If Yes**: You have the disease
- **If No**: You don't have the disease

So the final decision tree is:
```
Do you have S1?
├─ Yes → Disease
└─ No → Do you have S3?
    ├─ Yes → Disease
    └─ No → No Disease
```

### Will This Give Perfect Classification?

In general, this will give **perfect classification on the training data**? Sometimes—there's no guarantee. You might have multiple people with the same symptoms, some with the disease and some without. There's no guarantee the best decision tree will be perfect; it just does the best it can. There's no guarantee your $X$ perfectly tells you $Y$.

### Downsides of Decision Trees

There's a major downside to recursive partition trees:

If I start with $n$ people at the top, in general:
- The next level has $n/2$ people
- The next level has $n/4$ people  
- The next level has $n/8$ people

The **sample size is going down exponentially fast**. This is generally bad news.

The good news: you don't have to go very deep because you're dividing up the world exponentially quickly.

The number of leaves at each level (if evenly split) goes up exponentially too. These things are sort of horrible models—they really quickly divide people into tiny groups (e.g., "people with bald heads, blue glasses, and shorts are likely to not have liver disease"). They can **overfit super easily**.

We'll worry about that later. In the old days, people would **prune** decision trees (chop them off and not go too deep). But in the modern era, no one uses decision trees by themselves—they're just a component in an ensemble algorithm (which we'll get to Wednesday).

You should be thinking: "My God, these are terrible, they're going to overfit!" But they do a nice job of breaking up the data. You can break up everybody in America into Democrats and Republicans in like a dozen questions. It's not perfect (there are people like Peter Thiel who are gay and Republican), but statistically it works quite well.

### Alternative Approaches (Not Used in Practice)

You could try to do a linear combination of features, but that's turned out not to be commonly used. What's going to happen (on Wednesday) is we'll learn to introduce noise, do a whole bunch of decision trees, and average over them. This turns out to be an **awesomely good method**.

## Use 2: Loss Functions for Probability Distributions

Now we're going to give you a different use of information theory (though it's actually not that different).

### Review of Distances and Similarities

We've talked a lot about distances. The most common distance was $L_2$ (or any $L_p$ norm):

$$d(x, y) = \|x - y\|_p$$

These are awesome distances for real-valued vectors, widely used, with beautiful properties:
- Symmetric
- Non-negative  
- Triangle inequality

We also talked about one kind of similarity: **kernels**, particularly Gaussian kernels:

$$K(x, y) = e^{-\frac{\|x-y\|^2}{2\sigma^2}}$$

This is sort of like the reverse of a distance (not an inverse, but $e$ to the negative distance squared).

### Introducing KL Divergence

Now I want to introduce a third measure that's just as popular: the **KL divergence** (Kullback-Leibler divergence).

Suppose we have:
- A **true probability distribution** $P$ (which we typically don't know, but might be approximated by the distribution in our data)
- An **alternative distribution** $Q$ (this is like $\hat{P}$, but we call it $Q$ because that's standard notation)

Think of $Q$ as our estimated probability distribution coming out of our model (like logistic regression or a neural net).

We'd like to ask: **how far away is $Q$ from $P$?**

One distribution is $P$, which is often the distribution of the data. One distribution is $Q$, which is always an approximation to $P$ and is usually given by a model (like a neural net).

### Definition of KL Divergence

KL divergence is defined as the **expected extra bits per point** that you have to use to transmit (or represent) $Q$ instead of $P$.

If $Q$ were exactly $P$, the KL divergence should be **0** (you've nailed it!). If $Q$ is not $P$, it's a less good representation—it'll be farther away, somewhat different.

Here's the formula:

$$D_{KL}(P \| Q) = \sum_{j=1}^{M} P_j \log_2 \frac{P_j}{Q_j}$$

This is:
- The summation over $j = 1$ to $M$ (the possible values $X$ can take on)
- Of the probability of $X_j$ from $P$ (this is the expected value over the true distribution)
- Times the log of the ratio: $\log_2(P_j / Q_j)$

Let's look at this again: if $P$ and $Q$ give the same estimates, then it's $\log_2(1) = 0$. If we're off, it matters **how often that $X$ takes on that value** (it's an expected value).

Expected value cares about $X$'s that are frequent. If some $X$ never shows up (like unicorns), we don't care what the probability of a unicorn is. If $P(X) = 0$, we don't care about it. But for $X$'s that are frequent, we really want to get our $P$ and $Q$ correct.

### Alternative Formulation

You can write this as:

$$D_{KL}(P \| Q) = \sum_j P_j (\log_2 P_j - \log_2 Q_j)$$

The nicest way to factor this is:

$$D_{KL}(P \| Q) = -\sum_j P_j \log_2 Q_j + \sum_j P_j \log_2 P_j$$

This is:
$$D_{KL}(P \| Q) = H(P, Q) - H(P)$$

where:
- $H(P, Q) = -\sum_j P_j \log_2 Q_j$ is the **cross-entropy** between $P$ and $Q$
- $H(P) = -\sum_j P_j \log_2 P_j$ is the **entropy** of $P$

### What Are We Measuring?

We have something that says **how different $Q$ is from $P$**.

**First question**: Does this look like a distance? Is it symmetric?

**No!** Not symmetric. If you swap $P$ and $Q$, you get a different value:

$$P \log_2 \frac{P}{Q} \neq P \log_2 \frac{Q}{P}$$

One of them is the true distribution, one's the approximation. It really matters which is which.

**Can it be negative?** That's a little harder to prove, but the answer is **no**. It's strictly non-negative (well, it can be zero). If $P = Q$, then $D_{KL}(P \| Q) = 0$. If not, it's strictly positive.

The proof isn't hard (it relies on $P$ and $Q$ being probability distributions that sum to 1), but we won't go through it here.

### Properties of KL Divergence

It's **not a distance or a similarity** in the classic sense. It's technically a **divergence**.

Properties:
- The divergence is 0 if and only if $P = Q$
- It's strictly non-negative
- It's **not symmetric**
- Doesn't satisfy triangle inequality (which is less important)

The important thing to note is this **asymmetry**. This is the first time we've seen this kind of weird asymmetry.

It's not a distance metric in the classic sense, but it's a measure of **how well does the probability distribution $Q$ approximate the probability distribution $P$**.

### Why Can't It Be Negative?

For any distributions $P$ and $Q$, you will never be able to plug them into KL divergence and get a negative result. This is a mathematical property.

**Intuition**: If I'm trying to send data from distribution $P$, how many extra bits will it take with a different distribution $Q$? If I'm using the wrong distribution, it can only cost me bits (or the same number of bits if they're identical). It can never save me bits.

Approximating something always costs you more bits than the actual thing, unless you get it exactly right (in which case it costs you nothing extra).

The proof relies on $P$ and $Q$ being probability distributions (they sum to 1).

## Applications of KL Divergence

There are two main uses of KL divergence, and we'll see them throughout machine learning.

### Application 1: Loss Function for Classification

Often we'll have labels $Y$ that can take on multiple values. So far, we've had things with one output, but often we try to predict one of a thousand diseases or one of 10,000 objects.

$Y$ takes on a bunch of categorical values. If you have something that could be a dog, cat, horse, fire truck, sheep, etc., then there'll be a **true output**, which is a **1** for the thing it is (say, sheep) and **0** for everything else.

This is called a **one-hot encoding**. The actual label is always one 1 and a bunch of zeros. Note that this fits the notion of a probability (it sums to 1, all values are between 0 and 1).

We'll use models (like neural nets) that output a probability distribution. Recall we talked about **softmax**: it takes any $X$ and maps it to a probability distribution. You can ask: how well does my softmax output $Q$ match the actual $P$ (the true answer)?

I have a neural net that gives me $Q(X)$ (say, 0.2 for one class, 0.01 for another, 0.7 for another, 0.09 for another, etc.). I have the true value $P$, which is one-hot: $(0, 0, 1, 0, \ldots)$.

I want to know: **how close is $Q$ to $P$?** I want a loss function for my neural net. The loss function that's the best one (that people mostly use) is:

$$\mathcal{L} = D_{KL}(P \| Q)$$

Take the KL divergence between $P$ and $Q$: how well does my output $Q$ approximate $P$?

### Simplification for One-Hot Encoding

This is a special case of KL divergence because $P$ is zero for everything except one value (where it's 1). It's **one-hot**.

The loss function becomes:

$$\mathcal{L} = -\log_2 Q_{\text{correct}}$$

where $Q_{\text{correct}}$ is the probability the model assigned to the correct class.

Let me say that again: the nice loss function to use is to compute the KL divergence between the one-hot truth and the output $Q$ from the model. This simplifies to the **negative log of the probability of the correct answer**.

### Historical Context

I'm old enough to remember when people used to use neural nets with $L_2$ losses. $L_2$ losses work great if you have real numbers with Gaussian noise on them. But $L_2$ losses do **not** work well if you're doing something that gives you an output which is a probability distribution over objects, labels, or diseases.

In the modern era, **nobody uses $L_2$ losses over probability distributions**. They use what I would call KL divergence (or we'll see it has a slightly different name people use).

Does this make sense? The negative log of your predicted probability of the true answer, when averaged over all training points, gives you a KL divergence between:
- The true labels (the $Y$s)
- The output probability distributions from your neural net

### Example: Language Models

**Is this how ChatGPT models language?** Yes! This is the loss function that's used.

In that case, the output is a set of (ballpark) 40,000 tokens. GPT calculates a probability distribution over all 40,000 possible next tokens. You then see the actual next token, and you look up: what probability did I give to that token? Your penalty is $-\log_2$ (probability of that token), and you do gradient descent on it.

### Clarification on "True Distribution"

**Question**: The true distribution isn't one-hot though, is it?

**Answer**: Yes and no. In fact, what we're looking at is the **empirical distribution**. I have a big dataset (say, a trillion tokens). In each case, I'm predicting the probability of the next token.

You could say there's a "true probability" out there. I'm pretty close to it because I have pretty much all the data in the world. What I have is an empirical distribution: conditional on $X$, how likely is $Y$? I get a $Y$ of that token. I do that over and over for every example in the dataset.

That is an estimate of some sort of true probability distribution. What we're using for machine learning is not the "true probability distribution" in some Platonic sense, but the **empirical probability distribution** from our data.

Remember, it's an expected value—everything we have is an expected value, which means we sum or average over all the predictions we make.

### Why KL Divergence Over $L_2$ Loss?

**Question**: Why did we choose KL divergence over $L_2$ loss function?

**Engineering answer**: It works better.

**Theoretical answer**: In general, if you think about distances in probability space, consider: which is farther away?
- Distance from 0.8 to 0.9?
- Distance from 0.0 to 0.1?

0 to 0.1 is farther away! If I tell you the correct probability of something is zero, and the true answer is 1 (e.g., "The chance of the rocket ship blowing up is zero")—what should the loss be?

It's in **log space**—this is about log probability. If you say in $Q$ the probability is zero, but in $P$ it's not zero, you have $\log_2(0)$ in there. It's **infinitely wrong** to say something has probability zero if it happens.

**Is the converse true?** If something never happens and I say "this thing has a 0.01 chance of happening," and it doesn't happen, that's not so bad.

Note this **asymmetry**: if you promise me it's never going to happen and it happens, that's literally infinitely bad. **Log probability space** is the way you should compare probabilities.

The other piece: if you have 40,000 possible tokens to predict (standard these days, or a bit more), I don't care about the probability distributions you predicted for all the ones you got wrong. I **only** care about the probability you gave to the one that was the right one.

Looking at all of them (with $L_2$) is typically a bad idea.

### Distillation (Brief Digression)

There's one case (not covered in this class) where this isn't quite true: **distillation**.

It's common in deep learning to train a really big, expensive model, then use that big model to train a much smaller, cheaper model (because it's expensive to run these things, and there are geopolitical issues with getting enough chips).

What you do:
1. Train your large model
2. It outputs a probability distribution  
3. Use that as the "correct answer" to train the small model

The small model is trained over all 40,000 tokens to match the distribution of the large model. That's called **distillation**, and it works much better than training on single tokens, because instead of giving one token at a time, you're giving the whole distribution.

Everyone uses distillation (OpenAI, DeepSeek, everybody). They train the big model, get a probability distribution, then approximate that with a much cheaper, smaller model. The approximation they use is always **KL divergence**.

There's longer theory showing it's the only right thing to do, but either way, it's the right thing to do.

## Cross-Entropy Loss

The typical approach people use is to take the KL divergence between $P$ and $Q$:

$$D_{KL}(P \| Q) = H(P, Q) - H(P)$$

where $H(P, Q)$ is the cross-entropy.

If you take a standard deep learning course, they'll say: "Our neural nets will **minimize the cross-entropy**."

I'm thinking: awesome! But minimize with respect to what? **The weights in the $Q$ model!**

Minimizing cross-entropy is the same as minimizing KL divergence, because the entropy $H(P)$ is just a function of the training data—there are no model weights in the entropy of $P$.

You can either:
- Minimize KL divergence (like a good mathematician)
- Minimize cross-entropy (like a machine learning person)

It's **exactly the same minimization**.

### Terminology for Job Interviews

There's a lot of jargon in this course. In a job interview, when someone asks "Why do you want to minimize cross-entropy?", you should be able to give a coherent answer:

"Minimizing cross-entropy between the predicted distribution and the true distribution is equivalent to minimizing the KL divergence, which measures how well our model's probability distribution approximates the true distribution. We're essentially finding the model that best represents the data in terms of information content."

## KL Divergence in Decision Trees

The other thing to notice is that when we look at decision trees, **KL divergence** is what's being maximized for information gain.

If we're going to ask about some new feature $X'$ (symptom 1, symptom 2, symptom 3), we can compute the KL divergence between:
- $Y$ with the $X$'s we know so far and the new feature we added to our model
- The probability of $Y$ before we asked the question

I want to ask the feature $X'$ (the symptom) that's going to:
- **Most push me away** from my current distribution
- Give me the **most learning**
- **Maximize the KL divergence** (or information gain)

Decision trees (or the 20-questions game) say: **find the feature I can measure which will give the biggest information gain** (or equivalently, have the largest KL divergence).

### Training vs. Feature Selection

Note the difference:
- **In training**: I want to find weights that make $Q$ as close to the truth $Y$ as possible (minimize KL divergence)
- **For decision trees/feature selection**: I want to find the feature that pushes it away, that teaches me the most, that gives me the most new information about $Y$ in expectation (maximize KL divergence or information gain)

This is the feature that:
- Most changes what I think about $Y$
- Tells me the most about $Y$  
- Gives me the largest information gain

If you go back and review this carefully, you'll see that **information gain is, in fact, the KL divergence**.

## Summary

Today we covered:
- **Entropy**: $H(X) = -\sum_j P_j \log_2 P_j$
- **Information gain**: $\text{IG}(Y \mid X) = H(Y) - H(Y \mid X)$
- **Conditional entropy**: $H(Y \mid X) = \sum_x P(x) H(Y \mid X = x)$
- The standard **decision tree** (recursive partition) algorithm
- **KL divergence**: $D_{KL}(P \| Q) = \sum_j P_j \log_2 \frac{P_j}{Q_j}$
- **Cross-entropy**: $H(P, Q) = -\sum_j P_j \log_2 Q_j$

All this jargon you should know!

On Wednesday, we're going to use these concepts in actually doing **random forests** and other nice ensemble methods.

---

**End of Lecture**

---

# CIS 5200: Machine Learning - Lecture 10: Ensemble Methods and Boosting

# Ensemble Methods and Boosting

## Introduction to Bagging

If I have different 30% of the data, I run the linear regression models, I then bag them, average them.

**Helpful?**

Yeah, I got a lot of "yes" here.

Linear regression models are linear.

If I take $T$ models, all of the form:

$$Y = W^T X$$

And if I do that for the first dataset, and I do that for the second dataset, and I do that for the third dataset, and I average them.

The resulting model is going to look like... it's still linear. If you average a bunch of linear models, you get a linear model.

So, bagging linear models is not going to help.

**Doesn't it reduce variance?**

Well, in some sense, if you... this is only 30% of the data, you'll have more variance here. But, in the end, you haven't learned anything differently, and there is no lower variance by doing this step versus doing this directly.

Let's think about what this minimizes. This minimizes a squared error loss:

$$\min_W ||Y - W^T X||^2$$

It is the best solution possible to minimize the squared error. If you do a bunch of things that minimize the squared error on different samples, and add them together, you're going to get something that, within the sampling noise, but asymptotically, if you do an infinite number of these, will be exactly the same as this.

So again, combining a bunch of linear models in general gets you nothing because the linear models combined give you a linear model. You haven't enriched the form. In some sense, no, not even any sense.

We picked this to be:

$$\arg\min_W ||Y - W^T X||^2$$

If you want to find the solution to the argument of $Y - W^T X$, there's a closed form solution: $(X^T X)^{-1} X^T Y$, use gradients, whatever. That's the optimum. No amount of dividing stuff up is going to make it any better because this model is exactly the same as this in terms of its functional form. Right? Linear plus linear gives you linear.

No win. Sorry.

**So, bagging is useless for linear regression.**

Does that make sense? Yeah.

If we did a non-linear function, then we could potentially get some sort of benefit. In practice, doing it with logistic regression is also generally not helpful, because you've still really assumed a particular form which is linear under the hood.

So, in general, it's not... At least it's not mathematically trivial with logistic regression, but it turns out to be not particularly useful and pretty much never used.

Good question.

## Bagging Decision Trees

Um, how about decision trees? If I replace each of these models here with a decision tree number one, and a decision tree number two, and a decision tree number 3...

Now is bagging helpful?

**Yes**, right? Each of these has a different subset of the features in it. So I get out $T$ different decision trees, and they each have a different functional form, and if you add them up, I get something that's a much richer model than any single decision tree can represent.

**So, bagging decision trees is super good.**

Decision trees are pretty much all we use in something that's bagging-like.

Well, we'll see, it's both an ensemble, and it'll be done stage-wise, yep.

Think about if I have one decision tree that says: "Is it symptom 3? Yes, no. Is it symptom 1? Yes, no." And I have a different decision tree that says: "Is it symptom 2? Yes, no. Is it symptom 3? Yes, no."

If I add these together, is there any way to build a single decision tree that represents that?

**No.** This is a more powerful, more general representation. It has more degrees of freedom, it's more complex, it's more powerful.

It's just a different class of models when you add two decision trees. **Decision trees are not closed under addition.**

Linear models are closed under addition. You add any two linear models, you get a linear model. You add any two decision trees, in general, you don't get a decision tree.

Yeah.

### Aggregating Predictions

If I've drawn this, how do I average out? What I'm going to do is I'm going to take each data point, and for any point $X$, I will classify it. I use decision tree 1, and it goes boom, boom, and it says the answer is yes. I use decision tree 2, and I run it through, and the answer says no. On the average, the answer is zero, so I don't know, so it's a tie.

But then I do another decision tree, and it says yes, and I go, okay, I got two yeses and one no.

In general, if you're doing a small number of these, you would use an odd number. But we're in machine learning land. The default in scikit-learn for a random forest is 100. I think that's incredibly stupid. My default when I use scikit-learn is a thousand trees.

A thousand trees is not that expensive on the sort of size I have, which is, you know, 10,000, 50,000 patients, and 1,000 trees gives you less variance than 100 trees.

I suppose scikit-learn was built back when I was a boy, when 100 trees was a lot.

But the answer is, in practice, a default number for most applications for me is 1,000 trees. There's no point in my doing 1,001 trees. I could, but I've never had it split to 500, 500.

And so, in general, if you were in a tiny land, yes, but don't think about tiny numbers. Think literally about a thousand trees, yep.

### Question: Bagging Quadratic Models

**If all my weak models are quadratic, is it potentially helpful?**

Um... Won't the average also be quadratic? So if you have all... if it's the full quadratic with all the interaction terms, then sure, it's linear in the quadratic representation. Doesn't help.

So these are great for decision trees.

## Bagging Deep Learning Models

Um, what about deep learning?

First of all, if you run a thousand deep learning models to predict something, will there be variation?

If you subset, you train on a fraction $F$, you take 30% of your data, or 30% of your data for each of a thousand ones, and train... will each deep learning model be different?

**Yep.** And if you average them, you get something that's plausibly better than a typical one, right?

Again, do the thought experiment. I train up a thousand deep learning models for image classification, and I could have them vote. Would that be, in general, better than I trained up one CNN, which we haven't covered yet, deep learning model for image classification?

**Yep.** Yeah. In general, ensembling deep learning gives you better models.

Now, **in practice, nobody uses them** because rather than spending the time or cost to train up a thousand models, you should have one model that's three times as big.

So it's... so for... if cost matters, if cost matters, you're not going to ensemble. Right? If you have something that's general and flexible. So, in general, you're ensembling weak models.

So again, a neural net's a strong model. In general, you could ensemble a strong model, but mostly you don't. The benefit is small, and the cost is big, whereas if you ensemble a weak model, like a decision tree, the cost is small, and the benefit is big.

You look puzzled. Yeah.

### Question: Number of Trees vs Features

**Is the number of decision trees based on the number of features?**

Good question. Does it have to be?

Not really, right? So we'll see that we have a number of hyperparameters, of which one of the hyperparameters is the number of decision trees. But it doesn't really have to be based on the number of features.

Yeah.

### Question: Why Deep Learning Works Well with Bagging

**Is it because the deep learning is non-convex?**

Well, I think it's because each... Partly, yes. Each time you train a deep learning model with a different subset of data, you're going to converge to a really different sort of solution. Right? Whereas if you train a linear model, or even a logistic regression with a bunch of different subsets, you're going to converge to really similar models.

There'll be some noise—each different data point set gives you a slightly different model with some variance, but much higher variance than something that's very nonlinear and very non-convex.

Yeah, so good intuition. Yeah?

### Question: Bagging K-Nearest Neighbors

**Could you bag KNN?**

What would happen? I was thinking about that, let's take a KNN solution, and I have different subsets of the $K$.

So I'll get different predictions.

I've never thought about that.

It's going to end up maybe giving you some benefit, but not very much. Because you think if you take the $K$ nearest neighbors of each of the 30%, and do that a bunch of times, you'll end up getting sort of $1/0.3$, so you made $K$ sort of $1/0.3$ bigger.

But you're going to end up getting... averaging out things that in the limit looked just like picking a slightly bigger set of neighbors.

So it's not going to give you a big win to do the k-nearest neighbor, I think.

Nice question. That'd be a good midterm question. Yep.

### Summary: When to Ensemble

**Benefit of bagging neural nets is low, right? Why is that?**

They're strong models, right? They do a quite good job, and the better the model, the less need you have to combine a bunch of them.

Cool! Okay, that's for your notes, the answers I said.

---

## Random Forests Algorithm

Okay, first algorithm, which is... We'll get eventually to gradient tree boosting, which is very fancy. Random forests is a widely used, really simple algorithm that basically does bagging.

And the algorithm is as follows:

### Algorithm Steps

**Do $K$ times:**

1. Randomly select with replacement, a fraction $F$ of the $N$ data points. Right? Take 30% of the data points.

2. Build a decision tree. Typically, what they do is something that's a little funny—there's a lot of variations of this, and in fact, if you go to scikit-learn, you can do extremely random forests or whatever. But for each node of the tree, as I'm doing the decision tree, instead of choosing the best feature, you **randomly choose $M$ of the possible $P$ features** and take the best of those $M$.

   You with me? So it's not picking the best feature, it's randomly picking the best of a subset of the features.

3. Repeat until you have the tree.

4. I've done that $K$ times, then I just have the whole thing vote. Right? Pick the most frequent prediction.

Yeah.

### Hyperparameters

**How do I choose the right value?** So what are the hyperparameters here? There's two hyperparameters... there's... well, really three hyperparameters. Hyperparameters are:

- The fraction of the data $F$
- The fraction of the features $M/P$
- The depth of the tree $D$

How deep you go, and the number of trees $K$.

The standard sort of defaults are to use never less than 100, but I'd rather use a thousand trees.

And there's some nice theory and general practice that says a good way to pick $M$, the number of predictors to use, is to use **the square root of the number of features**:

$$M = \sqrt{P}$$

And what about $F$? Um... I always use whatever the default is in scikit-learn, and I forget what it is, but you're going to do something that's sort of like 20%. You don't want to do 2%, which is way too noisy. You don't want to do 80% where all your trees look the same. So it's going to be sort of like, you know, a third or a fifth or something.

### Why Randomly Select Features?

**Why do I want to do something that's stupid, like only picking $M$ rather than all $P$ of the features at each point?**

What is this doing to my model?

It's going to explore more stuff. It's making it less greedy. It's making it more of a weak learner.

So, **a standard modern random forest is doubly weak.** That's not a technical term.

But it's weak in the sense that you're using $F \times N$ of the points (bagging), and you're using $M$ out of $P$ of the features. Both of them make them crappier models, which increases the variance, makes the model weaker, and it means that it explores a broader set.

Right? And the weird thing about ensembling is that if you have a model that's really the best you could do, you do a full search of all possible decision trees, which is really expensive, and doesn't help much, but you find the best possible decision tree that's a strong model, in some sense. If you do the same thing a thousand times, you're averaging a thousand copies of the same model. It gets you no benefit.

Whereas if you make the model crappier, right? More variance here, more variance because we're not even trying some of the features. It makes sure that if there's one feature that dominates, most of the time, you didn't include it. It only came in $M$ out of $P$ times, or $M$ is the square root of $P$ here. So most times, you're not using the best feature, so you're trying another crappier feature. So it's going to come into the model.

**So this forces you to consider all the features.**

Think of the opposite of this. We did greedy searches, like stepwise regression. Find the best single feature. Find the second best feature. Find the third best feature incrementally.

In that case, if a feature is redundant or overlapping, it can throw it away. It gives you a minimal subset.

Whereas this gives you sort of a maximal subset. If the feature is ever useful, throwing away most of the other stuff, put it in.

And it's going to add noise, or variance, and then you can average that out.

### Runtime Cost

**How much does it cost me to run this at runtime?**

A thousand decision trees. But hey, they're pretty fast. Right? They're typically not very deep, they might be 10 features deep.

So, it's a thousand times making 10 choices.

Yeah.

### Tree Depth

**How do I find the depth of the decision tree?**

Most of these algorithms, you pick some number, a depth, say 10, and it will just keep going down the decision tree, and anytime it gets to 10, it just stops.

Yeah, so standard practice is—I repeat it so you can hear it—you might have 100 features, or 1,000 features, you would standardly have a depth of about... about 10, or 5, even, is plenty, and it will then go 5 features deep, which, of course, is of order $2^5$ decisions. Right? And then that's your $2^5$ decisions, then you stop.

And in general, if you think of these as statisticians call these interaction terms, this is $A$ AND $B$ AND NOT $C$ AND $D$ AND NOT $E$. It's got a bunch of ANDs in it. In general, it's not helpful to have that many ANDs.

So, if you want to think about this as searching over a space that says, I'm going to have a thousand terms that I add together, each of which has up to 10 ANDs in it.

And more depth is more overfitting, but remember, I'm not horrified by overfitting, because I'm going to average this a thousand times. If you average a thousand overfit things with enough noise in it, again, by enough variance in it, then it's fine to overfit.

**So it's perfectly fine in random forest that every one of these thousand trees is overfit.**

Right? As long as they have enough variation due to the variation over training points and variation over features at each node, you get something that ends up not overfitting on average.

So, random forests are really good at not overfitting because they're averaging out over the variance. You're averaging out overfitting.

Yeah.

### Question: Depth Bound

**Is depth bounded by the number of features?**

Yes, mathematically, how deep could your tree be?

Worst case, it could be $P$ deep if the features... if the tree was a balanced tree.

For a balanced tree, how deep is it going to tend to go? More like $\log$ of the number of features.

So I think worst case is a terrible... I know computer scientists like worst case, but that's really not the right way to think. It's better to think in machine learning in average case. So think of a plausible tree depth as being not more than the log of the number of the features.

Log base 2, because we're doing a binary split. So you really don't want very deep trees.

Yeah.

### Question: Order of Trees

**Does the order of trees matter?**

Great question. Is there any ordering on these trees?

**Nope.** Imagine I was in a hurry to get this done. Could I parallelize this over a thousand machines?

**Yes!** This is a fully parallelizable algorithm. I don't care about that so much, but I do wish to point out, because we're going to talk later about stage-wise regression, this is a parallel thing. Every one of these trees is exactly symmetric, and they're all independent.

Right? It's a thousand random draws of $F$ out of $N$ at each point, I pick a piece there, so the trees are all made entirely independently, and at the end, this is a MapReduce algorithm, for those of you who like the Google sort of structure, it's a MapReduce—you map it to a thousand machines if you care, and then you reduce it by having them vote.

In practice, I don't usually bother, because they're not that big.

Yeah.

### Why Make Models Weak?

**Why do I want each of the models to have high variance?**

Why am I trying... this is the key point, which is weird. I'm making them weaker. Make sense?

Each model is less good than it could be because I'm not using most of the features at any one decision point. Why is it good to make the models weak rather than strong?

Nope, it's not compute time at all. It's statistical, it's not a compute time issue.

More weight to things that were found unuseful before, but each tree is separate, so it doesn't... this is not... this is not stage-wise, there's nothing about what model worked before.

It's higher variance because it explores more, right? It's looking at a richer space of different possible models, and by trying lots of different models, it's looking over a larger space of possible models.

Remember we talked in the first class that there were 3 things that go into machine learning? One is sort of the feature set, the inputs and outputs, and the loss function. One is the model class, and the other is the optimization. This has got a funny optimization search sort of thing.

But this is a very rich model class. The class of all possible models that are of depth of 10 averaged over a thousand—it's a very big class of models.

And we're going to sort of semi-greedily explore it.

But if we fit each model using all of the training data and all the features, we'll get a thousand trees that are exactly the same. That's pointless.

I can average a thousand identical trees. I need the trees to be different.

Right? Averaging 1,000 trees has the same power as averaging one tree.

So I need the trees to be different, and the more different I get them, in some way that's still vaguely useful, the richer the space of models that I'm exploring is.

So it's not really searching a space, it's certainly not a gradient descent, but it is trying to do something that says that I really want to find something that averages a whole bunch of different trees, each of which is sort of optimal within its own subspace of its training points and its features, such that when I average all of them, I average out the pointless variance and I'm left with a complicated model that doesn't overfit.

Right? **Bias-variance trade-off for a fixed model is zero-sum.** You're sort of stuck. Right? You do the best you can, but if you make the model more complicated, well, in general, you'll get better fit and more variance.

But if you can do it with something that has lots of weak models and average it, you can get a richer model without having higher variance overall. So the total random forest is very low variance.

If you run the random forest algorithm five times, it's incredibly stable. The individual trees are all massively different because they're trained on different sets of points and different sets of features, right?

So it's a funny algorithm. **It's locally extremely unstable** because you pick random sets of points and random features, **but on average, it's extremely stable.**

It's a very low variance, low bias algorithm. It makes very little assumption about the data, and it tends not to overfit.

So it's a great algorithm, yeah.

Sorry?

### Feature Selection at Each Node

We're... at each node, we're taking... at each point in the tree, we're sitting right here, and we now say, okay, there are $P$ features, I'm going to randomly pick $M$ of them, I'll pick the best one with the highest information gain, I will make the decision, then I go here and I pick another random set of $M$ of them.

And I will compute the information theory gain for each of those and pick the best of those, so at every single point in the tree, I'm picking a different, potentially different, random subset of $M$ of the features.

Right? So it's pretty random.

Well, it could be done. Forget the MapReduce, that's not important.

## Random Forests: Regularization

So these are really widely used. They don't overfit because you're averaging out over the variance.

In some sense, these things have a magical sort of regularization.

So, so far, we've regularized mostly by putting a penalty in, or equivalently, by putting a prior in, which we showed was the same as a penalty.

A different... you can... we won't show it in this course, you can regularize by taking all your $X$'s and just adding a bit of Gaussian noise to all the $X$'s. That looks awfully like Ridge. If you run the math on it, um, not quite exactly the same, but very similar.

If you put noise into your system, it makes it harder for it to memorize it. It makes it more noisy, so by adding noise and then averaging, we are regularizing.

**So this is a new regularizing technique we haven't seen before, and the regularizing technique is: throw in noise and average over the noise.**

And here, the noise is not adding noise to $X$'s, the noise, or variance is subsetting the features, randomly only picking a subset of the predictors.

But we're adding, in effect, a source of variance. And when you add in this sort of piece here, this means that you can't actually memorize the data, because every decision tree has seen a different subset of the data. It can't perfectly memorize it.

So it's a... it's fitting crappy partial models, and then averaging them.

Well, regularization does... regularization has the benefit of reducing variance. You could also think of regularization as smoothing. It gives smoother... less tendency to overfit.

So they're all tied together. It's the same... the regularization tends to be a shrinkage, a smoothing, a reduction of variance. They're all tied together.

Cool. Um, and these are nicely rich models, they tend not to underfit either. So it's a nice model. I like it particularly for people who are not practitioners. If you're going to train a neural net, it's easy to have it fail.

You get the wrong learning rate, you have to choose the size of it, there's a bunch of hyperparameters that matter a lot.

In this one, it's not very sensitive. If you use the right out-of-the-box hyperparameters, although, again, I tend to use a thousand trees rather than 100 to get more averaging, it works pretty much reliably.

And so they're nice. And it's...

Why is it... decision trees are terrible? Decision trees are a very poor model for many tasks. They don't capture additive contributions, right?

Decision trees are just "this AND this," or "this AND that." Very unstable, complicated model, doesn't do good averaging, whereas random forests have both the interactions and the averaging.

Cool. Oh, that's weird, um... Why is Poll Everywhere trying to take over? We're not using Poll Everywhere today!

Cool! Um... No, not from the start. Okay!

---

## Stage-wise Regression

That was ensembles. Second big concept is **stage-wise regression**.

And stage-wise regression is a search procedure, different from stepwise regression.

And the idea of stage-wise regression is:

### Algorithm

Start out... we're going to start out with some initial model, which typically will say, hey, our initial model for regression will be just predict zero always:

$$H_0(X) = 0$$

And then I'm going to, for $T$ steps, for little $t = 1$ up to big $T$ steps:

1. I'm going to first **compute the residual**:

   $$R_t = Y - H_{t-1}(X)$$
   
   So I'm saying, hey, take my previous model, subtract it from $Y$, how far off was I?

2. Now what I'm going to do is try and **fit a model that predicts the residual**.

   In particular, I can fit almost anything I want, but I will do something that says, give me some model that predicts the residual.

3. And my **next model will be the old model plus the new incremental model**:

   $$H_t(X) = H_{t-1}(X) + \text{new model}$$

Make sense?

And one way to do that would be to say, hey, what I'm going to try and fit... my residual, write my residual of $t$ being equal to some model. This is my new $Y$, is the residual, and my regression, I can say that's equal to some weights:

$$R_t = \alpha \cdot \phi(X)$$

Call that $\alpha$ times... I could put anything here. In the general form I'll say would be the $X$, this could be... pick one of the features, right? So if you're doing your standard regression, pick any feature—feature number 3, feature number 7. You could do a stepwise, stage-wise, try all the possible features and see which one gives you the most benefit.

You could put a radial basis function here for your features, you can put whatever weak learner, put whatever you want, fit a regression coefficient.

And then, once I fit this, the next model $H_t$ is just equal to:

$$H_t = H_{t-1} + \alpha \cdot \phi(X)$$

The model I had before, plus the new contribution? Like, $\alpha$ times $\phi$ of $X$.

### Stage-wise vs Stepwise

Now, if I were to, say, try all possible features here, and pick the best one, how is this different from the stepwise regression we did before?

This is critically different. Standard stepwise regression is not stage-wise.

Well, remember stepwise regression? We don't look back.

**Everything we fit before in stage-wise remains fixed.** All the preceding alphas, right, we're incrementally adding one more weighted feature, and all the Model 4 has not changed at all.

In a classic stepwise regression as practiced by statisticians, each time you add a new feature, you fit the whole model again and get all the weights.

So this is a more greedy search. And it says, always what I'm doing is just fitting the residual.

### Practical Applications

And I find this surprisingly useful as a general philosophy.

Um, when did I last use it? Okay. Sometimes I have some text data, and I have some image data, and I have some audio data. And each of them has potentially a different model. Right? Take a different neural net or whatever for each of them.

I could ensemble them, I could fit one model for the NLP, one model for the image, one model for the audio, and then put those 3 features as weak learners into a model. That's not bad. Make sense?

But instead of ensembling them, I could stage-wise them. I first fit the NLP model, then I can compute the residual, and for the things the NLP model got wrong, now I fit the image model, and I add in that correction, and now I say, well, for the things that the combined image and NLP model got wrong, the residual there, fit those with the audio model.

I could train all three simultaneously with gradient descent. But I don't like to do that.

**Why not just train them all in one big model?**

Which, in some sense, is the right thing to do. I might, if I do them... if I do the residuals, get something better, but in general, if you have the choice, you get a better model if you train everything together, assuming you don't have a problem with gradient descent converging.

But there's a pragmatic reason that you often don't train the models together. **Often the models are a different type.**

So I have a different form of model with a different inductive bias for images than I do for text. Right, I might use a CNN for one and a transformer for another.

So I might use different model types, where it's not even easy to train them together.

And sometimes some of the models are ones that are given to me, I don't even get to retrain them. So I can ensemble somebody else's model. I can call an API at Hugging Face and take the output of their model, and put that into my ensemble.

Right? So I can ensemble anybody's model, not just my model.

Whereas if I'm trying to train it all together, I need to have all the models simultaneously.

So, it's nice to build all the models together, but very often, at least for a first step, and typically for classes like this, you'll build somewhat separate models for different modalities.

Right? And a lot of your final projects will do text and image, a lot of stuff right now doing text plus image.

And you'll think for each of the different types of data, what's the right sort of model? What's the right sort of regularization?

You'll need different regularization for imaging data than you will for text data, and you could try and do all the hyperparameters across all three models simultaneously, but that's a pain in the butt.

Whereas if I do it stage-wise, I can do the stage-wise of the first stage, find the best hyperparameters, I've now got all the predictions, that's locked, it's done. Now do the residuals, take the second stage, I can now do a hyperparameter search on the second stage.

**So it's a cleaner search procedure to do it stage-wise.**

So, stage-wise saves you effort in terms of hyperparameter selection, allows different models at each stage, and in many ways, it's a nice way to approach the world.

Yeah.

### Question: Ensemble Same Type of Models?

**Does ensembling have to be the same type of model?**

Yes, no.

I got one yes and one no. A great question, then.

Does ensembling need to be the same type of model? Yes? No? Yep, **no**.

No, it's very often nice to have them different models. Now, what you don't want to do is to train up a thousand models that are identical to ensemble them, but if you are given a bunch of different models from different people, it's super good.

If you want to make predictions, I do a lot of prediction stuff—get a whole bunch of people to give you a prediction, and ensemble the predictions. The crowd is better than anyone in the crowd. You're much better off averaging a bunch of people.

And our research shows the obvious. If I can get people with different knowledge—you know one country, you know a different country, you speak a different language. If I can ensemble people with different, diverse expertises, I'm going to get a better result than if I ensemble a bunch of people, all of whom know the same stuff, right?

I want someone who speaks Hindi, and someone who speaks Arabic, and someone who speaks Russian. I want someone who's an eco-boomer and someone who's an eco-doomer.

I want to... yeah, you don't want people to be wrong. You like all of them to be as accurate as possible, but subject to that trade-off.

And there's always this sort of bias-variance trade-off. If I could get a bunch of models, and the people I don't get to actually retrain the model. I get the people as I recruit them, right?

But averaging 50 people, or even 5 people, in general, way better than any one person.

Make sense? So averaging ensembles is a really good technique. Yeah?

Yep. Yep.

### Question: Finding the Best Model

So, what we'll see is sometimes we find the best one, and sometimes we don't. Often, we're going to try not to find the best one. And it's better to have a worse one, because this gives us often something that's like an ensembling method.

And the same way with a random forest, we said it's better to have crappier decision trees because I get more variety, I get a richer set. So often in stage-wise regression, we're going to be less greedy than we could be and not try to find the best next one.

It's very flexible. Stage-wise is a huge class of algorithms. I've described a couple of them. We'll see a couple more.

But let's, yeah, let's... let me put it on hold, I want to keep going a little bit. And because I want to go into gory detail, take, like, the rest of class today pretty much to talk about boosting, and then we'll very quickly wave our hands at gradient tree boosting.

---

## Boosting

But boosting is a good example of something which is, on the one hand, an ensemble method, so we're going to learn a combination of weak learners, right? Like before, where each one will be weighted.

But unlike a random forest, where we did them all in parallel, what we're now going to do is do astage-wise estimation. We'll estimate the first learner $H_1$, then the next one $H_2$ on the residual, then the next one $H_3$ on the residual for those two. So we're going to stage-wise learn the weak learners.

And this leads to methods like gradient tree boosting that are slightly better than random forests and widely used in data science.

### AdaBoost Algorithm

So, before I get there, I'm going to go on a long digression. We'll do a little bit of formal math. AdaBoost as an algorithm is not used very much, but it's got nice mathematical properties, and it's got a nice way to illustrate the intuition behind some of these.

So again, if you're an engineer, use gradient tree boosting. If you want good results, use random forest. But AdaBoost is nice to understand some of this notion of the philosophy of stage-wise: fit a model, find the residual, fit a new model to the residual, recurse.

And to show how these things converge, in different versions of this class, um, usually ones I'm not teaching, there's a lot of math on convergence properties. I mostly don't talk about them, because although the math is very nice, I don't find them particularly helpful for deciding what to use in real circumstances. But this case, there's... I can wave my hands at the convergence properties that are sort of nice.

So, we got a fairly messy situation here, but let's do it.

We're going to start with a training set of size $N$. And we're going to have some weights, capital $D$, on each of the observations.

So this is going to say how much weight should I give to each of the $N$ points in my training set?

Make sense? So $D$ is the weighting, and initially, we know nothing.

**So every training point has weight $\frac{1}{N}$:**

$$D_1(i) = \frac{1}{N} \quad \text{for all } i$$

Good. Now we're going to train some sort of a weak classifier. Maybe a decision stump? I haven't told you a stump yet. We know a decision tree.

**A decision stump just says you ask a decision tree of depth 1.** You get to ask, here's a feature, is it above or below some value? If it's above, it's either yes or no. If it's below, the opposite of it.

So a stump is just a decision tree with a single node in it. And that would be a classic, really weak learner.

Then what we're going to do is we're going to train our classifier on this data set, initially every point weighted equally, but, and here's the cool part, **we're going to shift the weights as we iterate, giving more weight to some features and less to others.**

And what we'll do is update the weights so that the weight on each feature, $D$, is the previous weight times... oh, let's write this out slowly, because it's so complicated!

And because it's sort of weird. We're going to have the new weight!

I know you can't read this in the back, but look up on the slide there:

$$D_{t+1}(i) = \frac{D_t(i) \cdot e^{-\alpha_t \cdot y_i \cdot H_t(x_i)}}{Z_t}$$

Can we read that there? Yes. The $-\alpha_t$ times $y_i$ times $H_t$ of $x_i$. So $y \times H$ is...

And we have this divided by some normalization constant.

$y$ times $H$ is what in the classification world?

**One, if you got it right. Minus one if you got it wrong.**

Right? So what are we going to do?

We're going to say **if you got it right, then down-weight it exponentially** with some weight $\alpha$, we'll get to in a second. If you got it right, pay less attention to that point in the future.

**If you got it wrong**, it's $e$ to the minus a negative number, **upweight it**.

So let me say that again. **The key insight of AdaBoost is: if you got a point correct in the past, put less weight on it. If you got it wrong in the past, put more weight on it.**

Right? Pay attention to the things you got wrong. Right?

This should vaguely remind you of support vector machines and the hinge loss. What does hinge loss say? If you're right past the margin, it has weight zero. If you're on the wrong side of the margin here, then the weight... the worse it is, the more it gets weighted.

Right? So it's got some of the same philosophy, but totally different math.

Um, the $Z$ here on the bottom is—the fancy term is a partition function. The $Z$, which I'm not going to write because it's up on the screen, just says, this $D$ is a distribution.

And by distribution, I mean a probability distribution, and that means that there are $N$ weights over the $N$ points. They must be between 0 and 1, and sum to 1.

So, this thing is certainly non-negative. $D$ is non-negative, it's like a probability, it's a weight that has the same form as a probability. You gotta divide by the sum of everything to make it still be a probability.

So the partition function's annoying to compute, but just basically says, hey, make sure these things really look like probabilities. Does that make sense? They will sum to 1.

And then my **output model** is going to be:

$$H(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t H_t(x)\right)$$

Output the sign of the weighted sum of each of these pieces.

### Computing Alpha

And the only thing I've skipped over is $\alpha$. Right? There's an $\alpha$ here, which is how to re-weight things. $\alpha$ is:

$$\alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)$$

One half of log of... hmm, $(1 - \epsilon)$ over $\epsilon$, but what is $\epsilon$? $\epsilon$ is the weighted sum of:

$$\epsilon_t = \sum_{i=1}^{N} D_t(i) \cdot \mathbb{1}[y_i \neq H_t(x_i)]$$

Here's a funny thing. One if $y_i$ is not equal to $H_t(x_i)$, so this has 1 if you got it wrong, zero if you got it right.

So, what is $\epsilon$? **$\epsilon$ is the weighted error rate.** Right? For every point that you got wrong of my $N$ points, I pay a penalty $D$ for everything I got right, I pay a penalty 0. So, $\epsilon$ is a measure of how badly I'm doing.

Right? It's an error. It's zero if I got it right, right? If I got everything right, $\epsilon$ would be zero.

So, $\epsilon$ is the weighted error rate.

**What is $(1 - \epsilon) / \epsilon$?** $\epsilon$ is an error rate. $(1 - \epsilon)$ is a correct rate. This is like an **odds ratio**.

Right? And remember that $D$ has the property of being like a probability. $D$ sums up to 1, so your error here... if $D$ of everything were equal, and you got everything wrong, your error $\epsilon$ would be one, right?

Everything we're... I guess, incredibly bad. If everything were wrong, you'd have $N$ times $\frac{1}{N}$, you'd have a... worst case, you'd have an epsilon of 1.

In general, you should never do better than... worse than random flipping. If you randomly flipped a coin and you had equal weight, then $\epsilon$ is going to be a half.

Right? And if you have an $\epsilon$ of a half, then this is the log of $\frac{1/2}{1/2}$. Which is... zero. Well, sorry. No, $\frac{1/2}{1/2}$ is 1, log of 1 is zero.

**So $\alpha$ would be zero if you were random guessing.**

If you did better than that, then, if $\epsilon$ gets small, as $\epsilon$ goes to zero, this is... goes to $\frac{1}{0}$, this becomes log of a big number, $\alpha$ gets big as you get things going smaller.

So this is a long way of saying the details don't matter. What does matter is:

- We have a weight on every point
- We compute an error, a weighted error rate
- We use the weighted error rate to decide how much should we weight the next round

Remember, $\alpha$ is the scaling that says how much should you downweight the right things, and how much should you upweight the wrong things?

And again, I want to note that this is an exponential function, so it's a pretty powerful down-weighting of the things you got right and up-weighting of the things you got wrong.

### AdaBoost Example

Let's walk through a concrete example. Um, I've got one linked through here. Take me there.

That's the exact same equation. Um, in practice, as $\epsilon$... if $\epsilon$ is higher, $\alpha$ is lower. I just said $\epsilon$ of 0.5 was $\alpha$ of 0.

Here's the function: as $\epsilon$ gets smaller, $\alpha$ gets bigger.

Cool! So, let's steal something from Rob Schapire, who sort of did AdaBoost. Here's a simple XY plot where I have some blue X's and some red minus signs. And I want to learn a classifier.

I'm going to have stumps. So what's a stump? A stump is either a line that splits $X_1$ at some point—so, yes and no—or that splits $X_2$ at some point.

Right? And initially, everything is weighted the same by the size. I can do a search for a good split.

Here, for example, is a nice split that says, hey, if $X_1$ is bigger than this value, call everything a minus sign. If $X_1$ is less than this value, call everything a plus sign.

Right? So I've done a single split, a stump, divides the world into two pieces. It's a weak learner, and I will then get something that says, well, these guys are all pluses, these guys are minuses. You can see I got most things right, but I got 3 things wrong.

Right? So I end the first round.

Now, what am I going to do? I'm going to change the weights. I can compute the error rate $\epsilon$, which is 0.3. I got 3 out of 10 of them wrong.

They're all weighted equally. I can compute $\alpha$, which is just a formula:

$$\alpha = \frac{1}{2} \ln\left(\frac{1 - 0.3}{0.3}\right)$$

One half log $(1 - 0.3)$ over $0.3$, so I've got $\epsilon$ and $\alpha$.

Now what I do is I redo the weights. And I'm going to take everything I got right, and I'm going to weight them by $e^{-\alpha}$ (what was $\alpha$? 0.42) times 1, because I got them right. And they're down-weighted, normalized.

And the ones that I got wrong are going to be weighted by $e^{-0.42 \times (-1)}$. That'll be upweighted.

So, what do you see? Here they are! The two, three I got wrong are more important now. The $D$'s are bigger, and the ones I got right are all less important.

So what have I said? I'm saying in a stage-wise regression, because I found the first of my weak learners, stage-wise regression, I found the piece there, I found a weight for it $\alpha$ by this funny formula, and now I've got new weights on the points, and I'm going to effectively fit a residual.

I'm going to say, hey, these things that I got wrong are more important to fit, and the things that I got right are less important.

Right? And that's the philosophy of residual. Residual says, spend your effort getting the things you got wrong, not the things you got right.

That makes sense? And I can do the same thing again faster, because, hey, it's exactly the same thing.

I start with this new piece here, I have already this model that said this piece, this is my $H_1$ or $H_1$, my weak learner. Now, here's the data points. This part right here, just copied down here. I find another stump that works well.

This problem's a little underdetermined, there are a couple that work, but here's one that says, everything left is blue, everything right is red on this weighted piece, which does a good job, and I say, what did I get right? I got all these guys right, and I got these 3 little reds wrong, and these two reds right.

But it counts for a lot, because these three were ones I got wrong before, so they're more important.

And that will then give me a new $\epsilon$, which will tend to go down monotonically, and a new $\alpha$, which will tend to go up monotonically.

I then say, okay, I've now got something that upweights the 3 that I got wrong—sorry, down-weights the 3 that I got right. Note that the three X's here are still bigger because I upweighted them once, and now I down-weighted them, because I got them wrong once and right once.

I now have this new model, I now repeat that again, where I say, hey, build a third model cutting here, classifying, say, these as blue and these as red. I now fit that, and the final classification is just, and this is the key part, a weighted sum.

So stage-wise, each time I fit a term, I never changed it again. I just incrementally added something. And at the end of the day, my model has 3 components—three hypotheses, weak learners, each weighted by their appropriate $\alpha$.

And if you add them all together, which is what you need to do and take the sign of it, you get something that's this nice decision surface.

So, note that **each of my weak learners is an incredibly stupid model**. It takes one feature and says, is it bigger or smaller than that?

But if I add up enough of them, I can fit a complex decision surface. Right? It's not linearly separable, it's no longer there. In this case, I happened to actually find the perfect solution.

So it's sort of cool. I can get a complicated solution by adding a bunch of simple, dumb, weak learners. And by incrementally putting more and more weight on the few things I got wrong, and less and less weight on the things I got right, I can get something that gets incrementally better and better.

**That is AdaBoost.**

And I say it's not used in its raw form very much, but the idea of putting more weight on the things you got wrong is super useful. And this idea that's sort of obvious once you see it—you can take really weak learners and learn a very complex decision surface.

Yeah.

### Question: Using Decision Trees Instead of Stumps

**Could we do AdaBoost with a decision tree instead of with a simple stump?**

Would that work better?

Maybe, but probably not.

And this is one of the weird intuitions I want... great question, I want you to get, which is that **if your weak learners are too strong, then they've done a good job of fitting the data, and there's not that much residual information left to fit.** And it's hard to fit the residual.

So sometimes you're better having a weaker learner. A stump is dumber than a decision tree—it's a proper subset of it. The weaker learner leaves more signal there to fit, and allows you to do a better job.

So in some sense, using a stump here rather than a decision tree is conceptually the same as a random forest using $M$ predictors instead of $P$ of them. It does a less good job fitting it, forcing you, or allowing you, but forcing the algorithm to keep searching and trying more stuff.

So, often you want the weak learners to be intentionally weak rather than to make them stronger.

If your first intuition is always, I want my weak learners to be as good as possible—and that's like, nope, that's driving you toward overfitting.

By making the weak learners intentionally weak, you can then encourage it to not do a good job fitting it, not overfit, and now it's better to average a much larger number of weak learners than a very small number of strong learners.

Right? Again, with this notion that if you got the perfect decision tree, learning a second decision tree gives you almost nothing, because you learned everything you could learn.

Right? So it's this weird sort of notion which happens a lot, we'll see the same thing in gradient descent. If you do too good a job of gradient descent, you zoom and you get stuck. Sometimes it's better to do a smaller gradient step and move more gradually down, which will get you to a better minimum.

So, often you try and make the weak learners weak rather than strong.

For a given problem, I don't know. Sometimes a full tree might be better, but often not.

Yeah.

### Question: Bias-Variance Trade-off

**Does this transcend the standard bias-variance trade-off?**

Yes, this transcends the standard bias-variance trade-off. The idea is, by having lots of variance across all of them, but low bias, you then average them and you take out the variance.

**And that is the big lesson of today: a good way to get around the bias-variance trade-off is, in fact, to have lots of variance in the weak learners and average them. Averaging takes out variance.**

And again, it's a rough approximation, it's $\frac{1}{\sqrt{\text{number of weak learners}}}$.

Um, but so this is a nice way of incrementally adding stuff in, each of these being a crappy learner, and then the end still doing well.

Cool.

### AdaBoost Loss Function

Let's go back to PowerPoint. Um... no, don't do that to me.

Okay, um, two comments about this, then we'll move to gradient tree boosting, which we will do unreasonably quickly.

The first thing to note, which I'm going to state without proof, is we talked about loss functions. **AdaBoost is an old-style algorithm**, which is to say, it's an algorithm rather than a loss function.

But, right, it's sort of backwards. In the old days, people gave you an algorithm, and then you found out what the loss function was that it minimized.

In the modern era, you take a loss function and you try and minimize the loss function directly.

But in fact, this... AdaBoost ends up minimizing, provably, something that is, in fact, an **exponential loss**:

$$L(y, f(x)) = e^{-y \cdot f(x)}$$

So we can actually, after the fact, say, hey, it's doing an exponential loss, which is... remember, we had set up a version of the slide before—not too far away from the logistic loss, which is the log of $1 + e^{-y \cdot f(x)}$.

So it gives you something that looks sort of qualitatively similar to the exponential style loss function of a logistic regression, but rather more aggressive.

Right? For better or for worse.

Well, it's more aggressive on both sides, because it goes up... if you get it wrong, you're paying a higher penalty, and if you get it right, you're paying a higher penalty.

Right? So it's... the blue is more extreme, is higher than the red here. And it's lower than the red on the positive side, so it actually is more extreme on both sides.

### Convergence Properties

There's also beautiful math, which if I had another day, I would give you, but I won't. Um, and so you don't need to... you're not responsible for this, but the fact is that you can prove, because it's an exponential model that's sort of multiplicative, you can show that this thing learns exponentially quickly.

So, each of the updates is sort of effectively a multiplicative update. And given this partition function $Z$, which I mentioned over here, then you can actually bound the total error, the average error you have, based on something compared to the partition function, which gives you something in terms of the $\epsilon$, which gives you something in terms of how many stages and how accurate the weak learner is.

So I'm not going to prove it, and you don't need to know it, and I won't ask questions on what the formula is, but it's nice to know that this has nice, simple, provable properties.

In practice, people usually use fancier methods that are harder to prove, but have sort of similar style behaviors.

Right? But the nice point is, you can learn exponentially quickly with one of these stage-wise methods that work on the residual.

Cool!

---

## Gradient Tree Boosting

Last piece. Okay, I've got 7 minutes. I'm going to go really briefly, that'll save me a little bit more on Monday. We're going to wrap up. But...

My normal **go-to algorithm for tabular data**, things that fit in an Excel spreadsheet, is, in fact, to try **gradient tree boosting**.

Gradient tree boosting is, in some ways, similar to random forests, which we saw in full detail, and in some ways, it's a fancier version of AdaBoost with less messiness.

And I'm going to sort of wave my hands a bit at how it works, but it's really going to be a **stage-wise ensemble method that fits pieces recursively**, where instead of using stumps, like we did for AdaBoost, **we use full trees**, like the guy in the front suggested, and this is sort of my go-to algorithm.

### The Model

The model says, hey, I'm going to fit something that, again, is a weighted average of a bunch of weak learners:

$$F(x) = \sum_{t=1}^{T} \alpha_t h_t(x)$$

And you can pick whatever loss function you want. People do gradient tree boosting for regression with an $L_2$ loss, they do gradient tree boosting for classification with logistic loss. It's a modern method. Plug in your loss function, and you can do it.

Whoops. Um...

The **base learner or weak learner is going to be a decision tree of specified depth**. Pick a depth like 5 or 6. Usually not less than 2 and not more than 8, but pick a depth of the tree, that is your weak learner.

And then if you want, you can do the same thing random forest does, which is to say, instead of using all $P$ features at each point, use $M$ of them. That would be called **stochastic gradient boosting**.

Same idea can be applied everywhere, and then we're going to do stage-wise estimation of each of the decision trees.

And now we're going to get a little bit messier about how we do it.

### Gradient Tree Boosting for Regression

Um, a simple version of this, the cleanest one, is **gradient tree boosting for regression**. Let's be concrete, $L_2$ loss.

Right? So $Y$ is a real number. Base learners are a decision tree, fixed depth, say depth 6, fit on the residual.

And what it's going to do is for... we saw decision trees for classification. A **regression decision tree** will say, you go down the tree, and for each leaf, it'll give you a number. And the number is just **the average of the $Y$'s of the points that went down to it**.

Right? So each leaf of a regression-type decision tree is the average of the $Y$'s of the training points. It's just a number.

Um, then we can actually do the whole algorithm. Okay, I only have 2 minutes, but this is fine because we covered all the stuff before, and life is now pretty straightforward, gluing lots of things together.

### Algorithm

So, we're going to:

1. **Initialize our initial tree value** to just being the average of all the $Y$'s:
   
   $$F_0 = \bar{Y}$$
   
   Instead of 0, doesn't matter much, but why not?

2. Then for each of the capital $T$ trees I'm going to learn, I'm going to:
   
   a. Do a **bagging step**: I'll pick $F$ of the $N$ observations, just like a random forest.
   
   b. **Compute the residual**:
   
   $$r_t = Y - F_{t-1}(X)$$
   
   c. **Fit a decision tree to the residual**.
   
   d. Then I need to find **how much weight to put on that decision tree**. I will fit a stage-wise regression, which is to say, just regress:
   
   $$r_t = \alpha \cdot h_t(X)$$
   
   The residual is equal to some number $\alpha$ times the prediction of the decision tree, so it's a single feature model. It's a trivial regression.
   
   e. And then I'm going to say, hey, my **new model is the old model plus some learning rate times the weak learner**, which is my decision tree:
   
   $$F_t = F_{t-1} + \eta \cdot \alpha \cdot h_t$$
   
   Made on the subset, times a learning rate.

So, the whole thing just glues together things we did. Unlike a random forest, this is stage-wise. But it has that same sort of property. I'm bagging, so I'm trying not to overfit. I have a learning rate, so I don't learn too fast.

And this works out quite nicely, and turns out, in a way that I'm not going to tell you, that this looks sort of like a **gradient descent**, whence the "gradient tree boosting."

### Hyperparameters

You've got a bunch of hyperparameters. I need to end, like, in one minute, or minus 1 minute. You have:

- The **tree depth**
- **How many stages** you run
- **What fraction** is in each bag
- The **learning rate**

And if you pick those better or worse, you'll tend to overfit more or less. So you pick the hyperparameters. If you learn too fast, you converge quickly and overfit. If you learn too slowly, it takes forever. If you make the right hyperparameters, then you learn the best model.

And... we're at time, so thank you.

I will hang out as usual for questions!

Awesome, awesome!

---

# CIS-5200 Fall 2025 - Lecture 11: Online Learning and the Perceptron Algorithm

---

## Administrative Announcements

### Exam Information

Good news everyone! Wednesday's going to be the exam, and it will be great and fun, right here in this room. People who've been building their cheat sheets are telling me it's helpful for learning things, which is good.

**Exam Details:**
- **When:** In class, usual time
- **Format:** Standard fill-in multiple choice
- **Important:** If you suddenly get COVID or food poisoning, let me know immediately, not later
- **Note:** Read questions carefully! Some answers require you to pick exactly one (that's the default), some ask you to pick all that are false, and some ask you to pick all that are true. We're not trying to be tricky, but please read the questions carefully.

The TAs who served as guinea pigs taking it say: read it carefully!

### Recitation and Homework Updates

There's no recitation this week, which you should have all gotten notification about.

The TAs and I have been debating the homeworks. I want them all due immediately so I can actually give you feedback and you can stay up to date. They're like, "Give them more time, they're all stressed out."

**Current Homework Status:**
- **Homework 4:** Extended deadline until Wednesday
- **Homework 5:** Infinitely far in the future
- **Solutions to Homework 4:** Will be posted today

You're responsible for the material on Homework 4 and Homework 5. A lot of Homework 5 covers things like random forests, gradient tree boosting, which are on the exam. However, the specific problems particular to Homework 5 are not going to be on the exam, so if you don't look at it, it's fine.

Frankly, if I were studying, I'd focus on the concepts, the terms, and the ideas, rather than on the homework details.

### A Note on Academic Integrity

This might seem crazy - posting solutions before the homework is due might encourage people to cheat. The TAs said, "Hello, you could post Homework 4 into Claude or GPT-5, and it will do a really decent job on it." There's a reason we stopped grading the homeworks for "did you get it right?"

Last semester, people who used GPT to cut and paste, on average, got higher scores than those who did it themselves.

**My goal is for you to learn.** That's really my goal.

Why do I want you to use a cheat sheet? Because the process of writing it out by hand (or typing if you want) will help you get the material. The homework is only 1% of your grade anyway. You're going to have to know this material to do well on the exam.

Try to learn. Yes, it makes no sense, but that's the reality we're in.

---

## Course Review and Structure

### The Big Picture

Before we cover new material, I thought it would be interesting to review what we've covered so far. As you look at things, you should be thinking about compiling a mental model of:

1. **All the representations/model forms we've used**
2. **All the loss functions we've used**
3. **All the optimization procedures we've used**

My claim is that's essentially all of machine learning.

### Key Concepts Covered

We've talked a lot about:
- **Distances** (L1, L2, L0, and other things that are not distances)
- **Convexity**
- **Kernels**
- **Entropy**

For each of these, you should think: why does it matter?

**Convexity:** Where does convexity show up? 
- In optimization - if it's convex, it has a single unique global minimum
- The model either is or is not convex

**Distances:** Why are distances nice? What kind of distances did we use?
- L1, L2, L0 norms
- Things that aren't distances, like hinge loss
- Divergences - what's my favorite divergence? My non-distance distance? **KL divergence!**

You should know the jargon of what cross-entropy is, because that's a vanilla job interview question.

### Focus on Overfitting

The first half of the course (up until today, really Wednesday) has been about fitting relatively simple models - we got up to gradient tree boosting, which isn't that simple - on relatively small datasets where we worry a lot about overfitting.

You should think about and list out:
- What were the different regularization methods we used?
- What are the different priors that correspond to them?
- Why do I call them "shrinkage"?

Try to go back and tie these things together.

**Other key topics:**
- Bias-variance trade-off
- Cross-validation methods

### Models We've Covered

What models have we used?

- **Linear regression**
- **Logistic regression**
- **Multi-class logistic regression**
- **Kernel regression**
- **SVMs** (which I hate as a term - I like the idea of hinge loss, but SVMs as a model form with hinge loss is imprecise)
- **Decision trees**
- **Random forests** (ensembles of decision trees)
- **Ensembles in general**
- **K-nearest neighbors**

For each of these: **Why do people like ensembles? Why do people hate ensembles?**

Start to think about each of these because one thing I'm hoping you're considering is: when is each of these better or worse? They have some sort of assumption about the world behind them.

We've covered a bunch of loss functions and optimization methods - hopefully over the next day and a half, these will all sort of solidify.

---

## What's Coming After Break

After the exam and break, we're going to spend time on:

1. **Week on PCA, embeddings, SVD**
   - If you're not up on eigenvectors and eigenvalues, now would be a good time to review those

2. **K-means clustering, Gaussian mixture models, and the EM algorithm**
   - A new optimization technique (well, very old, but new to us)
   - Unsupervised learning

3. **Neural nets** (super quickly)
   - "Hey, it's regression, take a real neural net course if you want"
   - All the same loss functions will show up, plus some new regularization

4. **Reinforcement Learning (RL)**

And then we'll all go home and celebrate!

---

## Today's Topic: Online Learning

**Note: This material is NOT on the exam.**

Today I want to cover something slightly different and slightly the same. We're going to look at **online learning**. It's sort of fun for me to go back and look at two algorithms from ancient history, back when I was a baby growing up in what's now called Silicon Valley, but in those days was just Palo Alto.

People invented these cool algorithms which, in retrospect, you look at and go, "Oh yes! This is just something that's optimizing some loss function for a simple linear model." But it helps give a little intuition.

These things are nice because they have beautiful theory, which if we had a lot more time we'd cover. I'll wave my hands a bit and explain some things.

### Two Types of "Online"

#### Online in Observations (Standard Online Learning)

Online learning in general is **online in observations**. You have a fixed $p$ number of predictors, and you have potentially an infinite number of observations. The world just keeps streaming you data.

#### Stream-wise Regression (Online in Features)

Before we get to standard online learning, I want to talk about the other side, which is weirder but certainly possible: **stream-wise regression** (not an official term).

You have:
- A **fixed number of observations**
- But you keep getting **more and more features** for the model

**How would this happen?** Does this make sense - can you have an infinite number of features and a finite number of observations?

**Example from Google:**
I worked at Google. We had a finite set of observations - all the web pages on the internet with some labels on them. $N$ is fixed. You can't collect more; you got the whole internet.

But **features**? You start with obvious features:
- What words are on the page?
- What pages are linked to it?
- Who clicks on it?

But then you can keep adding:
- All the pages that link to this page, and the words on those pages
- All the pages this page links to, and the words on those pages
- Click history of people who visited
- Geolocation data
- When the page was put up
- Who owns it
- What else is on the same domain

There's lots of information. You can keep generating more features.

For any one page, you have effectively an infinite amount of features. Make sense?

#### Why Stream-wise Feature Addition?

Why might I want to do stream-wise feature addition?

I can't do stepwise regression (try all features, pick the best one, try all remaining features, pick the next best one) because I have too many features.

Instead, I want something more like **stage-wise regression**:
- Try the best features first (e.g., words on the page)
- Then find other ones
- Keep adding stuff
- At some point, as I add more features, the marginal feature doesn't help me enough to overcome the complexity penalty

**What do I have more of?**
- Observations (number of pages on the internet)?
- Or potential features?

**Features!** Right?

If you're Facebook, it's the same thing. You can start with your 2 million users, then find all their friends, friends of friends, what they're doing, etc. You have more features than you can afford to put in your model because it's too complex.

---

## Standard Online Learning: Adding Observations

We're not going to talk about adding features today. We're going to talk about adding more observations - **streaming** or adding data points one by one.

### Why Stream Data Points?

Why would I want to update the model one data point at a time rather than take all data points and do the whole thing in a standard batch process?

**Reasons:**

1. **Data doesn't exist yet** - Data's coming in real time
   - The world keeps changing as data comes in
   - Most companies are continually collecting data
   - You'd like to update your models in real-time or quasi-real-time
   - Google doesn't batch everything - every time you click something, they're doing a little update of the model (streaming update)

2. **World drift concerns**
   - Your actions might change future observations
   - The world drifts exogenously (external changes)
   - The world changes because of what you've done
   - You want to adapt to a changing world

   **Tension:** How fast do I change my model?
   - **Learning rate too high:** Forget about old things
   - **Learning rate too low:** Don't adapt to new things fast enough

3. **Computational efficiency** (the most mundane but important reason)
   - Does anybody train large language models in batch? **No!**
   
   **What's the typical training regime?**
   - **Stochastic Gradient Descent (SGD):** One point at a time
   - **Standard Gradient Descent:** Whole batch
   - **Mini-batch:** Groups of 64 (or whatever) points at once
   
   For big datasets, it's too computationally expensive to put the whole thing into memory. Memory on your GPUs is the limiting factor. Often computationally, you want to stream stuff or do mini-batch.

---

## Computational Cost of Linear Regression

Here's a hard question for you today.

If you're doing linear regression, the solution says:

$$\hat{w} = (X^T X)^{-1} X^T y$$

So if you're going to compute that, you need to take $(X^T X)^{-1}$.

**Question:** How many multiplications does it cost you to do that?

### Matrix Dimensions

How big is $X$?
- $X$ is $n \times p$ (our standard notation)

So $X^T$ is $p \times n$, and $X^T X$ gives us a $p \times p$ matrix.

### Cost of Multiplication

To multiply $X^T \cdot X$:
- We're computing $p^2$ entries
- Each entry requires $n$ multiplications (inner product)
- Total: $O(p^2 n)$ multiplications

This is **linear in $n$** but **quadratic in $p$**.

### Cost of Inversion

Once you have the $p \times p$ matrix $(X^T X)$, how much does it cost to invert it?

If you do it naively: $O(p^3)$

We'll come back after break and discuss SVD and efficient methods of doing this that are much cheaper.

**In practice, the answer is the larger of:**
- $O(p^2 n)$ for multiplication
- $O(p^3)$ for inversion

### Important Takeaway

**You should never be inverting a matrix in a machine learning course** because it's too expensive.

We'll find a bunch of ways after break to avoid doing this. Even doing the multiplication starts to get very expensive for large $p$.

These things are, in theory, trivial - "hey, that's one line of code!" - but that's not how you actually do anything at scale.

After break, we'll discuss eigenvalue methods and other techniques. We'll also potentially cover MapReduce for parallelizing these computations.

**Quick Poll:** How many people have seen Singular Value Decomposition (SVD)?

*[Several hands go up]*

Okay, we'll cover that right after break.

**MapReduce:** How many people have seen MapReduce?

*[Fewer hands]*

Okay, we'll talk a bit about parallelization methods as well.

---

## Method 1: Least Mean Squares (LMS)

LMS is basically **stochastic gradient descent for linear regression**.

### LMS Algorithm

Super easy! We're basically reviewing the first week of class.

If we're doing L2 regression, what do we have?

**Error for each observation:**

$$e_i = (y_i - w^T x_i)^2$$

If you want all of them, you sum them up. But if we're doing one at a time for stochastic gradient descent, we take one at a time.

**Take the gradient with respect to $w$:**

$$\nabla_w e_i = \frac{\partial}{\partial w}(y_i - w^T x_i)^2$$

Using the chain rule:

$$\nabla_w e_i = 2(y_i - w^T x_i) \cdot \frac{\partial}{\partial w}(y_i - w^T x_i)$$

What's $\frac{\partial}{\partial w}(y_i - w^T x_i)$?

The quantity $(y_i - w^T x_i)$ is called the **residual**: $r_i$

The derivative of the residual with respect to $w$ is: $-x_i$

So:

$$\nabla_w e_i = -2 r_i x_i$$

**Gradient descent update rule:**

$$w_{i+1} = w_i - \eta \nabla_w e_i = w_i + 2\eta r_i x_i$$

We can drop the factor of 2 and absorb it into $\eta$:

$$w_{i+1} = w_i + \eta r_i x_i$$

### Geometric Interpretation

When I want to update my weight in stochastic gradient descent (one point at a time):

**The amount and direction of change (in terms of sign) is given by:**
- **Residual $r_i$:** How much to change it (signed - can be positive or negative)
- **$x_i$:** The direction to change it

**Can $r$ be positive or negative?** Yes!

The residual is $r_i = y_i - \hat{y}_i = y_i - w^T x_i$. You could be too high or too low. It's signed.

The residual tells you how much to change, and $x_i$ tells you the direction to change it.

**Key insight:** The weight is a vector, $x$ is a vector, they're in the same space (both in $\mathbb{R}^p$, the feature space).

Geometrically, what you're doing is **adjusting the weight** so that it does a better job with the current observation $x_i$.

### Choosing the Learning Rate

**How do we pick the learning rate $\eta$?**

Someone suggested cross-validation. Actually, we don't typically use cross-validation for the learning rate.

**Why not?**

Cross-validation is usually used for **hyperparameters that control complexity**.

**Should $\eta$ affect the solution we get from gradient descent for linear regression?**

We hope not! We're solving a **convex problem**. It should have a single global optimum.

So $\eta$ is really a hyperparameter, but **not a regularization hyperparameter**.

**Why do we care about $\eta$?** What can go wrong?

**It affects how fast you converge.**

#### Visual Intuition

We have our loss (error) which is quadratic as a function of weight:

```
     Error
       ^
       |     
       |    /\
       |   /  \
       |  /    \
       | /      \
       |/________\______> w
```

Weight is now a $p$-dimensional vector, so it's this quadratic surface in $p$ dimensions - a big bowl.

**What are we doing?**
- Find the gradient (tangent direction)
- Take some step going down

**If $\eta$ is too small:**
- We creep along slowly

**If $\eta$ is too big:**
- We go way down and overshoot
- End up bouncing to a higher error on the other side

**So $\eta$ controls convergence speed.**

If you're taking too big a step, you're bouncing to higher error. If you're taking too small a step, you're going slowly.

There are methods to optimize this, but that's the intuition.

**Think about which things are changing:**
- The **optimization procedure** (gradient descent)
- vs. the **overall penalty/regularization**

$\eta$ should not affect what you converge to - it's just affecting **how you get there**.

### Properties of LMS

LMS (Least Mean Squares) is online regression using the formula we just wrote down. It has beautiful properties (which I won't prove, but here they are):

#### Matrix Invertibility

If you take the matrix $X^T X$ that I'm trying to invert:

**Is it generally invertible or not invertible?**

*[Many yeses]*

I've got to say, most datasets I have, this is **NOT invertible**.

**Why not?**

Features are very often **collinear**. Often you have two or three things that are perfectly or nearly collinear.

**Example:** I was just looking at changes in intelligence over age and over time.

People have gotten smarter by about 3 IQ points per decade for 50 years (now we're getting slightly dumber). People who get older also get a little dumber, particularly in fluid intelligence.

So I can measure three things:
1. The year you were born
2. How old you are when you take the test
3. The year you take the test

**What's the problem?** These are massively collinear - perfectly collinear!

If Year Born + Age = Test Year, then these three variables are linearly dependent.

Anytime you have collinear features, $X^T X$ is **not invertible anymore** - it has a zero eigenvalue.

**In fact, I'm massively opposed to using linear regression without regularization.**

**What do I always use?** Ridge regression!

Just put a little ridge - $10^{-6}$, just a tiny bit of ridge - then it always has an inverse.

#### Convergence Properties

In general (we'll talk more about eigenvectors next week), if you take the covariance matrix $X^T X$ and make it well-behaved:

**The algorithm will converge if:**
- Your learning rate $\eta$ is positive
- $\eta$ is smaller than $\frac{1}{\lambda_{\max}}$ where $\lambda_{\max}$ is the largest eigenvalue of $X^T X$

**Convergence rate is proportional to:**

$$\frac{\lambda_{\min}}{\lambda_{\max}}$$

The ratio of minimum to maximum eigenvalue.

**If the matrix is singular, what's the minimum eigenvalue?** Zero!

And the convergence rate could be zero! It won't converge because it's singular.

### Takeaway

I don't care that you memorize details of eigenvalues, but I **do** care that you know:

**Running linear regression on a million features without checking for collinearity is asking for trouble.**

You should really know that:
1. **Taking inverses of matrices is generally a bad idea**
2. **If you're doing gradient descent:**
   - If your covariance matrix is reasonably well-behaved, it will converge reasonably
   - If it's singular, that's not reasonably behaved, and you'll have problems

**The intuition behind the math is super important.**

---

## Method 2: The Perceptron Algorithm

Let's do the same thing a second time, but for **classification**.

Many courses start with the perceptron algorithm, but I think it's nice to cover it now because it actually makes sense and reviews things. We'll see it as a **streaming version of hinge loss** with cool mathematical properties.

### Perceptron Setup

We've got a set of training examples $(x_i, y_i)$ where:
- $x_i$ are observations (feature vectors)
- $y_i \in \{+1, -1\}$ (typical machine learning binary classification)

**Output:** A hyperplane defined by weight vector $w$

**Wait, how is $w$ a hyperplane?**

$w$ is a $p$-dimensional vector, but we call it a hyperplane because:
- $w$ **defines** the hyperplane orthogonal to it
- The hyperplane is: $\{x : w^T x = 0\}$

### The Perceptron Algorithm

1. **Initialize:** Randomly initialize weights $w$
2. **While making errors:**
   - Compute the sign of $w^T x_i$
   
Let's visualize this:

```
        x₂
        ^
        |
        |    x  (some point)
        |   
        |----w (weight vector)
        |  /
        | /_____ (hyperplane ⊥ to w)
        |/_____________> x₁
```

We have:
- Some vector $w$
- A plane orthogonal to $w$ (a $(p-1)$-dimensional hyperplane)
- Some point $x_i$

When we compute $w^T x_i$:
- **Sign is positive** on one side → predict $+1$
- **Sign is negative** on the other side → predict $-1$

**$w^T x$ has a name:** the **score**

The **label** is the sign: $\hat{y}_i = \text{sign}(w^T x_i)$

### The Update Rule

The update rule is super simple (this was done in the late 1950s, before my time):

**If you got the prediction wrong** (i.e., $y_i \neq \hat{y}_i$):

$$w_{i+1} = w_i + y_i x_i$$

Remember, $y_i$ is either $+1$ or $-1$.

**If you get it right:** Do nothing.

### Connection to LMS

We can think about what we're doing using the exact same framework as LMS.

**For LMS (regression):**

$$w_{i+1} = w_i + \eta r_i x_i$$

where $r_i = y_i - \hat{y}_i$ (the residual)

**For Perceptron (classification):**

$$w_{i+1} = w_i + \eta r_i x_i$$

where now:
- $\hat{y}_i = \text{sign}(w^T x_i)$
- $r_i = y_i - \hat{y}_i$

**What is the residual?**

$$r_i = y_i - \text{sign}(w^T x_i)$$

The residual is either:
- **0** if you got it right
- **+2** if $y_i = +1$ but $\hat{y}_i = -1$
- **-2** if $y_i = -1$ but $\hat{y}_i = +1$

So $r_i \in \{-2, 0, +2\}$

**If I pick my learning rate $\eta = \frac{1}{2}$:**

$$w_{i+1} = w_i + \frac{1}{2} r_i x_i$$

When $r_i \neq 0$, we have $r_i = 2y_i$, so:

$$w_{i+1} = w_i + \frac{1}{2}(2y_i) x_i = w_i + y_i x_i$$

**This is exactly the perceptron update rule!**

### Classic Perceptron

The classic perceptron from my childhood (and I was small and cute once upon a time, with hair and no beard!) said:
- Just pick a learning rate equivalent to $\eta = \frac{1}{2}$
- Don't try to tune it and be clever

If you pick $\eta = \frac{1}{2}$, then standard gradient descent gives you back the perceptron algorithm.

You could use a different learning rate and be fancier, but the simple approach uses $\eta = \frac{1}{2}$.

### Geometric Interpretation

Let's think about what the perceptron update is doing geometrically.

**Setup:**
- I've got $w$ (in red)
- The hyperplane (in green) orthogonal to $w$
- Some point $x_i$

**Scenario:** The prediction was wrong. Say $y_i$ was actually negative, but the prediction is positive (since $x_i$ is on the same side as $w$).

**How do I update $w$?**

$$w_{\text{new}} = w_{\text{old}} + y_i x_i$$

Since $y_i = -1$:

$$w_{\text{new}} = w_{\text{old}} - x_i$$

**Geometrically:**
- We had $w$ pointing in some direction
- We subtract $x_i$ from $w$
- This **rotates the hyperplane** to be closer to $x_i$

Sort of cool geometry! Let's walk through it once more in slow motion:

1. We've got $w$
2. We got $x_i$
3. We said, "We got it wrong - $x_i$ should have been on the other side of the hyperplane"
4. So we're going to **move the hyperplane to be closer to $x_i$**
5. To move the hyperplane closer to $x_i$, we **shift $w$ in the direction of $-x_i$** (when $y_i = -1$)

**Boom!** Just like that, we've rotated the hyperplane.

The intuition makes sense.

### Does It Actually Fix the Mistake?

In the example shown, I didn't move it far enough - it's still misclassifying it.

Remember, this is **gradient descent**, so:
- There's no guarantee I shift it to be exactly right
- I might undershoot
- I might overshoot
- I haven't proved anything about that yet

What I **will** show you in a second is:
- There are strong theorems that if the world is **linearly separable**, this does magic things and converges nicely

**But yeah, the picture shows it's still wrong.**

However, the **score is lower**! The score is $x_i^T w$ (the distance), so the score is certainly lower after the update.

### What If We Got It Right?

**Question:** If I'm in this situation and $y_i = +1$ (we predicted correctly), what does the perceptron algorithm do for the update?

**Answer:** Nothing!

The standard rule of gradient descent says: if the residual is zero, you don't make any changes.

If you got it right, don't change anything. That's a sensible approach.

### Is This Online Learning?

**Yes!** The perceptron algorithm, by definition, is online.

It takes **one point at a time**, processes your data points one by one, and then if you want, you go through the whole dataset again if it hasn't converged.

You update after **every single point**.

This is **not a batch algorithm**. There is no "batch perceptron" - the thing called perceptron is inherently online, the same way LMS (Least Mean Squares) is online linear regression.

I'm going to use "online" and "streaming" totally interchangeably in this course.

---

## Convergence Properties of Perceptron

The perceptron algorithm has nice properties. Sometimes in this class (but not this year), we try to prove these properties. They're quite elegant.

### The Perceptron Convergence Theorem

**If the data are linearly separable** (meaning there exists a hyperplane that gives zero classification error), then:

1. **The perceptron algorithm will converge**
2. **The number of mistakes** (starting from any random initialization) is bounded by:

$$\text{Number of mistakes} \leq \frac{R^2}{\gamma^2}$$

where:
- $R = \max_i \|x_i\|_2$ (the size of the biggest $x$)
- $\gamma = \min_i y_i (w^*)^T x_i$ (the margin)

### Understanding the Bound

**$R^2$ (normalization):**
- Just the L2 norm of the biggest $x$ squared
- A scaling factor that says how big the data is

**$\gamma$ (the margin):**

$$\gamma = \min_i y_i (w^*)^T x_i$$

where $w^*$ is the true optimal weight vector we're trying to learn.

**What is $y_i (w^*)^T x_i$?**

If this is always positive, it means you can get everything right - it's linearly separable.

Because:
- $w^T x_i$ gives the score
- $y_i$ fixes the sign to be positive (when classified correctly)

**This quantity has a name:** the **margin**!

Remember from when we did SVMs - the notion of a margin is the distance of the points closest to the actual decision boundary.

### Interpretation

The technical piece is that we're dividing $\gamma^2$ by $R^2$ (the square of how big $x$ is).

**What does this say?**

$$\text{Number of mistakes} \propto \frac{1}{\gamma^2}$$

**The number of mistakes is inversely proportional to the margin squared.**

The more separable the data is (in a scaled sense), the fewer mistakes you make.

This is an **upper bound** - the **worst case**.

It's a cool theorem! It says:
- We're iteratively updating this weight really simply
- All we do is: if you got it wrong, add or subtract $y_i x_i$ to make it go in the right direction
- If you just run that algorithm, you're guaranteed to bound the number of mistakes **if it's linearly separable**

### What If It's Not Linearly Separable?

If it's not linearly separable, what happens?

**There are always mistakes** - it's going to keep bouncing around forever!

We'll deal with that in a second, but that's a technical detail. Theory people love it when it's linearly separable, which of course **nothing in my world is**.

But **if** it were linearly separable, this thing would converge nicely depending on the margin.

If you're in linear land, life works really nicely. And remember, the learning rate was trivial - just $\frac{1}{2}$.

---

## Handling Non-Separable Data

### The Voted Perceptron

If it's not separable, it's going to bounce around. There are **solutions** that are sort of ensembling-style methods.

**Idea:** If the hyperplane keeps bouncing around (because the data isn't separable), keep all the different weight vectors you generate and **let them vote**.

**Why does it keep bouncing?** Let's visualize:

```
        Hyperplane
           |
       x   |    o
        o  |  o
         x |
        o  |
     x   x | 
```

If I have an $x$ on the wrong side (in the $o$ region), the algorithm tries to move the hyperplane. But then maybe some $o$'s are now on the wrong side, so it moves it back, then it moves again... it doesn't ever converge.

**Voted Perceptron Approach:**
- Keep all the weights you've learned after every step
- Let them vote (majority vote)

This has nice theoretical properties, but...

### Why People Don't Like Ensembling

**Why do people not like keeping the last thousand weights and letting them vote?**

**You've got a thousand models!**
- Requires a thousand times as much memory
- For small models, who cares
- But for big models, **that's ugly**

Ensembling a bunch of things in a big model is ugly. So as a theoretical thing, letting them vote has great properties, but **it's not used very much in practice**.

### Averaged Perceptron

You could **average the weights** instead of voting.

**Is it mathematically the same to average a thousand weights as to let a thousand weights vote?**

**No, they're different!**

**Voting is nonlinear:**
- It's either $+1$ or $-1$
- If you shift from 499 voting one way to 501 voting the other way, the final vote flips
- It's not like averaging

**In linear regression:** If I fit a thousand linear models and average the outputs, is that the same as averaging the weights?

**Yes!** Under linearity, if you average the models, it's the same as averaging the weights.

**But in classification, it's nonlinear.**
- Voting is not the same as averaging
- Voting has slightly nicer mathematical properties
- But nobody wants to do it because it's expensive - you want **one model**

### Practical Approach

Super common approach: If the perceptron doesn't stabilize:
- Nobody uses perceptrons anymore anyway, so it doesn't really matter
- But in general, if a model's not converging, you could try to keep several copies and ensemble them
- Mostly **not attractive from a practical standpoint** if the model is large
- For linear models, at least it has nice properties

---

## Improving the Perceptron: Passive-Aggressive Algorithm

Let me now push this one step further. 

**Why is perceptron funny?** It's an algorithm that everyone covers in machine learning, and nobody ever uses.

The reason I'm still covering it is to give intuition about:
- What's happening geometrically
- What is the optimization doing

What I want to do now is say: as you start to look at perceptron-type algorithms, **can we use optimization thinking to improve them?**

### Standard Perceptron (Review)

For each observation:

$$w_{i+1} = w_i + \eta r_i x_i$$

where $r_i = y_i - \hat{y}_i$

In the classic perceptron, $\eta = \frac{1}{2}$.

### Can We Be More Clever?

Can we actually think about $\eta$ more carefully? Think of this more like true gradient descent?

There's something cutely called the **Passive-Aggressive Perceptron**, which says:

**What am I trying to do?** Actually minimize something that looks more like a **hinge loss**.

### Hinge Loss Connection

Let's think about what the hinge loss has to do with the perceptron.

**What did we say the perceptron looked like?**
- If you got it right → no cost
- If you got it wrong → update by some amount

**That sounds like a hinge loss!**

Hinge loss says:
- If you're right (or right past the margin) → no change
- If you're wrong → there's a loss

That's a hinge loss centered at the margin.

At the moment, we have an ad hoc update: just add $y_i x_i$.

But now we can say: **let's really do this more like a hinge loss problem** with proper gradient descent.

### Hinge Loss Formulation

**Loss function $L(\hat{y}, y)$:**

$$L = \begin{cases}
0 & \text{if } y_i w^T x_i \geq 1 \\
1 - y_i w^T x_i & \text{if } y_i w^T x_i < 1
\end{cases}$$

This says:
- If you're on the correct side of the margin (score $> 1$), loss is zero
- If you're on the wrong side of the margin, loss increases linearly

We've got this weird $y \in \{+1, -1\}$ setup where there's a symmetric thing at $-1$ as well.

**Make sense?** Hinge loss all over again.

Instead of doing hinge loss summed over all points, I'm now in **stochastic gradient descent land** - I'm going to do the hinge loss **one point at a time**.

### Passive-Aggressive Update Rule

Before, I said: just shift $w$ by adding/subtracting $x_i$.

Now I can say: **let me do something that looks more like actual gradient descent on hinge loss.**

**If $x_i$ is on the right side of the margin:**
- No change (gradient is zero)

**If $x_i$ is on the wrong side of the margin:**
- I've got a gradient: $\eta y_i x_i$

**The clever part:** Instead of just setting $\eta = \frac{1}{2}$, we can choose:

$$\eta = \frac{L_i}{\|x_i\|_2^2} = \frac{1 - y_i w^T x_i}{\|x_i\|_2^2}$$

where $L_i$ is the loss for this point.

Then the whole thing works out beautifully, and you've got an **optimal level of change** for this step.

### The Math (Optional Detail)

Let me walk through the fanciest, ugliest math we're going to do.

**Setup:**
- Initial weight: $w_i$
- New point: $x$
- Hyperplane orthogonal to $w$

Before, I said: if I got it wrong, just subtract $x$ from $w$ - that would move $w$ a long way.

Now I'm saying: **I can be more clever about how far I shift it.**

**The formula says I'm going to shift it by:**

$$w_{i+1} = w_i + \frac{y_i - w_i^T x_i}{\|x_i\|_2^2} \cdot x_i$$

**What happens to the score of the new weight?**

Let's compute $y_i (w_{i+1}^T x_i)$:

$$y_i w_{i+1}^T x_i = y_i \left(w_i + \frac{y_i - w_i^T x_i}{\|x_i\|_2^2} \cdot x_i\right)^T x_i$$

$$= y_i w_i^T x_i + y_i \cdot \frac{y_i - w_i^T x_i}{\|x_i\|_2^2} \cdot x_i^T x_i$$

$$= y_i w_i^T x_i + y_i \cdot \frac{y_i - w_i^T x_i}{\|x_i\|_2^2} \cdot \|x_i\|_2^2$$

$$= y_i w_i^T x_i + y_i(y_i - w_i^T x_i)$$

$$= y_i w_i^T x_i + y_i^2 - y_i w_i^T x_i$$

Since $y_i^2 = 1$ (because $y_i \in \{-1, +1\}$):

$$= 1$$

**The result is exactly 1!**

### Geometric Summary

Trust me (or look at the math): what I just showed is that if I do this update where my learning rate equals the normalized hinge loss score:

$$\eta = \frac{1 - y_i w^T x_i}{\|x_i\|_2^2}$$

Then I have **shifted $w$ exactly so that I moved the misclassified point to the margin**.

**Summarizing** (pulling back from all this math, which you'll never remember, and I don't even remember):

**Geometrically, what we're always doing:**
- Check if we got it right or wrong
- If really right (on the correct side of the margin), don't move at all
- If wrong, move it

**Ways to move:**
1. **Simple approach:** Move by $\frac{1}{2} y_i x_i$ (always converges, guaranteed)
2. **Fancier approach:** Compute how far you'd have to move to get the hyperplane to the margin
   - Not very complicated math
   - Geometrically shifts it over
   - Can get better convergence

---

## General Takeaways

### For All Loss Functions

You can do some sort of gradient descent:
- **Batch process:** All data at once
- **Mini-batch process:** Groups of data
- **Stochastic gradient process:** One point at a time

If you do it with stochastic gradient descent one step at a time, **if it's a linear model**, you can prove really nice theorems.

### Nice Theorems Work For:

1. **Linear world (regression):**
   - Convergence based on eigenvalues of $X^T X$
   - If you normalize things properly

2. **Hinge loss (classification):**
   - Convergence based on how big the margin is
   - Shows how "easy" the problem is

**Regression:** Easiness is defined by the $X^T X$ covariance matrix structure

**Classification:** Easiness is defined by the margin (how separated things are)

Obviously, **if things are not separable**, then all the theorem guarantees go out the window. But you can still do the same sort of gradient descent.

### Training Error Behavior

**What does the overall error rate $E_i$ look like if I'm doing any of these stochastic gradient descent algorithms?**

**Training error for stochastic gradient descent:**

```
Error
  ^
  |  
  |    /\  /\    /\
  |   /  \/  \  /  \  (noisy, bouncing)
  | \____/____\/____\____
  |________________________> Iteration
```

As you process each individual point:
- The world is noisy
- Sometimes it's going to get better
- Sometimes it's going to get worse
- That's what "stochastic" in stochastic gradient descent means

**Training error for batch gradient descent:**

```
Error
  ^
  |  
  |  \
  |   \___
  |       \____
  |            \______
  |___________________> Iteration
```

- Goes down **monotonically**
- Unless you mess up with a stupid learning rate
- Gradient descent means the error goes down (subject to reasonable learning rate)

**For stochastic gradient descent:**
- Looks incredibly noisy
- Bounces around a lot
- But **on average, or in expectation**, it's going to go down (if you have a reasonable learning rate)

**What we showed today:**
- Nice theory for linear regression gradient descent
- Nice theory for hinge loss gradient descent
- Guaranteed convergence properties

**For perceptron specifically:**
- Not only guaranteed to go down
- **If linearly separable**, we can guarantee a **bound on how many mistakes** you make based on the margin

Gradient descent is really quite nice - gives you a nice structure for optimization.

---

## Parallelization: MapReduce

*(Brief 5-minute overview)*

### The Idea

When you have **lots of machines**, here's what you'd like to do:

You've got a bunch of data $(x, y)$. You're going to **map** it to a whole bunch of computers (workers).

**Example:** If you had 10 computers, take a tenth of the data and send it to each computer. If you're Google, take a random thousandth and send it to 1000 computers.

You usually **shard it** - divide it randomly.

```
Original data: 100 million observations

Shard to 1000 computers:
Computer 1: 100,000 observations
Computer 2: 100,000 observations
...
Computer 1000: 100,000 observations
```

So now I've got 1/1000th of my data on each of 1000 computers.

You also have some initial version of weights $w$ (random initialization). Send the weights to all computers.

### Can We Parallelize Gradient Descent?

**Yes!**

On each computer:
- Do gradient descent on that subset of data
- Get some update $\Delta w_k$ for computer $k$

It's almost like a mini-batch algorithm. In practice, you might even do mini-batch **within** each of these machines (e.g., only 100 observations fit on GPU at once).

**After the map phase:**
- You have 1000 different weight updates: $\Delta w_1, \Delta w_2, \ldots, \Delta w_{1000}$

### The Reduce Phase

This is the **REDUCE** step:

Send all these updates back to one machine (or a smaller set of machines) and:

$$\Delta w = \sum_{k=1}^{1000} \Delta w_k$$

Just add them up!

**Then:**
- Take this combined update
- Apply it to get new weights: $w_{\text{new}} = w_{\text{old}} + \Delta w$
- Send $w_{\text{new}}$ back to all machines
- **Repeat the process**

### MapReduce Summary

```
MAP:
- Shard data across machines
- Each machine computes gradient on its subset
  
REDUCE:
- Combine gradients from all machines
- Update single model
- Redistribute updated model

REPEAT
```

Often with a small number of iterations, you get good convergence.

### Comparison to K-Fold Cross-Validation

**Question:** Is this like K-fold cross-validation?

**Similar:** Yes, like a 1000-fold cross-validation done in parallel

**Different:** 
- In K-fold CV, you train each fold to completion → get $K$ different models
- In MapReduce gradient descent, you:
  - Take **one gradient step** on each machine
  - Combine gradients
  - Update **one shared model**
  - Repeat

### Why Not Just Use K-Fold Cross-Validation?

**At the end of K-fold cross-validation, what's your final model?**

You've got $K$ models!

**But no one wants $K$ models** - they want **one model**.

You can ensemble them (average or vote), which is fine for tiny models. But **if they're big neural nets**, nobody wants to say "I've given you a thousand neural nets, run them in parallel."

**What is Sam Altman trying to do?** Save money! Make GPT-6 even cheaper than GPT-5. He wants smaller, distilled models.

### MapReduce for Gradient Descent

What we're doing is **very different from K-fold CV**:
- Use each machine to find **one step** on the same model
- Combine gradients back
- Update the **one shared model** with the combined gradient
- Repeat

**Goal:** A **single model**

We're gradient descending in parallel with all our mini-batches, then updating one model and repeating.

Very good point about the distinction!

---

## Feature Selection Challenge

One thing I didn't mention - let me touch on this briefly:

**Feature Selection**

Think about all this gradient descent I did. We did it all on the **whole model** with all features.

**It's not obvious how to build feature selection into streaming/online gradient descent.**

When we come back to look at other big models like neural nets:
- They mostly **don't do a decent job on feature selection**

**Comparison:**
- Small model with stepwise regression → gets feature selection
- Gradient tree boosting → doesn't really do feature selection
  - Though if a feature is always redundant, it tends to downweight it and show it up less
- Neural nets → not great at feature selection

### The Problem

**We're not doing great on feature selection** for these streaming/large-scale methods.

**How to handle it:**

Often you'll do a **two-stage process**:
1. **First pass:** Decide what features to put in (feature selection)
2. **Second pass:** Run your big model with those selected features

Or you'd have to put some sort of loop on top to do feature selection.

### Where Feature Selection Works Well

Feature selection makes lots of sense in:
- **Linear regression**
- **Logistic regression**

Where you can afford to do the search.

**But in many other methods, we're not really doing feature selection:**
- Perceptron
- Gradient tree boosting
- K-nearest neighbors

None of those really have feature selection built in.

If you want feature selection:
- **Two-stage process:** First select features, then build fancy model (most common)
- **Or iteration:** Some sort of loop that does feature selection

---

## Conclusion

### What You Should Know (Not for Midterm)

From today's lecture:

1. **Online regression** (LMS algorithm)
2. **Online classification** (Perceptron algorithm)
3. **Large margin classifiers** and their connection to SVMs
4. **Beautiful convergence bounds** for linearly separable data
5. **MapReduce** for parallelizing gradient descent

These algorithms have beautiful theory behind them, showing:
- How margin affects convergence
- How eigenvalues affect optimization
- How to parallelize training

### Looking Forward

With that, **I will see you all bright and shiny with your number 2 pencils** (or any pencils, it doesn't matter) **in class on Wednesday for the exam!**

Good luck studying! Focus on:
- Concepts
- Terms
- Ideas
- The intuition behind the methods

Rather than memorizing formulas.

---

*End of Lecture*

---

# CIS 5200: Machine Learning - Lecture 12: Neural Networks and Deep Learning

## Administrative Matters

### Midterm Results

Yes! Midterm, we've… I finally got everything graded. We lost a couple of exams where there were names that the OCR didn't recognize correctly. I think I've got almost everybody settled.

Um, don't panic. If there was a problem you've been told about, it's hopefully been resolved.

Um, I will release the exams today. I have one more sick person, I'll post these all on Ed.

This is, like, one or two people out of date.

But I think people mostly did fine, um…

What can I say? If you're in the bottom six, you might want to email me and talk.

I'll put that on Ed, too.

And Gradescope I'll release, you'll have the Scantrons and the answers and everything.

Questions? Yes?

These are all raw scores.

Uh, I would guess 60.

But we'll have… I will, you know, there'll be full transparency. I did personally check all the things that were possibly misgraded.

And the two people's names who got OCR'd wrong got fixed.

Good? Good.

And again, I'll share the… well, the slides are posted on Canvas.

Things will be there. Um…

## Course Overview and Transition

Cool! Where are we in the course? We're sort of halfway through, we're transitioning.

We covered supervised learning.

You should all remember what the non-parametric and parametric models are.

### This Week: Semi-Parametric Models

This week is what I like to call semi-parametric.

Not an official term, people don't use it, but…

Neural nets are parametric.

There are lots of parameters, millions or billions of parameters. They fit mostly into a very standard regression format.

What I'm going to do today is say a bunch of things that are really obvious to say: hey, it's all stuff we've seen before.

And… on the other hand, they're so flexible that there's not a lot of assumption.

But in fact, there will be lots of assumptions. We'll talk a lot about **inductive bias**.

And the sort of implicit assumptions that go in as you pick a neural net.

And also, we'll…

Note that most things are still $X$'s (features) predicting $Y$.

But often in neural nets, you have a sequence of $X$'s where you predict the next $X$.

Does that make sense?

## Self-Supervised Learning

So I've got something which is a sequence of $X_1, X_2, \ldots, X_T$, and in general, for self-supervised learning, we'll have lots of things. We have some $X$, that tries to predict $X_{T+1}$.

**Question:** Supervised learning or not?

$$X_{t+1} = f(X_1, X_2, \ldots, X_T; \theta)$$

Supervised, unsupervised?

Unsupervised?

There's no label! Yeah, that's sort of right and sort of wrong.

But I'm gonna argue it's more wrong than right.

Okay, and that's what I want to set up: we're used to predicting $Y$ as some function of $X$. But this is something which looks like supervised learning.

**Answer:** It is supervised! Right? We have… where does it show up a lot?

### Example: Natural Language Processing

In my world, I'm a language NLP guy.

I've got a whole bunch of sequences.

> "Yesterday, I went to see the…"

The input is: "Yesterday, I went to see the"

The next token is: **Cal** (or whatever word comes next)

**Cal** is the label.

That makes sense? Because it really is a weird sort of supervision.

But there are labels, and the label is the next thing you're going to see.

Questions? This is slightly weird, but insanely important.

Yeah.

**Q:** Are we going to use probabilities?

**A:** Yes, let's think about it.

### What Does This Function Actually Look Like in Language?

We take in… well, crap. First of all, can we take in tokens or words?

**No!** All we get in this life are **vectors**.

All of machine learning is just vectors, or matrices, or tensors, but they're all basically vectors or numbers.

The whole course of machine learning only gets numbers. So somehow, we're gonna have to take every token—for every word: "yesterday", "I", "went", "to", "see", "the"—and we have to map each of these to some vector.

**Do we have a fancy word for that?**

**Embedding!**

Right? We're gonna embed it, we'll talk a lot about that next week.

But for now, I'm gonna do things slightly out of order, because that's… hey, it's an advanced course.

So we map every token to an embedding—a vector. We concatenate them, we got a big vector here.

So that's the $X$, it's just a big vector, which is the concatenation of a bunch of small vectors for every word.

### Output Space

Now… what's the output space?

**All possible tokens you might emit.**

A token in this case is a word piece.

A typical vocabulary is on the order of 40,000 possible tokens.

Which you need to do because there are a lot of words.

In the old days, we used to have every word be a separate token.

And when I did that at Google, we used a vocabulary of about a million words.

Now, given the average person has a vocabulary of about 80,000 words, that's a lot of words, but remember:
- Every number is a word
- Every name is a word
- Every misspelled name is a word
- Every city and town and country in the world is a word

Oh, then there are lots of other words besides English.

Most modern companies in those days—Google did 40 languages, now they do about 50.

But there are about 50 languages that have enough words that somebody wants to actually make money selling to you.

And so, standard would be to take all the words in the 50 most frequent languages that will cover 90% of you statistically plus, right?

Make sense?

### Output Format

So, what does the output look like from this?

Is $Y$ a scalar, a vector?

**A vector of dimension 40,000** (the number of tokens).

Is it a probability?

**Yes!** There's a probability distribution over the 40,000 tokens.

Right? So a standard… and we'll see a bunch of different architectures, but the most common one is to say that we've got something that takes some vector, and then produces some distribution over a set of objects.

So it's a **multi-way classification**.

And we get a probability distribution, and then…

Okay, while we're on this topic: Do we want to produce the highest probability token as output?

**Mostly, yes.**

In practice, if you use any sort of API call to a large language model, it has a couple of features, one of which is called **temperature**.

And temperature says:
- If the **temperature is zero**: Pick the most likely one
- If you have a **higher temperature**: Pick one from a probability distribution, which is a renormalized version

So if you want to generate different things every time, you use the full probability distribution.

Right? The neural net—it's worth keeping in mind there are two things that are sold separately:

1. You estimate a probability distribution over, say, next tokens (it's like a score, but it really is a probability—the likelihood, we'll see)
2. You pick which one you want, usually the highest, the **argmax** (the most likely one)

Good?

### Self-Supervised Learning Definition

And this process is called… semi-supervised, or **self-supervised**.

Probably **self-supervised** is the best name for it.

So the label exists, but the label isn't done by paying someone, right?

There's a huge army of people—many of them in Africa, lowly paid; some of them in America, highly paid; some of them working for Scale, which was acquired by Meta for, like, a billion dollars—labeling stuff.

But the magic is, often you have **automatic labels**.

If it's a sequence:
- Words
- Videos (frames in a video)
- Sounds and audio

Lots of things are sequential.

And you can collect lots of data if you were, say, Google or Meta. Oh, Meta or Meta.

Cool, good? Good.

## Upcoming Topics

Then we will turn starting next week to:
- **Unsupervised learning**
- **Principal Component Analysis (PCA)**
- A variety of nonlinear autoencoders
- Ways of generating and improving embeddings
- Gaussian mixture models
- K-means and EM
- Maximizing likelihood

And then we'll do **reinforcement learning**, which will typically have neural nets under the hood for the function approximators, and use a different kind of gradient descent reward.

And then we'll talk about the **final project**.

### Next Week: PCA

Next week, PCA. At this point, you should know:
- Eigenvectors and eigenvalues
- An orthonormal basis

We will post soon a nice 3Blue1Brown video you can watch, and we'll set up people on… Friday morning, we'll post it to actually have a little extra bonus presentation and questions.

And, like, all this stuff—it'll be stuff you've seen but go super quickly.

**I should ask:** How many people have seen neural nets?

Everybody?

How many people know CNNs (convolutional neural nets)?

Half the people, okay. So I'm gonna do the neural nets really quickly, a review of history, and hopefully it'll help you to think about it a second time.

And then we're gonna go and do CNNs, and then we'll maybe do more advanced stuff on Wednesday.

Good, um, so you should all know what an orthonormal basis is.

## Final Project

**Final project!** We will post on Wednesday the next homework and information about putting your projects together.

So what I want you to do, and this course is very… dis… dissociated pieces.

There are core concepts here, which you've seen on the midterm. The final will look just like the midterm—cumulative, by the way.

Um, there are some homeworks where you do a little bit of coding, there are recitations, and then finally, you get some real datasets and run some real machine learning algorithms over the real datasets.

### The Easy Way vs. The Learning Way

Now, it's insanely easy right now to go to Claude or Perplexity and say:
> "Hey! Download that from Kaggle, and run 3 different scikit-learn models over it, and give me the answers."

This gives a fairly decent result. Takes a couple hours, preferable if you have a paid model, so you can actually do the full thing.

Um, so that's, like, **C-level work**.

And you could probably do it in 3 hours and vaguely pass the class, but I'm really hoping that you will come away from this class **learning to think about the different problems**.

And that means trying to say:
- Where is it something that you can't just slam the thing into either a Hugging Face model (which is where most people get their neural nets) or scikit-learn?
- Where is it you have to **think**?

### Inductive Bias

And almost all of those involve something that I would call **inductive bias** or **inductive priors**.

What is it about your problem that suggests how you should go about choosing either:
- The model form
- The regularization
- The optimization algorithm

Or often—we'll see a bunch of things starting today—versions of this.

Some of the versions are:
- Maybe I could take one dataset, use that to learn an embedding (dimensionality reduction), and then on my new small dataset, I can take the embedding I found before and use that
- This requires finding a similar problem where it's similar enough that the embeddings work

We'll see lots of examples of that this week.

It could be a case where it's **multimodal**, where you grab audio and video and text. You can't usually easily throw those into one simple model and have them train—they each require:
- Different regularizations
- Different sized models
- Different pieces where you might partially train the models and combine them
- Maybe ensemble them in some fashion
- Maybe stage-wise train them

### Project Guidelines

So I want you to:

**A)** Find some other people, and we'll set up a thread on Ed to meet people if you don't know people, but hopefully you've been meeting people in recitation and chatting and being friendly.

**B)** Think about a problem, preferably one you know something about.

And there are amazing datasets, everything from:
- Taxi rides in Manhattan (where they go, how much they cost, who takes them)
- RNA modification

Preferably, if you're doing RNA, you know something about RNA, so you can actually figure out what the right inductive bias is.

It's not generally a good idea to say:
> "Hey, I'm gonna go and solve recognition of breast cancer when I don't know anything about breast cancer."

Um, so I had people do professional wrestling! Fine! If you know professional wrestling and like it, I don't quite understand it, but that's okay. Not my project, right?

So something you know something about… find the data, and you all know something about text and audio and video. Many of you are multilingual. That's sort of nice.

Think about problems.

Questions? Yeah.

### What Do I Mean by Inductive Bias?

**Q:** What do I mean by inductive bias?

**A:** Prior. Inductive prior, or think of inductive bias.

It will be the case that when you build a model—we'll see today examples of images where you might think that things are **translationally invariant**.

If I shift an image over by 2 pixels, should it change the label?

**Probably not.**

If I rotate the image and flip it upside down, should it change the output? Are the outputs invariant to flipping?

**Probably not**, right?

Think about this image of you or me. Look at me, shift me this way, it doesn't change things very much. Turn me 180 degrees upside down—something is really weird.

So that's a particular **inductive bias about the world**, or a **prior**, right? A prior, both in the Bayesian sense, before I go and collect my data, I have some notion that some things are gonna matter for changes, and some things won't.

#### Example: Face Recognition

Do I think that an image recognizing a face here should be the same as an image recognizing a face over there?

**Probably**, right?

Mostly a face doesn't change who it is, depending upon where it is.

Oh, unless I have a bunch of seats—people tend to sit in the same seat most of the time, right? Occasionally, people who sat on that side switch over to this side. But that's fairly unusual, so that's a different sort of knowledge about the world.

It says:
> "Hey, I'm trying to recognize people from sequences of photos. They tend statistically to sit in the same place they sat before, but not always."

That makes sense? That… that almost always before you see the data, you have some expectation of what it's gonna be.

And again, we're going to see lots of examples, and in some sense, my main point of doing neural nets today is just to say:
> "Hey, they're exactly a regression, and oh, right! You're gonna build in different sorts of priors."

So that'll be a long answer, and we'll see more of it. Yeah.

### Project Timeline

**Q:** The final project timeline?

**A:** We'll post everything on Wednesday, but there will be:

1. **Initial submission:** You'll give, like, a one-page document with:
   - Here are my three people in the group
   - Here's what I'm gonna do
   
2. **Meeting with TA:** Then you're gonna meet with one of the TAs (you'll get assigned a TA), meet with a TA, get feedback. I will read all the ones that are problematic and meet with people, help you refine it.

3. **Checkpoint:** Then there will be a checkpoint, which is:
   - Yes, I've got a baseline, something running
   - Always in machine learning when you build a model, the first thing you do is take an off-the-shelf, what's-the-dumbest standard, simple thing that you could run?
   - Often it's a linear regression or a logistic regression
   - Sometimes it's an off-the-shelf, pre-trained neural net from somebody
   - But there's always some **baseline**
   - And you're gonna show that your model hopefully does better than the baseline

4. **Presentation:** Then there will be, in one of the recitations, you'll do a final, very short 10-minute presentation, get more feedback.

5. **Final submission:** Then the last day of class, which is the last time I can make anything due, you'll have the final project due.

So there'll be a whole bunch of pieces.

### Homework Adjustments

What's gonna happen is, we're gonna have homework still, but the homework's gonna get a little bit less, because more and more of the homework is going to be taken over by doing the project.

And so you'll start to see homeworks that are 2 weeks instead of 1 week that are the same amount that one week was before.

So, it'll depend on what week, but it's going to shift toward more project-based and toward the end, more of the recitations will be toward the projects.

Yes.

### Using Existing Code

**Q:** Do I expect you to write all the algorithms from scratch?

**A:** No! I'm perfectly happy with you using prefab everything. You can use any code you want.

- **scikit-learn** is nice for most of the tabular data
- **Hugging Face** is probably the first place to start for most of the neural nets
- There's a whole different set of things for RL

You are welcome to and encouraged to use (with attribution) other people's code, right?

And what I want you to do is to think about the code:
- How do you pick the right hyperparameters?
- How do you pick the right regularization?
- Is there some sort of embedding you should do?
- How should you structure and glue these things together?

But no, almost nobody actually writes code for machine learning. It's like writing compilers.

There's not a big market for writing a new compiler. There's not a big market for writing a new machine learning platform, right?

So, most people in the world are trying to take some real problem and figuring out how to solve it, and do a better job of it. And that's really what the project is trying to do.

So, you're welcome to write things, but **no points for writing things in Python or assembly language**, or whatever language you like to write in. It's fine, but… no.

Good. I think good.

## This Week: Neural Networks

Um, cool!

So, this week, neural nets. I'm going to quickly review **multi-layer perceptrons**, which you have all sort of seen, but to make sure we're clear on jargon.

Over the years, the names of neural nets keep going through phases, and now it's **deep learning**, which is trendy.

But we'll go through a bunch of these names. We'll quickly cover **CNNs** and make sure, which half of you haven't seen.

We'll talk a lot about **regularization** again.

Um, and then we'll talk on Wednesday about a few other versions of **transfer learning** and such.

## Coordinate-Based vs. Coordinate-Free Models

Cool! Before I start this… one of the key concepts that I see people get wrong all the time is a confusion between **coordinate-based models** and **coordinate-free models**.

And I just reviewed a paper that was using gradient tree boosting on PCA on embeddings.

We haven't covered PCA yet, but most of you have seen this notion that you take some sort of vector, and we run it through a neural net or through a PCA, and we get out another vector.

Where, if you flip any of the features or rotate them, or transform them, you get the same information.

### Which Methods Are Coordinate-Based?

So let's stop for a second and think, before we go any farther, which of our methods are coordinate-based.

Let's take the non-parametric methods:
- **Neural nets** versus **decision trees**

Which one really cares about coordinates, in the sense that each feature (inches, miles, etc.) matters individually?

**Decision trees?** Yes? No?

You see the question? Yep.

### How to Answer This

So, I think one way to think about how to answer it is:

When you're thinking about your model, are the individual features that show up in your feature vector—are the individual $X$'s that you have—meaningful and interpretable by themselves?

#### Example: Interpretable Features

So I could look at if I have some $\mathbf{X}$ that says that:
- The first thing is my **temperature**
- The second thing is my **weight**
- The third thing is my **height**
- The fourth thing is my **age**

This is something where the… if this is my feature vector, my $\mathbf{X} = [X_1, X_2, X_3, X_4]^T$ (or that transpose).

Each of these features actually **means something**, right?

And I might want to do **feature selection** and pick some of them and throw out other ones. I might want to show them to me or my doctor.

These coordinates, these features **make sense**.

#### Example: Embeddings

On the other hand, we'll see for a lot of things, if I take a token like "I", and I map it to some 568 or 2,000 dimensional vector, I'll get some vector of $X_1$ up to $X_d$ (whatever dimension). I'll get a whole bunch of numbers representing this token, which sort of means something.

But **no one of these numbers means anything** individually.

There's meaning in the sense that I will be closer in vector space to "we", or to "you", than it is to "cow" or to "see".

So, there's a **metric space**, a **distance** defined over those vectors, but each individual component is not particularly meaningful.

Does that help?

### Mapping Objects to Vectors

So this notion says that sometimes we have… we're mapping an **object** to a vector.

And one way to do this that works embarrassingly well is to take each object, each discrete object, and just **give it a random set of numbers** to represent it.

Okay, the similarities won't work—you won't have "I" similar to "you" anymore—but I could take that as a feature vector that describes this, and I can plug it into a neural net.

And often, in fact, that's used as an **initialization**.

And often it works surprisingly well, compared to even learning the optimal representation.

### Coordinate-Based Models

So if you think about it, some things treat each individual feature separately.

Think of a **decision tree**. You ask:
- Is height greater than 2 meters? Yes, no?
- Is age greater than 30? Yes, no?

That's **feature-based**, **coordinate-based**.

### Neural Nets and K-Nearest Neighbors

Now, how does a neural net work? Do individual features ever show up in the neural net? Sorry, in the **k-nearest neighbors**?

It has what? It has a **similarity measure**, or a **distance measure**, right?

So the neural net takes in two vectors (whole vectors) and computes a distance.

Sorry? No, **k-nearest neighbors**.

K-nearest neighbors takes two vectors and computes a distance.

**If I were to take the two vectors and rotate them by 90 degrees, would it change the nearest neighbors?**

**Not at all.** The nearest neighbor is **invariant under rotations**.

Right? How close two points are—if I rotate the world in whichever way, the rotation doesn't change the distance.

#### Example: Rotating Features

If I rotate my temperature, weight, height, and age… right? That's a four-dimensional space.

Okay, we can do it with two dimensions, let's do it in two dimensions!

Let's take just your **temperature** and your **weight**.

And here are some people (plotted).

If I rotate this axis, so that I have a new modified temperature axis and a new modified weight axis—we'll do this a lot when we do principal component analysis.

If you rotate this, what do the new features mean? Before, here's this guy's temperature, here's his weight.

Now I've got someone's new modified temperature and modified weight—it doesn't mean anything. It's some weird linear combination of temperature and weight.

**Does it change K-Nearest Neighbors?**

Again, **no**, right?

So anything that's based on **distances** (or similarities, kernels) —all these things are **invariant to rotation**. They don't really care what the feature is.

That makes sense?

### Summary: Coordinate-Based vs. Coordinate-Free

And so, some of the algorithms really rely on:
> "I want to have features that are meaningful, because I'm going to ask a bunch of questions about them and select some and not select other ones."

Those are **coordinate-based models**.

And it's really weird to take something that is coordinate-based, and give it an input which is arbitrarily rotated.

And we'll see most of the **embeddings** we learn in deep learning—starting from principal components—most of the embeddings are **arbitrary up to a rotation**.

There's nothing that's gonna make a lot of sense to look at the individual components. It won't make a lot of sense often to feature-select them (maybe, yes, maybe not), and it won't make a lot of sense to put a decision tree on them.

If what you're really getting is: how close is this in a metric space?

So we'll come back and revisit this, but I wanted people to start thinking about this—the difference between methods that take in two whole feature vectors and say "how similar or distant are they?" versus methods that go through and say:
> "Okay, pick a small number of the features and zero out the rest of them."

Questions? Yeah.

### Linear Regression with Kernels

**Q:** So, linear regression, once you put a kernel in…

**A:** You sort of destroyed the coordinate-based version, right?

If you think of a classic **stepwise regression** that statisticians love, that's by definition coordinate-based. You're trying features one by one, putting some in and not putting a bunch of them in.

Whereas once you put in a **kernel**, you've said:
> "Hey, all I really care about is how similar these things are."

And I drop this notion of feature selection, right?

**Feature selection doesn't play well with kernels**, and **feature selection doesn't play well with neural nets**.

Right? So there's a very orthogonal world, where we started with a world with:
- Simple models
- Lots of feature selection
- Lots of zeroing features out

And now we're shifting to a world which says:
- You got a fixed set of features
- Put them all into the big model
- It'll compute a bunch of stuff
- But we're not gonna zero things out

Yep.

### Decision Trees on Transformed Data

**Q:** Couldn't I run a decision tree on a transformed dataset?

**A:** So if I do a decision tree on something with temperature, weight, height, and age, it makes sense.

If I've got a new feature which is some combination of your temperature and your weight, that's a very weird feature to say:
> "Should I include or not include that feature?"

It might work, and in fact, if you run **Gradient Tree Boosting** over a coordinate-free embedding, it works. It just doesn't work as well, and it's not as sensible as if you do something where the features actually make sense.

So, sure, you can take something which is an arbitrary combination—you've made new features.

We're gonna do a lot of mapping: take original features, and we're gonna map them to a new space, **embed** them, right? A mapping from $\mathbf{X}$ to $\mathbf{Z}$.

$$\mathbf{Z} = \text{embed}(\mathbf{X})$$

So we'll embed the features, and my point is, once you've embedded them, you've usually **lost any clear interpretation** like "this is temperature, this is height."

And in the embeddings, it rarely makes sense to say:
> "Hey, I'm gonna look at this embedding, and I'm gonna try and keep the top 5 features of the embedding."

And I keep working with people who want to say:
> "Hey, I've got this beautiful embedding. I'm now gonna interpret my model that predicts whether you're warm or cold based on looking at the BERT embedding" (we haven't covered these yet, but whichever embedding you want).

And it's like, no—those components of that embedding **don't mean anything**, usually.

Unless you somehow **sparsified** them, or done something to them.

So if you've got a new feature which is a combination of all the old ones, then it's often very hard to interpret it.

We'll cycle back and show how you sometimes can with neural nets, because that's very hot right now. But it's a non-trivial problem, yeah.

### Definition of Embedding

**Q:** Do we define what embedding is?

**A:** We didn't. An **embedding** is some sort of mapping of any feature vector $\mathbf{X}$ to another feature vector $\mathbf{Z}$.

$$\mathbf{Z} = f_{\text{embed}}(\mathbf{X})$$

And this could be something that is a **one-hot encoding**.

#### Example: One-Hot Encoding

So this could be something which is one of 40,000 tokens.

And so it takes it, and it turns out that I've got:

$$\mathbf{X} = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, \ldots]$$

Where the one is at position corresponding to "this" word.

So I can map the word "this" to some vector.

Or I could take something that's already a vector and map it to another vector.

So "embedding" is a hopelessly broad term. It started initially when you have **discrete objects**, you want to put them in as input to your neural net, and the only way you can model discrete objects nicely is with something that's a vector.

And often, instead of having something that says:
> "Give me a 1 if it's a fire truck and a 0 otherwise" for the 10,000 favorite objects

It's often nicer to say:
> "Take that vector of that one-hot vector—a 1 for the firetruck and 0 for everything else—map it to a **low-dimensional embedding** (500, 1,000 dimensions), and then map everything else to that embedding, too."

Does that make sense?

An embedding takes any feature vector, or anything you have, and maps it to a vector.

And in some sense, everything, again, we do in neural nets and in all of machine learning, we need a **vector as input**.

Yeah. Yep. Yep. Yep.

### Example: Classifying Objects

**Q:** In general, the $X$ vector could…

**A:** No, no, so… let's say we want to ask the question:

For the final $Y$: **Is it dangerous or not?**

Whatever that means, we have a label: dangerous (yes/no).

We've got something which we want as input.

And I'm going to tell you:
- If it's a **fire truck** (like, I wish I could draw a picture of a fire truck, but I can't, so I'll just write the word "fire truck"): Is it dangerous?
- **Frog**: Is it dangerous?
- **Avocado**: Is it dangerous?

In general, it's not useful to try and say:
> "Hey, I take each of these things independently and plug them in."

First of all, I can't plug them in—I gotta have a vector to plug in.

Make sense? You can't plug in "fire truck" or "frog" or "avocado" into a regression. It's a type mismatch. You need a vector.

#### One-Hot Encoding

The simplest vector representation says: I take all possible objects in the world, I put a 1 for "frog", or I put a 1 for "avocado", and the rest are all zeros. Now I got a vector.

If I got a million objects, the vector is dimension a million.

Make sense? That's a **one-hot embedding**.

#### Dense Embeddings

Then what I generally do is say:
> "Hey, take that vector and map it to something that's usually a **lower-dimensional dense vector**."

Where I take every possible object and map it to a vector.

For every object—firetruck, frog, avocado—every object, every one of my million objects maps to some vector.

Nope.

I'm not sure… the mapping takes in a one-hot vector (one 1, and the rest zeros), and the output is a set of, say, a thousand real numbers.

So that's a **thousand-dimensional vector**.

The input is a **million-dimensional vector**.

But it's only got one 1 ever.

And I have a mapping that says: for any $\mathbf{X}$, give me a $\mathbf{Z}$.

$\mathbf{Z} = W \cdot \mathbf{X}$

So that's… that's an embedding.

Yeah.

**Q:** Now you gotta figure out… so one thing you could do if you had two fire trucks and one frog is put a 2 and a 1…

**A:** And have some way to map that now back to the same space. Somehow, you gotta do it.

So I think one of the big things that we're moving to neural net land is: we now often have inputs that don't look on the face of them like vectors.

### From Objects to Vectors

So the very first step, always, for neural nets, is: **if you don't have a vector, you gotta make one**.

Right?

Now, maybe if it's a scene with two frogs and a fire truck, you don't care about that—it's a bunch of **pixels**.

If it's pixels, it's a vector, right? It's RGB $\times$ X $\times$ Y, but you can flatten it out and put it in a big vector.

So you got the reds and then the greens, and the blues, and there's one big vector. So for pixels, it's pretty easy to put them in a vector space.

If I'm really telling you:
> "Hey, it's two frogs in a fire truck."

Now you gotta find some way to embed that. We will by, say, Wednesday, figure out how to do that.

And that's something that, say, GPT-5 does beautifully. It takes… you can say:
> "Hey, draw me a picture of two frogs in a fire truck."

It's gonna take that sequence of tokens, map it to an embedding, feed it to a model (which we'll probably cover much later in the course, like a **diffusion model**), that then generates an image.

But note that it's gotta take in a **vector** at some point.

Does that make sense? And it's gotta be a **vector of fixed size**.

Again, we're just doing fancy regressions.

**Regressions only take in a vector of size $p$.**

No matter what you give me, if I can't make it a vector of size $p$, I can't put it into my regression, which means I can't put it in my neural net.

Yeah.

### Invariances

**Q:** According to the thing only for rotation…

**A:** There are a whole bunch of different invariances.

And we'll cover a few of them, but for now, think of the main point as being:

**Does each individual feature make sense?**

In the sense of picking a small subset of them, or is it something where you're really only capturing distance or similarity?

And depending on how you capture it, sometimes due to the distance between two things—distance in general is **not scale invariant**.

But you could normalize everything and make them scale invariant. You could do a scale invariant distance, right? If you normalize the two vectors, now you would have a much broader class.

Good question, yeah.

**Q:** And so, over the real numbers, like, is it any… where it makes sense to do it over, like, integers?

**A:** Would you want to use something other than real numbers?

There's a small number of people that like **complex numbers**, which have beautiful properties.

**Integers** are not great, because think of what we're doing. Everything looks like linear regression or nonlinear regression, or logistic regression.

You could do integers, but people don't.

#### A Little Secret

Now, a little secret I won't tell anybody: If you actually look at a real computer, they don't have real numbers.

You just have a **fixed number of digits**, a small amount of things, so actually they're using integers in the neural nets.

And, in fact, there's a big business of taking things that used to have an 8-digit approximation to a real number, and replacing them with a 4-digit or a 2-digit, or a 1-digit.

And I guess the most famous in the last year: the clever people at **DeepSeek** figured out a better way than other people have been using to say which of the numbers in my neural net should be 4 digits, which should be 8 digits.

And managed with relatively little compute to do better approximations.

So if you think about it, there are **no real numbers**. The real numbers are in the math world. We're just doing approximations.

And how much of an approximation you get away with—that's a complicated story, and actually there's lots of interesting work on neural nets of when do you use 8 bits versus 16 bits?

Yeah, I've got… I'm gonna throw you out, and we can talk another time, so I'm gonna move on, and…

Two things. Okay, so, sorry, non-answer, but don't want to lose the class. So…

## Neural Networks: Core Concepts

Where are we? Quick review:

All of the machine learning, supervised learning we've done is:

$\hat{Y} = f(\mathbf{X}; \theta)$

And we take the argument of some sort of loss function:

$\theta^* = \arg\min_{\theta} \mathcal{L}(Y, f(\mathbf{X}; \theta))$

All we're doing this week is taking **very different functional forms** for $f$, somewhat different loss functions, somewhat better gradient descent, and then trying to make sure they can run them fast on GPUs (which is great, because it's all vectors!).

### Loss Functions

Yes, we've seen a bunch of loss functions. Which two are used on neural nets the most?

Yep. Sorry? L1 and L2? **Cross entropy**, and you have to pick two.

So, in general, there are two types of supervised learning problems:
1. **Regression problems** with real numbers
2. **Classification problems** that have labels

#### Classification: Cross-Entropy Loss

If you're doing a classification problem where you have 10,000 labels, there's pretty much only one thing anybody ever uses: **Cross-entropy**.

$\mathcal{L}_{\text{CE}} = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)$

#### Regression: L2 Loss

If you're doing regression, where the neural net's predicting a real number, the most popular thing is actually **L2**.

$\mathcal{L}_{L2} = \|\mathbf{Y} - \hat{\mathbf{Y}}\|^2$

Could I do anything? Sure! And a lot of them we'll see in reinforcement learning and stuff, you want to predict number of dollars made, which is much more like an L1 than an L2 sort of thing.

But in general, think:
- **Cross-entropy** if you've got a whole bunch of labels (which are very common in the deep learning world, 1 of 10,000 or 100,000 labels—right, one of 50,000 tokens)
- **L2** if you're doing real values as the default

But think deeper. And again, for your projects: **what's the right loss function?**

Cool.

## Modern Machine Learning Examples

So… everything will take in:
- A web page and an ad and predict if someone clicks
- Your past purchase and predict your net present value, your expected discounted cash flow
- People's Facebook posts and predict how old they are, and whether they're male or female, and whether they're extrovert or introverted

### The Key Point About Inputs

What's the key point about… I called these $Xs, but they're not really $Xs because… **they're not vectors!**

Right? Somehow, you need to go from the webpage (which is a set of string of characters) to an **embedding**.

Make sense? You can't give a string of characters to a machine learning model. You can only give a **vector**.

So, one thing that will show up with all of these as you look at things that are more sort of modern machine learning:

If it's not tabular and small, but if it's something like text with hyperlinks and other pieces, you're gonna have to think:
> "What's the right way to embed this? What's the right way to make this into a vector so I can give it to the machine learning problem?"

Make sense?

### Output Types

Click on the ad is **binary**.

Net present value is **real**.

Ages are more or less **real** (but we mostly have integers, but we'll pretend it's real).

Sex is **categorical**.

Personality is typically a **real value number**.

Right? So these things have different types.

There are lots of things you might want to do, um… yeah.

## Artificial Neural Networks

In some sense, lots of the neural nets—and now I've used the term **artificial neural net (ANN)**, a slightly out-of-fashion version; now you'd say, a **deep learning model**. We're doing sort of history today.

Takes in something, embeds it, and puts it into a model that predicts something else.

### Examples

- **Image** → **Cancer diagnosis**
- **Camera image** → **Objects in it** (if you're doing a self-driving car)
- **Text in** → **Text out** (machine translation is a machine learning problem)

You take a sequence of tokens, you embed them, you predict an output which is a probability distribution over the next token.

You do that recursively. Pick the most likely one.

Yeah.

### Output Format Questions

**Q:** Is the output an embedding?

**A:** What is the output if I'm predicting Arabic from English?

What's the form of the output of the neural net? Is it an embedding?

It puts out a **probability distribution over tokens**.

All language models—all commercial language models that people use—take in a sequence of characters, they break them into tokens (which are sort of like words), they embed them, they feed them in the neural net, and they output a **probability distribution over a finite predefined set of tokens**.

So it's a fixed-size embedding, mapping to a probability distribution over the next token.

Now, you could call that an embedding, but that's sort of a weird notion of embedding. It literally is a **probability distribution**.

Right? From which they typically take the argmax and pick the most likely one, but sometimes they don't, yeah.

#### Probability Distribution as a Vector

It's still a vector, right? So in general, if you have 40,000 tokens in your vocabulary, the output is a **40,000 dimensional probability distribution**.

Which you then pass to, or the code passes to something, which often takes the most likely one.

But in fact, often doesn't. Often, they take… they have a non-zero temperature, and often they're picking proportionally among the top 5 most probable, proportional to the probability of the output, right?

And that lets you run the same thing over and over and get a **non-deterministic output**.

And that means that every time you talk to it, if you ask the same question, you'll get a slightly different answer.

### Two Parts of Generation

So again, think of these as models that give out **probability distributions**, and then you as an engineer get to pick: What do you want to do with that distribution?

Sometimes you're greedy and do the argmax.

But if you go to ChatGPT and just talk to it, it's **not** doing the argmax. It's not running at temperature zero.

If you start a fresh one up and ask a question—the same question twice—you'll often get slightly different answers.

They'll be similar—they're drawn from the same high probability sequences—but they're not going to be the same.

Make sense? Yeah.

**Q:** A probability distribution is a real number vector, true. So you could use it as an embedding?

**A:** It's a very particular kind of embedding.

But… so yes, you could certainly use a probability distribution as an embedding. Most embeddings are not probability distributions.

So I think of… think of an embedding as partly how you're using it.

**Embedding** is typically used as a representation that's fed to something. You could feed it to a similarity:
> "Here are two embeddings of two objects, how similar are they?" 

Compute their distance or their kernel, right? Or you could use it as a feature.

So embeddings are… you could use a probability distribution as an embedding. But that's not a very common use of it. But yeah, yeah.

**Q:** What's the difference in neural nets between neural nets and a kernel model?

**A:** We'll get there. That's a good question for today, and I've got, yes, 35 minutes. We'll get to that question.

## Semi-Parametric Models

So, **semi-parametric model**: Flexible model form, typically used with lots of data.

And what we're gonna see is the way to think about these is they are often:
- Taking the input features
- Computing new features
- Taking those features, computing new features
- Doing that recursively, so you get **features of features**

And we'll see pictures of it on Wednesday, but for today we'll talk more generally.

The classic neural nets take a **logistic regression** and generalize it.

We'll see later, like, next week, unsupervised neural nets that will generalize **PCA**, which we haven't covered, and we'll see they're used in neural nets, and we'll see lots of ways to build in structure.

### Real Neurons vs. Artificial Neural Nets

Let me give the obligatory 30-second: if you take an actual real neuron, there's a picture on there, and you actually change the input, the output spikes over time, and then it stops, and if you drop it, it's…

**Real neurons are really different. Really, really different from artificial neural nets.**

So… if you want, you can sort of say, well:
- The weights in the artificial neural net sort of look like synapse weights
- The cell bodies sort of look like things that are going to take linear weighted combinations of the inputs
- The output activation functions sort of look like the spiking function

But I think the term "artificial neural net" is a little bit out of fashion, because they're so different from real neural nets (like, ones in your brain).

So, that's all I want to say about it, except to say: **inspired by!** But really different.

## Logistic Regression Review

Okay, so quick review. I think everybody has seen this, but if not:

We take a feature vector in, we're gonna put in $X_0$, the bias term, as a constant.

Right, so we have a $p$-dimensional vector (here, $p = 3$), plus a bias term.

We'll put it in, we're gonna take a set of weights times (which we're gonna learn) times that $\mathbf{X}$.

Plus, what they often call $b$, that's the weight zero (or $w_0$).

We'll take that in. The classic function that people use is the **logistic function**:

$\sigma(z) = \frac{1}{1 + e^{-z}}$

where $z = \mathbf{w}^T \mathbf{X} + b$

That's **logistic regression**, right? Solved with gradient descent, convex, beautiful.

Good? Go ahead.

## Multi-Layer Perceptron

What you do is take this and you run a whole bunch of logistic regressions with different weights, so now I have…

Happens to be the same number (3 hidden nodes), which could be 5 hidden nodes, or could be 2 hidden nodes, could be more, could be less.

Where I've now mapped the original feature space, $\mathbf{X}$, to some new feature space, which are the **activations** and the outputs of the neural net.

### Network Structure

Every line on this diagram is a weight, so what we have is: this neuron takes in:

$z = X_1 W_1 + X_2 W_2 + X_3 W_3 + 1 \times W_0$

Adds them together, passes them through a link function.

Takes the output—that has the activation. We do that 3 times, 4 times, 17 times, we get 17 different transformed features, which are the activations.

We then pass them to another layer.

Take the weights times each of those, sum them up, pass them through a link function, give the output.

Allegedly, you've all seen this? Question, yeah.

### Hidden Layer Dimensions

**Q:** The hidden layers can either have more features or less features?

**A:** Both are used frequently.

We'll talk later about how you might decide how many and what you want to do, but yes, important point.

Anytime you do a feature transformation, it's sort of like radial basis functions. You can have more of them, or you can have fewer of them.

Are you trying to get something that is high enough dimension that it makes linear something which is nonlinear? Are you trying to compress something down?

**They're both used**, and we'll see often they're both used in the same neural net.

#### Criterion for Dimension Choice

**Criterion for one or the other:** I think at some level, think about:
- Are you taking something **high-dimensional**, like a three-dimensional brain scan, where you only have a thousand brains?
  - And you need to **reduce the dimension**, because you just don't have enough data to label them as cancerous/non-cancerous?
- Or is it something that is relatively **low-dimensional**—you only have 40,000 tokens, but you have billions or trillions of tokens to train it on?
  - Now you probably want to **blow it up** and try and make it more linear so you can solve it better.

Yeah.

### Link Functions

Yep. Exactly.

**Q:** Can you use whatever link function you want, wherever you want?

**A:** Yes! Logistic regression was the first one people used historically.

But in two slides, we're going to cover all the ones people use today.

### Feature Transformation

I'm mapping to a new feature space. Yep. Yep.

Exactly. So instead of using a radial basis function, or something which is a way to do a transformation, now we have a **logistic regression transformation**.

Yeah.

### Network Connectivity

**Q:** Does it have to be fully connected? Can you randomly connect it? Can you do all sorts of different structures?

**A:** Most of them are **fully connected**, because remember, the thing you're concerned about on big data is **speed**.

And so what you want is things that are vectorized, because you're using vector processors, called **GPUs**.

And so, in general, what you want is to… getting them sparse doesn't help you a lot, because having a bunch of zeros in the vector doesn't speed up the vector processing.

But there are plenty of people who like to make them sparse. But mostly, we'll see people make them sparse on a much more macro scale.

## Deep Networks

Now, once I've taken my initial $Xs, I've transformed them by 3 different logistic regressions, so I have a new set of activations that are transformed features, I can take the transformed features and then **transform them again**.

In this case, I have dimensionality reduced from 3 to 2.

But here I took in these 3 transformed features, call them $\mathbf{Z}$, and the constant, $Z_0$.

I've now run them through different sets of weights. Run them through a logistic function, and I get a different output.

Which I can then take again and multiply by weights and sum and combine, so we can have **features of features of features**.

And it's pretty common these days to have ones that are **50 or 100 layers deep**.

Features of features of features of features of features.

Yeah.

### Deep vs. Wide Networks

**Q:** Why have deeper ones rather than wider ones?

**A:** There are beautiful representation theorems that say that if you use a wide enough net or a deep enough net, with an infinite number of weights, you can approximate any smooth (sufficiently smooth) function.

So any smooth function can be fit by either of them.

There is interesting math that shows that starting at two layers, you can get more representational power if you think about trying to find things that are **interaction terms**. It's easier to capture interaction terms with things that are a little bit deeper.

#### Hierarchical Feature Learning

And I think the answer is that lots of real-world problems work best if you think of taking an image:

You take the image, the first layers we'll see tend to do simple things:
- Recognize edges of things
- Recognize patterns that you might find the shininess of a head, or the shape of a beard, or the edge of a shirt

As the first layers, and as you get deeper, they start to have ones that are sort of more **macro**, that start to say:
> "Ah, okay, that's a shirt, that's a head"

And then as they get more macro, they have a bunch of pieces that combine those features and say:
> "Hey, it's probably some guy in a classroom waving around his chalk."

So, note that as you get deeper into the neural nets, in general, you have sort of a more **macro, big-picture, gestalt, combine-everything-together** view.

Whereas the first layers are more **superficial**.

Right? What color is it in this patch?

### Choosing Network Architecture

And there's a **black art** of how do you pick how deep and how wide, and we'll see there are also some cool tricks to make them work better.

But an awful lot of it is just **empirically try what works**.

And what seems to work are things that are of order **50 deep**.

Even though, in theory, you could do one that's, you know, a million wide, and it would also be able to do it.

Great question. Wish I had a clean answer.

Cool!

## Activation Functions

So… in the early days, people had a **logistic function**, which looks sort of like that:

$\sigma(z) = \frac{1}{1 + e^{-z}}$

Then, starting when we'll get to the story of Jeff Hinton in a second, what became popular was the **ReLU** (rectified linear unit).

### ReLU (Rectified Linear Unit)

The rectified linear unit is something that is:

$\text{ReLU}(z) = \max(0, z) = \begin{cases} 0 & \text{if } z < 0 \\ z & \text{if } z \geq 0 \end{cases}$

This is a term you should absolutely know. This is a super important technical term in neural nets.

**How many people know a ReLU?**

Most, yeah, awesome, right?

**Zero if you're negative. Linear if you're positive.**

You can view it as an approximation.

#### Why Was ReLU Super Popular?

Why was it super popular in the early 2010s, the late 2010s?

**Why is the ReLU nicer than a logistic function?**

**It's really cheap!**

Right? This is like an ANDing operation. If the sign bit says it's negative, give zero. If not, return the thing. It's **super fast** and **super cheap**.

**Is it convex?**

Yep.

**Is logistic convex?**

Yep, they're nice.

#### Problem with ReLU

Um, what's not nice about the ReLU? Why does nobody use it anymore?

Well, forget the "nobody uses it". What's the unfortunate property of ReLU for gradient descent?

It's continuously differentiable… it has a derivative everywhere except for 0, so that discontinuity is not a problem. There's a point of Lebesgue measure 0, there's a tiny point where there's no derivative, but that doesn't cause any real problems.

There's another problem with it.

Yep. **The gradient vanishes!**

For half of the things, the gradient is zero.

$\frac{d}{dz}\text{ReLU}(z) = \begin{cases} 0 & \text{if } z < 0 \\ 1 & \text{if } z > 0 \\ \text{undefined} & \text{if } z = 0 \end{cases}$

If you're doing gradient descent over here (in the negative region), you got no gradient.

And that ends up leading to what are called **dead neurons**.

A **dead neuron** is one where you try and do gradient descent, but the neuron always gives you zero for all the inputs. The output's always 0, the inputs are all negative, and the gradient's zero, so it never learns.

So these were super popular because they're incredibly fast. And they're nice, and they converge beautifully, and they're stable.

And they're unpopular because they have a zero gradient for a lot of inputs, and zero gradients make it really hard to converge. So you end up losing a whole bunch of your neurons that get stuck.

### Modern Activation Functions

So, what do we see today?

Today, we see the **SILU**, the **Swigloo**, and the **GELU**.

Which you do not need to memorize, but I think the bottom line is…

Sitting here in red is the ReLU: 0, and then constant (linear).

And if you look at… take the green **GELU**. The GELU looks really similar to the ReLU on the positive side.

But then it scoops down just slightly, and scoops just up again.

It's a **continuous version** that looks like a ReLU, but in fact, has a **non-zero derivative everywhere**.

$\text{GELU}(z) = z \cdot \Phi(z)$

where $\Phi(z)$ is the CDF of the standard normal distribution.

And I think last time I checked, the most popular one in Hugging Face was **Swigloo**, but… who cares?

These are all a **one-line swap** in your PyTorch code.

They're all doing the same thing, they're all more expensive than a ReLU, but they all lead to not having ever a zero gradient, and so they're just much nicer.

And if you care, they have lots of equations that have… funny things in them. Um…

But I don't really care… I mean, I don't get very attached to whether it's a sigmoid or a ReLU, or a SILU, or a GELU, or a Swigloo.

Because they're all pretty much the same thing. But they're critically **all non-linear**.

**Is the ReLU nonlinear?**

Yeah, it's non-linear.

Right? And, in general, people today are… you have faster computation, and you're more worried about getting gradients everywhere, so people have switched to Swigloos and things.

### Practical Advice

Cool. So… don't get attached to the link function if you care.

Ask ChatGPT:
> "What's today's favorite link function?"

Go to Hugging Face, what's today's favorite link function? But it's a **one-line code change**.

**Do not get attached to it.**

Cool!

## Why Neural Nets Over Fixed Basis Functions?

Why do I want to have neural nets rather than to have fixed basis functions?

And I wrote in my notes: see the picture on the board, which I'm about to put.

So, imagine you have some… I'll just do one dimension, $Y$ as a function of $X$.

And you've got some data that looks like whatever. Like that (drawing a curve with varying densities of data points).

And you want to fit a function to it. Now, of course, we're in 10,000 dimensions, but picture 1 dimension's fine.

### Problem with Fixed Kernels

If I do a **radial basis function**, what problem do I have fitting this curve?

What's my most important parameter for the radial basis function?

**The kernel width, the sigma ($\sigma$).**

What's the right $\sigma$ here? Is it gonna fit this one (tight cluster)? How is it gonna fit that one (spread out cluster)?

It's not going to work well. If you're picking one fixed radial basis function:
- **A)** They're radially symmetric, which is nasty, because, hey, we're in high dimensions, things are not symmetric
- **B)** You've picked **one length scale**

### Adaptive Kernels

What you'd like to have is an **adaptive kernel** that learns, given your data, how big the kernel should be.

Here, I want a really tight radial basis function. Here, I want a nice, broad radial basis function. Here, I want a really broad radial basis function. Or non-radial.

So, it turns out that you can show that for many problems, you get **exponentially better behavior** if you can **adapt the kernel locally** to fit the data, rather than picking any single kernel *ex ante* (*ex ante* means before you see the data).

Does that make sense?

All the methods we had before, these sort of kernel methods, all assume you're gonna pick some kernel with some kernel width, which assumes that across your entire dataset, you roughly… oh, there's one counterexample?

**K-nearest Neighbors was brilliant.** Because it, in fact, does expand or contract, so I love k-nearest neighbors.

But the kernel functions all have a **fixed kernel size**.

Yeah.

### Sinusoids as Basis Functions

**Q:** What about several distinct, different… got different frequencies?

**A:** But note that if you try and do this with a fast Fourier transform, a bunch of sines…

Could I use a sinusoid as a link function? Sure, people do. If you think the world is, in fact, sinusoidal, you're doing… you're doing audio? Sure, you could use sines.

But if you… sinusoids are not going to do a great job of capturing this model either.

So it's not a great fixed basis function.

So there is no good optimal sinusoid, single sinusoid frequency. You can do a whole bunch of frequencies, but it's not gonna work very well.

### The Key Insight

You really, really want to **adapt it**.

So again, I'm trying to give you math intuition without the math. What I'm saying is that if you have a complicated function with different sizes where some things are close and some things are spread out, you're really, really better off using an **adaptable kernel**—one that picks the dimensions appropriately.

And if you have something like this, you could fit a ReLU or a GELU or whatever that says:
- Okay, fit one here that goes like this
- Fit one here that goes like this
- Fit one here, and one here, and one here
- Fit a ReLU like that, fit a ReLU like that

I don't need very many ReLUs. I can put them right in there and just nail this, right, with a ReLU here and a ReLU there that squeeze in and just capture this thing.

### Summary

So, by being able to **adapt my features** (and by features, I only mean the transformations, right?)—the things that take the original $X$ here and map them to, in this case, a much higher dimension (right, I went from one dimension here to, like, 10-dimensional, embedded, right? An embedding)—these new features, by having the ability to **learn by gradient descent where to put the values**, or Swigloos, I can do **much better** than if I had to just pick a fixed set of feature transformations, all with the same length scale.

Okay, um, let's relax for 5 minutes and just quickly talk about the history of these things.

## History of Neural Networks

Looking back in the early age… back in the late 90s, there's a guy **LeCun**. Anybody heard of this guy?

**Yann LeCun?**

He's currently… people track these guys? No.

**Meta**, right? He's a senior AI guy at Meta.

They used:
- K-nearest neighbors, got **5% error**
- They used a radial basis function, got **3%**
- They used boosted stumps and got **1.5%**
- They used a support vector machine, got **1.4% error**

They started using fancy neural nets toward convolutional neural nets, and got enormous gains back in the 2010s, right? So, a little over a decade ago.

### MNIST and Handwritten Digit Recognition

You could learn to recognize… Yann is famous for being the guy who told the **U.S. Postal Service** how to actually recognize handwritten addresses.

This was the big first breakthrough of commercial neural nets—for recognizing handwritten digits.

You could OCR-type stuff, but you couldn't OCR handwritten stuff.

**MNIST** is… so

---

# CIS 5200 Machine Learning  - Lecture 13: Transfer Learning and Regularization

## Administrative Announcements

Mostly, I've been hanging out after class and talking to people, and then not making it back to my office. Today, I actually have to run back and take the first half of my office hours at 3:30 and meet someone else. I'll be free the second half.

In general, either grab me right after class—I'll be sitting here or right outside—or email me or send an EDM (can I say that?) and set up an appointment to meet later, because my office is hard to get into; it's locked.

### Project Information

You should be thinking about projects and groups of people. Sets of three seems to be a nice number, so I'm going to try and strongly enforce that. You'll get the project descriptions coming out today, along with the next set of homework, with lots of details. Feel free to reach out to me and the TAs in office hours to ask questions. I'm happy to give suggestions about datasets or questions.

Cool. Any more bureaucratic questions? Good? Yes.

### Midterm Solutions

Are we going to go through the midterm solutions? I'm going to look, and if there are things that seem super useful for people to review, I will cover them and explain the ones that are most important. Or I'll put it in writing—I'll figure out what to do. So I'm going to sit down and figure out where the major errors were and give feedback. But yeah, I'll try and spend at least a little bit of structure somewhere to give feedback, because yes, the midterm should be a learning experience, not just an assessment. Thank you.

### Course Pacing Note

For the neural nets, most of you have seen neural nets before. Most of you have seen PCA before. I'm doing a little bit of a second pass through them, so I apologize. Some things, as always in this course, will be super fast.

If you haven't seen convolutional neural nets, I covered it too quickly. Please go back to that link to the Stanford site and walk through the convolutions and how they work. That takes at least 20 minutes to walk through. Enough of you have seen it; I don't want to spend class time on it.

So if you haven't seen it, please do go back and review that. Most of the things I'm trying to do is give you the "how does this fit as part of machine learning" perspective. I'm also not going to cover a lot of modern self-attention or transformer architectures, because there are whole courses on that. What I'm trying to do with the neural net section is, on the one hand, make sure you've seen the basics of it as it fits as a piece of machine learning, trying to tie it into other pieces of the course, and to be able to use it later when we do deep reinforcement learning. I apologize—I know for some of you it's a little fast.

Cool! I hope that's clear. Let's see if we can find PowerPoint and start today's lecture!

---

## Why Neural Networks Haven't Made Traditional Methods Obsolete

Cool! So before I start today, I wanted to ask one big question. The first half of the course, pre-midterm, was all about logistic regression, linear regression, gradient tree boosting—widely used methods. Now we're sitting back in the early days, up until 10 years ago, of neural nets.

**Big Question:** Have neural nets put gradient tree boosting and logistic regression out of business?

**Answer:** No!

**Follow-up:** Why not? Given that neural nets can answer every single problem you've ever seen, who's using logistic regression in the 21st century? Why?

### Common Misconceptions

**Misconception 1:** People don't have a lot of compute power.

**Response:** No. The cost of compute is pennies. Nobody in general (there are rare exceptions) is using gradient tree boosting because they can't afford the compute for neural nets. Compute's really cheap.

### Interpretability

**Claim:** It's more interpretable.

**Question:** What's more interpretable? Gradient tree boosting or a simple neural net?

**Answer:** Partly, yes. Logistic regression, people like because logistic regression gives you two things:
1. It gives you a coefficient in front of every feature, which they like
2. It gives you log-odds that really are ready to produce a probability, which they like

So logistic regression is super nice. Gradient tree boosting, which for many, many problems is slightly more accurate than logistic regression and is widely used (say, in the medical school here), doesn't give you a probability in the same way. It's not a probabilistic model in the sense that a logistic regression is, or even a neural net is.

And you've got the average of a thousand trees, or maybe a small model with 100 trees. Not really interpretable. You can do feature importance, which we'll talk about, but hey, you can do feature importance on neural nets too.

So there's something else...

### Hyperparameter Complexity

**Student:** Harder to...tune neural nets.

**Professor:** Two things: neural nets have many more hyperparameters. How many hyperparameters does gradient tree boosting have?

Let's start with an easy one. **What are the hyperparameters—how many hyperparameters does logistic regression have?**

**Student:** Two, which would be...

**Professor:** If you use kernels, most people do not use kernels in logistic regression. So most people use off-the-shelf logistic regression. How many parameters?

**Student:** One, which is...

**Professor:** What? Function? No, the function is fixed. It's a logistic function.

What regularization do I use with logistic regression? This is all from the midterm. If you only got one regularization to use in regression, what would you use as the simplest one?

**Student:** L2, or Ridge, as it's called by the statisticians.

**Professor:** Right! So a standard logistic regression, you've got one hyperparameter, which is the Ridge or $\ell_2$ penalty. I like $\ell_2$ because it's mathy; statisticians like Ridge because it's sort of branded. That's like Wharton—it's branded. It's nice.

So, really easy! **How many hyperparameters does gradient tree boosting have?**

**Students:** The depth of the tree, some sort of learning rate, maybe two-ish? Maybe 3 for hyperparameters—what feature for the bagging.

**Professor:** How many hyperparameters does a simple neural net have?

Okay, **what hyperparameters does a simple neural net have?** A multi-layer perceptron, as they used to be called. What?

**Students:** 
- The number of layers
- The size in each layer (so that's a whole bunch of hyperparameters, because you have a lot of layers, a lot of sizes)
- Activation function

**Professor:** Activation function—I showed you 5 different activation functions last time. Swish, ReLU, whatever. Okay, but the old-fashioned ones just use the hyperbolic tangent. Fine, but yeah, that's a hyperparameter.

For CNNs, you have to choose—remember the AlexNet had 5 layers of CNN with a given size of how big the kernel was? And then 2 layers that were fully connected. So that's another set of choices.

**There's a whole other class of hyperparameters you're missing, which you have to choose for machine learning.**

**Student:** The weights?

**Professor:** Well, the weights are **parameters**, not hyperparameters. The weights on a neural net are parameters, not hyperparameters.

**Student:** The learning rate?

**Professor:** The learning rate, for sure, is an important hyperparameter, yep.

**Student:** Fully connected or partially...

**Professor:** You could do that, yeah. Big other class?

**Student:** The optimizer?

**Professor:** Yep. The optimizer, sure, that's sort of like a hyperparameter. The other one is the **regularization**.

---

## Regularization in Neural Networks

You're going to do, typically, an $\ell_2$ penalty on the weights. You might also do an $\ell_1$ penalty on the weights in the good old days.

### Early Stopping

You'll train these things typically with **early stopping**. So it's almost always the case in neural nets of all flavors that what you're doing is you're training the network, and as you have more gradient descent steps, the training error keeps going down! It always goes down. Eventually, you run out of compute time.

But is that the best model? No, because you have a separate test set.

**Question:** Do I do tenfold cross-validation on neural nets?

**Answer:** Usually not, because they're very expensive to train.

So I will train them, and the testing error looks like this:

```
Training Error:  \  (continuously decreasing)
                  \
                   \___________
                   
Testing Error:     \    /  (U-shaped curve)
                    \  /
                     \/
                      \
```

Oops! So a hyperparameter, in some sense, is **when do you stop?** How many mini-batches do you do in your training?

It's really, really, really standard in neural nets—your garden variety of neural nets—to train it for a long time and notice that, oh, the testing error is gradually going up.

And of course, this is pretty idealized. In reality, these things are mini-batched, so you know, the errors are going to be wiggling around there. But the wiggles from the mini-batches are pretty small compared to the number of iterations, so this is a very fine-grained graph.

Greg looks like that! Do you actually have a real one? [referring to a student's visualization]

So you've got a lot of hyperparameters, a lot of model architectures, so it's much more stable to just take something like gradient tree boosting or logistic regression and train that.

---

## Tabular Data vs. Multimodal Data

**Question:** What classes, what are the names of the classes of data that logistic regression or gradient tree boosting (the first half of the course) work on? The neural nets guys have a name for this data. More jargon. Yep.

**Answer:** **Tabular!** Yes!

**Tabular data:** Data that sits in a table, like an Excel spreadsheet or a CSV. Tabular data is the magic term that you should know—super useful jargon to know. Part of this course is just a crapload of stupid jargon.

If you have tabular data, you're mostly not doing neural nets for it.

**Question:** What's not tabular data?

**Students:** Time series, and...

**Professor:** Think of what examples I took yesterday. Images, audio, video, right? Things that are video, right? Things like here's all my medical records, or here's all my social network connections. Things that are sometimes called **perceptual** or **multimodal**.

So in general, multimodal data—you do much better on it with a neural net than you do with gradient tree boosting.

### Why Neural Nets for Images?

**Question:** If I'm doing image processing, why do I probably want a neural net and not gradient tree boosting?

Or if you don't like image data, think about: I want to try and predict career success by your grades in all your courses, what job you should take, you know, tabular stuff.

**Answer:** Images have an inherent assumption that nearby pixels influence each other, or more importantly, there's more correlation between pixels that are close. This would be called **inductive bias** in the model, and so you'd like a model that notes that and builds that in. CNNs build that in.

Modern methods like transformers mostly drop that inductive bias and no longer... So the ones from 10 years ago that we just looked at, the AlexNet, in fact has a strong inductive bias that local correlations are more important than global ones, that the first layer should find local correlations, deeper ones should find broader features.

The modern vision systems tend to use transformers, which I'm not going to cover, that take the whole input at once. They relax that inductive bias.

### The Cost of Relaxing Inductive Bias

**Question:** What's the cost you pay by no longer making an inductive bias? Inductive bias assumes nearby pixels are correlated, and the same feature detector here works also over there. If I relax that assumption and say, hey, any pixel could correlate with any other pixel, what do I need?

**Answer:** **More data!**

Right? The stronger your inductive bias—"I assume that things are linear!"—awesome! That's a really strong assumption. I need a lot less data.

If I say, hey, these things could have any possible relationship between each other whatsoever, I'm fitting a much more complicated model with much more parameters.

**Question:** Can a standard neural net represent anything that a logistic regression or a linear regression can represent?

**Answer:** Sure! Right? It's a more complicated model. It can fit any continuous function. Any reasonable function can be fit by a neural net.

**Question:** Does that mean it's a better model to use?

**Answer:** No! If you have infinite data and infinite compute, sure. But mostly companies are limited by compute or by data.

**Which one?** Data!

### The Data vs. Compute Limitation

I worked a decade ago at Google. I had access to all of Google's compute and all of the internet. Which one was more limiting?

**The internet!** I only had every text put on the internet. There was no more data. Compute? Not a problem. You want another thousand GPU hours? Fine, it's going to cost a little bit.

Right? I was **data-limited**.

And that was back when compute was slow. Now compute is fast, and the internet's bigger? Now, some things are still compute-limited.

I was doing static images. I wasn't doing... What else does Google own in terms of data? What's their big data source?

**YouTube!** They own YouTube! That's a lot of data. Still compute-limited for video.

So there are still cases where we're compute-limited, but more and more, it's data-limited rather than compute-limited. And certainly machine learning mostly sits in the data-limited world rather than the compute-limited world.

Cool. That's a long digression, but in some sense, my goal today is only to digress and give you insight.

---

## Regularization Revisited

So, regularization—to repeat what I was saying before:

Here I have error on the y-axis. I can draw the same picture with accuracy. In general, what happens is the training accuracy goes up with epochs, with the number of times you look at all the data, or mini-batches.

The accuracy on a separate validation set or test set generally goes up—you hope. Sometimes it goes up less, and we're going to talk a little bit about what causes that.

In general, lots of these systems just put an $\ell_2$ penalty, maybe also an $\ell_1$. Everybody uses early stopping. Don't train the thing all the way. If it's big enough, you might run out of compute power first.

### Additional Regularization Techniques

Sometimes people also use funny few tricks on the gradient descent. The most popular one—a jargon term you should know—is either **weight clipping** or **gradient clipping**.

#### Gradient Clipping

**Gradient clipping** says: Hey, if the weights get bigger than a certain amount—they're bigger than a million—I'll just truncate it at a million. That's less popular.

More popular is gradient clipping when I'm doing gradient descent. If the gradient gets bigger than a certain amount—a million, whatever—we truncate it. So we clip the gradient and say, don't take a step bigger than this.

These things are mostly no longer used in simple neural nets like we're talking about this week, but we're going to come much later in the course to reinforcement learning. In reinforcement learning, the algorithms are a little more unstable. They're still doing gradient descent, and lots of the deep learning in the reinforcement learning context uses gradient clipping.

**Make sense?** So it's just ways of making the convergence a little more stable.

Remember what happens with the gradient: If you take too big a step, you go past the optimum and up the other side of the valley. One way to solve that is just take a small step, which is the nicest way. But another way is to say, hey, just don't take a step above a given threshold.

So it's a super simple technique, but it's one you should realize sits in your sort of toolbox to avoid instability in gradient descent.

I'll try and remind you again when we do reinforcement learning.

---

## Dropout Regularization

Cool! **Dropout!** How many people know dropout? I'm not talking about dropping out of school. Lots of you! Okay. We're still going to cover it. Enough don't know it, so I'm going to do this at normal speed.

### The Dropout Algorithm

The idea of dropout—here's the algorithm. The algorithm says: I want to avoid overfitting, so it's a regularization technique.

What I'm going to do at each mini-batch is I'm randomly going to pick a fraction of the nodes in the neural net—neurons—and I'm going to temporarily drop them out. Make them vanish.

So here's my full network:

```
[Full network diagram]
```

Here is a network where I've taken half the nodes, roughly, and I've crossed them out:

```
[Network with some nodes crossed out]
```

Right? I've just dropped them out of the calculation. So I've got a network now that's sort of half of my original network.

I'm now going to do one mini-batch update: update the weights, take the gradient, update those weights.

Now I'm going to put back in the half that I temporarily dropped out—I didn't delete them, I just ignored them. And I'm going to randomly pick another half. Right? This is, again, **sampling with replacement**.

Just randomly pick 50%.

### Choosing the Dropout Probability $p$

**Student:** Yeah. What's the strategy for picking $p$?

**Professor:** The strategy for picking $p$ is to look to see what people have done in the past and pick some number that's somewhere between 0.2 and 0.8, so 0.5 is a really nice number.

And the answer is it's one more stupid hyperparameter that nobody has a good theory on why you should choose a particular value. But a half (0.5) would be a good starting point.

**Student:** Yeah. Instead of dropping out the random ones, could we take the 50% that are less active, that have zero activation?

**Professor:** No, that's not going to do what I want. The whole point here is...

### Why Randomness Matters

One way to regularize is **noise**. You can regularize by:
- Adding noise to the $X$'s
- Adding noise to the activations
- Dropping things randomly

And it turns out that this form of sampling with replacement has very nice mathematical properties, because you are actually searching through a space which is enormously big.

What you want critically—and it's the same way we needed in gradient tree boosting—I want to put noise into the trees. I want **weak learners**, right? We regularized before. We said, hey, how do you avoid... how do you learn lots of trees? Make each tree really random.

Only pick a fraction of the training points. Only pick a fraction of the features. We intentionally sampled over different trees to get a noisy tree for gradient tree boosting.

And now we're intentionally adding noise. So anything that you tell me that's clever—that says, "here's a way to throw out the worst things"—I'm going, no, I want randomness. **The randomness is a feature.** It gives me maximum exploration. It knocks things out.

What I don't want is to do a whole bunch of things that all look really similar. I want each of these neural nets to be as different as possible.

**Student:** It's like annealing, except annealing's a technical term which says start with lots of randomization and then have less and less randomization as you go along.

**Professor:** So annealing is a different process. Annealing says lots of randomization at first, less randomization toward the end, and that's fine, but that's not dropout.

Throughout the whole procedure of dropout, I'm going to repeatedly take half the nodes, temporarily zero them out—usually $p$ is a half, to answer your question. I never search over different values of $p$ for the hyperparameter; either I do a half or I do none (no dropout).

### Mathematical Properties of Dropout

And do this over many iterations. It turns out there's some beautiful math, which again I'm not going to show you because it would take an hour, that if you actually do a bunch of random sampling, it's equivalent in some sense to sampling over **exponentially many networks**, because you're doing lots and lots of samples. You're sampling broadly over the space of possible networks.

And what it does is it **avoids local minima**.

So think about what we said about gradient descent. If your world looks like this:

```
    /\    /\
   /  \  /  \
  /    \/    \
```

You can start somewhere, and if you walk down, you're going to find a local minimum.

**Question:** Are neural nets convex?

**Answer:** Absolutely not! They're massively non-convex. And so if I want to find a low optimum, I might want something that says, hey, put some randomization in, bounce me around.

And one way of bouncing that has proven to be quite effective... These are not used in modern transformers, but are used in most of the CNNs that are still used. A lot of CNNs are still used in vision and audio. In CNNs, people mostly use this sort of bouncing around with dropout.

### Using the Final Network

Then once you've done this for the final network, you're going to use all of the weights, because remember, I've now done this 10,000 times—10,000 mini-batches, each with a different 50%.

I've converged to something. I put all the weights back in. But realize I trained everything at each point with half the weights. If I'm going to put twice as many weights in, I've got to divide the results by two.

If I use 10% of the weights each time, I've trained something that has numbers that are based on 10%. If I'm now going to have 10 times that much, I'm going to divide that by 10.

**Does that make sense?** It's a technical detail I don't really care about, but I think the important thing is this concept that says that one way to make your gradient descent not get stuck in local minima is to do this dropout.

And particularly, in the CNN world—the convolutional neural net world, which is the vision and audio world—a lot of these are pretty standard. Yeah?

### The "Exponential" in Dropout

**Student:** Where does the exponential come in?

**Professor:** Um, what's happening is... See if I can give the intuition for the math.

Obviously, I'm not literally sampling over exponential networks. I'm doing... if I'm doing 10,000 iterations, I'm doing 10,000 networks. But each of them has a different 50% of the nodes dropped out.

And so I've done something where at each step I've checked lots and lots of different combinations of neurons. So we're throwing together at each step a random, different set of neurons.

And one of the key things about dropout-style regularization—and this happens a lot in gradient tree boosting and random forest—this idea says that if you just train one network or one decision tree, if one feature's important, it comes to dominate, and the other ones get thrown out.

Whereas if half the time you throw out that feature, then whatever is the second-best feature needs to come in. And that, then, is like an $\ell_2$ regularization, almost, in the sense that it shrinks the biggest feature and says, hey, the second most important feature should matter too.

Now, in deep learning—or neural nets, I mean—we've got not just raw features, but features at every level. And now we're saying, hey, at every level in the network, don't just let the dominant feature that's currently doing the best... Throw it out with 50% chance—this is extreme, 50% chance—throw out the really useful feature and force whatever's the second-best feature to take a step in the right direction.

So this is now sampling and resampling over all the features at all layers, right? Not just the raw features, but the intermediate features. So it's doing lots and lots of samples.

I think you should drop the "exponential" terminology, although there is math that says that, and just take the intuition that says there's something very nice if you're doing a search to say, don't just let whatever first seems to be the right feature dominate. Throw it out and let the other features also come in. Right? It's almost a boosting sort of theory: throw it out, get the other pieces.

**Student:** We are sampling over all possible subsets of the neurons?

**Professor:** Yes! We are sampling—good way to put it—we are sampling over all possible subsets of the neurons. And we can't literally sample over all possible subsets because there's exponentially many. But by repeatedly, randomly taking 50%, we get a provably good approximation to sampling over all subsets.

Thank you, that's a nice way to put it. I wish I had said that.

**Questions on dropout?** Cool. Okay.

---

## Invariances and Inductive Biases

So all of these things... When we build neural nets, we're thinking about what are the **invariances**—which things matter, which things don't—and a CNN assumes **translational invariance**. It assumes if you have a feature detector, right, one of these little filters or kernels, a little filter that no matter where the filter's detecting the feature, should be the same.

**Question:** Is that true in the real world? Does it matter where you have a feature being detected?

If I'm looking at an image like me, sort of... Is the top of an image and the bottom of the image usually the same? If you take a random set of images, say videos on YouTube?

**Answer:** Absolutely not, right? The top of the image has got the lights in the sky; the light comes from above in general. Almost all images have light from above. It's very rare to have light from below.

So this is a strong assumption which is not quite true.

**Question:** Are most images invariant to a little translation?

**Answer:** Pretty much, right? Whether you take the camera here or there, a couple of pixel shifts don't matter. So translational invariance this way doesn't make any sense for large shifts. Flipping really changes it.

If you have a picture of a cat, the odds are the feet are down and the head is up. I haven't tried with my cats, but I'm told if you throw a cat out a fifth-story window, they will fall feet first. Right? They really, really have this physics reaction, very fast reaction speeds.

My cats are amazing—they can slap mice or dogs way faster than a dog. Anyway, the feet tend to be down, the heads tend to be up. Yep, all of you have your heads up and your feet down. There's not one of you that's inverted.

So some things are invariant. I shift you a little this way, you don't change. I flip you upside down, somehow you're really different. It's not to say that people are never upside down, but statistically it's rare and it's highly... has information.

### Building Inductive Biases into Models

So as you build your models, as you do your final projects, you're going to think about what are the invariances, what are the things that I don't want to learn, what are the things that I do want to learn?

And a typical sort of trick is to say, hey, I don't have enough data, I'm going to cheat by building **synthetic data**. And that's still super popular in my field of natural language processing. Lots of companies are generating huge amounts of synthetic data, having LLMs generate data and then training LLMs on the same data.

We'll get to that one. Or if you flip an image this way, that usually doesn't change things. Or sometimes you build things into the loss function: I don't care if this matters or not. I want something that doesn't repeat itself too much, or I want things to give longer answers.

These are all **inductive biases**—something you know about the world.

---

## Data Augmentation in AlexNet

And if you look back to the AlexNet, which I talked about on Monday, one of the many tricks that Alex [Krizhevsky] and Ilya [Sutskever] and Geoff [Hinton] did was they said, we don't have enough data. We're going to take a bunch of our images—and there you can see an image of a cat, because, hey, the internet's full of cats—and we're going to... They said their data overfits, so let's actually look at the numbers.

This is a very old network, right? This is a dozen years ago. Now tiny by modern standards: **60 million parameters** in 600,000 neurons.

Right, so 60 million parameters. **How big is a modern model?**

**Answer:** Billions, right?

So a standard model that my grad students use, because they only have crappy GPUs, would be somewhere between **2 billion and 8 billion parameters**. 2 to 8B, as we call it—billion. An 8 billion parameter model, like Llama or Qwen, is a **small model**. That's one that you can afford to train on a single GPU.

A medium-sized model is **80 billion to 400 billion**. That's one that we could, if we really wanted to, rent enough GPUs to train on. And a big model's going to be a lot bigger than 400 billion, so we're talking a **trillion parameter model**.

Right? So 8 billion is small. These ones [AlexNet] are only 60 million!

Well, I'm looking at my phone. 60 million parameters! Will that run on my telephone? What's the memory on my phone? It's a modern iPhone. How much memory?

**Student:** 8 gigs.

**Professor:** And how big is a gig compared to a million?

A gig is a billion, right? Just to be clear, so we're mapping between math terms and computer terms. So my phone, and probably yours, fits about **8 billion parameters** on it. These old models fit really well on a modern phone. Just to think about the size.

Anyway, they have something with a pathetically small model of only 60 million parameters, but they have fewer than 60 million images with labels, and so it's overfitting like mad.

### Regularization Techniques Used

And the... One of the regularization techniques—of course they do early stopping, of course they do $\ell_2$ regularization, of course they do dropout—and the other regularization they do is they take the image of the cat, or every image, and they **shift it by a few pixels**.

Still a cat. Shifted by a few pixels. Still a cat. Shifted by a few pixels. Still a cat. You can generate a lot more labels.

**Question:** How similar is a cat image to one that's shifted by 3 pixels? Very or not very?

In fact, if you take the $\ell_2$ distance between them, they're actually surprisingly not very close. Because cats have lots of hairs and whiskers, and a hair is like a pixel size or smaller. So unfortunately, if you shift something by even a couple of pixels, the $\ell_2$ distance in picture space is pretty high.

You with me?

### $\ell_2$ Distance for Images

In the old, old days, before neural nets, people tried to retrieve images by looking at the $\ell_2$ distance between my image and the image I'm searching for. Doesn't work very great. It'll find trees and forests that are mostly green, or people that are mostly flesh-colored, but in terms of actual matching, $\ell_2$ distance between pictures is a terrible similarity distance.

That make sense? Yeah.

**Student:** How can I just... what's... how do I compute the distance between two pictures?

**Professor:** What does the picture look like? A picture, first of all, what's the dimension of a picture? It's number of pixels across, you know, maybe 600 pixels by 300, or 600 pixels by 400.

So it's a... what do you call it? A tensor, right? $600 \times 400 \times 3$ RGB, right? Most pictures are colored.

Take those, just put them out into one big, long vector, and take the $\ell_2$ distance between it.

**Question:** Can I take the $\ell_2$ distance between two matrices or two tensors?

**Answer:** Yeah, what is it? Let's just think about this sort of distance. I've got two tensors.

Right? I've got a tensor $X$ and I've got a tensor $Y$, both $P \times Q \times R$. I take the difference between them:

$$X - Y$$

I've now got something that's $P \times Q \times R$. I take all those, sum over $P$, sum over $Q$, sum over $R$ of the squares, added up:

$$\sum_{i=1}^{P} \sum_{j=1}^{Q} \sum_{k=1}^{R} (X_{i,j,k} - Y_{i,j,k})^2$$

Take the square root, I got the $\ell_2$ distance:

$$\ell_2(X, Y) = \sqrt{\sum_{i=1}^{P} \sum_{j=1}^{Q} \sum_{k=1}^{R} (X_{i,j,k} - Y_{i,j,k})^2}$$

So I got... it's no problem computing distance in matrix or tensor space. I take the norm of them—$\ell_2$ norm.

This makes sense? Hopefully, yeah. Okay. If it's unclear, grab me afterwards.

### Data Augmentation as Regularization

So **data augmentation** is another form of regularization. It avoids overfitting to the particular training data you have. It creates a whole bunch more data, which then averages over more of it.

**Student:** Yeah. What do you say?

**Professor:** Correct. Yes, you as an engineer need to look at your problem. And I'm saying for images, for sure, shifting it a couple pixels up or down for most images does not change what's in it. Whereas flipping it upside down would change the meaning of the image.

Flipping it left to right, for most images that I deal with, doesn't change the meaning of it.

Make sense? But there could well be a different world, like looking at text. If your image has got text in it, shifting a little bit doesn't matter. Flipping it this way—the text is backwards now, that's not the same at all.

So cats are pretty invariant to being flipped this way. Text is absolutely not invariant. If you're looking at RNA, it's got different invariances.

Right? So depending upon what your data looks like, it's up to you to think about it.

### Other Data Augmentation Techniques

Some things—another thing which they didn't do, but they could have done—is **zoomed in a bit** on the image, **zoomed out**. That would be another invariance where cats tend to vary how close or far they are. It doesn't change their "catness."

That's one they didn't use. Why? Because they've got enough data by just doing the translations. But note that this is an inductive bias, coming from your knowledge of the problem, of the world.

And some things, if you shift them a day earlier or a day later, it doesn't make any difference. In my world today, a day earlier, day later, it makes a huge difference, because you go from a weekday to a weekend day. So a one-day shift—things are not invariant to adding one to the day of the week.

Make sense? Saturdays do not look like Fridays in my data, so things are not invariant to a time shift. Again, in my daily world, my world mostly—my data comes daily.

If it were seconds, if I had data every second, which I mostly don't, yes, shifted forward or back a second, I probably don't care.

Right? So note that the invariance you build in is something that comes from **your knowledge of your world**.

### Final Projects

And that's why I want your final projects to involve finance, or biology, or physics, or something where you can talk about what the invariances are in your world and build them in.

And for most things, you'll be data-limited. And if you are compute-limited, then often we'll just say, well, pick a smaller shell of the data. Pretend you have infinite compute and be data-limited, because in the real world, most employers are going to say, sure, I'll give you more compute. More data? Oh, that's going to cost you.

Right? Especially if it's labeled data. The labels are very expensive.

So note that by shifting the cat image, they don't need to collect more labels.

**Students:** [Questions about application-specific invariances]

**Professor:** Yep. It depends on your application, yes! Yep. Yep. Yep.

So yes, the inductive bias is totally determined by the problem you care about. Right? That is sort of the most important thing to think about inductive bias—is you need to think about your problem and build in the inductive bias into the model that makes sense for it.

Do you think the world is linear? Do you think the world is translationally invariant? Do you think the colors matter?

Sometimes yeah, sometimes no. For cats? Of course they matter. Right? I got an orange cat and a black cat. They don't look the same.

---

## Gradient Descent and Optimization

Cool! We talked about gradient descent before. I'm not going to review mini-batch. I mentioned gradient clipping; I'll just say again that you should keep that in mind. We talked about momentum. We talked about learning rate. I think I'll mention one more learning rate version.

Okay, we talked about either doing a numerical approximation to the gradient, or you could do it analytically. Lots of PyTorch will automatically take the gradient without you worrying about it. There's always a momentum option.

There are lots of algorithms which I talked about last time, which I'm not going to cover again, for adjusting learning rate over time.

### Adagrad Optimizer

The one I sort of want to mention was **Adagrad**, which is a funny notion that says that sometimes it's useful to not just have a single learning rate for all weights, but for each individual weight to change the learning rate over time.

And... whoops, I've got weird pieces here. And the idea here is to say for each weight... So I've got $j$, the subscript of all my weights in my neural net. I'm going to have a global learning rate times the gradient $\frac{\partial \text{Error}}{\partial w_j}$, right? That's the gradient for that single weight.

Normalized by something which is the $\ell_2$ norm of all of the changes in that weight that I have had so far.

So in particular, let me write it out here, because I think it's maybe not in the slides. What I want to say is, I want to take my change in my $j$-th weight at time $t$ to be equal to a global learning rate...

$$\Delta w_j^{(t)} = \frac{\eta}{\sqrt{\sum_{\tau=0}^{t} (\Delta w_j^{(\tau)})^2 + \epsilon}} \cdot \frac{\partial \text{Error}}{\partial w_j^{(t)}}$$

Something which looks like the summation of $\tau$ goes from 0 up to—okay, let's make this capital $T$—up to capital $T$ of all the changes I have made, the deltas in the weights, this weight, how much have I changed this particular weight at each of the times.

Oh, and then I want to take the $\ell_2$ norm of that, which is I take it, I square it, I sum it up, I square-root it! Right? So this is the $\ell_2$ norm of a vector of length time.

Times the derivative, right? How much is the slope—the error with respect to this weight?

#### What is Adagrad Doing?

**So what is this doing?** This says, how much have I changed this weight so far?

Some weights get changed a lot. Some weights don't get changed very much. Sometimes it's because some features are almost always zero. And if you have a feature that's almost zero, it's multiplying the weight. If that feature's always zero, you don't tend to change the weight.

Whereas if you see some feature that you see a lot of times, so it's big and it's small, you get that feature a lot of times, a lot of variation, that weight's going to change more.

**And the more I've changed the weight so far, the less I need to change it now.**

Right? If this is the first time I've seen a non-zero example of that, I've got some weird thing, you know... Do you speak Tagalog?

And there's just not a lot of Tagalog speakers here. Okay, it's a rare event. Not so much in California where I'm from, and certainly not so much in the Philippines, but in many data sets, that's a rare feature, and you'll see, like, lots and lots of training points where you never have that, and there's no gradient because that feature was always zero.

Once you see someone who speaks Tagalog, you go, "Whoa, this feature popped up! Ah, very exciting!" Lost my chalk.

Now I want to say, okay, I want to take a fairly big change. Make sense?

#### Adding Epsilon

And usually, to be super mathy about that, I'm going to put an **epsilon** ($\epsilon$) here:

$$\Delta w_j^{(t)} = \frac{\eta}{\sqrt{\sum_{\tau=0}^{t} (\Delta w_j^{(\tau)})^2} + \epsilon} \cdot \frac{\partial \text{Error}}{\partial w_j^{(t)}}$$

Because if this has been zero so far, and I've never changed the weight in the past so far, I've got to divide by zero and make an infinite change, which turns out to not work very well. Computers hate to divide by zero.

And if you go back and look at the Adam optimizer we covered in the first half, you'll see it also has an epsilon down there as a sort of, yep, never divide by zero, just put epsilon.

So I think the intuition... Yes, question?

**Student:** This one does not actually give more emphasis to earlier or later?

**Professor:** This one does not actually give more emphasis to earlier or later. This is the vector of all the changes I've ever made since the beginning of my gradient descent. So this is the cumulative total—well, in squared space. Right? But it's the length of the vector which is all the changes I made in the last... in $T$ time steps.

So you could be cleverer and try and adapt this over time. And instead of going from 0 to big $T$, maybe you'd like to do this from $T_1$ or some time. That'd be clever—I like that idea. I've never seen it done, but I could totally imagine that that says, hey, have I changed this recently?

That'd be fun. Not called Adagrad anymore—it's an improved Adagrad++. You can patent it. So yeah, that would be a reasonable version that says, hey, there's usually a burn-in period. The early descent looks different than the much later one, so maybe you'd want to do something more clever.

But I think the one big takeaway is that typically you do some either global adjusting of the weight based on how much you've been changing it, or you can adjust for each of all weights—or for each individual weight if you want—you can adjust it based on how much you've been changing it.

Yeah?

#### When to Use Adagrad

**Student:** When would you want to do this?

**Professor:** Mostly for data which are very **sparse**. So if it's sparse, by which I mean most of the features have lots of zeros...

So if you're doing something like images, it's not sparse. Every pixel has an RGB value. There's no zeros. In images, this is stupid for images.

If you've got something where you're asking people "Do you speak which of these thousand languages?", most people don't speak most thousand languages. You've got an enormous number of zeros, and then everybody speaking English or Mandarin or Spanish.

And so in that case, every time you see a rare language, you should say, "Whoa! I need to really update that weight to the language." Whereas you see one more English speaker, you're like, "Yeah, right, I've already seen a million of them." Or one more Mandarin speaker—fine, I don't want to make too much change because I'm averaging over vast numbers.

So I think in some sense, you want to upweight changes on things that are super rare and downweight changes on things that are super frequent. And that makes sense in a world of sparsity.

Cool. I'm going to push forward. Yes, if this were a deep learning course, I'd spend a whole lecture on this.

### Choosing Optimizers

But I think there's a super important question, which is: There's a bunch of different optimizers you can choose with different learning rates, and mostly you'll use **Adam**, or **AdamW**, or some variation of... The optimizers are sort of like the Swish and GELU and ReLU activations.

I don't get too attached to which one is popular this year. But... And the big comparison—SGD, yeah, it doesn't use any of this; the default one for PyTorch, it works fine.

But there's an important conceptual concept, which was: I've said there's the model you use, the loss function you use (including the regularization), and the optimization.

**Does the optimizer, the gradient descent, affect what you converge to?**

For non-convex problems, it should affect—and in fact it does—right? If the world is convex...

**Question:** Full bonus points. If it's logistic regression, it's convex. Any sensible optimizer gives you the same solution. There's only one solution. It's a convex problem.

So if it's convex... But now we're in non-convex world, and different optimization techniques will give you different results in ways that are somewhat predictable.

### Learning Rate and Convergence

So in general, if you look at your epochs, right, your mini-batches, how many iterations you do...

**If you have a really high learning rate**, things tend to spin out of control. Remember, if you take too big a step, you jump past the optimum and go out the other side. So if your learning rate's too big, you diverge.

**If your learning rate is really low**, you creep down really nicely, but it takes you forever, and you sometimes run out of money.

**If your learning rate is high**, what mostly happens is you will converge fairly quickly to a **suboptimal solution**.

So **a lower learning rate** tends most often to—you know, no guarantee; these things are stochastic—but in general, a lower learning rate will take you off into a **better solution**.

**Why would a greedy method, a fast method, find a worse solution?**

In some sense, it's... if you do a greedy search, the first thing you come to is the first thing you come to.

**What is dropout doing? Making it more greedy or less greedy?**

**Less greedy!** So it should come to a better or worse solution?

**Better solution!** Is it free?

**Oh God, you're doing more computation!**

Right? So in general, that sort of trade-off... And I think one thing that neural net people tend to—unless you're professional—tend to overlook is that you are doing **regularization by the very nature of your gradient descent**.

And in general, if you're going faster, then you're converging to a less good solution. And there's a whole bunch of very cool work, which... Oh, I don't have time to talk about either, which in general, if you do gradient descent nicely, it's more likely to end up in a **big flat minimum** than in a super steep one.

### Flat vs. Sharp Minima

And in general, if you have something that has a loss function that looks like this, right?

```
        A      B
        |      |
   \    v      v    /
    \   |      |   /
     \  |      |  /
      \ |      | /
       \|      |/
        +------+
```

So this is $X$, and this is error. Would you rather be at point $A$ or at point $B$?

**Answer:** $B$! This is a more stable solution. It means that if your weights are off a little bit one way or the other, it's not going to make a huge difference. This one [point $A$]—equally good solution in terms of the training error loss.

But hey, we don't care about the training error loss; we always care about the **test error loss**. And this one [point $A$], the odds are you get a test error and test data set, and you're actually here or here [slightly off from the minimum], and you've got a much higher test loss. This one [point $B$], you're not getting the exact right weights, but if your weights are off a little bit, you get pretty much the same solution.

### Gradient Descent as Regularization

And there's a beautiful magic about gradient descent, which is, especially if you don't do it too fast, the gradient descent tends to find a **flatter minima**. And that tends to give you more stable results.

And an awful lot of the very, very big neural nets—think of the Gemini and Claude level big nets—the major regularization they use is **gradient descent**. They're worrying less and less about... they don't do gradient clipping, they don't do early stopping, but that's because they can't afford it. They're mostly regularizing effectively by gradient descent.

And in the case of linear worlds, I think I deleted this slide because it was too technical, but in the linear world, where you actually know exactly what you're doing, you can actually prove theorems about gradient descent going to a **minimum norm solution** in some cases. So there are some limiting cases where you can prove theorems.

But certainly, empirically, gradient descent is just a nice... I mean, imagine I could run a randomized search algorithm, a genetic algorithm or something, where I exhaustively try and search through all possible weights in the neural net and find the one with the minimum error.

Imagine your hypothetical quantum computer does all possible combinations of weights and finds the best one.

**I'm Sam Altman. Do I want to use that?**

It's a good interview question for a job. I can find magically, with lots of compute, the best solution, right? Minimize the training error.

I've got a model with a trillion parameters. I've got a trillion tokens of training data. I want to find the best model. Do I want the lowest error model?

**No, it would be a disaster!**

The lowest error model is something that's right down here [pointing to a sharp minimum]. I want this solution over here [pointing to a flat minimum]. It's not as low a training error, but it's at a much flatter area, right? Remember, this is weight, and this is error.

I want to be somewhere where if my weights are off by a little tiny bit, I still have roughly the same prediction. I want a stable solution that's not going to matter. If I find the lowest solution in a neural net, it's mostly a disaster.

Fortunately, I can't find it, and fortunately, I always use gradient descent. But if I could find the best solution, that would be terrible.

Right? So note that **gradient descent is doing regularization**. And **the learning rate is a regularization hyperparameter**.

High learning rate, low learning rate, different learning rates—I get different actual outcomes after a fixed amount of computation.

Make sense? Super important.

I don't know—it took me a long time to get my head around the fact that the learning rate was a regularization hyperparameter. Partly because the research in the last 5 years has been very supportive of it.

Yeah?

**Student:** [Question about convex functions]

**Professor:** It's only true for a non-convex function. For logistic regression, there are a couple of different algorithms people use to do gradient descent in logistic regression. Do they have different results?

**Answer:** Nope. Except...

I have seen once... We were training on a medical data set, and I trained the logistic regression and the loss went like this [diverging]. So it is possible to find commercially produced code for logistic regression that will, in fact, diverge to infinity.

I have experienced it. But by and large, most logistic regression code gradient descent will, in fact... In fact, it was built into scikit-learn. Most logistic regression code will, in fact, not overflow... will, in fact, converge. And if it converges, it will converge always to the one local optimum, which is the global optimum.

---

## Scale Invariance and Feature Standardization

Cool! Another thought question: **Is deep learning scale invariant?**

Should I... Does it matter if I normalize or standardize my features before putting them in? So sometimes we said it's often, for some models, you want to take the features, subtract off the mean, divide by the standard deviation, make them all of size 1.

Is that a good thing to do for neural nets? Is it unnecessary?

I got a no. Just tell me more.

**Student:** Yeah. Depends on the problem?

**Professor:** How does it depend on the problem?

Does neural nets care... Is neural nets a metric-based method? Is neural nets more like k-nearest neighbors, or is it more like linear regression?

**Answer:** It's a regression model. It's more like linear regression.

If I multiply... If I change from inches to centimeters, can the neural net compensate for that?

**Answer:** Sure! Make the weights off by a factor of 2.54 or whatever. I can't help but use inches in America. Stupid, but... Wasn't my choice.

So **in theory, a neural net should be scale invariant** because, like linear regression, if the features become 10 times as big, the weights can be 10 times as small.

Does that make sense? Yep.

### When Regularization Makes It Non-Scale Invariant

And once you put **regularization**, it becomes **non-scale invariant**. And among other things, you're doing search, so in reality, they are, in fact, de facto not scale invariant.

Right? So if you think about what some levels of equal loss look like... So I've got something that's sitting in a $\theta_1$, $\theta_2$ dimension. So picture two features, $X_1$ and $X_2$.

And if there are lines of equal error, people have got me the pictures—these are farther out, bigger circles of higher error. If you rescale the features and standardize them, you'll rescale the weights to make the weights equally big.

If you're regularizing with a Ridge penalty, then in fact you'll have different penalties depending upon whether you scaled or not. So if you... Once you put regularization in, you've made it not scale invariant.

And if you're trying to do gradient descent, it's much easier to do gradient descent on something that's sort of well-balanced, that's got the same distances on all dimensions. And remember, we're sitting in, for a small network, 60 million, or for a big network, a trillion dimensions. So you've got a lot of dimensions here.

### When to Standardize

And so in general, it's super common to **standardize features for a neural net** because it makes them more stable on the convergence, and it means the weights apply equally.

And again, I should comment: when you would and when you wouldn't.

**If I have images—I'm recognizing cats or breast cancer—should I standardize the pixels?**

Yes? Take every pixel, divide by the average... subtract off the average, divide by the standard deviation.

**Answer:** No!

The pixels have a **natural size of brightness**. If you take brain images, there's a whole bunch of brain and a whole bunch of black out here that's not brain. Right? You've got a square image, or cubic for brain, right? It's a voxel, but it's still $X \times Y \times Z$.

So for images in general, standardizing's a terrible idea. The things that are zero are zero. Right? The black part outside the brain is outside the brain. Don't make it equally big.

If you've got things where some things are in kilograms and some things are in inches, then it's probably good to standardize.

Make sense? That they don't have natural sizes? Yep.

**Student:** Isn't there no difference if you standardize the pixels or not?

**Professor:** You might hope that would be the case. Right? And if you were doing something that was a true linear regression... But is it the case?

Imagine I've got some pixels that are always 0 because they're just black—they're outside of the brain—and some that have a variety of intensities. If I standardize them, what happens? The ones that were black before, that were all zero, become just as big as the ones that were actually within the brain.

You with me? And when I do regularization, I'm much less likely to zero them out with an $\ell_1$. And I'm much more likely to just shrink them a bit, but they're contributing **noise**.

Right? So what you've done by standardizing the pixels is you've made the noise bigger. The place outside my brain really should not be contributing to my brain cancer diagnosis. If it does, we get a problem.

And it happens sometimes—there's a watermark in the image or something—but that's always a mistake.

So in general, I don't want to introduce more variance in the features that I think are noise. And features that, when small, really mean small—you want them to stay small.

### Temperature Example

Imagine I'm measuring temperatures across the U.S. Should I standardize the temperature in every city?

**Answer:** No, because Atlanta really is warmer than Philadelphia, and Boston is colder. I mean, Atlanta's warmer than Philadelphia; Boston's colder. I don't want to standardize them so each one is all centered at zero. The **absolute temperature matters**. So you're throwing away information if you're standardizing things that are actually on the same scale.

Make sense? Super important point to think about.

And again, your knowledge of engineering needs to come into: Are these things that are all measuring roughly the same thing, where the absolute size matters, like temperature or voxel intensity? Or are these things where there's no way to compare my weight in kilograms to my temperature in degrees centigrade?

Which one's bigger? 50 kilograms or 32 degrees? Okay, 32 degrees is bigger than... Right? But for pixels and temperature, it makes sense to keep the absolute scale.

### Summary on Standardization

Cool. So in general, if you rescale and standardize the features, gradient descent works better, and most of the deep learning methods have something which is an automatic first step, which they may tell you about or not, which will take all the features and standardize them for medical data—that's great. For images, that's probably a bad idea.

---

## Hyperparameter Selection

Cool! Lots of regularization parameters: dropout (it's typically yes or no, 0.5), early stopping (people always do), learning rate (you're going to pick something). Lots of hyperparameters.

**How do you choose them?** There's, like, 15 hyperparameters to choose for a neural net.

### Practical Approach

You're hired at a company, and they're trying to diagnose failures of turbine blades by taking the audio signal of the sound of the blade whirring. Actually, these really exist. If you're actually selling a big turbine blade, it'll come bundled with an automatic detection that's trying to detect: Is it going to fail, based on the sound?

Things that are spinning, as they're starting to fail, start to wear unevenly and go [makes sound]...

Okay, you want to... How do you pick the right architecture? This is an **engineering question**, not a math question.

**Answer:** You look at someone who did it before, yes!

The way to do it is to try and find someone who's published something on that, and start with something that worked before. And one of the big advantages you have if you're at OpenAI or Anthropic is a lot of people around—or Google even more, or Alphabet—who have done this before. Start with something that looks like an architecture that someone has used before, and then tweak that and do a search in the parameter space starting from someone else's.

Yes. Yes. Yes. And as you do your projects, you should be looking to see who's done something similar before.

---

## Image Recognition Performance

Cool! Let's just do a few... How well do these things work, even a decade ago? Trying to recognize: What is this? Is it a mite or a container ship, or a scooter? It's mostly good.

Lots of the errors that you get these days are errors where it says this is an "agaric," but in fact the label was "mushroom." Or it says this thing is a Dalmatian—no wait, is it a cherry? No, it's... Which one? This one is a Dalmatian, but the label is cherry.

So often what's happening now is the AIs are slightly better than the people you're paying to label the stuff. And most of the AI errors you find are effectively errors in the label.

Make sense? Or ambiguities. Is that agaric or mushroom? Does anybody... mycologist here?

Both in some sense, right? It's... Do you want the more general category of a mushroom, which... I wouldn't know one mushroom from another. I don't know if that's poisonous or good. But apparently it's a particular [species]... So these things are superhuman labelers—slightly superhuman—and therefore the errors are often made by people.

### Funny Errors

Mostly they're really good. Sometimes you get... I love this error. This is labeled as a "cellular telephone."

Why would a beautifully trained neural net call that—it's called a boombox in my era when people had them—why would they call that a cell phone?

**Answer:** It hasn't seen boomboxes, and what is it? It's a device that's being held up to someone's ear, just like a very big telephone.

If you were—okay, maybe you are of a generation where you didn't see people... But if you're of my generation where people actually saw people walking around with these very big... There's no cell reception on that device.

So note that **out of sample**, these things still often fail. And out of sample, they do something that's reasonable, but, hey, who knows?

---

## Embeddings and Feature Learning

Cool. Often, of course, you do searches, so rather than doing a whole image, you'll have a window and search across it. In a deep learning course, I would take a full lecture talking about how to parallelize search over lots of windows within a big window, but we won't today.

What I do want to do is emphasize: Looking forward to next week, we're going to talk a lot about **embedding**.

### What is an Embedding?

**Embedding is just a vector that represents anything.** You can map a token (a word) to an embedding. You can map an image to an embedding. And a popular way to map either of them to an embedding is to train a neural net.

In a supervised fashion—so this is the AlexNet from before, which had a bunch of images like I just showed you and a bunch of labels like I showed you, like Dalmatian or Mushroom or Planetarium. You train up that supervised model, and then what you do is you **throw away part of the neural net** and you keep something very toward the end—the output of maybe the layer before the last, or right before here, where they're getting something that's, in this case, a **4000-dimensional vector**.

So I've learned something which takes any image, maps it through, say, 6 layers, and produces a 4,096-dimensional embedding. You with me? Which are the activations of those neurons.

### Why Embeddings Are Useful

**Why is that vastly nicer than having raw images? Why do I care? Why do I want embeddings?**

I just have to embed tokens, so I'm just embedding images. They're already a vector, right, or a tensor. Why do I love those embeddings?

**Answer:** It's got lots of high-level information.

We'll see it's not so much edges and colors and textures, actually, at that level. Those tend to be in the very early embeddings. We'll see the early ones have textures and colors. But what do I want with an embedding?

One use is **similarity**. I can take: How similar are two images in pixel space, or how similar are they in embedding space?

If one's got an elephant here and the other's got an elephant over there, pixel space? Really different. Embedding space? It's going to be heavily in the "elephant" embedding space.

Right? So **similarities work beautifully**, and $\ell_2$ similarities work—distances work beautifully—in embedding space.

### Embeddings as Features

And they can be used as **features**. So an embedding is a mapping from an original feature space $X$ to some sort of $Z$, right? One that's been learned here for labeling. And once you have that embedding $Z$, you can use it either for measuring similarities or distances, or for putting as features into a regression, or a neural net, or whatever.

Now, this is super popular right now.

### Image Retrieval Example

You can do **image retrieval**. I can take in an image of a t-shirt, map it to embedding space, and then find the other images in my dataset that are closest in embedding space. And you can see a t-shirt over here that's really far in pixel space from the original one, but it's very close in embedding space.

Similarly, I can take a flower here, and I get another flower over here that is similar, but it's similar in embedding space. It's not very similar in pixel space.

Right? And you're really good at this. But if you look closely, you realize that the purple flower here and the purple flower there, if you actually put them on top of each other, mostly the purple's on top of green, and the green's on top of purple. They're not close in pixel space.

So **embedding space is a much more sensible space to do almost everything in**—for sure for similarities, and much nicer for features.

### Semi-Supervised Learning

And we can now do **semi-supervised learning**, where we take an enormous dataset, use it to learn something that maps images to labels, and then take the embeddings and go to an entirely different domain—well, not entirely different, but a somewhat different domain—take those embeddings and now train up a separate model.

So these things are widely used in image search. You can go to Google today and type in something where you want to search for "Find me other things close to this statue," and it can find you other gorillas that may be very different in pixel space but are very close in embedding space. Or you search for images that look similar to the sea. It finds things that are close in embedding space. Each ocean looks very different in pixel space, but the oceans all map close to each other in embedding space.

Oh, yeah. Cool. I'm going to skip the transfer learning details and do one last piece here.

---

## Case Study: Transfer Learning Example

Sort of a fun study from a while ago. **Michal Kosinski** at Stanford—very contentious study—went to a dating site, downloaded 40,000 profiles: 20,000 men looking for men, 20,000 men looking for women.

Let's call the guys who are looking for men "gay" and the guys looking for women "straight." With me? So I've got 40,000 labeled things based on who you're looking for in the dating site.

By the way, it's not quite clear if he got people's permission, so there are all privacy issues, but okay.

Then you can say: **How can I predict the label—looking for a man or looking for a woman—from the image?**

Make sense? It's a mapping from pixels to who you're looking for—gay or straight?

There's a piece there which I won't read for you, but I'm going to walk you through the algorithm because it's a **classic transfer learning** piece.

### The Algorithm

1. **Download a bunch of images and labels.** It's a small set, right? About 40,000.

2. **Try and filter them.** Some images—it's me with my dog, or just my dog. Throw those away so you only have images that are just a single face in them. So you filter the data to make it cleaner. And there's plenty of pre-trained code that does this. Okay, now you're back down to about 35,000 faces.

3. **Restrict it to only look at white people.**

**Why is he only looking at white people?** Not because he's white (which he is). Why is that an easier problem?

**Answer:** The narrower the class of people, of objects you're trying to classify, the cleaner it is. He wanted data that was easy. And if there are lots of people of different skin colors and different beard types and different hair types, it would be a harder learning problem. He'd need more data.

So he's not being racist; he's just trying to make the problem really easy. And it's easier if you have a more homogeneous population, so they're more of the same.

4. **Then what he does is he goes to the web and grabs [a pre-trained model].** Someone else has trained a convolutional neural net on millions of faces, just labeling them: Are they male or female?

Right? So it's a network that's trained on a different dataset for a different problem—much bigger dataset. You with me? This is a **classic transfer learning**. It's easy if you're, say, Microsoft, to get... I think it was... a million faces labeled male or female. Click the box, train the model up. You now have a CNN that does that.

5. **Now, if you recall, you say: Take that pre-trained model and use that to give you an embedding that maps an image of a face to a 4000-dimensional vector.** You with me? One that is a set of activations that are optimal for predicting: Are you male or female when you check the box?

So far, so good.

6. **Now, take that embedding.** And that original thing he used was trained on 2.6 million faces. Takes the embedding from that.

7. **Then he takes those embeddings, dimensionality-reduces them** in a way we're going to cover next week, using **singular value decomposition (SVD)**. This looks a lot like PCA. So he does a dimensionality reduction, which we'll cover next week, to project them down to a smaller dimension.

8. **And then he takes that smaller dimension embedding and just pops it into a logistic regression.**

### Results

And in a group that's 50-50 male-female—sorry, straight-gay—with one picture, you can get about **60% accuracy**. With 5 images, maybe **80% accurate**.

It's not super accurate, but it's **way, way ahead of chance**.

### The Process

So note:
- **A, the process:** Take someone else's pre-trained model, use the embedding, dimensionality-reduce with PCA/SVD to a smaller dimension, plug into a simple model.

- **Why is he using a simple model, like logistic regression?**

**Answer:** Because this dataset is small. He's only got 35,000 labeled examples. **Data is the limiting part**, not because he's worried about the compute. He's at Stanford; he's got lots of compute. But he only has 35,000 labeled examples, and therefore he's taking an embedding and dimensionality-reducing it.

I guess the method is clear?

### Interpretability

Other comment: You look for interpretability. It turns out, what is it really keying off of? Is it finding the intrinsic nature of "gayness"?

Turns out that:
- **A,** the gay men tend to wear glasses more than straight guys on dating sites. Apparently, gay guys think on average that glasses make you look more attractive. I'm straight; I didn't know that.

- And the gay guys tend, on average, to have somewhat different **facial hair**. I've got straight hair, or on a gay head, maybe, and... Sorry, I stole my... a friend of mine, I copied his hairstyle.

- And it's the case that the gay guys have slightly **narrower, thinner faces** than the straight guys. It's a culture that puts a little more emphasis on being fit and trim, and so they're a little bit less likely to have the higher body mass index.

So there's interpretation in his model, and it's hard to tell from what it was doing. It may be fairly trivial features, but he didn't have to do any feature engineering. He didn't say, "Hey, is that a gay mustache or not? Is that a gay beard or not?" It's not [explicitly engineered].

And it's a **beautiful example of transfer learning**.

---

## Conclusion

We'll come back later and talk about visualization and distillation, which you're not responsible for... Someone said, for the slides I skipped, do I need to know them? No, nope. Either I'll come back and do them, or I won't. But so don't worry about the slides I didn't do; I'll delete them and post the new ones.

But this is the stuff you've mostly seen, and I will see you all next week.

**Please look at PCA and SVD if you don't know eigenvectors.**

See you next week!

---

*[End of Lecture]*

---

# CIS 5200 Machine Learning - Lecture 14: Principal Component Analysis (PCA)

## Introduction and Course Context

The median is... not a failing grade, but maybe not in the midterm is... great! Right?

Amen.

So, try to disentangle the growth in a world where 90% and above is required to pass the course, the application.

Cool, um... yeah, I think I'm gonna say a little about that. I have posted a little bit on that... but I think I want to really move the talk today. We're switching to the second of our three topics.

### Course Structure Overview

We did **supervised learning**, we're now into **unsupervised learning**, then we'll do **reinforcement learning**.

And to sort of give a sneak preview of the next couple weeks... we're going to talk this week a lot about **spectral methods**. There's a fancy jargon term for eigenvectors, eigenvalues, singular vectors, singular values. Most of them are sitting in the $L_2$ loss world. If you're, like, nice and beautiful, so we're gonna redo things we did with linear regression before from an $L_2$ loss and from the likelihood. We're gonna do unsupervised learning from an $L_2$ loss.

## Autoencoders and Reconstruction Methods

A version of this which will also focus on as a rebranding of it, is these are often called **autoencoders** or **reconstruction methods**.

There's a whole bunch of methods that take some sort of input. And they map it to some... oh, in with $X$? Whoa, let's do it!

That's something book nuts? We're going to have the space $V$, it will auto... we will encode it. $Z$ will do a transformation, mostly linear now, but we will do neural nets later in the course, which are just nonlinear versions.

### The Autoencoder Framework

And then from $Z$, we're gonna go back and reconstruct $\hat{X}$.

So instead of going from $X$ to $Y$, we're going to go from $X$ to $Z$ back to $X$.

$$X \rightarrow Z \rightarrow \hat{X}$$

So we're going to spend all week doing that, and then we'll come back and do it again later. And these have names like **PCA**, or **ICA**, or fancier versions are called **autoencoders**.

There's denoising on the $X$, you could add noise to it, which is almost like a regularization, and you can reconstruct $X$. And very popular for a while in language. We take $X$ as a bunch of words, or envision taking a bunch of pixels, take 10% of the words and remove them. Take 10% of the pixels and remove them.

Now you've got a masked $X$, use that, map it to some encoded $Z$, use that to reconstruct the full $X$.

### Why Autoencoders Are Popular

Make sense? Why is this massively popular in machine learning?

Here's something that's entirely pointless on the surface. We're taking something, we're hiding part of it, we're compressing it (for PCA, or sometimes expanding it for other methods), and then we're reconstructing.

Why is that such a good idea?

So, like, if we can do reconstructions, we have $Z$... right? Or is it in American? $Z$, and then we do stuff with it.

**Student response**: Yeah, so information. Why do we prefer many, many times to have an encoding of $X$ rather than the original $X$.

**Answer**: One reason is because $X$ is large... as you might want, it's smaller for just your computational cost. But in general, we're mostly not limited by computational costs. Usually we're limited by the cost of labels. The first half of the course, we mapped $X$ to $Y$, and I said, without proof, that's mostly the expensive part is getting a bunch of the $Y$s to label data.

If you want to know what's going on in this image, you can try and use a caption, but it's mostly crap. If you want a good label, just pay a bunch of people to label it. And that's why, like, COCO got created by, what, a \$6 billion investment by Meta, because there are a whole bunch of people - some of them expensive people here in America, some of them cheaper people in Kenya or Nigeria, but a whole bunch of people who labeled data.

### Reasons for Using Encodings

So, mostly not just computational. It does mostly reduce the dimensionality, but why is this good?

**Student**: Yeah, try to remove correlation between variables.

**Answer**: I might want to try to remove correlation between variables. In general, my $X$s are highly redundant, I might want to get some sort of encoding that is decorrelated - the fancy word in neural nets is that it's called **disentangled**.

So I might want something that says, hey, I've got something with a bunch of pixels, but I'd like the pixels to actually represent things like the Golden Gate Bridge and my hairline, or my ears. I like to disentangle those, I like to have some representation of these, where the $Z$s are in some sense, deeply meaningful.

Today, we'll do the simplest version of decoupling, which is making them **orthogonal**. But the neural net people do all sorts of ways to drive what they call **sparsity**, right? Which is another form of decoupling.

So, yeah, one version that pulls stuff out. But I think partly we want dimension reduction. You'd like these things in an ideal world, though they often aren't, to be meaningful. You'd like to capture the essence. 

### Regularization and Noise Reduction

The other piece is they often reduce noise. So this is a form of regularization. If we start with something that's a high-dimensional thing, and we project it to a smaller dimension, there are fewer degrees of freedom to fit in the future. So if we go down from some $N$ by $P$, if we go back to some $N$ by $M$ piece here, we've taken something that was $P$ dimensions, and made it $M$ dimensions. Now, if you ever learn something on $Z$, we only need to have fewer degrees of freedom.

And often, if we're doing a non-linear encoding, now we get something that's actually maybe capturing more of what we care about.

### Dimensionality Discussion

**Student question**: $N$ by $N$, like I mentioned earlier. $\hat{X}$ is going to be $N$ by $P$.

**Answer**: But it's not, in some sense, full rank. So we are willing to take all $N$ observations, we're going to compress them from $P$ dimensions - today, what we call compression. Going to PCA, we're compressed from $P$ dimensions down to $M$. We'll use the $M$ to go back and reconstruct an estimate of the $\hat{X}$. It'll be an imperfect estimate.

And we look at the difference:

$$X - \hat{X}$$

By default, we want to minimize the reconstruction error. Well, it's worth it if we can make it orthogonal. I'm going to cover all of them in great detail. We're gonna get fairly mathy, because this is one case where the eigenvectors and eigenvalues are beautiful. And the math is fairly clean, and there are just nice intuitions, unlike the combinatory world of neural nets.

## Matrix Norms

And we're gonna have to talk a little bit about what sort of a norm... well, it's gonna be something that looks like the equivalent of an $L_2$ norm for a matrix.

Right? So far, we had norms on vectors, but here we're going to say this is a matrix, right? This is $N$ by $P$, this is an $N$ by $P$. We're going to take something that is the $L_2$ equivalent, which will turn out, if you're fancy, to be called the Frobenius norm, but we'll get there. That lets us say how close are we to reconstruction.

### Goal: Reconstruction vs. Embedding

**Student question**: This is our goal - to reduce the dimensions and get a $Z$, or is it to reconstruct the $X$s? Is our goal to reconstruct the $X$s?

**Answer**: No. Our goal is to find a good $Z$. We want to find a good embedding, where we'll have a quantitative measure - hey, does the machine learning have a mathematical test for good embedding? We want to find a good embedding. In fact, an optimal embedding in terms of reconstructing the $\hat{X}$.

But I think the key point is that a good embedding is one that allows you to reconstruct the original $X$. If I start with something with 2,000 dimensions, and it's a pretty image (2,000 dimensions), I can project them down to 10 dimensions. If it's good, I can get a good approximation of reconstructing the original 2000. If it's a crappy embedding, I can't recover the information. 

Our constraint is we want something that's smaller, that lets me reconstruct, it captures the most information, in some sense, well, in $L_2$ sense, the most information that allows us to do it.

Then we can either take these $Z$s and see how similar are they in some sense, or we can take these $Z$s and mostly use them as features. Instead of going from $X$ to $Y$, we'll go from:

$$X \rightarrow Z \rightarrow Y$$

That, as supervised learning, we'll build a standard supervised learning model. And then we can go back to $X$ when we... and then let me go back to $\hat{X}$.

## Next Week's Topics

Cool. Um, then we'll go next week, and we'll talk about a bunch of more Bayesian models, so we're all in the $L_2$ space this week. Next week, people go back and look at the Gaussian space.

We will show that K-means has both an $L_2$ loss as clustering and a Gaussian interpretation. There's, like, regression going, you will find a new optimization method called **EM** (Expectation-Maximization), which is perhaps the most widely cited paper in all of statistics. And eventually, we'll get to more fancy Bayesian models.

## The Reconstruction Problem Framework

Cool, that's excellent. So... this week, I want to focus on the reconstruction problem. And the idea is, again, we want to learn some mapping from $X$ to $Z$.

And bizarrely, there was a $\theta$ in this thing yesterday when I typed it, this morning I looked at it. So, some function that has some sort of parameters, right? And so there's your linear model.

And then we think of this as... we think this is $P$. I'm mapping from $X$ to $Z$, and it's something that I wish would be the inverse that goes from $Z$ to $X$. But that's not really invertible. So this is going to be something that looks like a **pseudo-inverse**.

Make sense? If I've written $\phi^{-1}$? But realize that if you start in $P$ dimensions and go down to $M$, you can't truly invert it.

Make sense? We're losing information, we're gonna lose as little information as possible in the $L_2$ sense, and then we'll do as good a job as we can at reconstructing it.

### Objective Function

And what do I want? I want to minimize the distance between the original $X$ and the reconstructed $\hat{X}$:

$$\min ||X - \hat{X}||$$

Oh, it's on the screen. Such a bad day. It's really just $X$, I can say $\hat{X}$, but, on the screen here, it's right. It's $X$... it's a matrix minus $\hat{X}$, a matrix of the same dimension. I take the difference between them. I want to make them as close as possible, but keeping, if you will, some sparsity on the $Z$ to get to the embedding.

**Student question**: I don't believe... if we want $\hat{X}$ as close as possible, why don't we just use $X$?

**Answer**: Great point. What we're gonna do is... our goal is to reduce from $P$ dimensions down to $M$. So, $\hat{X}$ is going to be reconstructed back from $Z$, right? This $\hat{X}$ is a function of $Z$.

Make sense? We take the $X$, as often called a bottleneck. We have an $X$, from big $X$, we map it down, right? That's maybe dimension $P$. We map it down to a much smaller $Z$, which I've been calling $M$ dimensions. And from the $M$ dimensions, we expand it back up to $\hat{X}$.

### Information Bottleneck

So this is often called an **information bottleneck**. We have reduced the original space to the small one, and we want this because this is going to capture, hopefully, all the signal without the noise in a way we'll be able to define precisely, so that this $Z$ is in fact capturing what we care about, for example.

**Student**: Yeah, so the goal with this problem is to find a $Z$ that is...

**Answer**: Well, the goal is not just to find $Z$, but remember, there's a mapping. Find some mapping that permits the $Z$. Yeah. And in such a way that this $Z$ does a good job of capturing the signal without the noise.

**Student**: And then after doing this, then you just take this, uh, this mapping you found, and then use it as, like, a...

**Answer**: Exactly. Once I found this, I get a new $X$ in my test set, I can map a new $X$ to $Z$ because of this mapping. In our case, today it'll be matrix multiplication or projection. You find this new mapping, it says, given any new $X$, map it through this embedding.

## Semi-Supervised Learning Framework

Or this... this is a huge concept actually. It's a two-step process. This is **semi-supervised learning**. We start with a large amount of $X$s with no labels. We learn to compress the $X$ down to $Z$, or in general neural nets, we could learn to just embed it.

So we go from the original $X$, think $X$ in pixel space, or in word spaces. On Wednesday, we'll talk about tokens, and compressing tokens into vectors, embedding them. So we'll take the original $X$ in the feature space, we'll map it to a $Z$. 

Once we learn that mapping for the large, unlabeled data, right? I've got crap loads of images. I have all of YouTube. It's cheap. No labels. I learn the embedding or the reduced dimension mapping. 

Now I go with a small number of labels, and I want to label which things are safe for work or not, which ones am I willing to show up in the search or not show up, right? So that's given some sort of embedding here that hopefully captures an approximation to $X$ that does a good job, and we use that in the supervised part, right?

$$X \rightarrow Z \text{ (unsupervised)} \rightarrow Y \text{ (supervised)}$$

### Clarifying the Process

**Student**: Still a little bit confused, you said, please start with a big dataset with no labels.

**Answer**: Right, and we try to find a, uh, embedding method that maps to, you know, something with smaller dimensions, that part is fine. Yeah, if I learn the $\theta$ to map it. And then, are we talking about, like, a different problem? Like, once we have that mapping, we...

Once I have that mapping, now, what I do is I go to a different data set that has labels. And then we just use the embedding as a feature tool. And now you have everything in $Z$ space.

**Student**: Okay, but why do you call it...

**Answer**: Well, I like the name!

**Student**: Why don't I call it semi-supervised?

**Answer**: The first map is unsupervised. We're trying to do the embedding calculation. So the first part, this part is purely unsupervised. I learn nothing from $X$ to $Z$ - unsupervised, no labels.

Then I take that and I'll transfer with it. And I take that, I put it in a supervised learning problem. So the $Z$ to $Y$ part is supervised, the $X$ to $Z$ part is unsupervised.

And an awful lot of modern machine learning relies on those two pieces. You take a lot of unlabeled data, and you learn a good representation, a good mapping. And given that good mapping, you then train something supervised.

### Example: Predicting Sexual Orientation

Last time at the end of class, I talked about predicting if people are gay or straight. And what I said was, hey, take a large set of images of faces, and you can learn some embedding from those. You could even do that purely unsupervised.

Once you've learned a mapping and embedding from a face to some sort of embedding, now take that embedding, well, then they used a PCA to make it smaller, and then use a supervised label. If you've got a small number of labels, 40,000, roughly 35,000 labels, right? But that's 35,000 labels of faces is tiny compared to the millions of faces you usually have out there.

Great, good, okay.

## Matrix Norms: The Frobenius Norm

Um... okay, got to talk a little bit about measuring the size of a matrix. And we're going to use today what I would call $L_2$. But $L_2$ is really technically only for vectors, and so if you think about the equivalent of an $L_2$ norm for a matrix, it is called the **Frobenius norm**. A term which unfortunately you should know.

### Computing the Frobenius Norm

And it has several different ways of computing. One makes a lot of $L_2$ sense. Take the sum of all of the elements of the matrix, square them up, and add them up. That's why we call it $L_2$, right? If it were a vector, take the sum of the squares. It's a vector, right? It's a matrix.

$$||A||_F = \sqrt{\sum_{i,j} a_{ij}^2}$$

It turns out for matrices, magically, this is the same as taking the trace of $A^T A$:

$$||A||_F^2 = \text{tr}(A^T A)$$

And more interestingly, this is equal to taking the sum of the squares of the singular values of $A$:

$$||A||_F^2 = \sum_{i} \sigma_i^2$$

Or if $A$ were a symmetric matrix, it's the sum of the eigenvalues.

Uh, we'll be looking at these a lot. So I think one thing to think about is... when we talk about the sign of a scalar, it's positive or negative, we don't even talk about the sign of a vector, or big or small.

But in terms of a vector being positive or negative, or big or small, what we really care about are the eigenvalues. And we're going to work mostly with matrices that have only non-negative eigenvalues. That's the equivalent of a non-negative matrix. I use a fancy word for it: **positive semi-definite**.

And I'm saying it's not just the sign, how big the matrix is in any reasonable sense is a function of how big the eigenvalues are. Sort of cool, right?

### L1 Norms

Um... we're going to talk about $L_2$ norms and equivalent versions of $L_1$. As you would expect, $L_1$ is a little nastier to deal with, but still convex, and it tends to gain some sparsity. So, at the very, very end of the day, we'll come back to $L_1$ norms. For now, we'll stay mostly in the $L_2$ space.

Then you will see is that eigenvalues keep showing up as how big things are, which is super cool.

## Eigenvectors and Eigenvalues Review

Okay, so I'm going to cover eigenvectors in one slide, because you're supposed to all know it already.

Um, matrix $A$, $Az = \lambda z$ - $z$ is the eigenvector, $\lambda$ is the eigenvalue.

Eigendecomposition of a symmetric matrix, $A$ is:

$$A = VDV^T$$

$V$ forms an orthonormal basis. What does that mean? You take any two eigenvectors $z$. The eigenvectors, if you take the dot product, equals 0 if they're not the same eigenvalue, equals 1 if they are (ignore repeated eigenvalues for now).

And $D$ is the diagonal matrix containing the eigenvalues, and if $A$ looks like $X^T X$, they'll be all non-negative. When you report to me, generally put the biggest one first. Also, all this is old and boring!

### Expressing Vectors in the Eigenbasis

Um, I also want to remind you, and we'll come over and over today, is we're going to use these as a basis. You can take any matrix, any, sorry, any vector $X$, and write it as the summation:

$$X = \sum_{k} z_k v_k$$

Some coefficients, $z_k$, times some eigenvectors $v_k$. If $X$ is $K$-dimensional, this would be a complete basis.

And... yeah, I could write $X$ in terms of coefficients times basis vectors, etc.

Thanks, good. You give me a vector, I write in terms of coefficients times $V$, and $Z$s are easy to find.

### Finding the Coefficients

Why? If you dot $X$ into $V$, if you take the dot product of $X$ into the eigenvector $v_j$, what you will get is that:

$$X \cdot v_j = \sum_{k} z_k v_k \cdot v_j$$

But what is $v_k \cdot v_j$?

**Students**: Zero!

**Answer**: Zero? No, zero if and only if $k$ is not equal to $j$. It's delta $_{kj}$, it's 1 if $k = j$, it's zero if it's not.

And so, this will just project out exactly one of these. $j$ is equal to $k$, it's 1. If it's not equal to $k$, it's 0, so:

$$X \cdot v_j = z_j$$

Whoa, so... what was I trying to do? I wanted a mapping from $X$ to $Z$.

I could take any vector $X$, and transform it into the $Z$s by multiplying it by each of the $K$ different eigenvectors.

And I can take any $Z$, and unmap it back to $X$ by taking:

$$X = \sum z_k v_k$$

by taking the $Z$ times the eigenvectors, and reconstructing $X$.

### Reconstruction Quality

If I keep all $K$ of the original $P$, I can get a perfect reconstruction. If I make $K$ smaller, I get a less than perfect reconstruction. The smaller $K$ is, the more the approximation. But it's a smaller representation. The more I've thrown away.

And we're going to spend a lot of the time being very precise about how good the reconstruction is, what you keep and what you throw away.

## Geometric Interpretation

**Student question**: This is like... super basic and super fundamental, yeah. Uh, look here, I know that $X = \sum z_k v_k$, we use the eigenvectors, it means the eigenvectors? And then $Z$ is the coefficients.

**Answer**: So, what's the... $Z$ is... these are the... coefficients. You can think of this as a basis, then $Z$s are the coefficients.

Or, if you will, you can think of $Z$ as changing from the original $X$-dimensional space, which is one space. And then you're transforming the $X$s by dotting them into each of the $V$s, they give you a new representation $Z$.

### Two-Dimensional Example

And what we'll see... a bunch of times. If I have data points, $x_1, x_2$, two-dimensional $X$. What we're going to do is find the first eigenvector, which, if you center things around zero... well, let me get back through this way, so this is gonna be $v_1$ in two dimensions, then $v_2$ has to be orthogonal to it.

I can then ask, what is $z_1$? If I take some point here, there's a point $X$. What is $z_1$? $z_1$ is the projection of $X$ onto $v_1$.

So this is... this distance is $z_1$. Right? It's a scalar, you're projecting the vector for the single point. And I could ask what is $z_2$ for this $X$. And what's the other one? The other one is I'm going to project the point $X$ onto $v_2$, and this distance here, the distance is going to be the $z_2$.

Make sense? I've taken something I had an original basis $x_1, x_2$. The original orthogonal bases of the $X$s, I have now found a new basis $v_1, v_2$, which is also an orthogonal basis. 

I can take any point here, and I transform from being in $X$ space to being in the $Z$ space. And you can see the projection, right? It projects here, I project this $X$ onto the $v_1$ to get its $z_1$, I project this $X$ onto the $v_2$ to get it to $z_2$.

### Approximation with Fewer Components

So if I'm doing a full... if I'm completely approximating, if $K$ equals... here, there's no loss of information. $K$ was equal to $P$. My complete reconstruction, two-dimensional, I've lost nothing.

But, if I want to approximate it... whoops, there it is, I shrunk it. If it goes down, I can just keep the first one. So I could now say, hey, I want an approximation. Let's not worry about $v_2$. Let's take every point $X$ and represent it only by the projection with $K$ equals 1. We'll project $X$ onto $v_1$.

And so, $X$... here is this... this is its $z_1$. And the $\hat{X}$ is what? $\hat{X}$ is... this is the approximation, yes. My $X$ is the original vector here. We're getting hard to see with the axes.

And you can see that the $\hat{X}$, the projection on the first principal component, approximates $X$. Now, what we want to do is to make the approximation as good as possible, maybe in $L_2$ norm or Frobenius norm. We want to find the best $V$, and the best, hence the best $Z$, such that we get as good an approximation as possible for my $N$ different points, right?

You can look at the geometry. This is hopefully review, or we're gonna get a lot of knowledge, and hopefully review, um, super important. If it's not clear, there was a nice tutorial that can be recorded, there are videos.

## Computing Eigenvectors: Practical Considerations

Um, one minor deep point, in reality, when I work on big data sets in places like Google, I never compute eigenvectors. Why not?

**Answer**: Because big matrices are expensive, right? And remember, we've got some $X$ in my world, your world, right? Your data set $X$. If you want the eigenvectors, what matrix do we mostly care about?

Oops, that's not the right. Hey, computer! What's the matrix we're gonna do eigenvectors of?

**Students**: $X^T X$!

**Answer**: There's two versions of it, and they're almost the same. $X^T X$. I don't want to do this multiplication. $X$ is the number of people on Facebook, or the number, in my case, of websites in the world. Well, that's $N$. And then I've got the feature vector. I do not want to compute this!

### Using SVD Instead

So we're going to use **SVD** (Singular Value Decomposition), and get something that's equivalent to the eigenvectors of $X^T X$. So when I sit in math land, which is most of today, I'm going to talk about eigenvectors and $X^T X$. And, like, in engineering land, I'm going, forget that. All I got is $X$. I'm never going to multiply it out.

Make sense? Sure, happy, and... there's sort of two things in $X^T X$ that show up. Where have we seen this matrix before?

**Student**: Sorry? I mean, eigenvalues...

**Answer**: Well, we saw that if you wanted to estimate $W$, it was $(X^T X)^{-1} X^T Y$. So, often we'd like to invert that matrix. Never invert a matrix. You want to approximate the inverse of this. We'll get to there eventually. Approximate this to find the linear regression.

### The Covariance Matrix

And where else does $X^T X$ show up? It has a name, almost. It's the **covariance matrix**, but it's not quite the covariance matrix. You have to center the $X$s first.

Before you compute any eigenvectors. And I'm gonna go back and forth a bit, talking about $X^T X$, but in general, if you're doing PCA and all these methods, you're trying to find something about the covariance matrix, and you're going to center the $X$s first.

### What Happens Without Centering?

What happens if you don't center the $X$s first? Let's say I've got data points scattered over here. What does the first principal component look like?

It looks... like that. That's okay, but it's not so okay. What does it actually look like?

That. The first principal component is... uh-oh, no, it's not! This is still the first principal component if you don't center them.

You with me? If you got a bunch of points that sit like that, and if it... when you do the math which we'll work through in a second, if you don't center them, your first principal component is just the line from the origin to back here. Yep.

**Student**: What's the principal component?

**Answer**: Uh, that's something that I just covered. And we'll talk about more in depth there, but tonight, I just walked through the math of why you center things, we'll cover the eigenvector concept.

You take it the moment, for the moment, they could be eigenvectors. But, but, yeah, we'll see that. So let's... let's digress, we'll come back with it formally.

## Singular Value Decomposition (SVD)

Um, in theory, you should have seen this, but I'm gonna quickly cover SVD before we get to that.

So, if you have a matrix $X$, and you don't want to do $X^T X$ to find the eigenvectors of it, you can look at the SVD of $X$, you can factor $X$ in the form:

$$X = UDV^T$$

Where $U$ will turn out when we do this to be... the left singular vectors of $X$ will be orthonormal, $D$ will be a diagonal matrix, which looks like the eigenvalues, but in fact will be the singular values.

Think about it, $X^T X$ is like squaring stuff, right? That's like squaring a matrix, going back to the guaranteed non-negative. It's like squaring a matrix. So this is a non-square matrix, remember, right? But $X^T X$ is going to be square. $D$ will be the singular values, and $V$ will be another orthonormal basis, which are the right singular vectors of $X$.

### Relationship Between SVD and Eigendecomposition

And... there's a clean relationship between something like $X^T X$, where I might do that times $V$ equals $\lambda V$, right? Eigenvectors. And $X$ times $V$ equals... I can't call it $\lambda$ anymore. I'll use $\sigma$ for the singular values times $V$.

Not gonna quite work out the same. But what I want to be able to say is that if I have:

$$X^T X V = \lambda V$$

If I want to synthesize something, I'll get another... oh, now it's messy. Well, I have $U$ and $V$ transpose. What I want to do is to say, hey, we're sitting in singular value space in something that allows an $X$ which is not square, right? Because we have a data matrix in this class. And it might be $N$ by $P$. So we'd like something that gives me these singular values.

And... yeah, I'm not sure it's totally clear from my phone. Um... so let's think about how these things are related, right? The eigenvectors, which we're going to talk about for most of today, we've hopefully seen, and the singular vectors, which are just from the SVD of $X$.

### Deriving the Relationship

So... what shall we do? Let's... multiply this by the transpose. We have:

$$XV = \sigma U$$

Let's multiply it by... $X^T$ on both sides:

$$X^T (XV) = X^T (\sigma U)$$

$$X^T X V = \sigma (X^T U)$$

So I took this thing, I multiplied it by $X^T$, I get now something, oh, this looks much more happily like something that's gonna give me... I didn't get back to this yet.

But what was $XV$ equal to? I said it's equal to $\sigma V$, so that's equal to what? $X^T$, that's... $\sigma V$... or something very much like it, let's see.

But $X^T U$ should give me something that is... what did I say? I said that $X^T U$ was equal to $\sigma V$. I got that right.

So this $X^T$, this... whoa, whoa... this is going to equal $\sigma V^T$ transpose. Yeah, I mean, you got the same $V$ transpose here, I'm gonna get $\sigma^2$, which I wanted, and a $V^T V$, which is my identity.

So that's going to be equal to:

$$X^T X V = \sigma^2 V$$

But I wanted... don't worry too much about the math, but I wanted to drive the notion that says that you start with the singular values. If you square them (the way you do is by transposing and multiplying), you get out something which is, in fact...

### Key Result

**The $V$ is the same as the eigenvector, and the $\lambda$ is the singular value squared.**

Make sense? I'll say that once more, because it's so important.

**The right singular vectors are the eigenvectors of $X^T X$. And the singular values $\sigma$ squared gives you the eigenvalues.**

Which means I don't ever need to actually compute $X^T X$. I get a singular vector and singular value, I got the answer.

Make sense? Where do the $U$s come in?

**Student**: They're the eigenvectors of $XX^T$. They're in the dual space.

**Answer**: So, if you want those, you get those too. Cool. 

### SVD Dimensions

Okay, so $X$ which is $N$ by $P$ is going to be broken up. What is $U$? $N$ by $K$. $D$ is going to be $K$ by $K$. And $V^T$ is gonna be $K$ by $P$.

Make sense? $N$ on the left, $P$ on the right, and we're going through... $K$ is some intermediate dimension.

And when we approximate, we're going to keep $K$ small - always, in reality, we're going to keep a much smaller $K$, right? $N$ might be 10 million, $P$ might be 1,000, $K$ might be 20.

Right, so we're going to take something which then maps those. 

**Summary:**
- Eigenvectors of $X^T X$ are $V$
- The eigenvalues of $X^T X$ are $D^2$ ($D$ is the diagonal of singular values)

Oh, and that's it. Okay, moving fast.

## Principal Component Analysis: The Math

The core thing I wanted to do today for the next half hour is to walk through the math of principal component analysis (PCA), showing that PCA is a change of basis, like I showed over here, showing that PCA minimizes the reconstruction error, that's the reconstruction error, showing that PCA maximizes variance captured.

And... we'll be doing... so, as I said before, the idea is I'm going to approximate the original $X$, in $P$ dimensions, by some $K$-dimensional $Z$s, and we're going to do it in terms of the eigenvectors.

### Uses of PCA

I should say eigenvectors, right? People use it for making pretty pictures - project a high-dimensional space into two dimensions and look at it. That's probably not the best way to make pretty pictures, but it's used a lot for dimensionality reduction, uh, projection from $X$ to $Z$ used in supervised learning.

And there's a long tradition of taking anything you have and embedding it. In the early days before neural nets, people did **eigenfaces**. They took faces and projected them into a set of eigenvectors, and we do a modern version of it.

I will show you on Wednesday how to do word embeddings! Which is a linear version of embedding words, which eventually became **Word2Vec**. And modern embeddings of languages. There are people in robotics doing graphs, where you want to take a graph and embed the graph, right? 

So you can take a sequence of images, map them to an embedding. So, lots of things, whether you're in the robotics world or the language world, you'd like to map things to some sort of an embedding with lots of unsupervised data, ideally, and then use that embedding in supervised learning later.

Makes sense? That's the motivation. And we're gonna really stick with linear today, but you can think non-linear if you've seen it before.

Okay. Um... I said all of this. Okay, so the idea is... we're going to minimize distortion.

## Two Notations for PCA

And I'm going to show you two different sets of notation. I'm going to flip back mercilessly between them. And one thing I want is for you to get comfortable with the math notation, which is slightly crappy. And there are two things that really are the same.

One, which looks like what I said before, let's actually draw it on the board, is to say that... what I want to do is to minimize how far away $X$ is from $\hat{X}$.

### Matrix Notation

And what is $\hat{X}$? $\hat{X}$ looks like:

$$\hat{X} = ZV^T$$

Let me be precise. So, the two notations are that you can either have $Z$ times $V^T$, which is a matrix multiplication.

**Student question**: What's $Z$ again?

**Answer**: Good! Okay. Right? $Z$ is... let me write the dimensions. $Z$ is $K$ by $N$ times $V$ transpose, which is going to be $K$ by $P$.

By the way, for matrices, just keep... I always repeat to myself the dimensions, right?

So, $\hat{X} = ZV^T$. That's the matrix notation.

### Summation Notation

If you prefer, you can write it as the second notation down below here. We can take the summation over the $N$ data points, of the distance of each of my $N$ data points:

$$\sum_{i=1}^{N} ||x_i - \hat{x}_i||^2$$

What is $X$ in this notation? $X$ is now a vector, right? This big $X$ is a matrix, this is a vector $x_i$, dimension $P$, minus what? $\hat{x}_i$. What is $\hat{x}_i$? $\hat{x}_i$ is the summation over $K$:

$$\hat{x}_i = \sum_{k=1}^{K} z_{ik} v_k$$

of each of the $z_k$s for that particular $i$, right, for that $x_i$. $x_i$ has a $K$-dimensional $z_i$, times each of the $K$ basis vectors, or eigenvectors.

I should note here in the more traditional formulation, you take the square root of this Frobenius norm squared, remember the norm is the square root of the sum of squares. We're also going to go back and forth. You can minimize the norm, you can minimize the norm squared, who cares? They're equivalent optimization problems.

Well, the thing that I want you to... feel a little comfortable with is this... this annoying switching back and forth, going, oh, just make this rotation and writing out in two forms.

Cool.

## Centering the Data

Okay, and again... pretty much always for PCA, the first thing you're going to do is take each of the features! Take the average, subtract it off, so that you move all the points from here, over... yes, so they're centered around the origin.

You also sometimes may want to standardize, you might want to divide by standard deviation or not. That I'm less concerned about, you have to think about your problem. But in general, PCA takes things and centers them.

Because now you're dealing with the covariance, not actually... just covariance matrix.

### Exception: Sparse Data

One counterexample to digress, if you're at a place like where I worked at Google, where my $X$ is incredibly big and incredibly sparse, because it's all the web pages, and almost all the features are zero. It's too expensive to center, in fact, subtract off any mean.

Make sense? The mean count for each word on a web page is roughly zero, right? Most words don't show up on most web pages - a web page has got a few hundred words, there's a million words, they're mostly not on any web page.

So there are occasions that you can't afford to subtract off the mean. But if you're a mathematician, ignore that.

## PCA Terminology (Jargon)

Okay, stop. Jargon! Jargon! More jargon! I can never remember this, but you should remember. Write it down, put it on the cheat sheet.

We're going to talk about $Z$, which I call the coefficients in the expansion, are called the **principal component scores**.

And $V$, which are the eigenvectors, or the basis are called the **loadings**, or the **principal components**, um, or the factor coefficients, which I would never use.

The terms that you should know, annoyingly, are **scores** - $Z$ are the scores - and $V$s are the loadings, or the principal components.

### Understanding the Terms

The principal component makes sense. That's the direction of the basis vectors, right? Make sense?

Why are they called the loadings? What's loaded onto a principal component, right? On top of it.

Anyway, I've got a principal component that points out here. What space do the loadings live in? What's the dimension of it?

**Student**: $P$, right?

**Answer**: Consider the original feature space. So the eigenvectors, the $v$s, sit in the original feature space. And they say, how much is each feature contributing to this principal component direction. Those are called the loadings of the features.

The $z$s, that says, in this new space, right? Here's my point $X$. This distance here is its score for $v_1$. There's $v_2$. This distance here is its score for $v_2$.

Thank you, actually. Make sense?

I can't help it, but I always forget the jargon, but it's a good thing to know.

## Variance and Reconstruction

Okay, so... you look at a picture up there, which looks a lot like the one I did. What's going to happen is the first principal component will magically (okay, we will prove it) magically project in the direction of maximum variance of the points. Where the points are most spread out. We're gonna have the highest standard deviation.

Right? Or variance. And the second principal component will be the second highest dimension orthogonal to that. And the third principal component will be the third direction orthogonal to that.

### Two Key Properties

So, the first one, it will magically do two things:
1. They'll point in the directions of maximum variance of the data, where things are most spread out, the most information (but it's not information in an information theoretic sense, it's gonna be in $L_2$ sense).
2. And secondly, they will magically do the minimum reconstruction error.

And we're going to see that those two things add up, and the more principal components you keep, the better... the lower the distortion, the less error you have in reconstruction. And each point gets you lower, we're explaining less and less variance.

## The Mathematical Derivation

Okay. We will do math. We will do math, but before we do math, let me walk through it, because it's getting a little bit messy.

### Full Representation

So, we're going to say that $\hat{x}_i$ for the $i$-th observation, if you keep all of the possible principal components, so here $K$ would be $P$, all the principal components:

$$x_i = \bar{x} + \sum_{k=1}^{P} z_{ik} v_k$$

$x_i$ is equal to the average of that $x$, right? Because remember, we subtracted off the mean, that's the mean of that $x$ plus the summation of $z$ times... you, and I'm gonna be a little sloppy here, because I like these for the right eigenvectors, and these for the... for the eigenvectors.

I think they're the same thing? And so...

Does this make sense? What is $x_i$? $x_i$ is equal to its average plus just to make sure we're all following, all of the basis vectors, all the eigenvectors, the summation of $z$ times $v$ for that point.

### Keeping Only K Components

And what we'll show is you can take this summation of all of the eigenvectors and eigenvalues, and you can keep the first $K$, which are the ones with the biggest eigenvalues. And that will be the ones we'll keep.

We'll take $K+1$ to $M$, and we'll throw them away. And what we'll show is that the ones that we drop, well, of course, that's the error, right? We've... instead of taking all $M$ of them, we took only $K$ of them, we've thrown away the rest of them.

### Proportional to Eigenvalues

And what we'll show is that the amount we're throwing away from each of them is going to be proportional to its eigenvalue. Right? So I've got $M$ total eigenvectors, each one has an eigenvalue.

And what we'll show is that this summation breaks up into two parts. The first $K$ are the ones that explain the signal. They capture the variance in the data. And the last $K+1$ to $M$ we throw away will be the error. The ones we miss.

And the amount we keep will be the summation of the first $K$ eigenvalues - we'll explain how much signal we keep. And $K+1$ to $M$ will be the amount of reconstruction error we're incurring.

All proportional to the total number of eigenvalues.

So this is where we're going. And now we're gonna get that result. Good.

## Minimizing Distortion: The Derivation

Okay. So, first subtract the average $\bar{x}$ from all of them, then express $\hat{x}$ as equal to $ZV^T$. And then we're going to minimize the distortion. The distortion is the summation:

$$\text{Distortion} = \sum_{i=1}^{N} ||x_i - \hat{x}_i||_2^2$$

$i$ goes from 1 to $N$ of the distance between the $i$-th point and the $i$-th point's approximation in $L_2$ norm squared.

Or if you prefer summation notation:

$$= \sum_{i=1}^{N} \sum_{j=1}^{M} (x_{ij} - \hat{x}_{ij})^2$$

where we use $M$ instead of $P$ here, because the PCA people like $M$ instead of $P$, because they're not predictors anymore, they're just features. So $M$ is the number of features.

of $x_{ij} - \hat{x}_{ij}$ squared, right? So this is the Frobenius norm squared.

### Perfect vs. Approximate Reconstruction

Make sense? If we keep all $M$ of them, we should have a perfect reconstruction. If we pick $K$ of them, we're gonna lose some signal. We want to pick the best $K$, which will probably be the ones with the largest eigenvalues. That'll be the best $K$. And the ones you throw away will be the ones with the smaller eigenvalues.

**Student question**: Can eigenvalues really be negative?

**Answer**: No, remember it's $X^T X$, it's a covariance matrix, it's positive semi-definite, so we don't need to worry about negatives. Can a singular value be negative?

Yeah... we're not gonna worry so much about those edge cases. Okay.

### Matrix Form

Or, let's write it a little bit more elegantly. The Frobenius norm of:

$$||X - ZV^T||_F^2$$

where $Z = XV$. So we have the transformation both ways:

$$X \xrightarrow{\times V} Z \xrightarrow{\times V^T} \hat{X}$$

We've got $X$ times $V$ that takes us to the $Z$ space, right? And in matrix form. And then we take $Z$ times $V^T$, and it takes us to $\hat{X}$.

So we've got both our forward transformation, right, $X$ times $V$ giving me $Z$. And then the $Z$ times $V$ transpose, giving me $\hat{X}$. And we can do the same thing if we're very fancy, and maybe we'll have neural nets. Instead of a linear transform, we'd have...

If you do a neural net, you can have a neural net that maps $X$ to $Z$, and a pseudo-inverse of that neural net that maps $Z$ back to $\hat{X}$, right? And again, oh, let's look at it. Right? $X$ to $Z$ by multiplying by $V$, or via a neural net. $Z$ back to $X$ by multiplying by $V^T$, or by doing the inverse of a neural net.

The nice thing about the matrices, it's linear, the math is beautiful. You hope it's good. 

## Working Through the Math

Okay, now we get messy. And I can actually get access to my notes here. Let's actually click through... and... I did a bunch of notes here. Let's try and look through at a bit of the math.

### What You Need to Know

What do I want you to know about the math? I'm mostly concerned that you'd be able to take away what comes out at the end. So, I'm not too concerned if people get a little bit lost in the middle of the math. The math is not super important. But the ideas are.

Which is to say that you should know this representation, which I've shown 3 times:

$$X = \bar{X} + \sum_{k=1}^{P} z_k v_k$$

You should know the definition of distortion. This is the Frobenius norm squared.

Now I plug in $\hat{x}$ in there, now you get something that has... that's $x - \hat{x}$.

$$x = \bar{x} + \sum_{k=1}^{P} z_k v_k$$

$$\hat{x} = \bar{x} + \sum_{k=1}^{K} z_k v_k$$

$x$ is equal to $\bar{x}$ plus the full summation of all of the eigenvectors times coefficients. $\hat{x}$ just keeps the first $K$ of them. And the $\bar{x}$s are going to cancel out.

As they should. Now, what I have is I have the full model minus the approximation, which keeps $K$, which is going to give us something in the end that goes from $K+1$ to $P$:

$$x - \hat{x} = \sum_{k=K+1}^{P} z_k v_k$$

### Orthonormality Properties

And now, once I go through and multiply, then all sorts of magic happens. We take the transposes, we multiply it, we do the... I'll go very fast. The fact that this is orthonormal, so if the two bases, if the $v_j$ and the $v_r$ are not the same, it's zero, if they're the same, it's 1.

And that then gives us the distortion is just the summation over the $K+1$ to $P$ of the $z$s.

So, what did I say? I said that we kept... remember, we kept the first $K$ eigenvectors, we threw away the rest of them. The error we made was due to the ones that we left out.

### Expressing Z in Terms of Covariance

Now, the cool thing is, the $z$s can be expressed by what? The $z$ is equal to $(x_i - \bar{x})^T v$. And if you look at what the covariance matrix is, we use the matrix formulation you should recognize and note, the covariance is:

$$\Sigma = \frac{1}{N}(X - \bar{X})^T(X - \bar{X})$$

divided by $N$, right? That's the covariance matrix. Here it is in matrix form.

If you now go back and say, here's the distortion, which is the $z$s, the $z$s were equal to this equation here, which looks like... you plug it in, and trust me, if you work the math out, it looks like the summation (remember, only $K+1$ to $M$):

$$\text{Distortion} = N \sum_{j=K+1}^{P} v_j^T \Sigma v_j$$

$v^T \Sigma v$, where $\Sigma$ is the covariance matrix. So, cool.

### The Key Result

What I just showed? I just showed the distortion, the $L_2$ error, the Frobenius error, is equal to something which is going to be some matrix, vectors, eigenvectors times covariance times eigenvectors.

The ones that are going to make this as small as possible, if the $v$s are the eigenvectors, I'm gonna wave my hands a little bit, and say, **mathematically, the eigenvectors are the best possible basis functions you could use**.

And, furthermore, if I now take $v^T \Sigma v$, that's going to give me back out... think about it, right?

$$\Sigma v = \lambda v$$

So:

$$v^T \Sigma v = v^T (\lambda v) = \lambda (v^T v) = \lambda$$

because $v^T v = 1$ (orthonormal).

People see that? That piece of magic you should follow. If $v$ are the eigenvectors, then $\Sigma$ times $v$, by definition, is equal to $\lambda$ times $v$. And the $v$ transpose $v$ is equal to 1, because they're orthonormal.

### Final Distortion Formula

So, I just showed... then if you work all this out, that the distortion is:

$$\text{Distortion} = N \sum_{j=K+1}^{P} \lambda_j$$

the number of data points times the summation, $j$ goes from $K+1$ to $M$ (or $P$), of the eigenvalues.

Oh, cool. So... if we do an optimal reconstruction, the best possible linear reconstruction, where we keep orthogonal basis vectors, we will actually use eigenvectors as the basis.

**And the amount of information we're losing per point is the eigenvalue for each of those $K+1$ to M$.**

We'll go through it very quickly. And you can see I glossed over some of the math details that I don't really care about. What I care about is the intuition of saying: **This is really the best basis you could use under $L_2$ loss.**

## Maximizing Variance Explained

Okay, let's do the second half of that, which is really the same piece, which is... the other thing I might want to do is, on the part I kept, right, I threw out one part, I showed that's distortion.

The reconstruction error? The other piece is, can I show that the directions maximize variance? Which is the same thing the other way.

### Variance Formula

And what's the variance? The variance is, for my $N$ data points, for the $K$ principal components I'm going to keep:

$$\text{Variance} = \sum_{i=1}^{N} \sum_{k=1}^{K} (z_{ik} - \bar{z}_k)^2$$

the difference between $v^T x_i$, that's the $z$. And $v^T \bar{x}$, that's the mean. The variance from each direction, each of the $z$s, is the square of $z$ minus the average, right? That's variance - just take the value minus the average, square them, sum them up.

### Working Out Variance

If you look at that, lo and behold, factor it, you get out, that's equal to... oh, look at this!

$$\text{Variance} = N \sum_{k=1}^{K} v_k^T \Sigma v_k = N \sum_{k=1}^{K} \lambda_k$$

$N$ times the summation of the $K$ components I kept of the eigenvector transpose covariance matrix times eigenvector, which, again, is equal to the summation for each of those of the eigenvalues $\lambda_k$.

### The Complementary Result

What I just showed was that the variance is the summation of the first $K$ eigenvalues. And the distortion is the summation of the rest of the eigenvalues.

Between them, so it shows you how well can you approximate something? Well, how well I'm doing is the eigenvalues I kept. And how badly I'm doing is the eigenvalues I threw away.

It's a nice quantitative measure. As long as I stay in Frobenius norm space, as long as I require orthogonal basis vectors. And it's nice, and there are nice algorithms for computing it.

## Scree Plots and the Elbow Method

That's what I just said. As an engineer, people tend to do something very similar called a **scree plot**.

And this scree plot says, I'm gonna... and if you'd like, compute right singular vectors, which are the eigenvectors, and I will compute the eigenvalues, which are the squares of the singular values.

And I will then say, hey, how do these eigenvalues change as I take the first principal component, the second, the third, the fourth, the fifth, when I order them?

And remember that the reconstruction error is the summation of everything I threw away, or the variance explained is the sum of all the ones I kept.

### The Elbow

And now we do a magic piece of engineering, we look for something we call the **elbow**. There's no math in the elbow. The elbow's just a little bit hand-wavy. We try and find where it goes down quickly, and then we say, hey, we're going to keep, in this case, the first 3...

Or, if you will, you can take a cumulative sum, and if you take the sum of each of these, you'll get something that looks almost exactly the same, which is... if I take principal components 1, 2, 3, 4, 5, 6, 7, 8, and I take the percent of the variance explained...

I'm having bad handwriting, we can either say percent variance or variance explained. This is 100% if I've explained everything. What you'll see is this looks just like that one reversed.

And often you'll say, hey, I want to explain something at some distance, or at some point right here. This is the part I got right. This is the error, right? That's the error up here.

My error is the sum of the eigenvalues I threw away. And you say, hey, I'm gonna go up until I get 80% explained, or 90%, or 95%.

Right? So I might say, hey, I want 95% of the variance explained, and that's 1, 2, 3, 4 principal components. Keep that, right?

And often I've got hundreds of them, or thousands of them. And just a quite small number does a really good job of capturing everything I care about.

## Quick Review Question

Okay, oh, it didn't do the poll you wanted, but we'll do it anyway for thought.

If $X$ is any matrix, and $X$ has SVD, $X = UDV^T$, the principal components, scores, or coefficients...

Are they the columns of... $Z$?

Crap, what was the score? What are the scores? Remember the jargon? Which parts are the principal components? $Z$ or $V$?

**Students**: $V$!

**Answer**: This is a trick question moment, right?

Okay, what do we call the $z$s?

**Students**: The scores!

**Answer**: Okay. The $X$ is mean-centered, PCA finds the eigenvectors of $X^T X$, the right singular vectors of $X$, the projection directions of maximum covariance of $X$. Satisfied with them? All the above, yay!

Yes. Cool. I'm sorry. Cool. 

## SVD and PCA Connection

Um... I'm not going to walk through this slide, but I want to note that PCA can be done with SVD, because SVD finds everything you need. It finds the right singular vectors, and $Z = XV$ gives the component scores, and $V^T$ are the loadings.

I'm gonna skip that... so, standard PCA is optimal in terms of reconstruction under a Frobenius or what I would call a $L_2$ norm, right? Everything goes down to that. It's great, it lets you do eigenvectors and eigenvalues.

## Sparse PCA

Often, machine learning people and computer scientists would rather have a sparse solution rather than the optimal reconstruction.

So I waved my hands and said that the $L_2$ norm, you know, the eigenvectors give you the optimal reconstruction under orthonormality. But it's fairly common to say that I would like something else.

And there's a bunch of variations people use. But the most popular one for me as a computer scientist, as opposed to someone in statistics who have a different variation, is to say, I'll minimize the Frobenius norm, but I might want to put a penalty on it. I can add a regularization penalty.

### Sparsity Penalties

And things I might want is I might want sparsity on the $z$s, or I might want sparsity on the $v$s. And the easiest way to do sparsity is to put an $L_1$ penalty.

Why do people like $L_1$ and not $L_0$?

**Students**: It's convex!

**Answer**: So you can still solve this, and in the modern era, people are doing these same reconstructions with neural nets. And so, it's just still nice to have convexity, you're not doing any, like, stepwise regression.

So, you can put whatever sort of penalty you want on these. There's no free lunch. And... in math? What did I give up? I put a penalty that puts either $Z$ or $V$ more sparse. I get each one of them. What am I giving up?

**Students**: I give up orthogonality!

**Answer**: Certainly going to give up optimal reconstruction. Right? If you zero out some of the things and shrink them, you're going to have less good reconstruction. Of course!

### Training vs. Test Error

What was odd about all of today's lecture, which was math rather than machine learning itself, is why do we use machine learning? The reconstruction error is a loss on the training set, right? Everything here was on a training set. All the math was on the training set. Everything was on the training set.

But of course, I'm an engineer. I only care about the test set, right? So, any sparsity might be nice for interpretation reasons, the doctors I talk to like sparse solutions. And the regularization might help me overfit less.

### When Overfitting Matters Less

I'm usually not so concerned about overfitting in PCA. Not that many parameters, I have only one hyperparameter, $K$, how many components, and... the data's cheap, right? Think about brain images. If I want a brain image with an autopsy, $20,000 bucks. I gotta pay a doctor to slice up the brain, look at it, right? That's expensive. 

If I want an unlabeled brain image, I've got tons of them available. There are lots of them, right? Brain images are relatively cheap, the labels are pretty expensive.

So often, I don't worry so much about overfitting. But realize that in reality, I always want to at least be thinking about overfitting, right? So everything I said was on the training set, all of the math was on the training set. But in reality, you might want to test the reconstruction error on a test set for some information theory reasons.

And you might do a little bit better on the test set if you regularize. Although, frankly, most regularization is done for interpretability.

## Biconvex Optimization

Cool. And these are **biconvex**. So this is the first of a set of algorithms we'll see in unsupervised learning. We want to find both $Z$ and $V$. And it's convex in $Z$ given $V$, it's convex in $V$ given $Z$. And so these things are often solved by an **alternating iterative algorithm**, which we'll talk about later in the course, yeah.

### Understanding Sparsity

**Student question**: What does sparsity mean in PCA? As we discussed last lecture in each principal component...

**Answer**: Sparsity means either for the $z$s, right, the coefficients in the basis expansion, what you're going to get is some of the $z$s are zero, or for the $v$s, the loadings, some of the features will not appear in some of the loadings.

So if you want a principal component that doesn't have all the features in it... you'd hope something, you're trying to explain either well-being across countries we'll look at, or you're trying to explain brain activity.

If many of the pixels or voxels have zero in the first principal component, you're saying, hey, all I care about is this one brain region. The rest of the pixels are zeroed out.

### Why Use Sparsity?

Why do I use sparsity? Because decision makers like it, because the decision makers want to interpret what's going on. And if I say, well, every voxel in the brain contributes a little bit, they tell me, no, no, no, tell me which regions of interest. Is it the amygdala? Is it the hippocampus? Where is it that you care about?

Right? So it's much nicer for interpretability. If you can zero out some of these, if your principal components capture something a little bit more intuitive.

And we'll talk later about there's a Bayesian dual, everything was $L_2$ so far. You can have priors over what the reconstructions look like. There are ways to build Bayesian priors into these.

Nice question. Cool, okay, we're... getting very close to the end.

## Summary of PCA

Um, summary of the first piece... you should know PCA as minimizing reconstruction error or distortion. You should know it maximizes variance. You see all the jargon, we'll cover more stuff next time.

I want to say, in the last 2 minutes, two other things very quickly.

## Pseudo-Inverse via SVD

We've talked early on about finding weights for $y = X^T w$, and what we'd like is to have a pseudo-inverse of $X$.

So, in some sense:

$$w = X^{+} y$$

where $X^{+}$ is the pseudo-inverse.

But $X$ is not invertible. $X$ is not even a square matrix. It has no inverse, but I want a pseudo-inverse. And again, you should know the jargon, Moore-Penrose inverse.

And we found this in the very first part of the class by taking:

$$w = (X^T X)^{-1} X^T y$$

Right? We're going back and reviewing linear regression. This was what we multiplied by $y$ to get $\hat{w}$. So this is a pseudo-inverse. It's an approximation to the inverse of $X$. It won't be perfect. But it'll be there.

### Computing Pseudo-Inverse with SVD

And if you do the singular value decomposition, super cool! You can write $X = UDV^T$, you could take the $X$ pseudo-inverse by finding this decomposition, and what you're gonna find is that magically, if you're in the singular value space, all you have to do is take the diagonal matrix $D$, take 1 over each of those singular values, and recompose it:

$$X^{+} = VD^{-1}U^T$$

So, with SVD, you can decompose a matrix into left singular vectors, singular values, right singular vectors. It allows you to super cheaply compute an inverse (or pseudo-inverse). And I'm not going to walk through the math, because I have exactly one minute. But, um, but trust me, it works beautifully simply, and it's a good thing to do.

And... I didn't mention the power method, which also works similarly.

## Key Takeaways

So, the things to notice for all these is we've got the same notion of SVD lets us break up any matrix and decompose it. That makes it cheap to do a pseudo-inverse of it. It makes it really easy to approximate stuff with it.

Having a nice orthogonal basis makes life super clean and super nice.

And with that, I will end, and on Wednesday, we'll come back and look at examples of it. And don't worry about attendance, it's all good, you're all here.

---

## Post-Lecture Q&A

**Student**: You can zero out as many features as you want...

**Answer**: I don't like sparsity for...

So, I actually promised you last week. So it actually belongs to them. But I was hoping to really do this type of... one, I wanted to... and if you see your design.

So it's, like, not... Yes, I think it's... I think it's... okay, as long as it's... yeah. So, so part of the... yeah, so part of the, you know, if you're working... we get involved with projects.

So, so no, I really do want to talk about your projects. We can help you with... there's a thread on Ed for people. Would you want to work with? It's going to be... we have a lot of students here interested in it. There should be probably enough time. We have a lot of robotics people here.

Sounds good. Okay, sounds good. Thank you.

---

### Project Discussion 1: Protein Folding

**Student**: Yeah. Hi, so I have a couple of questions that are very simple about what you've been asking. So, I'm, um, I'm a visiting student, and I do, like, biology work. So the project that I kind of wanted to do was to be, like, a smaller version of the ESM Fold model that, like, does protein folding.

**Professor**: Sure.

**Student**: Um... and with, um, select data, so...

**Professor**: Yeah, you want to find the dataset.

**Student**: Yeah, yeah, so we would do that, and it would be...

**Professor**: Yeah, well, yeah, so as long as you... yes. And, like, the data set, like, the PDB, you know...

**Student**: That's great. So, my couple of questions are, um... okay. So, like, none of my friends want to work on that in particular, so I need to, like, recruit people to...

**Professor**: Yeah, so post them on Ed.

**Student**: Okay.

**Professor**: The biology part of students, the bioengineers, a bunch of them on CIS. There's a bunch of CIS people available. I actually know some of them.

**Student**: Okay.

**Professor**: No, I'm not super close to them, I'm not super... no, I... no. Yeah. So, having the idea posted... don't need everybody to understand biology.

**Student**: Yeah.

**Professor**: It's something I can... legally technical stuff.

**Student**: Yeah.

**Professor**: Should be able to understand how ESM works.

**Student**: Yeah.

**Professor**: It's not terribly, like, complicated. As long as they're able to think about... I mean, it's basically just, like, ChatGPT for proteins.

**Student**: Yeah.

**Professor**: So... so I think the answer is post it and hopefully go ahead and put the hookup. So, you know, carefully.

**Student**: The second question, because that dataset that we're going to use is, like, actually kind of big, and I think it's much bigger than most people... that I could double-dip and use the project for my... for ESE 546, also, if your team says that's okay.

**Professor**: Yeah.

**Student**: Okay.

**Professor**: Yeah, because there also is a project requirement there, and I was thinking, like, because it's basically, like, a sequence encoder to a latent space and a structure decoder.

**Student**: Yeah.

**Professor**: So I was thinking, like, I could do, like, one of them for this class.

**Student**: Yep.

**Professor**: Or, like, whatever one I don't do I'll do with ESE.

**Student**: Yeah.

**Professor**: Sounds okay.

**Student**: Yeah, sounds fun. I mean, I'm... I'm flexible, it sounds... yeah, sounds good.

**Professor**: Okay.

**Student**: Alright, thank you so much. So, so yeah, so find, find... I will, I'll try to find people.

**Professor**: Alright, thank you.

**Student**: Cool.

---

### Clarification Questions

**Student**: Questions? Or a question related to the PCA part. So, it seems to be, like, so, or equally, is it just, like, minimizing...

**Professor**: No, no, okay. It's a single optimization which ends up doing both. Okay.

**Student**: So the simplest way to look at it is you choose a basis that keeps the principal components, the first $K$, you keep them.

**Professor**: And you throw out $K+1$ through $M$.

**Student**: Make sense?

**Professor**: Yeah. You can either pose it as maximizing the variance, which is how well you do on the reconstruction with the $K$ you keep. Yeah. Or you can pose it as minimizing the reconstruction loss, which is how much you lose on the $K+1$ onwards.

**Student**: Okay, so these two are not separate? They're exactly the same?

**Professor**: They're exactly the same. And they're fully equivalent. They're literally mathematically equivalent, and in particular, the total size of the matrix is the summation of the eigenvalues. And the eigenvalues of the ones you drop are your reconstruction error. And the ones that you keep are the ones that explain the variance.

So it's two precisely, it's two very different conceptuals. One is what explains the most signal? Yes. And the other is what loses the least signal. It's the same problem.

**Student**: Yeah, it's the same thing.

**Professor**: And also, like, so if you consider the original problem of minimizing the reconstruction loss, like, do we have any... like, any, like, assumption of $Z$ and $V$ yet, or, like, we're just assuming, like, we're minimizing the problem of $X$ minus $ZV^T$?

**Student**: If you do no assumption.

**Professor**: Basically, what comes out magically is... are the eigenvectors.

**Student**: Yes.

**Professor**: They are magically the ones that... okay, that minimize that.

**Student**: Yeah, I see, because I also thought, like, okay, like, you're solving this, you've got the SVD, but there's also something related to, like, maximizing variance, so I thought, like, because we do another... another optimization problem that we end up choosing the top $K$.

**Professor**: No, so I think... the way I think about it... because I use this also as a basis for future things. Same thing's done for neural nets. Is minimizing distortion, minimizing reconstruction error is the way I think about it. That, for me, is the most useful.

**Student**: Yes, I was...

**Professor**: And then it just turns out that that also means that if you... if you minimize the reconstruction error, you will, if the... the first principal component will be in the direction of maximum variance.

**Student**: I see. And also, like, another thing that... this one is more, like, specifically, like, this is all related to the lecture, but, like, how you use PCA.

**Professor**: Yep.

**Student**: So, like... when we use PCA...

[Lecture ends]

---

# CIS 5200 Machine Learning - Lecture 15: PCA Applications and Autoencoders

## Pre-Lecture Discussion

But, oh my god, what's going on?

Can you ask me about, um…

If this is bad, I fail it again, but it's always composed.

Yes.

Hopefully they know, I don't know.

But, I mean, there's quite good software that will put CRA. I tend to be most easy.

I know, I used to retire early tech business course.

It's not somebody. It also means still writing, handwriting. So, yeah. Yeah, I mean, it's good that I was sitting there.

I think we'll start talking about it now.

So, it's fast.

Like, the two devices can also read everything on my Kindle and not my laptop, and I don't have any pencil.

It is good. Now, I've got to keep an app on the laptop, but I never use it. It's a physical device.

Yeah, terrific.

So, there's no secret there.

And download all the library books, it was great.

I lived in my home, it's automatically connected to my homework.

It's so fooled, like, they said that…

She says those counter fires more security on the phone.

It's like, it's okay. I'm sorry, the internet word, but…

This is fine.

Five times.

It's always embarrassing.

It's super…

Alex, we wanted to bears all over the…

You'll find book club members, you know, professors. I have a statistical knowledge.

I like that.

You're still pretty well intrigued.

Just like somebody that's gonna be music to you.

I can only imagine putting it…

Well, we can't…

## Introduction and Office Hours

Okay, welcome, welcome, um…

Quick reminder about office hours: what I've been doing is spending the first half hour hanging around here, and mostly right outside there. Then I go over to my office, but please email me if you want to come to my office, because it's locked behind a glass door.

But mostly, I've been meeting people, which is good.

And…

Yep, sorry.

## Today's Topics

So I want to do two topics today, and the slides are both posted. I want to talk about using PCA—a very data science-y topic, the opposite of the math of last time. And then I want to talk about autoencoders, which are the neural net generalizations of PCA.

Right? The same way that we went from linear regression to standard neural nets, we'll go from PCA to reconstruction, ICA, and autoencoders.

And it's really very much the same notions. So, I'm gonna do a lot of stuff sort of quickly and hopefully it's fun. I'm also going to show a bunch of embedding examples from the linear world, and all of them have analogs in the deep learning world.

## Principal Component Regression (PCR)

### Overview

To remind people of the standard applications, the most common usage of PCA or any autoencoder, any embedding, is as features.

And when jargon term is **principal component regression (PCR)**, which is a fancy way of saying: take your feature vector $\mathbf{x}$, run SVD on it, factor it into the scores and the loadings (the weights and the principal components), keep the biggest $K$ of them—that's your one hyperparameter to control the complexity of that piece of representation.

You now have a mapping which takes any $\mathbf{x}$ into the $\mathbf{z}$ space. Right? You project it onto the principal components, and then you fit a regression like a linear regression: 

$$y = \mathbf{w}^T \mathbf{z}$$

in the transform space.

And then you pretty much have your model, except $\mathbf{z}$, of course, is a function of $\mathbf{x}$.

Right? And one thing you should be comfortable with is going back and forth, right? You go from $\mathbf{x}$ to $\mathbf{z}$, and you go from $\mathbf{z}$ back with the pseudo-inverse to $\mathbf{x}$. Those are just projections.

So, then we have a simple model: 

$$y = \mathbf{w}^T \mathbf{V}^T \mathbf{x}$$

That's it. Nice. Cool.

### Computing the Projection

So, for a new $\mathbf{x}$, what am I going to do to get the $\mathbf{z}$?

Just showed it, right? Hit it with the $\mathbf{V}$. And again, I want people to be comfortable with this notion that you can either write matrix multiplications or write dot products.

And if you think about these pieces, again, I want to think about this going back and forth, right? You go from $\mathbf{x}$ to $\mathbf{z}$, and from $\mathbf{z}$ to $\mathbf{x}$.

So if I take $\mathbf{x}^T \mathbf{V}$, and $\mathbf{x}$ is what? $\mathbf{x}$ is $\mathbf{V}^T \mathbf{z}$.

So $\mathbf{x}^T \mathbf{V} = \mathbf{z}^T \mathbf{V}^T \mathbf{V}$, which is equal to $\mathbf{z}^T$. 

Why is that? What is $\mathbf{V}^T \mathbf{V}$?

It's the identity matrix, right? We're working with an orthonormal basis.

Good. Hopefully this is trivial and boring. If unclear, please ask questions.

Cool.

## Semi-Supervised Learning with PCA

So, this is all done as if you had a single $\mathbf{x}$ and a single $y$. But in fact, most of what we're gonna do is we're gonna have one set of $\mathbf{x}$'s—look, I'm fading in and out here.

Fading in and out? We'll have a much bigger set of data for learning the embedding space for learning PCA, and then we're gonna take, once we've learned the embedding, we're gonna go to our smaller labeled set and use that, right? 

And this would be called **semi-supervised learning**, and I sort of pictorially represent it as: I've got one big set of $\mathbf{x}$'s with no $y$'s. I will use that to learn an embedding, either PCA or an autoencoder we'll cover later today. And then I take a smaller set of $\mathbf{x}$'s in the same space (right, my $p$-dimensional space), and then I learned the model there.

Yeah.

**Student question:** Is $y$ like a test set here? Yes or no?

Yes, no? No, I got a no, I got no's. No, $y$ is not a test set. This is the training set! This is $\mathbf{x}$ and $y$ for the training set. If you want a test set, you need some more $\mathbf{x}$'s and some more $y$'s.

So, no, this is all training. I got no testing here, but the piece that I'm stressing is there's two things that are sold separately.

One is: how do you learn a good way to take $\mathbf{x}$ and transform it? Given the $\mathbf{z}$, then I can actually learn some prediction of labels $y$.

**Student question:** Training-wise, the second stage, yes. I'm using my $\mathbf{x}_0$ to derive my $\mathbf{z}$, yes, right? 

The $\mathbf{x}_0$ with no labels is a big, cheap set. That's a crapload of videos, or brain images, or web pages, whatever I have a lot of. I use that to embed the $\mathbf{x}$. That gives you an $\mathbf{x}$ to $\mathbf{z}$ mapping. Then once I have that, I can now train up a model that says $y$ is some function—here instead of $\mathbf{z}$, I've written it $\phi(\mathbf{x})$ because that's how I'm representing the $\mathbf{z}$.

### Why Semi-Supervised Learning?

Why is it, in general, you don't have a lot of $y$'s? Why is it so much easier to get $\mathbf{x}$'s versus $(\mathbf{x}, y)$ pairs?

**Labeling is expensive.** You're gonna pay, in the best case, some underpaid, cheap person in Kenya or Nigeria, and in the worst case, some overpaid physician at HUP to label stuff for you.

Regardless of which one you're paying, you're still paying someone money, usually to get good labels.

Or, you're gonna get labels by showing the product to someone and seeing if they buy it or don't buy it. That's expensive to run an experiment. Companies hate running experiments.

A/B test it! Try showing different ads, see which one works. They're going, "No, if that one's probably better, I'm gonna use the one that's probably better. Why would I collect information by showing a worse ad?" That's expensive!

So, labeling, no matter how you do it, costs you money, whereas often, the unlabeled data is pretty cheap.

Yep.

**Student question:** How useful is the healthcare data?

If you look at HUP, how many brain images do you think they get per year, and how many autopsies of brains to check the nature of the neurodegeneration that they do per year?

Which one's more expensive? Putting your living head into an fMRI machine, which is sort of a noisy, expensive device—they spin around, you can't wear metal, and they're terrible. But they're still really cheap compared to doing the autopsy to actually check to see exactly where in your brain was the problem.

**Student question:** Why would you not use $\mathbf{x}_0$ plus $\mathbf{x}$? 

You can. And if you have all the $\mathbf{x}$'s, you can learn the mapping on the full $\mathbf{x}$. It's really a question of how the data set works. Usually, this one is so much bigger than that one, I don't really care, but sure, you could throw them all together and use the ones there.

Cool!

### PCR as Semi-Supervised Learning

So, PCR, if I write it as semi-supervised: do PCA on a big $\mathbf{x}$ Excel unsupervised to get the loadings, project $\mathbf{x}$ to get the z-scores, do the same thing I did before.

It's exactly the same equations—no need to cut them down again. It's the same ones as last time, except rather than using the same $\mathbf{x}$ for learning embeddings as I used for prediction, I use a much bigger $\mathbf{x}$, maybe adding the same ones in, and then I use that.

Cool!

## Geometric Interpretation

So why does this work? Let's try and get a little geometric interpretation. Imagine I have a small label set. In fact, I've got two items, right? I got a green one and a blue one. I can try and put a hyperplane to separate them. The best hyperplane is probably the one I drew, halfway through.

Right? It's a large margin hyperplane.

But imagine I had more unlabeled data. Now it looks like the old hyperplane's not the best one. I've redrawn one. Note that all these black dots are unlabeled, but nonetheless, if I were gonna do some sort of a dimensionality reduction or projection, I can say, ah, this green one probably belongs close to these guys, the blue one probably belongs close to those ones. I can do a better job.

Right? And it's often the case you've got to a high-dimensional piece—a brain image, a video. If you got a video, and you want to do a label on it, it's a very high-dimensional video. If you can embed it in a lower dimension—you know, 500,000, 10,000 dimensions, whatever—that's still low-dimensional for a video.

Then I can actually, if I learn a sensible embedding, I can now fit a hyperplane that says, is this a video of a happy or a sad video? Is it a murder mystery or a comedy, right?

Cool.

## Visualization with PCA

These things are used a lot for visualization, which is probably, again, probably not my favorite use of visualization, but what you can do is take your high-dimensional $\mathbf{x}$ (your video), project it down into two principal components or use some other way of dimensionality reducing it, and then you can say, "Hey, here's 10,000 videos, and now put them in two dimensions."

And you can start to see, okay, at different times of day, are people watching different subsets of the videos? You can label some of the points with the labels of the videos, or the captions, and see what's going on.

So there's a lot of ways of taking high-dimensional images, documents, whatever, and embedding them. Often it's nice to look at them.

And we'll see some examples. For some things, like medical records, you can then look back and say, okay, I've got these principal components, what are the loadings? Which was it—the patient's temperature? Was it the patient's age, which is important for children? Was it the patient's whatever that was most important in deciding each dimension?

### Sparsity in Loadings

And people often like sparse loadings, and we said last time, how do we typically, as computer scientists, drive sparsity?

In general, it's an $L_1$ penalty on the loadings will make the loading sparse. There are many other penalties you could put, but that's the popular one that tends to show up. If you're over in statistics or psychology, you'll use a slightly different sparsity method, but it's all the same idea. And the $L_1$'s mathematically cleaner.

Cool!

## Example: Well-Being Across Countries

So, I actually study well-being, looking at countries, what makes countries happier, not happy. Let me just do a quick demo, so I grabbed a bunch of labels from the OECD of different measures across countries. I can then take a PCA—super easy, just run a PCA with two components, plot them out.

And here's a bunch of OECD countries, projected. I got a bunch of information about them: how wealthy they are, how educated, how much crime there is, how satisfied people are with life. Project them down into two dimensions.

And I can try… well, first of all, we can sort of look at the countries. We've got South Africa way up there, and we got, on the other side, sort of Switzerland and Germany and Japan.

On this piece, which countries are probably better off?

I've been to South Africa, I've been to Switzerland. Switzerland's sort of a happier, more stable, lower crime rate country than South Africa. South Africans are wonderful, but the country's a little problematic.

And you can see, in many ways, countries that in some sense are similar: Japan and Korea are close, Australia and New Zealand are close.

So I've got some visualization, but it's hard to tell what these things are without seeing the loadings, right? I haven't even shown you what the data I downloaded was.

### Examining the Loadings

So let's look at the loadings of the first principal component for these countries. And what do we see? And loadings can be positive or negative.

**Question:** Is the overall sign of an eigenvector meaningful?

If I swap the sign on $\mathbf{v}$, is the sign of the eigenvalue meaningful?

The eigenvalue matters, right? But the eigenvector is arbitrary up to a sign flip.

By the way, occasionally this causes problems. You run the same code twice, and there's numerical round-off, and you get an eigenvector that's entirely different, because it's precisely the negative of the other one.

Right? So, be careful not to over-interpret the sign of the eigenvector. It's a vector, but if you put a minus sign in front of it, you have not changed the equation at all. And the code is, like, "too bad."

**Question:** Does the magnitude of the eigenvector matter?

We've normalized it, they're orthonormal. A standard eigenvector has norm of size 1. So the size is arbitrary—the magnitude is always one, so it's not helpful. 

Okay.

Anyway, so let's look at these and see what we see. So on the positive side: having dwellings without basic facilities, labor market insecurity, unemployment, homicide, and people working long hours.

Okay, when I say positive, I guess I don't mean that in the good sense, I just mean that the sign happens to be positive. On the negative side, all sorts of other things.

Right? And you can see some of them are less important, like how much you spend on houses, and, uh, whatever—stakeholder engagement for developing regulations and voter turnout. And other ones are more important, like time spent on leisure, oh, feeling safe walking alone at night. Very good.

Right? So if you consider Switzerland versus South Africa, I walked alone at night in both of them.

In Switzerland, all the women I talk to say, like, "Why would you worry about walking alone at night?"

In South Africa, the restaurant says, "Don't worry, we'll send an armed guard to walk you to your car a block away."

And I'm like, "I live in Philly, I walk around West Philly at night, I know what danger is..." They're going, "Thank you very much, the armed guard will walk you to your car."

### Interpretation of First Principal Component

Okay, so what do we have? We have a whole bunch of things that are highly correlated, right?

Remember, we have the first principal component. It's the direction of maximum variance. These things are all taken from the correlation matrix, and we've got some pretty good question of which things are good and which things are bad.

Make sense? I can go back and look at the first principal component. Ignore the vertical one, we're on the first.

Bad: South Africa, Mexico, Brazil, Turkey, at least in terms of this principal component. 

Good: Switzerland, Iceland, Norway, all better.

It says UK, so where's U.S. in here somewhere?

Oh, we're not… are we not in here? Maybe we're not in here. Oh, there's the United States. Yeah, yeah, United States, we're over, sort of, here, just to the right of Denmark.

Okay, we're not doing too badly. I mean, we're not pushing the edge, right? Switzerland is winning here.

Safe to walk at night.

Okay? So, note what we've done, is we've said, "Hey, can we visualize where countries are in the first principal component? Can we analyze what's going on? Can we get some set of things that all vary together and see how much each of them are important?"

Yes?

**Student question:** PCA is linear, yes, it's totally linear.

In some sense, PCA is useful for interpretation, right? This is explainable AI, if you will. We're doing interpretation.

### Example: Predicting Sexual Orientation from Faces

Does PCA help with prediction?

So let's think back. I talked about the example of looking at faces to predict gay or straight.

And remember that what Michal Kosinski did was he took a bunch of faces, ran them through an autoencoder neural net to get an embedding, then he took a PCA of the embedding he got, and used that PCA in a logistic regression to predict gay or straight.

Why did he… He's not a dumb guy, he's a professor at Stanford. Why did he take an embedding, which was already a perfectly fine embedding, and use PCA on it before he went in to run his regression?

It's a linear transformation, it doesn't make any difference at all mathematically. Why do it?

**Student answer:** Dimensionality reduction, yes, and why does he care about dimensionality reduction?

More features, less statistical power. You don't have enough labels, usually.

Right? So he had 35,000 labels, it's nice, but he was worried about overfitting. And he could have just run a ridge logistic regression, which would have worked fine on his dataset. Instead, he did something that's extremely similar, which is he took a PCA of it, did a dimensionality reduction, then fit it. But he fit far fewer parameters in his supervised learning than he would have had to had he fit it on the raw embeddings.

The raw embeddings were high enough dimensional that he thought with his pathetic 35,000 labels—right?—he thought he would be safer off doing something… Frankly, I probably would have just run a ridge myself, but it's pretty similar. But in some sense, you're always worried about overfitting, because you don't have enough data.

Right? How many faces can you download? Millions, easily. How many people who have said, "Hey, I want to date a man," "Hey, I want to date a woman?" That costs you something. Those you have to ideally pay people, or steal them—I'm not sure about his ethics—but, you know, you have to worry about the data.

Cool!

### Second Principal Component

We can also look at the second principal component, and it will also have a set of loadings. These ones are frankly harder to interpret. I'm not going to spend a lot of time on them.

Yep.

Yep.

Yep.

**Student question:** Doesn't the sign of the… you have to be consistent, so let's… there's two different questions. One is:

There's a nice final exam question. If $(1, -2, 1)$ is an eigenvector, is it the case that $(-1, 2, -1)$ is an eigenvector to the same equation?

And the answer is… yup!

If one satisfies this equation, the other one will, if and only if, right? So the eigenvalue… the eigenvectors are arbitrary up to a sign.

If this is an eigenvector, this is just negative of that eigenvector.

Now, interpretations: the fact that 1 and -2 are opposite signs is critical. But does this one—is dwellings without basic facilities being positive or negative—mean anything?

It doesn't. You have to be consistent if you're using this as a $\mathbf{z}$ in a regression. You better always use the same eigenvector. You can't swap the negative of it.

Right? In your regression, so once you pick one, you should stick to it and be consistent. But as to which of these mathematically is the better eigenvector, the sign matters within the vector, but again, you can pick either one of these (positive or negative versions).

It's not going to tell you whether it's good or bad to have dwellings without basic facilities. Whether that correlates with happiness, if you predict happiness, you'll have the same prediction of happiness. If you swap the sign here, you'll swap the sign of the coefficients in the principal component regression.

Yeah. Yeah.

Cool.

### Variance Explained

However, when I look at the equivalent—I showed a scree plot before, which was the eigenvalues getting smaller. I said you could look at percentage of variance explained.

If I keep increasing number of principal components and ask how much variance is explained, the first principal component explains about 95% of the variance. And if you add the second principal component, it's up at, like, 99%.

The marginal contribution of adding in a second principal component in this case to my principal component regression to predict well-being—pulling the question "How satisfied are you with life?"—is pathetically small.

Right? The first principal component captures, again, 95% of the variance. I don't even want to look at the second principal component. And the third one… I can't even see that small a difference.

There's always a tiny bit of noise, right? But it's pretty much the first one's nailing it.

So again, if you think about analyzing stuff, if you're picking the hyperparameter of how many principal components (right, I wrote number of features, but I mean number of features in the regression, which is number of principal components I kept in the reconstruction), one pretty much does it.

Cool.

## Word Embeddings: Eigenwords

Shifting to another example, what I like to call **eigenwords**—trying to get some feeling of embedding words. And again, large language models like Claude or Gemini take tokens, which are pieces of words, typically of order 40,000 of them (but it varies), and they map each of them to an embedding.

And it's all magic, but it turns out you can do the same thing really easily with eigenvectors. And I think it gives a little insight as to what's going on.

So I want to sort of show how one might take some word counts, and how you might build them.

### Simple Corpus Example

And so I took here a really small corpus. I couldn't afford to write a trillion words up there, so I wrote a few, like:
- "I ate ham"
- "and you ate cheese"
- "and you ate"

So I've got 3 sentences in my training data.

I can take each word—ate, cheese, ham, I, you—and I can say, how often does each word come before it? How often does each word come after it?

Right? So this is a word, and this is its context.

Right? Most of the modern language models only use a left-sided context. Right, so if you think of GPT-4, it's what's called a **causal model**, in the sense that its context is only the words before, not the words after, which is nice for generating words.

Make sense? But plenty of other models use both right and left context on words. If you already have your corpus and you want to embed stuff, it's often useful to have a right context as well as a left context.

Make sense? So I've got statistics on word versus word before, and word versus word after.

### Distributional Similarity

And the hypothesis behind all of these is called **distributional similarity**: words that have similar meanings should have similar contexts, and they should be mapped to close embeddings.

Make sense? So, a really obvious principle, but this is really what's happening in the large language models. They're taking the words before (the context), they're using that to predict the next word, and when you find an embedding of a word, or a token—I'm going to use those interchangeably today—it's based on showing up in similar contexts.

Cool. So what I want in the end is to get from these discrete tokens of ate, cheese, ham, I, and you to a vector embedding, that I can actually plug into something like a neural net, or a machine learning algorithm, because I can't actually put tokens directly. I could, I said before, embed them as one-hot.

But one-hot, what's closer in one-hot embedding? Ham and cheese, or ham and I?

They're the same distance away. They both differ by two positions, right? If you're one-hot, everything is equally far and equally close. You can't compute sensible similarities in a one-hot embedding space.

Right? It's either the same word or it's not! It's not a particularly helpful embedding to go one-hot.

So we want to put these in a sensible embedding space where I would hope that "I" and "you" are closer and more similar than "I" and "cheese."

Make sense? And if you think about it, if you look at these, cheese and ham both follow "ate," they are both things that one eats. That's a semantic piece, but statistically, they're both likely to follow the word "ate."

"Ate" is likely to follow either "I" or "you."

I and you have nothing before them, they're likely to start in sentences. And they're likely to have, um, "ate" after them, or be followed by something we may call a verb, if we had such a notion.

Cool?

### Computing SVD on Co-occurrence Matrix

Now I can take this matrix, this co-occurrence matrix, and in reality, these things are big, right? And the old ones of a decade ago, eigenwords would have, you know, 5,000 or 10,000 words, and in a modern context, you might have—a small one would be 40,000, the big models have a million words in their context.

But they're still putting it in context of words.

Cool. I can now take this matrix here, and I can compute SVD on it, right? It's just a matrix.

It's going to have a set of left singular vectors, which will be embeddings of the words. It will map each word to an embedding in the space, in the context space. I can also compute right singular vectors, ones attached to the other side (the context), which will give me an embedding of the context.

### Embedding Contexts

Is it useful to have embedding of contexts?

If you go to OpenAI and go to GPT-4, they'll let you type in a whole sequence of words called a context, and you can download from them the embedding of that context. I use that all the time.

It's a super nice feature, because it maps any sequence of words in basically any language to an embedding. I can then take that embedding and throw it into a machine learning model along with some labels of it.

Was this a happy sentence or a sad sentence, or from a depressed person? Was it a confused sentence? Was it a direct question, indirect question? Whatever label I have, I can take the embedding that GPT gives me of its context, and I can then train up a stupid simple model, right, that says, given that context embedding, predict my label.

I could also directly ask GPT, "Hey, here's the context. Was this happy or sad?" Which is better or worse than training up my own model from their embeddings?

Sometimes it's better, sometimes it's worse. Often it's really good if you're dealing with sort of typical American things that show up in lots of contexts, then it's hard to beat them. If you're looking at subtle differences—how is somebody from rural Alabama different from somebody from downtown Philly? Oh, even how is someone from Mumbai different from someone from downtown Philly?—GPT is not so good at capturing a lot of those pieces. A little bit of supervised learning helps.

But it's semi-supervised, right? The context embedding is based on, say, a trillion words of self-supervised labeling, right? A bunch of words, predict the next token.

Given that embedding, I can now take a pathetically small sample—a thousand labels, a few hundred labels even, right?—and use that in learning.

Cool.

### Projecting to Low-Dimensional Space

So, we're gonna take this vector here, we're gonna project it to a low-dimensional space, right? Computing the singular vectors, I can now have a mapping from each word here to a singular vector. I can now measure the distances between them.

And the right distance to use usually is an $L_2$ distance, because all of our SVD is sitting in $L_2$ land, right? Everything we did was compute the singular value decomposition, right? Remember, PCA is optimal for $L_2$ reconstruction.

So we're sitting in $L_2$ space. So these things give us embeddings that really are $L_2$ similar. And the left singular vectors are going to embed the words again, the right singular vectors are going to embed the context.

### Visualizing Word Embeddings

And I can now take any set of words, so I just grabbed a bunch of words, I took a moderate-sized corpus, I just ran SVD on it, which is pretty cheap. I have an embedding for every word. I can take a set of words, and for every word, it's a vector, might be 300 dimensions as sort of a typical moderate, small one. A big one might be a thousand dimensions.

**Question:** Is that big enough? How big is 300-dimensional space compared to the number of words in English?

Very roughly, how big is the 300-dimensional space? 300 squared, 300 cubed… About $2^{300}$ would be a good approximation, right? Just to start thinking about it.

Words in English compared to $2^{300}$?

Smaller, right? There's plenty of room in a 300-dimensional space to put a million words.

Make sense? It's hard, because I have base 2 versus base 10, but trust me, 300 dimensions is pretty big.

So, I've got all these words, each embedded, in this case, for this example, in a 300-dimensional vector. I now take all those words, and I take a set I'm interested in, and I'm going to visualize them in a two-dimensional projection!

I can't plot 300-dimensional space. So what did I do? I took these 300-dimensional words just for these words, and I did PCA on them. And these are the first two principal components of these particular words, and you can see words that are close, like:
- density, viscosity, permeability
- barrels, meters, tons, degrees
- teacher, farmer
- boy, boss, husband, son, uncle

Laughter, okay, who knows? There are a lot of words. But in this two-dimensional projection, words that are close tend to be ones that are similar.

Make sense?

And everybody should get the two-step procedure. Large data set, find the embedding, then take the small ones of interest, and for these particular words, visualize them by putting them into two dimensions.

### More Examples of Word Clusters

I can take other ones. Agree and disagree—very close in distributional space. River, house, dog, car, home.

What's funny about drink and sleep?

Drink's a funny word in English.

Yep. It's a noun and a verb.

Right? So distributionally, nouns tend to look like nouns, verbs tend to look like verbs. Nobody in modern natural language processing cares about nouns or verbs anymore. We don't label them, we don't do them, but they're distributionally different.

And all of these embeddings are an average over all occurrences in the collection, right? All of machine learning is just averaging. PCA's averaging too, and you can see that these words like "talk" are, in fact, averages over all the talks in all their contexts.

And talking is somehow close to pushing and sleeping, and far away from listening and caring, and really far from dogs and homes.

Right? So you sort of get a noun-verb average. And again, these are purely embedding the tokens, we haven't gotten to the context.

Make sense? If you add the context, you could tell apart the different kinds of "drink" (to drink versus a drink), which are different contexts and show up in different probabilities.

We can take pronouns! I love it! He and she are close. I and we are close. His and her are close.

Right?

You can take numbers! Distributionally, 1, 2, 3, 10 is very far distant from, mostly from 1990, 2000, 2001, 2004, 2008, 2009, 2010.

What's interesting about these numbers? They're not really numbers—I mean, they're… you could argue they're digits, but they're not really numbers.

They're years, and they're very far apart.

And what's interesting about 2009 and 2008 versus 2000, or 2004?

The closer the year is, the more distributionally similar it is.

Right? And what I'm trying to give you is a hint: if you look—this is not a deep learning course, it's just a machine learning course—but if you look at a modern deep learning model, think of Claude or Gemini, what it's doing under the hood is it takes each of the tokens, like 2001, and it maps it to a vector embedding like this, and it puts it into a model, and then it optimizes using gradient descent.

And it generalizes in the sense that 2008 is much closer to 2009 than it is to 2000.

Right? And I've, of course, shown this only in a two-dimensional projection. Remember, these sit—for the simple model, which is small enough to run on a tiny pen computer—this sits in a 300-dimensional space. OpenAI will use a bigger embedding space, but it's still the case that things that are distributionally similar are close.

Yeah.

**Student question:** Why is it closer? 

Remember that this is a small data set, because I had to run it on a tiny machine I had. I don't have a trillion tokens worth of text. And remember that I'm showing you a two-dimensional projection.

I think the thing to note is these projections are not perfect.

Right? What you're getting is something that's good enough to predict the next token, or to compute the correlations between these things, and they're not perfect. They're impressive, but they're not exactly the same. And I have no idea why that one is closer or farther.

Right? So I'm not claiming it's a perfect representation, I'm claiming there's a lot of signal here. And if you had a larger data set, you'd have less noise, and the signal would be cleaner.

### Names and Gender/Formality

I picked a bunch of names! Trisha and Betsy and Patricia and Joseph. There's Tom and Joe and Mike and Dan.

So again, I've taken—this is the same data set, same 300-dimensional embeddings, everything. This is the two-dimensional projection.

What does the second principal component of names represent?

What's the difference between Patricia and Margaret, and Mike and Dan?

Sorry?

There's male and female, so let's look at them. We've got Jennifer, Liz, Betty, Dan, Michael, David, George, Charles.

So there's certainly, I think, more importantly, the first principal component mostly has females to the left and males to the right.

So, the first PCA—are male and female names distributionally different on average?

Sure! Even "Mr." or "Ms." would be a distributional difference. So I think the main first principal component is capturing gender, which shows up very strongly in language.

And the second one?

It's nicknames, right? Mike and Dan is different from Michael or William, right? Bill and William. It captures formality.

Do nicknames and formal names show up in different contexts, statistically?

Sure! "Yo, Bob!" versus "Yo, Dr. Robinson."

Right? There are clear differences.

So, note that these capture style differences as well as content differences. They're all captured in the embedding.

And note this is a dumb, simple embedding. This is simply an SVD. I know, because I wrote it myself, yeah?

It's funny, I'm looking, and it's like, are these the Beatles here? But no, it's actually not the Beatles, it is a certain style of things, and I think many of these things are embedded. I also don't know how much…

It turns out that the Bible shows up a lot on the internet, and so if you're not careful, you get things that are overrepresented.

I've seen, and I've done various—I do a lot of Twitter. I got billions of tweets. I find all sorts of weird names, including… I got one thing, like, what are all these guys? Like, oh, right, it's One Direction.

So they were doing a tour, and somehow people were tweeting out the tour, so all sorts of things tend to co-occur.

### How Training Data Affects Embeddings

And remember that these are a mixture of all the Georges that show up all over the internet. It's George Washington, and any George you can think of all averaged together.

How much does each contribute? Based on how many times it shows up.

Right? The more times it shows up in the dataset, the more repetitions.

Oh, right. Imagine you're hired by OpenAI, or that's one that's more public, Meta. I actually know what they used. You're designed to make the next Llama.

Do you worry about how many copies you have of each document in your training set?

Hugely, hugely, hugely! Two copies of Wikipedia would be good, let's over-represent that. Twitter, let's downsample it.

Right? So, machine learning engineers spend a lot of time, if you have the luxury of having, say, the entire internet—say you were Meta—you spend a lot of time trying to figure out how much to represent each of these, because your embeddings and your predictions are going to be highly driven by the training data.

And almost anything important shows up many, many times on the internet.

Somehow my news is being flooded with 6-7 people.

Yeah, okay, you have the same problem, too. I was wondering if it was just me. I'm glad to hear it. It comes from Philly, but it's really not important to be in my training data set.

Yeah.

Yeah, how are they spread out? I don't… I mean, it's hard to interpret the details of what's going on, I don't know.

### Context Embeddings

Okay, so that was embedding of words or tokens. The other piece I wanted to just say very briefly is that you also want to be able to embed contexts.

And in the eigenword thing I said, that was the other—we had the left singular vectors, the right singular vectors, which is the context. Every context. Remember, we're trying to decompose both the words and their contexts, and every word—like port and tender—every word in English, almost, and most languages has lots of meanings. It has different pieces.

And so, in fact, you'd like an embedding for the word that depends upon the words around it.

And that's precisely not the tokenization which maps from a token to an embedding, but when you run through the neural net, the other side of it gives you the embedding of the full context.

And now, the same token, given its full context, will give you a different embedding.

So the contexts have embeddings. I think I'm going to say very little about that.

Cool.

### Historical Context: Word2Vec and BERT

Oh, I did want to say… yeah, just one piece to note that classically, people used to sell these things separately, so there were old things called **Word2Vec**, which was an embedding of each word. In the more recent era, people have models such as **BERT**, which are an autoencoder sort of model that maps words and context to embedding.

And in the modern era, the thing you should note is that if you go to GPT, they will have an API that lets you take any context—any sequence of tokens—and produce an embedding.

Make sense? Just a super useful tool to know about.

Great.

Good! So that finishes the first half of today, which is applied PCR.

## Autoencoders

And what I want to do is switch topics slightly, but not entirely. And these are all posted slides.

Now I want to take—we've looked at linear. Everything we've done has been linear so far this week. We've looked at linear embeddings, we've looked at linear autoencoders, and I now want to talk about **non-linear autoencoders**.

Cool!

### Concept of Unsupervised Neural Nets

So we'll see a few of these. So, sort of a classic concept of an unsupervised neural net is: can I take the concept behind PCA and generalize it with a non-linear $\phi$, right? 

The PCA says, given a vector, hit it with some linear transformation projection $\mathbf{V}$, such that you get $\mathbf{z}$'s that you can optimally invert (pseudo-invert) to get back the $\mathbf{x}$'s. Make sense?

And make them orthogonal.

So, autoencoders take in an image, or a text, or whatever, and try and do the same thing again to avoid reconstruction error.

You need to—if you take something, if you embed it into a big enough space, you can perfectly reconstruct it.

So, either you have to have an **information bottleneck** (right, pick $K$ smaller than $p$, so you compress it somehow), or what you can do in neural nets is impose **sparsity**—put an $L_1$ penalty on the embedding. Now it could be bigger in dimension, but forced to be sparse.

Make sense? What you don't want to do is memorize the trivial encoding, which is: take the thing, encode it to itself, reconstruct itself.

And we'll see a couple ways to avoid that.

### Denoising Autoencoders

And one of the ones that was popular for a while—it's out of fashion this year, but I'll cover it because it may come back in fashion—is you take an $\mathbf{x}$, take an image, add noise, take 10% of the pixels and make them black.

Now use that in a neural net, a supervised neural net. Take that image with 10% of the pixels blacked out, and predict the same image (the original, clean image).

Make sense? Super cheap to generate. Take as many images as you want, randomly block out 10% of the pixels, and reconstruct it.

So I've now taken a supervised learning algorithm—pixels to pixels—and I've done an unsupervised version of it.

Make sense? It learns to embed an image in a way that optimally reconstructs that image.

So, those are called **denoising autoencoders**. And again, a piece of jargon you should know: "denoising" in the sense that I add in the noise to get the $\mathbf{x}'$ (noisy version), and I take the original $\mathbf{x}$ and make that the $y$ (the target). Very cheap way of generating lots of data.

Cool.

The classic first ones, mostly done in convolutional neural nets like we saw before, take $\mathbf{x}$ in, make $y$ be the image.

Note that the intermediate neural outputs, the activations, are in fact embeddings.

There was a period when people used a bunch of methods in NLP, like BERT, which is the same idea. You take a sentence, remove 10% of the words in it, and predict the sentence.

Right? And again, sentence—each token goes to an embedding, but 10% of the tokens, I just put zeros for the embedding, and I predict the original word.

So with either images or with text, with pretty much anything, you can throw away 10% of it, predict the other 10% of it, and that turns out to be very nice when given to a supervised learning algorithm as a way to learn embeddings.

Cool. Very easy, but such a nice idea.

Cool.

## Independent Component Analysis (ICA)

A more complicated embedder, and to do that, I want to show something that's similar to PCA, but a little bit different.

There's a whole class of embeddings that people use that involve the same concept of: take some input, transform it to a new space in some way that has some constraints on the new space, but allows me to optimally reconstruct the original image, or the original text.

Make sense? One of those that's popular in electrical engineering is called **independent components analysis (ICA)**.

And let's do it sort of formally, because it's going to look almost exactly like PCA, but not quite. So I get to review PCA.

### ICA Formulation

So given a set of observations $\mathbf{X}$, right, my usual $n \times p$, find some weight matrix $\mathbf{W}$, such that the transformed—

$\mathbf{S}$. So what used to be $\mathbf{Z}$, I am now calling $\mathbf{S}$.

One tends to think of these things, these embeddings sometimes as a state, or a hidden state.

Right? So, let me just rename things. $\mathbf{X}$ is a bunch of observations, or percepts. $\mathbf{X}$'s are sound, image, whatever. And behind this $\mathbf{X}$ are some hidden states, the true nature of the world that you want to know, from which you will reconstruct it.

Make sense? So, I'm now switching to follow ICA notation, but $\mathbf{S}$ looks a lot to me like $\mathbf{Z}$.

I might want the $\mathbf{S}$'s to be as independent of each other as possible.

So, how do we… what piece of math do we use if we've got a bunch of things and want to have them be related or unrelated to each other?

Sorry?

So you do a joint probability distribution, and we quantify how similar probability distributions are. We always use KL divergence, right? And ICA uses a bunch of them.

### Independence and Disentanglement

One is, I would like each of these channels (to use the word for each of these latent states)—right, I got an image coming in, I break the image into 10 channels, 10 features, 10 latent states—I'd like each of these latent states over time, over observations, to be as uncorrelated as possible. I want to maximize the KL divergence between them. I want them to represent different information.

And in neural net land, I want to **disentangle** them. I want to make them less correlated.

Right? And there's a bunch of ways to do that, but my favorite one would be to use KL divergence.

There are other ones which you're not going to worry about.

Usually people pre-process them by whitening them, which is something I will not put on the midterm or final. But the same way with a single feature, you standardize it—you subtract off the mean and divide by the standard deviation—with a bunch of features together, you might want to not just divide by the standard deviation, as now we have a covariance matrix. So I might want to hit them with something that makes them look more like Gaussians, independent.

And I get out of something that's really similar to PCA, but it's not quite the same, because instead of just making things orthogonal for perfect reconstruction, I'm now trying to make each of the channels be as independent as possible.

So you can't use SVD directly. You have to use gradient descent or something, which is nice.

Oh, talk to me, computer!

### Terminology: Disentangle

And again, it's probably worth knowing the jargon term that the neural net people often want to either **sparsify** the representation.

So if you read the latest Anthropic papers, they take deep learning models where everything is a distributed representation, and they try and retrain a sparse version of it, such that something like Golden Gate Bridge will get represented by one neuron, rather than by a linear combination of a whole bunch of them.

Or, if you will, it's sometimes called, I want to **disentangle** the different activations.

Because you'd like, in some ideal interpretability or controllability world, to say each neuron shouldn't sort of represent something somewhat different.

And that's not achievable perfectly, right? You never get a true PCA where they're truly orthogonal, but at least you can aspire to putting in something in your penalty that says, hey, force these things not to have too much correlation between each other. Not too much mutual information.

Does that make sense? That's the fancy word: to disentangle the information. In an original image, all the information's entangled. Every pixel tells you a lot of everything else, sort of.

Whereas you hope you'd have a representation that's more disentangled.

### ICA Reconstruction

So, given a bunch of observations $\mathbf{X}$, I want to find $\mathbf{W}$ and $\mathbf{S}$, such that the $\mathbf{S}$'s should be independent. I can then reconstruct $\mathbf{X}$, just like I did with PCA, such that $\mathbf{X}$ will be approximated by:

$$\mathbf{X} \approx \mathbf{X}\mathbf{W}$$

which is what I called $\mathbf{Z}$ before, multiplied by $\mathbf{W}^+$ (pseudo-inverse). Now again, like with PCA, you're not gonna perfectly have an inverse, but you'll have a best approximation, right, in some $L_2$ sense to the inverse.

And if you do that, you say that:

$$\mathbf{X} = \mathbf{S} \mathbf{W}^+$$

Right? And your hope is that $\mathbf{W} \cdot \mathbf{W}^+$ gives you something very close to the identity matrix.

Right, so this should look a lot like PCA, with the one piece that I said I want the sources, the $\mathbf{Z}$'s, to be independent.

So the $\mathbf{S}$'s look like principal component scores, the $\mathbf{W}^+$ looks like the loadings, which we also called the principal components.

Right? Loading's the weight, the numbers of the original components.

So this looks like taking $\mathbf{X}$ in terms of principal components and the weights on them. Right? It's mathematically exactly the same, except I have, instead of orthogonality, I've got high KL divergence between the $\mathbf{S}$'s.

Same idea over again. Cool.

And I can then do $\mathbf{X}$ in a basis expansion, like I did before. And this is an autoencoder. It's now taking $\mathbf{X}$ and encoding it as $\mathbf{S}$, and then decoding it.

Cool!

### Formal Optimization Problem

So having done that, let's be a little more formal.

Again, I'm going to minimize what last time I called distortion, or what I will equivalently call **reconstruction error**.

And I sort of want to apologize for the jargon. I sort of want to say, hey guys, part of what I'm giving you is a crapload of jargon, so if somebody asks a question in a job interview, you've heard the jargon. So I both apologize for the jargon, but that's what it is.

And you can see this really is a reconstruction error, right? And I put the two—I could have put an F for Frobenius.

Right? The mathematicians prefer the F for Frobenius. I just remember the two better than the F. But it's the same thing, right? It's a Frobenius norm.

How far away is the $\mathbf{X}$ matrix from the $\hat{\mathbf{X}}$?

$$\text{Reconstruction Error} = ||\mathbf{X} - \hat{\mathbf{X}}||_2^2 = ||\mathbf{X} - \mathbf{X}\mathbf{W}\mathbf{W}^+||_2^2$$

Which is $\mathbf{S}$ times $\mathbf{W}^+$, so it's $\mathbf{X} - \mathbf{X}\mathbf{W}\mathbf{W}^+$.

So I want to minimize the reconstruction error in a Frobenius or $L_2$ sense, and I want to minimize the mutual information between the sources.

### Mutual Information

Where the mutual information sort of looks like the entropy of each of the $K$ different sources, summed up, compared to the entropy of them all together.

$$\text{Mutual Information} = \sum_{k=1}^{K} H(s_k) - H(s_1, s_2, \ldots, s_K)$$

And I'm being a little hand-wavy, and we're not going to go into the math, unless it shows up in the homework or the recitation. We won't actually force it.

But the idea should be clean, right? That what you're doing is saying, hey, I want big KL divergence, low mutual information between these. If you view real-valued numbers, it's worth noting that entropy looks like:

$$H = -\int p(x) \log p(x) dx$$

Right? For summation, integration, same idea.

Yes, so that's the core concept: you want to minimize the $L_2$ reconstruction error, and you want to maybe keep them independent.

### KL Divergence Between Activations

Cool, let's shift to neural net land. Now, before I do that, let me just say one more word about this. I want to have the chance to actually remind you about mutual information between things.

What we're talking about is the KL divergence between the joint distribution over all of the $Y$'s (in this case would be the activations, or the hidden state, the embeddings) versus the information, the probability, if they were each independent.

$$D_{KL}(P(Y_1, Y_2, \ldots, Y_K) || P(Y_1)P(Y_2) \cdots P(Y_K))$$

Right? If the $Y$'s were truly disentangled and fully independent, the KL divergence would be zero.

Right? But in fact, you never get a perfect disentanglement. You always have some correlations between your activations, and this is a nice way to quantify how much correlation there is across all of the different $Y$'s.

Right? And again, the $Y$'s here are the $\mathbf{Z}$'s, or the $\mathbf{S}$'s, or the embedded state, or the activations of the neural net, right? These are all the same thing.

If you want them disentangled, I want them to be as close as possible to independent, so I want to make this KL divergence small.

I'd like to make them look as if they were independent, because I'm trying to drive these—the smaller they are, the more interpretable they will be. You can look at one embedded feature by itself.

If you don't do something like this, any one activation in a neural net tells you almost nothing.

Cool.

## Autoencoder Architecture

So let's talk a little bit more about autoencoders.

I think I've said most of this, but let's look at a picture. We're gonna have an $\mathbf{x}$ input, say an image. We're gonna go through a bunch of layers of neural net, typically through a bottleneck in the classic older versions. In the modern versions, often not so bottlenecked, but with some more regularization.

And we expect, and we'll see pictures as we get later, that the earlier neurons here will be doing feature detectors—low-level features—and the higher ones will tend to be more object-level. We then get to some embedding $\mathbf{z}$.

We then feed that into another neural net, which is typically an exact mirror image in architecture. It goes from the embedding back up.

We can now train the whole thing all the way through. The first half is an **encoder** (more jargon)—maps $\mathbf{x}$ to $\mathbf{z}$. The second half is a **decoder**, maps $\mathbf{z}$ to $\mathbf{x}$.

And if we do, for example, a denoising autoencoder, all we put is a noisy $\mathbf{x}$ here, clean $\mathbf{x}$ there, and we don't have to do any other supervision.

Make sense? No data labeling. Unsupervised.

But it looks like supervised learning. The noisy $\mathbf{x}$, the clean $\mathbf{x}$ are, in fact, given to the exact same code I would use for supervised learning, except the output here is same-dimensional image as input.

### Loss Functions

KL divergence, $L_2$ norm, what would I use, probably, for this one?

Cross entropy? We love cross-entropy. But are we asking, how good is a picture of the pixels of an image compared to the image?

It's probably an $L_2$ norm. This is not a probability distribution. We're sitting in pixel space, RGB space—RGB by number of $x$ by number of $y$, maybe RGB pixels. Right, so this is much more an $L_2$-type loss function, rather than a KL divergence one.

Great.

## Linear vs Non-Linear Manifolds

So, one way to look about what happens is: in linear space, we're taking every point $\mathbf{x}$, and we're mapping it onto a low-dimensional—okay, new word—**manifold**, a hyperplane in the space.

Right? And in the case of PCA, we're mapping it onto $k$ dimensions. That's a $k$-dimensional subspace of the original full $p$ dimensions.

And it's mapped down to that linearly—mapped down to that low-dimensional space, and everything is linear in PCA.

In neural net land, things are no longer linear, and if you take a bunch of images, so here's a bunch of images of faces, I'm showing them in sort of a two-dimensional PCA space.

All faces sit close to a low-dimensional subspace in the space of all possible pixels.

Right? All possible images—really big space.

Make sense? Faces, especially if they're centered more or less in the center of the screen, are all mostly living very close to some small subspace within that big space.

But it's no longer a linear subspace. It can be quite non-linear, right? So you could picture, sort of, I can't do it in three dimensions, but I could say, here's a manifold that spins around like this.

Right? Here's a subspace, a two-dimensional subspace in a three-dimensional space.

And like I say, all faces sit sort of close to that. Well, two faces aren't so good, but you can often do a good job, and as you—this is, like, one guy smiling and frowning, and whatever—as you move your face around, you move locally within a fairly low-dimensional subspace.

So I want you to take away this intuition that says that PCA maps things to a linear subspace, deep learning models map things to a non-linear subspace, but it is important to know this: that all faces are relatively close, and you can have dimensions in that subspace to look older, younger. 

Have you seen these cool websites? You can plug in your face: "What will I look like at 80? What did I look like at 8?"

They're really good. They're just remarkably good. And what they do is they say, "Hey, take your face, map it to embedding, now find the direction of increased age in the embedding space," which is a direction where the manifolds are gonna be bent, and it just moves you in the direction of increased age or in decreased age.

They're quite fun.

Cool.

So, in some sense, an autoencoder is like a nonlinear PCA. Or to put the other part that's mathematically correct: a PCA is a linear autoencoder.

Right? It's an autoencoder, but just very particular, strong constraints. Cool!

## Case Study: Google/Stanford Autoencoder

We got a couple more pieces. There are some hacks—actually, no, I'm gonna skip the stacking. I think the stacking one is just not that important. Let me instead tell you about just one little case study. 

There's a group led by Quoc Le at Google and Stanford, they did one of the early papers. You see famous people—Andrew Ng would be probably the most famous of that set of co-authors. He's all over the internet. He's like the 6'7" guy—every feed I have is from Andrew. He's a good guy.

They took a bunch of images and said, "Hey, can I learn an autoencoder for them?"

And it's old now, but I think it's still very relevant and clean.

And what I've done is pasted the exact equations from their paper. Stolen from a slide from Quoc. Thank you, Quoc.

### The Optimization Problem

And what they're minimizing is something that is a reconstruction plus ICA-style penalties.

And it's a little bit hard to read, but what they have is something that looks sort of like our equations, but I gotta warn you: often—I've always done $\mathbf{X}$ with $n$ being the number of samples going this way, and features going across that way.

About 30% of people use the exact transpose. Occasionally, I ask GPT, "Hey, generate some nice LaTeX equations for me," and half the time it generates the transpose of the way we do it in this class.

So, do be careful when you read things, when you talk to people. I have tried to be super consistent.

Right? $n \times p$, not $p \times n$, but Quoc and a bunch of other people here who are just as smart as I am chose the other direction.

So, what does he have? He says, I have:

$$\frac{1}{m} ||\mathbf{W}^T \mathbf{W}\mathbf{X} - \mathbf{X}||_2^2 + \lambda \cdot \text{(penalty term)}$$

Cool. So what does he have? $\mathbf{W}$ times $\mathbf{X}$, but remember, he's got it backwards for me, because it's transposed.

The $\mathbf{W}$ is, in fact, like the eigenvectors mapping it, and then the $\mathbf{W}^T$ hitting that is now mapping it back again.

Make sense? So he's taken $\mathbf{X}$, mapped it to $\mathbf{Z}$, and mapped it back again.

And rather than using a nice F, like a good mathematician, he just uses a 2 because, hey, who cares—Frobenius or $L_2$ norm squared? So this is precisely the reconstruction error.

And then he's got a $\frac{1}{m}$ to divide by the number of points, and he's got a $\lambda$, because you have a hyperparameter of how much regularization you're going to apply. Some people also put the regularization in front of the loss functions. I like to write:

$$\text{Loss} + \lambda \cdot \text{Regularization}$$

But other people, including the SVM people and Quoc, put the regularization hyperparameter in front of the loss function. It's exactly mathematically the same, he's not wrong.

Make sense? And then he has adding on to this some other penalty, which we'll come to in a second, and it's such that $\mathbf{W}\mathbf{W}^T = \mathbf{I}$.

Oh, he's making the $\mathbf{W}$'s be orthogonal, just like a PCA.

Right? Except he's gonna do this whole thing with deep learning, and there's a whole bunch of stuff in the paper saying how sparse coding and autoencoders and ICA and all these things look really similar. And he's got something, this $H$ here will be an entropy sort of term, which I don't have time to cover in detail, but think of it like the mutual information KL divergence—make these things independent.

So now he's got a bunch of loss functions. He can then try and take this hard constraint and put it in also as a penalty and optimize the whole thing.

## Learning Features

And he takes an image—oh, we got mangling, this is sort of weird. I didn't know PowerPoint could mangle things—but runs it through a few layers of a nonlinear transformation.

And you can look—that's the last thing I want to cover in my last few minutes—is you can look at what sort of features it learns. And you can ask the question for any neuron: given any image, which image maximizes the output of this neuron?

Right? So it's the $\arg\max$ over images of the output of a neuron.

And what you find is the neurons in the early layers of the neural net tend to be detecting patterns. They detect colors, they detect textures, they detect edges.

Right? What you're seeing in the first layers of the neural net are things that are really detailed feature detectors—things that years ago, electrical engineers spent a lot of time building clever wavelet-based ways to extract, but now everybody just learns.

And if you look deeper, you can find, if you look far enough in a very large network, you can find some neuron that is maximally excited by these actual images in the space of real images.

Or if you do a search, a gradient search in the space of all pixels, sets you can put in, this is the pixel set that most maximizes the output of this one neuron.

Make sense? It's learned to be a face detector! This is a fairly deep one.

Right? I don't know if he has it—yes. I also got a lot of press on the news at the time. There's a different neuron that maximally responds to cats.

### Why Cats and Faces?

By the way, why is Quoc's and Andrew's, guys, why is it developing special neurons for cats and faces? What's special about them?

Has anybody ever gone on the internet? What do you see on the internet?

Cats and faces, widely available. But again, this is a statistical learning algorithm. What is it going to develop the most degrees of freedom on weights for?

The things that are the most frequent!

Right? The $L_2$ loss is averaged over all items in the training set. If you want to reconstruct images taken from the web or YouTube (this was done with Google), images from the web have lots of faces and lots of cats in them.

That's an efficient way to reconstruct an image: learn a good representation of a cat or a face.

Make sense? So it's not a coincidence. The $L_2$ loss is averaged over the training set. They didn't do a clever sampling to down-weight cats. They just took whatever they got.

My internet, at least, is full of cats, and I try to—I like cats, but I like real ones.

Okay.

## Summary

So, the key ideas:
- First of all, this idea of a **denoising autoencoder**, a way to make your own supervised training data from unlabeled data
- Note that adding noise is like regularizing. It prevents overfitting, well, at least reduces it, prevents memorization
- We talked about **ICA**—like PCA, but where you're actually driving the separate channels or $\mathbf{Z}$'s to have low mutual information, disentangling them
- I talked about neural nets being autoencoders as well. They generalize PCA or ICA
- We have not covered **variational autoencoders**, which we'll probably come to much later in the course
- I didn't talk about stacking, so I'll remove that

And the last thing I wanted to note is we've talked about linear world, where you want to have a basis that's smaller than the original space. You can never have more principal components than you have original features.

Doesn't work that way, but in the non-linear land, deep learning land, you will often start with some feature set and expand it to something bigger in space than the input space.

And because you're doing non-linear transformations, you can learn something that's a richer basis. It has more features in the transform space than the original, and the fancy math term is this would be called an **over-complete basis**. This is sort of more than you need.

And a lot of the current deep learning systems are not just information bottlenecks going down, like PCA, but actually take a feature space and make it bigger.

And of course, then you're gonna have to force them somehow to be disentangled or not too correlated, otherwise they will memorize everything perfectly and be useless.

But it's pretty common in modern deep learning to learn a larger basis space, and by the way, a couple of you got it wrong on the midterm: if you're doing kernels, can I have something that has more kernel dimensions in the feature space than the original one? The answer is sure.

After I take a small feature space and make it bigger, such that in the bigger space, things are decoupled and are then good representations. So lots of embeddings now are, in fact, big and over-complete, and then you take these embeddings and use them in supervised learning.

Awesome, awesome! See you all next week!

And I'll put up the—I know some of you didn't have time to get the peace here.

## Post-Lecture Discussion

**Student:** Canvas. Yes, there is, and I'm gonna pull it up in just one second.

**Professor:** I started saying it, and I interrupted myself. So there wasn't one last time? Yesterday, the world was down. Two days ago, yep. 

There's a dataset that has, for around 1,000 patients, it has, like, their EHRs and things with, like, simplified clinical text information, and then synthetic genotype information from each patient, and then it's focused on cardiovascular disease. And we were thinking about trying to model, like, longitudinal patient trajectories of, like, output of probability distribution for, like, at this time point, there's the probability of, like, these different disease states.

But it—like, a lot can be done there, and it feels really complicated, and we've been talking about different modeling approaches, and what the best way to make use of the data is. We're hoping we can talk about what is, like, maybe a more effective alternative.

**Professor:** Yeah, so I think what to do is email me, or we can talk at—for today, if you want, because this is not going to be a short conversation. Yeah, yeah.

So let me just triple check, but I think—yeah, so if you want to come to my office, 420C Levine Hall, or today? 

**Student:** Yep.

**Another Student:** Hi. August 10 projects… I just have one query regarding…

[End of lecture transcript]

---

# CIS 5200 Machine Learning - Lecture 16: Loss Functions, Evaluation Metrics, and Recommender Systems

## Introduction and Project Guidelines

A little bit more math-y content today, because I'm feeling like we should do that.

Great! So let's see where we are in the projects.

### Project Expectations

Let's actually start with the projects piece. Projects, I think, are mostly clear, except for one piece: what am I looking for?

Let me say what I'm **not** looking for:

It's easy to go to Kaggle, download a dataset, run four methods in Scikit-learn, and put a table that shows the results. You can pretty much do that at the \$20 level of subscription to a modern deep research tool. For sure, with a \$200 one, you could do that with one command.

So what do I hope you will actually do? I know some of you I've talked to are doing that sort of thing. As you take a real problem, things get messier.

#### Real-World Challenges

Often the datasets are too small, and so you have to use some supervised learning. You might need to find some other dataset that learns an embedding. How do you map from things that are categorical? I'll talk a little bit about that. You know, we have labels on things, sets of things, discrete things to an embedding—that's a feature set.

Often things are multimodal, and you have to combine different models together with some sort of ensembling or combination method.

Often the loss functions that you use are not the standard ones that are built into scikit-learn and standard packages. So we've talked about standard loss functions: $L_1$, $L_2$, cross-entropy. That's awesome, and often you'll be optimizing those. But in general, the real world has some utility function—something you care about, which is the real loss function.

Right? Like, how many dollars will I make? Oh, but maybe not just the expected value of the dollars, because losing a whole bunch of money would be really bad. So maybe there's some non-linearity in my utility function.

Often, accuracy is not the right measure. Why is accuracy a terrible measure for almost everything in the world? Why don't you care about the percentage correct?

**Student response:** They're more correct than wrong, maybe, so... but yeah, so what's the problem? Is there anything wrong with getting 95% correct?

**Student:** Yep.

So, one problem, but it's not the typical one. The niche one is often the test set that you're going to look at looks different than the training set. Often there's distribution shift. You have one training set you're training on, but you want to apply it to a different dataset. That's an interesting case for machine learning to think about. But that's not why accuracy is bad, or at least not the main one, yeah.

**Student:** Cost of false positives and false negatives are different, right?

**Exactly!** Saying you have cancer when you don't is not the same as saying you don't have cancer when you do. Now, cancer's rare, happily, but there's still an asymmetry, right?

And so, as you choose methods and think about optimization, often you're going to say, what is the real piece? And so I'm really pushing in the projects is to find something that you know enough about to think about what makes sense for a loss function here, what makes sense for other information I could pull into it, what makes sense that I can do something that's more interesting.

So, lots of this is building in some sort of **inductive bias** or doing some sort of **semi-supervised learning** or thinking about what the right loss function is. So I'm looking mostly for something where you do something that's more interesting than just uploading your dataset into a standard package and running it.

Make sense?

#### Using AI Tools for Projects

A few people have asked, is it okay to use GPT to generate ideas? Good, bad?

**Student:** Bad, I got one bad.

No, it's good! Brainstorm with it. If you hope that GPT will tell you what project to do, it'll give you emotionally boring ideas. If you say, "This is what I'm thinking about doing. What are 3 good things and 3 bad things about it? How could I make it more interesting? Can you find me datasets?" Think of it—you've got an assistant to help you. Ask questions, so feel free to use it to help.

If you try and do it all the way with GPT or Claude or Gemini, you'll get a crappy project, and you'll get a crappy grade. But it's not cheating, it's just doing mediocre work. But feel free to use it to find things and ask questions and understand and brainstorm.

Do I have to write the software from scratch? No, but document where you got it. Your report should say where you got it.

Okay, other questions? What can I tell you about the project?

Good, I've been talking to a number of students. I think it's been super helpful. The ones who've talked to me already are smiling over there. So do reach out to your TAs, reach out to me, and we will give you lots of feedback. Should be fun.

I think the deliverables are all documented. Clear? Cool!

## Loss Functions vs. Utility Functions

Want to do a few things—two things today. One is we'll talk about measuring loss and accuracy. Talk a little bit about missing values, which we'll come back to and do the math on next week. We'll do imputation, but we'll do at least some practical piece here.

But one of the big things I want to hammer is this **distinction between the loss function you optimize for and what you actually really care about in the world**. And very often they're different.

### Why Are They Different?

Why? Let's just take something simple like logistic regression. What is it optimizing for? What's the loss function?

**Student:** It's... it looks like a cross-entropy sort of thing.

Remember what logistic regression was? It was saying the log odds are a linear function of the data, or it's an MLE estimate for a particular probability model, right? It estimates the probability of some outcome.

In fact, neural nets mostly estimate—if you're doing categorical neural nets, they're estimating cross-entropy, they're estimating the log probability of the correct answer.

Is that usually a utility function, in the sense of what most people care about? No, in some sense, what are you doing with these things? In the end of the day, you're often **making a decision**.

### Probability Estimation vs. Decision Making

Statisticians like to separate two things:
1. Build a probability model and estimate the probability of some event (what I might call a **score**)
2. Take the probability and threshold it to make a decision

Does that make sense? The two are really separate. One is modeling the world: write a model of the probability—a neural net or logistic regression—estimate that probability using an MLE, estimate the model. Then, given the probability, there's a separate problem: if the probability is bigger than 0.5, say yes, or if it's the most likely item, say yes. But the decision is **sold separately**.

Do I need to say, "Hey, if your probability is over 0.5, then you have cancer?" No, it's a totally separate piece, right? Estimating the probability is sold separately from the decision rule.

Or, you could be a machine learning person and say, "Hey, what I really care about is my true loss function. Let's put that into the loss function and optimize it," typically then using gradient descent, right?

So, as you look at your projects, you look at things in the real world, **distinguish**—this is a great thing for a final question—**distinguish between the loss function you're using to fit your model and the utility function you're using to evaluate the decision you made**.

And maybe you want to make them the same, maybe you don't.

**Student:** Yeah, great question. Why do people not train directly to optimize utility?

### Why Separate Probability Estimation from Decision Making?

And the first thing is that many big companies—I worked at Google—do they train directly on utility function? Right? Gradient descent on expected lifetime earnings from a person, right? So often you **do** train directly.

Why might you want to do these two things separately? Why might you... you're trying to decide whether to give someone a loan. Why might you first estimate the probability of their defaulting, and then separately say, "Hey, given that probability of defaulting, I will make a decision to give it to them or not?" Why separate the two of them out?

**Student:** Gives a little chance for reasoning, it's a little bit better. I can stop to think a little bit about, hey, I want to trade off this probability and do things, yeah?

And the probability may not be the only thing you care about for the decision—it may be other information. The other piece is that sometimes **your probabilities are more stable than your decisions**.

We should put it differently. Your decision depends on lots of things. What's the current interest rate out there? How much money do I have available? Right? So often, I've got a fixed pot of money I'm going to allocate over a bunch of projects.

If I can estimate for each project how much money it's likely to make me in return, now I can build my portfolio. But note that trying to simultaneously optimize everything—what's my portfolio of projects that optimizes my return?—that's a complicated problem. And it's hard, and it may change over time. I get more funding, I have less funding, the interest rate changes, the economy changes.

Whereas, "Hey, for each project, how much money am I likely to win or lose?" That's maybe a much easier piece. So often you're **feeding information to a much more complicated decision process**. And so, optimizing some part of it may be more stable, easier, more understandable.

And in general, the more data you have, the bigger your company—if you're Amazon, fine, you could try and optimize your entire supply chain simultaneously, and just run the whole thing as one big reinforcement learning algorithm, right?

But if you're a smaller company, you might want to estimate something much more mundane, like, "What's the probability of this person buying this next week?" And you don't have enough data to fit a whole reinforcement learning algorithm to say, "Here's how I should optimally price and distribute things."

Make sense? So, in general, the world's really complicated, and you're going to think about how to break it up into pieces as engineers. And there's not one right or wrong. You should both be thinking about, "Hey, can I optimize the actual thing I care about?" And, "Hey, are there pieces I should break up? Which of those are stable? Which of them will still be the same if I move to a new market or a new environment or a new population?"

### Covariate Shift

By the way, if I train a model on people at the University of Pennsylvania Hospital, and then they go to Bryn Mawr Hospital, what's the same, what's not, right? The fancy word is **covariate shift**. The distribution of $X$'s is different at Bryn Mawr than it is in West Philly.

Make sense? If you have a good model, it'll generalize well. If you don't trust it, yeah, okay, maybe I want to get some estimates of things I can make decisions on.

Good. Super important concepts.

### Decision Documentation

You'll make a bunch of choices as you go along about how you want to do these, and one thing I'll look for in the final project reports, and one thing I look for when I hire data scientists is: **What decision did you make, and why did you make it?**

There are an awful lot of hyperparameters. Why did you pick the ones you did? There are a lot of model forms you could do. Why did you pick that one?

And we're talking a little bit about interpretation. Depending on your problem, sometimes you're happy to just make predictions, and sometimes you want explanations. "Why did you make that decision? Why did I do that?" Again, for your given problem, some of them require explanation—"Tell me why I should believe that's the right decision." Other ones, it's like, "Fine, closed loop, just do it."

There's not a right or a wrong, but for your problems, think about: is that important?

Cool!

## Missing Data

I want to say a word about missing data. We're going to come back next week and look at imputation, which is how statisticians handle missing data. Machine learning people just mostly do something really hacky. You should look at a lot of yours and say, "By the way, why not just throw out observations with missing data?"

**Answer:** Almost anything I have has something missing in it. If I throw out all the data that's missing, then I got nothing left, typically. I might have a million observations and, like, 20 of them are complete.

You just don't know everything about everyone, and stuff's not there.

### Handling Missing Values

A typical sort of version would be: imagine giving—I need a vector that goes into my neural net or my gradient tree boosting. Take the missing item, and say if it had a real value, put in 0, and add one more feature to say, "Was it missing or not?"

Okay? And does the fact that something's missing matter? For example, if it's not missing, often things are 99% missing, and the 1% that's there is still useful, so I wouldn't necessarily say it's there.

### Missing at Random vs. Not Missing at Random

There's a key question: is something **missing at random** or **not missing at random**?

#### Example: PhD Admissions

I did PhD admissions for computer science here. You're supposed to put your GPA on the front page. Some people's GPA is not on the front page of the application.

Missing at random? Should I replace it with the average GPA?

**No!** There's a funny phenomenon. The people who didn't put their undergrad GPAs on, people who didn't report their SAT scores—they were much worse. It's **not missing at random**.

#### Example: Dementia Assessment

I had a friend who was building a questionnaire for measuring dementia of people's parents. And one of the most informative questions was: "Does your mother have trouble remembering to turn the stove off?"

If the answer is, "I don't know," that's not good.

Make sense? The fact of missingness is important.

### General Rules for Missing Data

In general, **do not throw it away**.

If it's categorical, if you have things that are red, green, blue, or missing, fine. You can just call it one more category. And you're either going to one-hot encode it, or you're going to embed it.

**Student:** Yeah, so how do we...

We'll cover how we do this in a sec—hang on a second. So, in general, for things that are categorical, right? One of a thousand categories, or five. You're either going to make it one-hot, or you're going to learn some sort of embedding. An interesting question for your problems is: what's a good embedding?

If it's real-valued, you could replace it with the average, but again, you want to know the fact that it's missing. So let me try and be concrete.

### Encoding Categorical Variables with Target Averages

Here's a super cool, weird trick that people use, and maybe we'll come back and look at the math behind it, because I asked more from Chris for math, but...

If I have something where $X_1$ is either $A$, $A$, or $B$, and here I have $X_2$, and here's my label $Y$:

| $X_1$ | $X_2$ | $Y$ |
|-------|-------|-----|
| A     | ...   | 1.5 |
| A     | ...   | 1.5 |
| B     | ...   | 4.0 |
| C     | ...   | ... |
| D     | ...   | ... |

One thing I could do for the $A$'s is replace them with the average of the $Y$'s. Replace the $B$'s with the average of their $Y$'s. Replace the $C$'s with the average of their $Y$'s, the $D$'s with their $Y$'s. I mapped a categorical label to the actual $Y$ value.

#### Is This Cheating?

Is this cheating? One thing you should—I worry about tremendously is cheating, because cheating...

**Student:** Cheating why?

I've used an element of the output with input, but what's the gold standard of making sure that I'm not cheating?

**Answer:** Did I touch the $Y$'s in the test set?

This is only fair if you do it in the training set. So I can take the training set and take every feature that's missing, and replace it like this with this mean value. That's still legit, as long as I don't look at the $Y$'s in the test set to do it.

Now, there's a danger of overfitting. If too many things are missing, and this has too much variability, I'd have to regularize, I'd have to cross-validate to make sure I'm not overfitting. But it's still legitimate.

Right? So the rule that you're going to enforce really, really is: you're going to take a test set, and you're going to hold the test set way out and not look at it, and you can do whatever you want in the training set. Yeah.

**Student:** Yeah, if there's a missing value in the test set that you haven't seen before, now you're out of luck, right?

You could put in the global value of all the $Y$'s in the training set, for example, right? Anything you want to do, as long as you didn't touch the test set.

**Student:** Yep. When I have the real input from when I go to run this on the real world, again, every time in the real world I see an $A$, I'm going to replace $A$ with 1.5. Every time I see $B$, I'm going to replace it with 4.0, and if there's no $C$ in the training set, okay, maybe I'll replace it with the grand average of $Y$.

But I need some sort of rule I make up only looking at the training set that doesn't look at the test. Yeah.

**Student:** If $X$ doesn't represent $Y$ very well, what's going to happen?

Remember, I'm giving this to gradient tree boosting, or linear regression, or a neural net, or something. If the feature is not important, it should get low weights on it. Either a feature is... so think of this again as a **preprocessing step**, right?

We've done this a couple of times. We said, "Hey, take the $X$, map it to some new feature set $Z$, now train your model on $Z$." This is a new way of building a $Z$.

Right? I could have taken $A$, $B$, $C$, $D$, and looked up in some other dataset and looked up for an embedding of them, right? Found a neural net that maps them to something that maps all these objects to some embedding.

Here, I'm just doing a different form of embedding them. I'm just saying, "Hey, take each of these features and represent it with the average $Y$ in the training set for it."

### Example: Zip Codes

Right? Imagine these are zip codes. The U.S. has—we're in 19104 right now—we've got a lot of zip codes. You could either take the zip code and learn an embedding of the zip code and put that in your feature for your marketing algorithm, because, hey, zip codes encode all sorts of information, like how much income and education people have, and what sort of products they'll buy, and how many students, all those.

Or you could take your zip code and look up and say, "Hey, in this zip code, what's the average wine value?" And then say, "We'll plug that in."

**Student:** Yeah, why are you not using the category as is?

You could. So how—great question. So could I... if I wanted to take—assume there are 26 categories, $A$ through $Z$—how could I encode those as a feature?

**Answer:** I could one-hot encode them.

Why might I not want to one-hot encode zip code?

**Answer:** There are too many possible zip codes in the U.S.

I've tried it, and that's even the five-digit ones. We also have nine-digit zip codes. Even the five-digit zip codes, I rarely have enough information to take the five digits of zip code and one-hot encode them.

Right? I need to do something with them, embed them somehow, right? Or take some sort of average of something for that zip code. Look up for that zip code, what's the average age, income, whatever I care about. But I need to map the zip code somehow to something that's much lower dimensional, like the average age or income or gender, or whatever I care about that might be predictive.

Does that make sense? So often, you're taking something that's a category, and you're thinking, how do I embed it in some space? Because one-hot is too many of them.

### Example: Amazon Products

Right? And imagine you work for Amazon, and you have, ballpark, 10 million products. You could one-hot the 10 million products, but it's really sparse! Right? It's fine to one-hot, you know, Harry Potter books, because there's a lot of data on them. But there's most books and most products—right? Most things that Amazon sells today, they didn't sell yesterday. Right? The long tail.

So categories, if you have a lot of categories, one-hot is just not enough observations. You're a startup. You've got 10,000 customers and a million products. You still need to pitch the product to a person somehow. You're going to have to find some sort of way to start learning a model and embed them that makes a reasonable thing.

Now, you might cheat and just say, "Hey, GPT, here's my customer, what product should I sell her?" That's okay, but that's an embedding. You with me? Yeah.

**Student:** Why am I taking $A$ as the average of the $Y$'s?

Because in the end, I'm trying to predict $Y$, and that's a pretty good prediction of what $Y$ should be.

**Student:** If they have multiple columns, if I have a different column, I could also do the same thing for the other columns?

If $X_3$ and $X_4$ are also categorical, we could also replace each of them with the average $Y$. If that's really rare, it might help, it might hurt. I don't know. Nope, this is a choice. I'm not saying it's the right thing to do. Sometimes you'll one-hot embed it. Sometimes you will find third-party embedding. Sometimes you'll replace it with the $Y$ value.

So, this is the sort of thing you'll be exploring, and what I'm hoping is you find datasets as you look that are interesting. Interesting datasets are the ones, for example, that have categorical objects in them.

Right? They've got genes with SNPs in them. They've got credit card data, but not PCAs. Someone has to be able to...

### PCA and Missing Data

**Student:** Can I take the principal components of the credit card data and use that?

I'm like, you've just thrown away all the stuff you knew about what was going on in the credit cards by PCA-ing them. You shift from a coordinate-based system where I could look at the credit card information, think about it, think about how to embed it, think about how to deal with missing data, to something that somebody else has given you a PCA on it.

Whoever gave you the PCA of that credit card data **made all the important decisions before you saw it**. They've already handled the missing data for you. I hate it when people do that for me, because they often do a really bad job about thinking how to do the missing data and how to do the categorical data.

Right? Someone says, "I'm going to give you the PCAs of my raw data." I'm like, "Why are you hiring me?" You've already made all the good or bad decisions and hidden them from me.

I got nothing against PCA, it's a great method, I use it a lot, but before you can do PCA, you've got to deal with the missing data. You've got to deal with the categorical data. You've got to figure out how to scale the data.

Oh, crap, they didn't even tell me, was the PCA data based on scaled or unscaled data? Is PCA scale invariant or not?

**It's not scale invariant!** How you scale the data is hugely critical to the success or failure of your PCA. I'm not going to go to Kaggle or to my boss and download a PCA that they give me. I don't trust them.

Right? So this is what I'm looking for you guys to be doing, is thinking about how do you map raw stuff where the stuff has different features of different sizes and different—sometimes missing, sometimes not, but often categorical—and to think about how to process that in some way that makes sense. And all these nice things like scale invariant are important because it means do I have to worry about that or not?

Okay, good.

## Loss Functions and Real-World Applications

So, I think I said about all of this. Okay, so I'm going to do a little bit of stuff which will be—most of you will have seen, and I'm going to review this slightly too fast to remind you to go back and just put into your cheat sheet the names of these things.

And I said several times, think about the loss function and the utility function, and not to confuse the probability you're estimating with what you care about. I said all this before.

### Example: Housing Price Prediction

So, let's just take a simple case. I'm going to do a linear regression. I'm trying to predict housing prices based on how many square feet the house is, and what zip code it's in, and what condition it's in, and how many bathrooms it has, and whatever.

Make sense? Common problem. Zillow does this sort of stuff.

What's the loss function that linear regression is optimizing?

**Answer:** $L_2$, squared error.

#### Residual Plots

What does the residual plot look like? We haven't covered residual plot, cool.

So, let's look at the housing price—the actual housing price—and let's look at the residual for each point, as a function of housing price. Uniform, non-uniform?

First of all, the residuals for linear regression should be mean zero. But that's the mean. It might look something like...

Why? Yeah, let's look at it. Here's a \$10,000 house—there are a few of those in Philly, not great. Here's a \$100,000 house price. Here's a million-dollar house. Here's a \$10 million house. Not many of those. We got a couple in Philly.

And what's the error going to look like on a \$10,000 house versus a \$10 million one?

**Answer:** Much bigger error.

Make sense? The magnitude is more! Can you estimate the sales price of a \$10 million house, plus or minus a thousand bucks? Or 10,000 bucks? No! Whereas 10,000 bucks... okay, could actually go for a negative value. I'll pay you to take my house, it's so crappy. But not very much, right? This is maybe 10,000 plus or minus 10,000. This is 10 million plus or minus 2 million.

Right? Crap, did I use the right loss function?

#### Using Log Transform

What might make more sense?

**Answer:** Yep, log dollars.

The answer is always you should be looking at something like log dollars. Log dollars are just nicer than dollars.

Right? So often, you should say, "Hey, just upfront, transform all the $Y$'s into $\log(\text{dollars})$." And you're just going to get something much more sensible.

Because now the errors are much more closer to homoscedastic—much more of the same across the different ones—and I'm not driven entirely by the three houses up here, because, hey, I could fit the model, and none of this error matters at all.

Right? If I'm looking at squared error, anything under 100,000 bucks is not even worth worrying about. I got one house at 10 million? It's the entire loss function. Everything else here doesn't matter.

### Homoscedasticity Assumption

So as you think about loss functions, you want to think about, "What do I care about?" And note that you have an assumption that the residuals are drawn from Gaussian noise. But these are not homoscedastic, right? They're not the same noise across the function of the different $Y$'s.

So dollars is a terrible loss function, even if what you sort of care about is dollars. So, again, what you want to do for many things is look at log dollars, concentrations if you're a chemist, sometimes temperatures like that—each doubling of the temperature is comparable in the physics world. Absolute temperature. Not Fahrenheit, or something stupid.

Right? But so think about what it is that makes your data look good. If you're building a probability model like neural nets, think about, if you're doing real values, think about what the loss function should look like, and think about what pieces are.

If you do log dollars, this is going to look much more sensible.

### Non-linearity

If it's non-linear—so the noise piece—you also might want to put in something that can be a neural net instead of a linear regression. But is a neural net going to solve the problem of the high non-linearity?

Maybe, sort of, maybe not, but I wouldn't count on it. Right? You're really better off thinking in terms of log dollars, and you're going to get cleaner results.

Okay.

## Evaluation Metrics for Classification

### ROC Curves

Do people know ROC curves? Not enough. People don't know ROC curves? Yeah, okay, we got 50-50, the worst possible combination.

Great. So, we'll get there.

So, in general, as you saw, for binary classification, there are four possible outcomes. You have something either truly being yes or truly being no, and you can either call it yes or no.

#### Confusion Matrix Components

You can have:
- **True Positive (TP)**: Correctly predicted positive
- **True Negative (TN)**: Correctly predicted negative
- **False Positive (FP)**: Incorrectly predicted positive (Type I error)
- **False Negative (FN)**: Incorrectly predicted negative (Type II error)

People also talk about Type 1 and Type 2 error, but I won't, because I think it's a terrible nomenclature.

And you have **accuracy**, which you can compute as a function of these:

$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

People like, in different fields, to measure trade-offs between false positives and false negatives differently.

#### Precision and Recall

The symmetric one is accuracy, which means just what you think it would, right? Percent correct. True positive plus true negative over all.

In the natural language world, my NLP world, people talk about:

**Precision**: How likely is it to be yes if you say it's yes?

$$\text{Precision} = \frac{TP}{TP + FP}$$

**Recall**: Of the ones you said were yes, how many were yes?

$$\text{Recall} = \frac{TP}{TP + FN}$$

#### Sensitivity and Specificity

In the medical world, people like to talk about:

**Sensitivity**: Same as recall

$$\text{Sensitivity} = \frac{TP}{TP + FN}$$

**Specificity**: If it is, in fact, no cancer, how often do you predict it's no cancer?

$$\text{Specificity} = \frac{TN}{TN + FP}$$

It's probably not worth memorizing these, but it's worth watching them, and as you read literature, write them down. I've looked them up 20 times, and I think I finally remember them now.

Questions? But I think the important thing is: note the asymmetry and think about how people in your area want to report the asymmetry. Usually either precision/recall or sensitivity/specificity.

### Asymmetry in Costs

Symmetry or asymmetry is: is a false positive and a false negative equal in cost? And it almost never in the real world is. I mean, it sometimes is. If there are small perturbations and low-cost things, you know... but most real-world problems have an asymmetry between a false positive and a false negative.

And I implore you, as you do your final projects, to think about what's a sensible false positive cost, and what's a sensible false negative cost.

Do you need to deal with the costs? If you optimize for accuracy, are you getting the best results in a world that has asymmetric cost? In general, no.

I'm not going to walk through these, because you're not going to remember them if I show them, and it would just be annoying. But if you haven't seen them, I suggest just take 10 minutes and work through the precision, recall, specificity—work through the numbers, play with them, and then psychologically, the fact that they exist is important. The jargon terms are not super important.

### Context-Specific Priorities

Okay, but let's think about it. In general, if you're doing medical diagnosis, do the doctors care more about—let's go back and say what these things are.

Do doctors care more about specificity or sensitivity?

We got a majority for sensitivity, but a non-trivial minority for specificity.

In general, **sensitivity is more important to doctors**.

If someone's got cancer, you really want to flag that they have cancer. Right? If they don't have cancer, and I say you do have cancer, bummer, you're getting a few extra tests. But you're not going to die from it.

Right? So the cost of missing a cancer is really high, because you go home and die, and the doctor feels bad and gets sued, and you know, it's just not a happy ending for anybody.

On the other hand, if I say you have cancer when you don't, you come and do a few more tests, I get paid a little bit more, and you go home and you live longer with just a little bit of stress.

Right? So when you're looking at cancer detection, you really don't want to miss the cancers. Make sense?

#### Spam Detection

What about spam detection? There we use precision and recall jargon in the spam world.

So I'm going to change jargon on you. I want to build a spam detector. Do I care more about precision or recall?

Or should I think about sensitivity and specificity? Let's see. Let's start with sensitivity/specificity.

I don't want to be... think about what I want for spam. I don't want to be throwing important mails to spam. But if a few spams creep in, it's a lower cost, probably.

Again, you can think of where your trade-off happens, but if you're building a spam detector for someone, the first question you should start thinking about is: what's the cost of false positives and false negatives? Very asymmetric.

Okay.

### F1 Score

One other jargon you should know in the NLP world, people like to report what's called an **F1 score**. The F1 is basically a geometric mean. It's:

$$F_1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

And if you think about what that does, if you multiply two things, you should always think of that as, which is that AND or OR?

**Answer:** AND, right? You want to have precision AND recall.

Make sense? If I want precision OR recall, can I do that easily?

I can make really good recall. Everybody's got cancer! 100% recall! No precision, but the recall's great. I haven't missed a single person.

Right? So having pure recall at the expense of precision is useless. Pure precision at the expense of recall—pick the highest, most likely person to have cancer. "Okay, you've got cancer?" Not everybody else is fine. That's high precision, but really low recall. I need both of them, right?

Is F1 a good score to use if you're a doctor or a spam detector?

No, it's one arbitrary—it weights the two of them equally. It doesn't want to think about the end goal that we care about.

Right? So it's a fine average, but it's an average. I mean, it's a geometric average, right? It's a product, but it's an average, just rescaled.

### ROC Curves in Detail

Okay, ROC—super important concept. **Receiver Operating Characteristic** curve.

ROC curve: I'm going to take, run my algorithm, and get out a score for every single observation. Right? Score can be a probability, score could be a score, I don't care.

By the score, I sort them from highest to lowest. Make sense? Highest score, highest probability.

"Will you buy the product?" I take everybody in my set, I sort them from most likely to buy to least likely to buy, as estimated by my algorithm, right?

And I could go down the list—the first one, the first 10, the first 20, the first 5,000, whatever—from top to bottom.

And I'm going to pick some threshold. Everybody above this threshold will be labeled as someone I should sell to—yes. Everyone below will be called a no. Make sense?

In probability, think of pick a probability. Everybody above the probability has cancer, by my prediction. Everyone below doesn't.

Make sense? And then I can see for each threshold what fraction are true positives.

And the picture looks sort of like that, so I'll sort of draw the picture on the board.

So we're sort of slowly... so I've got on the x-axis, everybody sorted from the highest probability to the lowest probability. Often we normalize them, so we'll divide by $N$, so 1 is 100% of the people.

And then for a given piece here, I can say, of the people up to this threshold, here's some threshold of people. Everybody below this, I can say, of these at the scoring piece, what is the **true positive rate**? What fraction of the total population did I catch?

Now, it's got to go up monotonically, because if I had more and more people, I'm capturing more and more of the whole piece there.

#### Random Guessing Baseline

If I were to put people at random, label them randomly as true or false, the curve looks like a 45-degree line, right?

If I randomly accept someone, the fraction of correct people I get goes up monotonically and linearly. Make sense?

So, that's an ROC curve from just random guessing.

#### Real ROC Curves

In reality, you hope you'll have a curve that typically looks sort of like that, which says that, hey, if I'm really selective, picking really good people compared to random guessing, I'm doing much better.

And at some point, I'm going to saturate, because I've got all of the cancer cases. I got all the yeses. And as I go up and keep adding more and more people in, I'm just gradually adding more and more. But most of the people are not cancer, because at this point, I've gotten 90% of the cancer people.

Questions? This is, like, a super nice way to look at it.

**Student:** This x-axis is... the fraction of the people that I'm saying... is this the prediction part, or this is the...

It's going to be the **false positive rate**, or 1 minus the specificity, but think about as I go up along this curve, I'm saying, "Pick all people that have a probability bigger than 0.9. All the people that are probability bigger than 0.8, all people that are probably bigger than 0.7, all people that are probably bigger than 0.6."

As I keep changing my threshold, accepting more and more people, eventually I'll accept 100% of the people, and I'll have gotten all the cancer cases.

**Student:** Yep. Why is the random one...

If you randomly take more and more people and say, "If I say that 10% of the people have cancer," and then I capture 10% of the cancer cases. If I say that 90% of the people have cancer at random, I'll capture 90% of the cancer cases.

Right? The true positive rate will be 90%.

#### Area Under the Curve (AUC)

What do I want? If I compare two different algorithms, what I want is to have something that's higher up—that's a better algorithm. And we typically measure these by the **area under the curve**, the **AUC**.

Random guessing is going to give what area under the curve?

**Answer:** 0.5, right?

Take that bottom triangle, it's half of a $1 \times 1$ square. It's got 0.5, so an AUC of 0.5 is guessing.

#### AUC Less Than 0.5

Sometimes I've had students come and give me an AUC that looks like that of 0.4.

How do you get an AUC of 0.4?

**Student:** Yep.

Yeah, but how does that happen? How can my students—and they're good, honest people—this is on the test set, not the training set—how do you get an AUC less than chance?

**Student:** Nope. That would be dumb but good. No, it's usually a different error.

**Student:** Yeah, that one doesn't happen much either.

No, the standard mistake people make is they **overfit**.

If you overfit to the data, you can actually build a model which is worse than random guessing.

#### Overfitting Example

Why is that? Because let's go back to, like, the third lecture of the class. Yay! And think about something.

I'm fitting $X$ versus $Y$. I got one $Y$ and one $X$, and I got a bunch of data. And there's an optimal model, which looks like, yay! And instead, I fit a really good model [highly complex, overfitted].

How well does that model do compared to random guessing? It's going to be worse in many ways, right?

So if you've overfit, you've done worse than just say, "Hey, for most cases, what's the best for classification problem? What's the best dumb model? What's the dumb baseline?"

**Answer:** Majority class.

Right? If something is 70% true and 30% false, the dumbest model you should use is "everything is true." You'll get 70% accurate.

If you fit an overfit model, rather than saying "everything is true," you'll be thinking you know, and you're guessing true, false, true, false, false, true, true, false, false, which is worse than just saying "everything is true."

Right? So, in a world that's noisy, the best model is one that's very simple. You want to regularize it.

So, pretty much the only time I've ever seen an AUC over 0.4 is somebody has overfit a model. And at least they reported—if I look at the training data, they're getting 0.8 on the training data and 0.4 on the testing data.

Cool! Or not so cool.

Right? So at the end of the day, I want from your projects the testing data, but it would be nice if you'd also report along the way the training data to see if they're pretty similar.

Good. So, that's ROC and AUC, all jargon you should know.

### Application to Search/Ranking

Let's think about things you're doing—Google search. You're looking for web pages. You put a probability distribution over the web. A, B, C—what do I care about? AUC? Where does Google want to be on the ROC curve?

Is AUC a good measure to use if you're trying to retrieve web pages? You get a probability for every page on the web, you sort them from the most likely you are to click on to the least likely.

**Student:** Sorry?

**Student:** C! We have a vote for high recall. We want to capture all the pages on the web.

I got a no. No, why not?

**Answer:** All I really care about for Google are the first 5, maybe the first 10 pages.

I want really high precision and really low recall.

If you look for a web page, do you hope to find all the pages? I'm looking for an Airbnb in Paris. Do you hope to find all of them?

No, you won't find all of them, and if you did find all of them, you wouldn't know what to do with them.

Right? You're looking for a high-precision, a very small number. You want to be way down on the bottom level. You want to be right here, right?

I mean, the only actual trick with Google is they won't try and give you the top 10, they'll try and find the top 500 or 1,000, and cluster them, because so many of them are duplicates. So they wanted to find the top 5 non-duplicate webpages.

**Student:** Yes.

Yes, let's do that! So, point A, way down here. I'm only picking the top 10 out of the billion possibilities. Is that high or low precision?

**Answer:** High precision.

High or low recall?

**Answer:** Low recall.

If I'm way over at the other point, C—high precision?

**Answer:** No.

High recall. I'm capturing everything, doing every possible thing.

The AUC in the middle—AUC is averaging them, right? That's precision and recall, right? Multiplied divided by the sum of them, right? So normalized. So AUC says precision and recall are roughly equally important. I'm going to balance them equally.

#### When to Want High Recall

When do I want high recall? I told you an example. Which one?

Someone help them out? Where do I want high recall?

**Answer:** Doctors, cancer, don't miss the cancer.

Right? Right? So again, sometimes missing something is really bad. Sometimes, there's a million things out there, just give me one of them. I don't need to find all the products I might buy in Amazon. That's not going to help me. Give me 3 of them that are the best three.

Right? So a lot of cases, we want very high precision. Sometimes we want high recall. Mostly algorithms are trained to maximize something like AUC, because it's the average over the whole area.

Make sense? Which is not what people care about. So Google is not going to optimize an algorithm for AUC.

### Optimizing Different Metrics

**Student:** Yes. So think about what you might be optimizing...

And let's think about two things. If I optimize logistic regression, is logistic regression closer to optimizing precision, or recall, or their average?

It's not emphasizing any of them, right? But let's think about what it is closest to doing. It's doing what? It's optimizing the log odds averaged over everything, right?

So it's averaging everything. It's sort of going to be more like a middle rather than either side of it. Make sense?

You can think of it differently. Does the logistic regression care more about really low probability events, or really high probability events, or are all events sort of equally important?

It's closer to an accuracy sort of measure. Well, it's not, because, unfortunately, it's got log probabilities, so it really cares about some of the extremes. But okay, apart from that problem, it's still symmetric.

High and low probabilities are exactly equivalent to logistic regression, right? We write it as 0 and 1, but if we were computer scientists, you'd write it as $-1$ and $+1$, and that would make it completely symmetric.

There is no asymmetry in logistic regression between low probability and high probability events. They're equally important to get right.

But for neither doctors nor for Google Search is that the correct thing to optimize. That makes sense?

#### Optimizing AUC Directly

Now, if I wanted to, could I optimize AUC directly? Could I take logistic regression as a model, right? It's an equation that gives me a probability from something, and could I use that to directly optimize AUC as a loss function?

**Student:** Yeah?

**Student:** A little louder?

**Student:** Yeah, pick a bunch of parameters, pick the parameters that gives you a model, calculate the AUC, and then I could do gradient descent on the weights.

Can I do a clean analytic derivative? Probably not. If I can't do a clean analytic derivative, what do I do?

**Answer:** I do a numerical one.

I change each weight a little bit. On the weights, I then see, "Hey, what is the derivative of AUC with respect to each of these weights?" And I can do gradient descent.

So you can use AUC as a loss function and directly optimize the weights using gradient descent. And I've had students do that—works.

Make sense? Any loss function I have that I could compute, I can do a gradient on that loss function.

**Student:** Yeah. When I optimize my AUC, what am I really optimizing?

I'm not sure what to say. I'm optimizing the AUC. Does that make my sensitivity or my specificity better?

**Answer:** Neither, right? It's the harmonic mean of them, it's the product of them.

Why do I want to optimize AUC? Sometimes that's a good measure of the overall performance of the system.

**Student:** You're... yes, AUC is good if you're not willing to commit to a threshold.

Right? Sometimes in life, you say, "Hey, here's the threshold. All I care about are the top 10 hits of my 10 million, or my 10 billion items." All I care about the top 10, right? So that's one sort of threshold. Or "I'm not going to—my threshold is a probability of 0.9."

Sometimes, you know the threshold in advance, in which case, AUC is a stupid measure. Sometimes you don't know the threshold in advance, and you want to have something that's agnostic as to the threshold.

**Student:** Isn't it more useful to determine the threshold?

Sometimes, yes, sometimes no. In practice, as an engineer, what I typically do is generate the ROC curve, then I show it to a doctor, and they then say, "Okay, this is acceptable or not, or here's where I would sit on it." And then, if you want, you could re-optimize for that particular threshold.

So, sometimes... I mean, again, each problem is different. Some problems, you know what some things you want an average good performance. There's not a right answer.

AUC is some function of your model. It's a loss function, right? But AUC is a function—I'll call it $G$, a loss function—of your estimate $\hat{Y}$ of $(X, \theta)$ and the actual $Y$'s.

You got a whole bunch of $Y$'s, you have estimates for them, which are a function of some parameter. I can take the gradient of AUC with respect to the parameters:

$$\nabla_\theta \text{AUC}(\hat{Y}(X, \theta), Y)$$

That's a legitimate gradient. It may not be closed form, but I don't care.

### Support Vector Machines

Support vector machines! Are they designed, in terms of their loss function, to do a good job at covering the whole spectrum? Or are they focusing their loss at some particular place?

**Answer:** They're focused, right?

Support vector machines don't give you a probability. They don't try and be accurate across all things. They're designed to ignore everything you got right and not to put too much weight on things you got wrong. And right and wrong already assume that you've got a specific threshold.

So SVMs are really designed around knowing the threshold in advance, unlike logistic regression, which for better or worse, cares about the whole probability range.

Make sense? And again, as you look at your real problems, you should be thinking about how do the properties you've been studying all semester—what the loss function is, what the functionality is, what the inductive bias—how does that fit in?

**Student:** Yes. Well, the SVM is a good question. What? Where's the threshold in an SVM?

At some level, the SVMs give you out a score, and the threshold is always set at zero, right? So think about how an SVM works mathematically. It says, "Hey, give me from this function a score, and if that score is positive, the answer is yes. If it's negative, it's no."

So there is by definition someplace which is zero. Above that is yes, below that is no, and you're not getting a good model of the world far away from zero, right? From the orthogonal hyperplane or far away from the decision surface in SVM. By construction, you're not modeling the data much. Make sense?

Particularly if you're very right—if you're outside the margin and correct, there's no reason you should get that right. So if you do an SVM, and then you want a high-precision model, you've said, "Hey, as long as I'm outside the margin correct, everything is equally good."

You spent no degrees of freedom fitting the super positive ones. And if you want to do high precision, it's going to be crap, right? You still get a score, you could actually threshold up there, but why would you?

Make sense? Thank you, good question.

Cool. Okay, onward!

## Confusion Matrices

Yeah, and when you go back to study, I posted the slides up on Canvas, you can look at the true positive, false positive, all these jargon terms.

The other jargon term you should know, which I look at all the time, are **confusion matrices**.

I just had a student today, and she's got 8 different emotions classified, and people disagree with each other, and I'm going, "Sonny, why don't you give me a confusion matrix? When people confuse two emotions, they're not confusing anger and disgust. They're going to confuse shame and embarrassment."

Make sense? Some emotions are closer to each other in embedding space, others are farther away, and I want to see, for a given prediction, what mistakes were made.

So we move away from binary classification to multi-label classification. Super useful to look at a confusion matrix.

### Multi-Class Classification

The diagonals are you predicted it correctly, so the true positives are on the diagonal. The off-diagonals are the errors.

Note there's no longer a true positive or false positive, because we're no longer in yes-no binary land, we're in multi-class land.

But in general, if you're making predictions across lots of categories, you're more likely to confuse a pickup truck with a fire truck than a pickup truck with a dog.

Make sense? And if you're doing error analysis, it's super useful to know those, because sometimes you find your labeling is so crappy that you can't even tell an agaric from a mushroom, because agaric is a subclass of mushroom, or your human labelers just couldn't tell these things apart.

So, the fact that you got something wrong is not always super informative. Sometimes you get things wrong that are just wrong—calling a dog a fire truck is just wrong. Whereas calling a dog a Dalmatian... hard to say that's wrong, it's just like your labeling was sloppy because it's part of the same category.

### Clustering Similar Categories

And often what you find on a big dataset is you should have clustered the Dalmatians—either you're doing Dalmatians, Chihuahuas, and Beagles separately, or you should have clustered them all together and just called them dogs.

So often, if you've got too many categories and are making too many errors, cluster them together, and you'll find you can tell apart the clusters in your supervised learning, even though you can't tell the individual labels.

Make sense? I work with people who do more genomic stuff. I find all the time the biologists give me, "Here's my 20 kinds of small RNAs," and it's like, "You can't even tell these two apart yourself. Why do you think my large language model will tell them apart?"

Right? So, things that sound like they're different are sometimes really different, and sometimes they can't be distinguished by the machine learning algorithm. And sometimes they can't be distinguished by the humans that are labeling them.

Make sense? So, confusion matrices are just a great thing to look at when you have categories.

**Student:** Yeah.

Cool.

## Summary: Loss vs. Decision

Yeah. Okay, I said that one.

Great, so wrapping up this piece... let me summarize, and then we're going to start Netflix, but not finish it. We'll finish on Wednesday.

So, distinguish between the **score** that comes out, which is probabilities for an awful lot of our models, or a real value, versus the **loss which is made after a decision**.

Right? And often you do want to estimate "How many dollars will it be—positive or negative?" and then you want a separate decision: "Should I take the contract or not?"

Make sense? The thresholding is often stored separately from the prediction, and sometimes you want to optimize end-to-end: "Should I take the job or not?"

But often you want to optimize some intermediate: "What's the probability I should take it?" or "How many dollars will I make if I take it?" Or whatever it is.

We talked about a bunch of confusion matrices. You should go back and review true positive, true negative, false positive, and all these crappy jargons like precision/recall, sensitivity/specificity, which I can never keep straight, but I mostly can now.

You should know ROC and AUC without having to look it up.

Cool!

## Introduction to Recommender Systems

Okay. Let's start thinking—we're not going to go through the solution, but I want to start thinking about a different class of problems. So this is one lecture today and Wednesday on a different class of machine learning called **recommender systems**.

Recommender systems are popular from organizations like Amazon and Netflix. Given a bunch of stuff that you might buy or watch, which one should I recommend to you?

Make sense? And it sort of looks like supervised, and it sort of doesn't, and it's a nice chance for you to review a bunch of stuff. And it's a nice way for me to show you, as you think about your projects, start from the problem, and then think about, of all the different methods we've seen, which ones apply or don't apply.

Cool! So it fits in my real-world category.

### The Netflix Prize Dataset

There is a classic dataset that Netflix released, which allows me to tell a whole story about it. They released a dataset of:
- **500,000 users**
- **18,000 movies**

Movies are pretty small compared to books, but this is still just the most frequently watched movies, not all movies they have.

And they had **100 million ratings**.

This is fairly typical of lots of recommender systems in terms of size, right? This is a portion of their data. But a portion that they're willing to share, and it gives a flavor of the size of things.

Big, small? How big is 100 million? In computer science terms? Are we talking megs, gigs, terabytes?

**Answer:** We're talking in the gig level, right? Bigger than a meg, less than a terabyte. We're talking gigabyte level sort of things.

So it's not hugely big, it's small enough that it's reasonably done for a project here. You can still run this on a reasonable machine.

### Sparsity of the Data

BUT! The data is really **sparse**.

Most people have not watched most movies. There are 18,000 movies. How many of you have watched more than 10,000 movies?

I know one guy who has. He's a professor of neurology here at Penn, very old, and has watched movies forever, and just watches movies all the time. He's in the English department. Anyway...

Right? If you look at the number—we got 100 million observations out of a matrix that is $500{,}000 \times 18{,}000$.

You with me? Picture the matrix? Picture this sparsity—it's a density of 0.01, because Netflix only gave the highly-watched movies. The real Netflix data is much more sparse.

But the toy dataset they released has 1 in 100 of the entries in the matrix with something in it. Make sense?

If you go look at real data from something like Amazon, it's much, much, much, much more sparse.

But! This way underestimates the sparsity. Why? Why does this 1% sound tractable, but why is it much lower than 1% in a reasonable sense?

**Student:** Yep. Few people are watching a lot of movies, most people are watching one or two movies, and a few movies—okay, a few thousand movies—are widely watched, and things fall off pretty quickly.

### Zipf's Distribution

For those who like distributions, almost everything follows what's called a **Zipfian distribution**.

Oh, I gotta say that, because it's so cool! Zipf said that for all sorts of things, if you look at the things that happen once, you've got a certain count of them. The things that happen twice are half as much as the main one. The things that happened 3 times, the movies that are watched 3 times, are a third as much as the top one. But things that are watched—the thousandth most likely movie—is watched a thousandth as much as the top one.

Why do things happen that way? That's a very long debate, of which there are hundreds of papers and no clean conclusion. But there are a number of generative models that will generate a Zipfian distribution.

And a lot of things are Zipfian. If you look at how often the number 1 comes up compared to 2 compared to 3, compared to 4, the number 9 shows up roughly $\frac{1}{9}$ as often as the number 1.

If you take addresses like 1023, it's about one over a thousand times as often as the address number one. And you sort of think about it, how many places are address number one? But it's weird. But Zipf distribution is incredibly common.

### The Long Tail

And what it means is that things have a **long tail**. And you can fit other distributions, like a geometric distribution—I don't really care which one. But the point I want you to remember is in most real data, the frequent data is frequent, and the rare data is rare.

Zipf started by studying language. The most frequent word in English? "The." And a fraction of total words? Maybe 1 in 10?

You get to a word like "backpack." You might use the word backpack twice a year. The word "macaroni" twice a year. There are a lot of words that sound sort of common that most of you know, but in fact, really don't show up very often. The frequent words are really frequent: I, you, the, a, some, and, and, right?

And there's a long tail.

So, anytime you look at data—real data—what you're mostly finding is stuff where there's a lot of something—cats on the internet—and a moderate number of things like dogs, and then a huge tail of rhinoceroses and elephants and whatever else you want, cheetahs.

Make sense? So, the data tend to have this distribution where the frequent stuff is really frequent, and there's a crapload of stuff that's quite rare.

But nobody thinks of an elephant as obscure, I think, or a backpack, right? Even though if you look on the internet, backpacks are just vastly obscure, right? 10,000 times as obscure as cats. I'm making that number up, of course.

Does that make sense? So what we have is this dataset which is really sparse in the sense that the people who haven't watched many movies haven't done many movies, and the movies that haven't been watched by many people have been watched by many people. There are a lot of movies out there without a lot of viewers, and that's going to sort of drive things a bit.

### The Netflix Prize

Netflix offered **a million bucks** if you could beat their baseline by more than 10%. Real offer. Million bucks, cash, U.S. dollars. All you have to do is beat the standard method they were using by 10%.

So, a lot of people thought this was worth doing. It looked good, and hey, it's a non-trivial amount of money.

They also set things up as you will probably do for your final project. They had:
1. A **training dataset** which they gave to anybody—you could download it
2. Then they had a **validation set**, or what they called a **quiz set**, which was a leaderboard where you could upload your algorithm, run it against it, and check to see how you did
3. And then they held off a **test set**, which you could not see

And if you were at the top of the leaderboard, and you beat the 10% threshold, then they took your algorithm and ran it on the test set and saw how it works.

So you can't be overfitting to the test set, because you only get it after you pass the 10% on the leaderboard, right? It was just to make sure people really didn't somehow cheat.

Make sense? And you can see that they're reasonable-sized, right? 1.4 million for each, which, again, is a sensible number.

The guy who designed it—his name is Elkins—is a reasonable machine learning PhD guy. Used to be at Columbia, designed this to make sure that this was not going to succeed by chance.

Cool!

## Supervised vs. Unsupervised Learning for Recommender Systems

So, last question for today. Supervised learning, unsupervised learning? What's $X$ and what's $Y$?

So, one version is $X$ is the users, and $Y$ are the movies. You could try and build a model from predicting movie ratings—did you like it? How many stars points did you give it? Or did you watch it?

That's sort of a hard model to fit, because it's one-hot over all the users—that's half a million users—and it's one-hot over the movies—that's 18,000 movies.

Unless you can do something for embedding people, you're sort of out of luck. So it sounds like a supervised learning problem, but it's vastly intractable with standard supervised learning.

Make sense? You can't one-hot all the people. There's no generalization across people, so that's not going to work.

### Unsupervised Approach

Unsupervised, I could group sets of people together that are common. Okay, and what makes some users close to each other? I need a distance metric.

**Student:** Sorry?

You can use the kind of genres that they like.

Yeah, unfortunately, the genre label—I and other people tried it—doesn't work very well. You could use the actual movies that they liked. If you and I like a bunch of movies in common, then, in fact, we are close to each other.

So now we start to be there. I can cluster people together based on having movies that we liked in common. Well, on Wednesday, we'll actually make this formal, right? So I have a distance between people, or similarity, based on joint movies. I'll have to formalize that in the missing space.

And then, does that give me a prediction? I've unsupervisedly clustered people. I can also cluster movies.

### Duality Between Users and Movies

Mathematically, is there any difference between a person and a movie in this world?

**Answer:** It's exactly symmetrical.

The size differs. There are a lot more people than movies. But that's a numeric detail.

Mathematically, every algorithm we do can be done two ways. You can cluster people based on the movies, you can cluster movies based on the people, right? A movie is similar if it's watched by the—liked by the same people.

Make sense? So there's a **duality** there that allows two sets of algorithms.

Cool, that's sort of fun.

## Conclusion

And we'll come back to it, so I'm going to leave us here, because we're at time, and I promise to give people time to go to other classes. We're going to come back in on Wednesday. We're going to go through and do a bunch of ways to solve the Netflix problem.

Cool, cool! Give me one second here.

---

# CIS 5200 Machine Learning - Lecture 17: Recommender Systems
## Introduction and Course Logistics

Hey! Yep, totally. Either wait till everyone has participated here, or we're dealing with a conflict with my office hours, but I think people will probably be good. So yeah, thank you.

Alright, I'm gonna get back to it. So we are going to cover recommender systems today. We started last time talking about Netflix, which I'm going to talk about in detail today.

My goal really is to give an example of taking the concepts from the first half of the course—supervised and unsupervised learning—and think about, as you attack an actual problem, what are the sort of considerations that come up in terms of what sort of algorithm to use, what sort of method of formalization, what sort of loss functions to employ. Recommender systems are nice because there have been a variety of ways that people have approached them, which often corresponds to the techniques we've covered before.

Let me remind people—and I see there's more and more people joining remotely on Zoom—we are giving credit for remote attendance, but it's really hard to see the board on Zoom. I apologize, but the course is optimized for people who are present.

## The Netflix Problem Setup

### Matrix Structure

The idea was we had a whole bunch of entities which we're calling **users**, and a whole bunch of **movies**. We've got a big matrix, and the matrix has in it, for example, ratings on a scale of 1 to 5: how much do you like it? And most of the entries are empty.

**Question:** How many... what fraction was empty in the Netflix toy problem?

**Answer:** 99% empty in the toy problem. In reality, it's 99.99% empty.

Right? Almost all people have not seen almost all movies. This is also typical. Usually, which is bigger: users or products?

**Users** are much bigger, right? This is a small sample of, whatever, half a million users, and whatever, 20,000 movies. But if you're Amazon, fine, you have, you know, hundreds of millions of users and only tens of millions of products. But it's very sparse.

### Problem Formulation

Cool. I want to just sort of think about how one might formulate that. So we've got users, movies, and some sort of a rating of user $i$ for movie $j$.

In some sense, what's the signature of the prediction problem—the machine learning problem? You could write it that you're trying to predict:

$$R_{i,j} = f(\text{user}_i, \text{movie}_j)$$

The rating of user $i$ for movie $j$ as some function of the user and the movie.

There are really two problems:
1. **Sparseness** - We're missing most of them, but maybe that's not a problem
2. **Categorical variables** - The other problem is that these are categorical

So, eventually, by the end of today, we'll get to say, hey, we could put these in and give some sort of embedding of the user and some embedding of the movie, and now we've made it back to a machine learning problem. But we're going to start by doing a whole bunch of ways that don't do embeddings, and then we'll see how they generalize to embeddings.

Cool?

## Dataset Details

Let me get rid of that, get rid of this. Okay, I said before, this actual training set—which I think you can probably still grab—has 100,000 ratings and is sparse.

### Supervised vs. Unsupervised Learning

**Question:** What class of problem is this? Supervised? Unsupervised? We talked about this last time, but...

**Student:** Unsupervised?

I don't know, this looks supervised to me. I've got an $X$, I've got a $Y$, I'm predicting $Y$ as a function of $X$. $X$ is user and movie. No?

There's not a right answer to this. What I'm trying to do is to sort of force a little bit of thinking about what's going on. Unsupervised and supervised sound really good, but a lot of things are **self-supervised**, right? Given a sequence of words, predict the next word—that's a supervised machine learning algorithm. But there's no label that's separate, so it's sort of unsupervised. It's self-supervised.

So let's try a forced choice. Supervised learning? Unsupervised?

Okay, we're almost 50-50 split, maybe the supervised camp has it.

**Student:** Unsupervised. Why is it unsupervised?

I saw a few hands back there. Yeah.

**Student:** There's no ground truth.

Let's be careful. One generally has a set of $X$'s and some $Y$'s that are labeled. We call the $Y$'s ground truth. Now, whether they're truth or not truth, that's not really my problem as a machine learning person. Sometimes the $Y$'s are: did you buy it, did you not buy it? But that's, in some sense, true. So, arguably, these are ground truth.

Some human, right? 100,000 humans clicked a rating of 1 to 5. Or, if you will, 100,000 humans either watched it or didn't watch it. So, in some sense, they watched or they didn't watch it—that's ground truth.

Now, if you're concerned with something different, like, did they like it? I don't know if they liked it.

**By the way, does Netflix really care if you like a movie?**

Well, sort of, but it's not at the top of their priority list, just to be frank about it, right? So they don't know whether you like it, they know whether you watched it. And whether you rated it, and if you rated it, they got a number. So I'm still gonna call that ground truth, because I'm in a machine learning class.

So that doesn't disqualify it from being supervised. Good enough, thank you.

### The Prediction Problem

So we have something which is: we're trying to predict the rating of the user and the movie, right? They took a bunch of these—mostly they're empty—they gave you a bunch of them, and they kept out a couple million entries here, and hid them away, and said, hey, we've taken a small number, 2.8 million entries of user by movie. We've hidden them, and your job is to build a model to predict the missing items.

Right? Tell me the ratings that somebody would have given on a movie. Now, they're mostly missing-missing, but there are some that they actually have and they just hid from you, right? It's a holdout set.

**Student:** Couldn't it be a mixture of unsupervised and supervised?

I don't know, how is it a mix of supervised and unsupervised?

Well, one thing you can do is look at which movies are close to each other, which... now we're in the method piece. Supervised or unsupervised is more a function of the problem. Right? So we might use an unsupervised method as part of it. Lots of things have unsupervised methods. We use PCA first, or whatever, embedding as an unsupervised embedding method, or self-supervised, then we take that and use that in our supervised method. It's still a supervised problem.

Right? So, arguably, this is supervised. We're trying to predict ratings from a user and a movie. It's a function, and we're trying to learn the value of that function. And we can get some loss function—$L_2$ loss, or whatever—on how well we predict the ratings.

### Matrix Completion

So, arguably, I say, yes, it is supervised. It's also often called a **matrix completion problem**. I'm giving you a bunch of items in a matrix, and I'm asking you to estimate the missing items in the matrix.

Now, there's a different problem you could be solving. What else might Netflix actually care about? This is estimating a problem. They posed it, and the challenge was: estimate the hidden ratings. That's a machine learning problem.

## What Netflix Actually Cares About

What are they actually trying to do? Are they trying to predict your rating of a movie?

**Student:** So, one version is to try and predict what movie you will watch.

Right? Now, that's related to the ones that you're rating. Presumably if it's a highly rated movie, you're more likely to watch it. If it's a movie you hate—I don't watch horror films. So, you shouldn't be pitching horror films to me. I like comedies and, you know, Wes Anderson. I watched last night, you know, fun little light things, silly things. So I don't like scary movies, so it should learn for Lyle—here I am, I'm in one little row there—I've watched a bunch of Netflix, it should know which other ones I would rate highly and be able to not just predict them, but you're right, predict what ones I might watch.

So, in some sense, maybe what they really care about is not the rating which we're going to focus most of today on, but:

$$P(\text{watch} | \text{movie}, \text{user})$$

The probability of watching, which is a function of a movie and a person.

And now when I think about that, do they care about my probability of watching all the movies? No, they care about...

**Question:** Precision or recall, what do they want?

**Precision!** They've got tens of thousands of movies, probably over 100,000 movies, and I'm gonna watch what? A thousand in the next 10 years? 20 years? 30 years, right? They want the top thousand out of their 100,000 movies, and they want the top one or three for today, right? So they want really high precision.

So they really only care about the highly-rated movies. Make sense?

So, you gotta think always about the difference between the problem that you may be solving for the machine learning—predict the rating—versus maybe the problem you care about: will the person watch it or not? Which is related but different.

So I'm going to spend a lot of time talking about how to predict ratings. It looks like a chance for you to review a lot of stuff we've done before in the class, and a lot of it will show how loss functions look the same and different. And then we'll talk at the end a little bit more, maybe, about the real problem.

Cool? Cool.

## Model Selection: Ensemble vs. Single Model

The other piece we'll come back to is: better a single model or an ensemble?

**Question:** Ensemble, single model?

Every time someone says the word "better," you should stop and ask them: **what metric of better are you using?**

And in the machine learning world, the typical "better" is accuracy on a held-out dataset, right? Which, yes, the ensemble's gonna win.

But in fact, there are other metrics of "better." Does Netflix care about interpretability? Actually, no, they don't really. Do they prefer simple algorithms over complicated ones? Somewhat, right?

We'll come back again, but think about: there are many reasons a business might choose an algorithm. It might be interpretability, might be simplicity, might be ability to throw in other information they have and make a more complicated cost function.

Okay, cool.

## K-Nearest Neighbors Approach

So, I'm gonna cover mostly today two methods. We'll do k-Nearest Neighbors once again, showing how it's done in sort of a fancier version, and then we're gonna do matrix reconstruction. And we've done one version of matrix reconstruction already in this class: **PCA!** So now we're gonna do alternates to PCA, right? There's a class of matrix reconstruction methods. They won't have beautiful mathematical properties—there'll be no eigenvectors, no eigenvalues—but it's still matrix reconstruction. And they're still widely used by companies like Netflix.

### Basic k-NN Formulation

So, let's start with the simplest version of what is the k-nearest neighbor approach. I might have the rating:

$$\hat{R}_{u,i} = \frac{1}{k} \sum_{j \in N(i,u)} R_{u,j}$$

The estimate of rating of user $u$ on movie $i$ equal to the average over the $k$ neighbors of the summation over $j$ in what set?

It's in... I want to find what? I want to find similar ratings, and now I can think of what does "similar" mean.

And what am I gonna look at? There are a bunch of ways you could do this. Remember, it's symmetric between movies and people, so everything I do will have a dual.

But I'm gonna say I'm gonna pick a set $N(i,u)$—a set of movies where the user $u$ has provided a rating. And I want movies that are similar to the movie I'm trying to rate.

Let me say it again. There is a **target movie** $i$. That's the movie I'm trying to get the rating on, right? So the $i$ is the target movie. And what I want to do is find $k$ movies that are similar to that one.

And the problem is there are 40,000 movies that are there, and lots that are similar, but the ones I want are the ones that I, the target user, have rated. Because then I can say: how well did I like movies that are similar to the one I'm trying to rate?

Make sense? Yeah.

**Student:** What if he's not rated anything that's similar?

We're doing k-Nearest Neighbors. So, A, if you haven't rated 20 movies, we're out of luck. So this is not going to solve the startup problem when someone first shows up. This requires me to have rated 20 movies.

But again, it's k-Nearest Neighbors. I'm gonna pick the 20 most similar that he's rated. If he's only rated 20, that's the 20 most similar ones. They could be really far away. If he's rated 20 right nearby, because he's just watching horror movie after horror movie—he's rated 40 horror movies, and however many others—we take the 20 most similar horror movies he's rated.

Make sense? So, k-Nearest Neighbors has this nice property, right? It's not a kernel method where you find the ones that are close enough. If there are any movies there, some of them are close enough, because we're going to find all of them.

So this is great for some place where I just don't even have a good metric of how many movies you've rated, how similar they are. If you've rated 500 movies, we'll find better recommendations. If you've only rated 5 movies, those are the five most similar movies.

### k-NN vs. RBF

**Student:** Isn't it better to use RBF for k-means?

Great question! This would be a great final exam question. For Netflix, am I better off doing radial basis functions with a Gaussian or k-Nearest Neighbors?

Let's try things. RBF functions—fixed bandwidth? I got one vote. k-Nearest Neighbors, I got everybody else's vote. The majority's not always right, but it's not a bad thing.

Why is it... okay, well, why is it that this way, if there's nothing close, you at least find something?

So I'm going to say I want the average rating:

$$\hat{R}_{u,i} = \frac{1}{k} \sum_{j \in N_k(i,u)} R_{u,j}$$

How did user $u$ rate movie $j$ for each of the $k$ closest ones?

Great! That's it. It'll find something. If you've got at least 20 movies, you get an answer. With the Gaussian, maybe there's nothing close, or maybe there should be. Yep.

### Defining Closeness/Similarity

I did not mention what closeness is. So now the big problem we gotta solve is: how are we gonna define closeness?

How do we define closeness in this sort of a setting?

Here's user $u$—that's me, that's Lyle. These are you. And what do I want? I've got a bunch of movies. Here's a movie—there's Avatar. It's a movie. How do I find the movies that are closest to... I rated it, right? There it is. So, 5. Okay.

How do I find the movies that are closest to Avatar?

**Student:** So I could look at the attributes of the movie.

You could look at movies—they have genres, they have producers, they have years, they have studios. You could try and find a distance metric over the attributes of the movie. I've tried that, it actually works quite badly.

Right? So that would be something that's based on **content**, right? What the movie is. And in some sense, it makes sense that you pick the movie by the actor, or do you want a new movie or an old movie, and features... The genre labels are fairly crappy in terms of, you know, what's comedy or what's crime or whatever. But that's not an unreasonable idea. Empirically, people tried it here, it doesn't work.

### Collaborative Filtering

So, recommender systems, rather than doing that somewhat sensible idea, mostly, instead, say something else: **only use the data here**.

What's similar to movies that would be similar to Avatar?

So a movie is what? A movie is a column here. What makes two columns similar?

I could do some sort of measure of how similar or distant the two movies are. In particular, I could take $L_2$ distance, I could take a dot product...

What properties do I want in terms of a distance metric, right? So each movie is a vector of users, yeah.

**Student:** So I got movies that are liked by similar users, but now I've just flipped the problem again.

Because similar users are going to be ones that watch similar movies. So I gotta still do the same problem on users. How similar are two users? It's the flip version.

Users are similar if they've watched the same movies or similar movies. But I still, at some point, it doesn't quite help.

I mean, I could have users that are similar because they have the same glasses or the same haircut, or whatever, but those features of users are not helpful. What's the best description of a user? **What movies did they like?**

Make sense? Descriptions of how old are you, or how male are you, or whatever, are not really good predictors of movie liking compared to what movies did you like. That turns out to be tremendously predictive.

Right? Tell me the last 5 movies you really liked—that tells me more information.

### Similarity Metrics

So, in some sense, this is a vector. I've got a vector here, which is: this movie is a set of users. And this vector here... If they were ones and zeros, just watched or didn't watch, what's the right measure of how similar or dissimilar they are? Can I take the dot product?

What does the dot product count? Similarity, but in English. If I take the dot product, if I take:

$$\text{movie}_i \cdot \text{movie}_j$$

**Student:** Sorry?

**Student:** The number of users that both movies had, right?

Good similarity metric or bad one?

**Student:** It's missing something.

What's gonna happen? If it's a movie that everybody has seen—Star Wars, or whatever, right? Then Star Wars is going to be similar to Harry Potter because everybody has seen both Star Wars and Harry Potter. Right? Yep.

**Student:** We have to use the rating, maybe, but let's even start before we use the ratings...

That'll be good to use, but before we get to ratings... The dot product is missing something that we should be doing.

**Student:** Remove the popular movies, yeah, we could do that.

That's a hard feature selection. Probably not a good idea. How about we **divide** this by how big these are?

Right? Divide by the number of movies here, and divide by the number of movies there. You want to normalize it, right? This would be a **cosine measure**, rather than just a dot product, right? The cosine is a number that goes between, well, minus 1 and 1. But in this case, these are non-negative, so from 0 to 1, because you can't negatively watch a movie.

So, one version says that what you want to do is to take each of your movie vectors and ask: what fraction... control for how many movies there are, right? Divide by the size. So, things that are more frequent, I don't really want to be more similar just because they're more frequent.

Yeah.

**Student:** How does this solve the problem?

If this thing has a thousand movies in it, I'm dividing by a thousand. If this thing has got three movies in it, I'm dividing by 3.

**Student:** It's one movie column. So it's the number of people that have watched that movie.

What's happening is: the more popular a movie is, the more likely it is to have a movie in common with some other movie. And I'm exactly dividing by how many? If this has got 10 times as many movies in it, each movie that we've watched in common only counts one-tenth as much.

So if you've watched Harry Potter, which a lot of people... **how many people have seen the Harry Potter movie?**

Yeah, pretty much the whole class.

Is it gonna tell me a lot about your movie taste? Secret—it's not my favorite movie. But I've seen it. And it doesn't tell me much about how similar you are. How many people have watched *My Dinner with Andre*?

Nobody? One hand, that would be a big similarity, because it's, like, incredibly obscure.

Right? In between, you know, Avatar is more common than *My Dinner with Andre*, less common than Harry Potter. It's got more signal.

### Normalization Principle

So, when you're measuring similarities in everything you're doing in your final project, in your projects in life, think about how you should be normalizing things to get a meaningful distance measure of what similarity or what importance is.

Cool! That was the question we just spent some time looking at.

So, here's what we said, I'll repeat it. We take... We're trying to say for a given user, how much we like movie $i$. Find the $j$ movies that the user has rated. Of those, find the $k$ that are most similar, using a similarity metric that says: how many do we have in common?

Okay, if it's ratings, then we could take the... something more like a distance. We could take:

$$d(i,j) = |R_{u,i} - R_{u,j}|$$

The rating minus the rating in the other one.

Of course, will that make sense? If we both rated a movie... Let's think about it. Does the distance of how far apart the ratings are somehow... it's sort of not going to work. First of all, it's the summation only over the things that we both rated.

Right? We've got two movies, I can't even compare them if I rated them and you haven't. There's nothing there, so that distance doesn't include... it's only the summation over the things that exist.

And, if I say how different they are, I'm gonna end up having potentially more distances if there's more frequent movies. How to put it.

I want, in some sense, the dot product is obvious. This is how many movies in common, divided by number of movies. If I take how distant are these two movies, if a movie is really popular, it's gonna be more distant from another movie because it'll have more differences being summed up.

Does that make sense? So again, I've got to normalize it. I've got to normalize by how many are in, or how big this is.

### Formal Similarity Measures

Cool, so I find the movies, I find the $k$ that are most similar, I average the ratings. Cool!

This summarizes, sort of, the two things we said. The second one says, if you're gonna have a vector for a movie and a vector for another movie, what you want to do is to take their dot product of the cases where we both have a user rating both movie $i$ and $j$, and divide it by how many users rated $i$, and how many users rated $j$:

$$\text{sim}(i,j) = \frac{\sum_{u \in U_{i,j}} \mathbb{1}_{u \text{ rated both}}}{\sqrt{|U_i| \cdot |U_j|}}$$

You can do the same thing for a Euclidean-style distance. Take the rating of user $u$ for movie $i$, and user $u$ for movie $j$—two movies—take the distance between those, square it, but again, I'm going to divide by how many there are:

$$d(i,j) = \frac{\sum_{u \in U_{i,j}} (R_{u,i} - R_{u,j})^2}{|U_{i,j}|}$$

So just think: as you're actually looking at any sort of similarity or distance, often you want to control if almost everything is missing. You're only averaging the distance or the similarity over the things that are present.

Good? Questions? Good. Yes?

### Handling Missing Data

**Student:** So what we're doing...

Great question. So, in the case where this one has a rating and this one has got nothing on it, what we're doing is just dropping that entirely from the distance or the similarity.

Right? And it's obvious in the dot product. The dot product—you're only taking the cases where both of them have a rating. Right? So, if you're saying, is the rating higher or lower or the same, does it make any sense if there's only a rating here and nothing there? So we're summing in all of this only over the things that are not missing.

Right? It doesn't contribute, right? So, note that there are two different things you could ask. One is, how different are the ratings? The other is, you could ask... you could take just the sheer dot product: did I watch it? Did I not watch it?

And now, if you watched it, if you both watched it, if this was watched by the same people, that's a 1. If this was watched and not watched, that's a 0.

Make sense? So, there are two different things. The rating case, we can only say how similar is this rating to that rating if there is a rating. Right? So that's a summation ONLY—that's the top one here—that's only the summation over the case where you have ratings on both of them.

If it's a "did you watch it or not watch it," now it's the summation over everything. But you still gotta be... so there's no missing data. Watch/not-watch for every single user, for every single thing—that's a complete matrix.

Right? It's almost all zeros. Almost every person hasn't watched everything. So it's massively zero, right? There's basically 100 million zeros with 1 million non-zeros in the middle of it.

So it's 100 million zeros, but the watch-not-watch is a **dense matrix**.

### Missing Data in General

But the thing I'm trying to hammer home is: a lot of data, and especially in biology, a lot of data—almost all the data—is missing.

Right? You could say, does this chemical bond to this protein? And you'll have some of them that are there—how well it bonds—but for almost all chemicals and almost all proteins, you don't have any measurements. No one has actually made the measurements. Increasingly, now you use AlphaFold to do a simulated estimate, but for the real laboratory measurements, almost everything is missing.

Make sense? And so what I'm saying is that all of the math we've done before sort of works with most of the data missing. You can't run PCA with missing data—if you call the PCA function, it barfs if you give it missing data.

Right? You can have ones and zeros in PCA, but if it's missing, PCA won't do it. But we'll see, you can do a matrix factorization that looks a lot like PCA using an alternating gradient descent method.

And certainly for regression, or something, you can do something that says, I want to do regression, but I'm only gonna sum over the things that are there in my loss function. Everything missing just drops out.

Right? And that's the big takeaway: all the math works—well, not quite, because the eigenvectors and eigenvalues don't—but almost all the math works. All the $L_2$ loss, all the gradient works, if you just sum over the things you have.

Make sense?

### Frequency Effects

And the other big takeaway is: if some of these are really frequent, and some of these are really rare, be very careful how you measure distance. Because anything that's super frequent is more similar—it's got more in common.

How many movies will you and I have in common? The more movies you watch, the more we'll have in common. That doesn't really mean we have common tastes. We probably do, but the number of movies isn't gonna prove it. Make sense?

Cool. And realize that many users have watched a lot of movies, have watched movies. Someone joins, they've watched very few movies. Does that mean they're really different from everybody else—they've watched very few movies? No, it just means I haven't watched many movies.

Cool. Awesome.

## Improving k-NN: Baseline Models

There are a bunch of ways to improve this. And I want to first start by thinking about a different problem that is mathematically pretty much the same.

It's popular in some crowdsourced classes to have random sets of 3 or 5 students rate each other students' homework. Make sense? So instead of users and movies, a similar problem is: have users and homework ratings.

Make sense? And it's massively sparse because most person's gonna rate 5 homeworks. I have this problem all the time, because I have TAs rating homeworks, like, your final projects.

And... what's unfair about having 3 or 5 people rate your homework?

Well, certainly there's sort of noise, so you can reduce the error, which is pretty noisy with 3 or 5 people. Some people are nicer graders, and some are pretty harsh. It varies a lot. I won't call out people from India by name, but they seem to have a little more hard-ass grading than Americans, where I grew up in California, where we're all nice and friendly in California, and we don't give hard marks.

Right? So there are individual differences—to use the psychology term. Each person's a little bit harsher, or a little bit more generous.

Make sense? Okay, can we put that into our algorithm?

### Adding Baseline Terms

Let's try and do that. And the idea here is, what we do is put in what I will call a **baseline**, which is gonna look like a constant term, or a bias, or something.

But here, the idea is that I can say, what's the average rating this person gave? When I look at what rating they gave the movie or the homework assignment, it'll be the rating they gave that homework assignment, subtracting off their overall average.

If they've only rated one movie, I've destroyed all the signal. If they only rated 3 movies, this is still pretty crappy, because I've subtracted the average of three. But if they've rated 20 movies, like in this nice data set, now the average over 20 movies tells me: are you a generous grader or a hard grader? And I can just subtract that off for every person.

Make sense?

### Symmetry Between Users and Movies

The world is symmetric. There is no mathematical difference between a movie and a user. You could, if you will, take for any movie its average rating and subtract it off. Right? It's like the constant term in the regression.

Each movie will have an average rating across all users. And I don't want to try and predict the actual movie rating, I want to try and predict the **residual** from having subtracted off the average rating everybody gives.

Make sense? That's sort of nice, because if I don't have enough information, I'm falling back on the average.

Right? So a common sort of trick used a lot is: take a dumb model, like the average movie rating, and now build all your models on the residual.

Make sense?

### Stagewise Prediction

And to digress a little bit, it's pretty common if you're building a model where you have different modalities. I have text and I have images. I might try and predict a model on the text of how much you'll like it, and then use the image to try and predict the residual.

Why not just predict them both at once? It's sort of nicer to have a simultaneous prediction. Why would I do a stagewise prediction, right? First, predict the baseline, then predict the residual?

**Student:** They're different models.

Right? You might have one piece—the average being predicted by just averaging them, or by a fancy model. One might be a convolutional neural net, one might be something else. It's hard to train them together.

Also, it's easier to regularize. You're always worried about overfitting. If you're fitting a residual, at least you sort of have a clearer idea of what's going on, if it's mattering, so it is cleaner sometimes to break things up.

In theory, it's always better to simultaneously optimize everything. But in fact, the convergence is sometimes unstable and harder to do, and often you have a baseline model that somebody else gave you, and you're now fitting the residual with your model.

And again, I'm trying to talk toward your projects. Some of you will find someone else's model that's a closed-box model. What can you predict? Well, maybe you'll predict where does that model mess up. Maybe good, maybe not, but worth thinking about.

### Baseline k-NN Formula

Okay, so what do we have here? Let's look at what this looks like. For every rating of user $u$ for movie $j$, I can subtract off a baseline rating, say the average rating that user $u$ gave to everything:

$$b_u = \frac{1}{|M_u|} \sum_{j \in M_u} R_{u,j}$$

Make sense? Subtract that off. I now am gonna have a weighting on that—we have to go back and show the last slide in a second, a weighting on that. And then I'm gonna fit my model, adding back in that baseline.

So the prediction is, for you, user $u$, liking movie $i$:

$$\hat{R}_{u,i} = b_u + \text{model}(u,i)$$

What's user $u$'s average rating? Or the movie $i$'s average rating, plus something that has the model piece here. And I need to go back and do this, because I didn't quite do the ordering.

So, let's just write out what this model looks like.

### Soft k-NN with Baselines

Here, we did a hard k-NN. Right? Last time? Instead of doing this hard k-NN, I can do something that's a **soft k-NN**, closer to kernel methods. I could say, hey, instead of having one in front of these, I'm going to put in **similarity** between movie $i$ and movie $j$:

$$\hat{R}_{u,i} = b_u + \frac{\sum_{j \in N(i,u)} \text{sim}(i,j) \cdot (R_{u,j} - b_u)}{\sum_{j \in N(i,u)} \text{sim}(i,j)}$$

Right? So there's movie $i$ that I'm trying to do, my target movie. There's movie $j$ that are my nearest neighbors. Before, I had: you're 1 of the $k$, you're similar—one, or you're not, you're similar—zero. Now I can do a similarity that's soft. And if I'm going to put the similarity there, then I need to normalize and divide by the sum of all the similarities rather than by $k$.

Make sense? So rather than a hard k-NN, you can do a soft k-NN. The farther away it is, the less it counts. We defined similarity before, right? A dot product...

So now I have the equation which is, great, I've got this similarity here, but I've just subtracted off the baseline—my average rating for my user—and then add it back in.

So that says, great, predict these residuals. Right? These are residuals:

$$(R_{u,j} - b_u)$$

The rating minus the baseline—this is how well I like that movie compared to how on average I like movies. Fit a model for the residual that predicts movie $i$, and then add in my baseline.

And I hope the nomenclature's not confusing. I did baseline $b_u$. It could either be the average that I, the user, gave, or it could be the average for the movie $j$.

Right? So either we're... or both. How much does this deviate from how well I normally rate a movie? Or for a movie, how much does my rating of it deviate from how the average person rates it?

**Student:** Sorry? Which one's better?

There's always one answer to that question in this class. Anytime the question is "which one's better," it's almost always: **it depends**.

Right? So it depends what's the case. Is it the case that people are very hard or very generous in their ratings? Is it the case that movies differ a lot in their average?

It also depends a lot on: think about what you're gonna do if you don't know much about a person. If I'm ignorant about you, what's a good guess of your rating of a movie?

**The average rating everybody gives the movie.**

Right? There's some movies everybody likes, and there's some movies that nobody likes. And if I don't think about you, I'm gonna just revert to the baseline.

Or conversely, if I know for sure that you're a really hard or a generous rater, and there's a movie I know nothing about—how well do you like the movie? Here's a movie that hasn't been released yet, I know nothing about it, call it X. How much would you like movie X?

**How much do you like the average movie?** I haven't even told you the movie, I don't even know what it is.

Right?

Which do I like better? Probably in this case, I'd go with the baseline of the movie, because I have a lot of data for movies, right? All 40,000 movies, I've got enough ratings to know: is that a good movie or not a good movie on average? And that's at least a baseline, whereas I have lots of users I know nothing about. They watched one movie.

Right? So at least you might start by putting in the movie as a baseline.

## Advanced k-NN: Regression Approach

Cool! There's a bunch of other things you could be fancier on, which is... The standard models in linear regression and most optimization assume that the observations are IID—they're independent observations, you grab an $X$ and a $Y$.

But movies and lots of features are **clustered**. There's a whole bunch of Star Wars movies, and they're all sort of the same. And some are better and some are worse. But, you know, if you like one MCU movie, you probably like a bunch. Again, there are good ones and bad ones, but there's a bunch of, you know, Doctor Strange movies that are all sort of the same.

And so you might say, hey, I should be doing something that sort of downweights things that are redundant.

Make sense?

### Does Linear Regression Handle Redundancy?

Does linear regression downweight features that are redundant?

How does the formula look? The weights are equal to:

$$\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$

$\mathbf{X}^T\mathbf{X}$ is, if you subtract off the mean values of $\mathbf{X}$, the **covariance**. This is the inverse covariance.

What's the covariance? That says which... what does the inverse say? **Downweight the features that are redundant.**

Make sense? So, linear regression is, by construction, saying, hey, if I got a bunch of features that are highly redundant, don't add them up all counting—downweight each of them.

Make sense? So regression is downweighting redundant features.

### k-NN with Learned Weights

Cool. So maybe I can do something with neural nets. I said before, let the rating equal some sort of a hard or soft average of some similarity times the rating:

$$\hat{R}_{u,i} = b_u + \sum_{j \in N(i,u)} \text{sim}(i,j) \cdot (R_{u,j} - b_u)$$

Instead of putting a similarity, I could just put a **regression coefficient**, $w$—a parameter. And now, I can go back and do linear regression:

$$\hat{R}_{u,i} = b_u + \sum_{j \in N(i,u)} w_{i,j} \cdot (R_{u,j} - b_u)$$

I've got the rating I give to a movie $i$ is my average rating, plus the sum of nearest neighbors of some weight times "how does my rating differ from my average rating of that movie?"—of the similar movies, right? These are the $j$ most similar movies.

I can now do a regression, but it's a regression only over the movies that are the $k$ most similar ones. Right, it's a **local regression**.

Yeah.

**Student:** How do we use linear regression again?

So, this is a model form. In it, I've estimated the $b$ by just... I could do it by regression, but I've estimated it by just saying, hey, what's my average rating?

The $R_{u,j}$, I know—that's the rating that I gave to movies that are similar to my movie. And how much should I weight them? I'm gonna learn a regression coefficient which is a function of two things, right? It's a function of movie $i$ and movie $j$:

$$w_{i,j}$$

So instead of computing before... We said how similar is movie $i$ to movie $j$? We measured a similarity, and we measured the similarity of $i$ and $j$ by saying, hey, they're vectors.

Now I'm saying instead of being a k-nearest neighbor sort of person, measuring how similar are the two movies, now I'm just saying, hey, forget all this stuff. I'm gonna have—I just want a weight $w_{i,j}$, where this weight $w_{i,j}$ says: **how much weight should I give to this movie $j$** in terms of how much I like it?

And now what's the regression gonna be over? We're gonna have to take a summation over all the people and all the movies—almost all of which will be missing, so it'll be summation over the movies and people we have, right? We'll take a loss function over the ratings we have in the training set.

Right? Because we don't... most of the things, 99% are missing. But over the things in the training set, for everything in the training set, for every person and every movie, for every rating I have in the training set—my crappy 1 million out of the 100 million ratings, my 1 million ratings I have in the training set—I can now say: what weight should movie $j$ be given in terms of predicting movie $i$?

### Parameter Count

If there's 18,000 movies, how big is this matrix?

**18,000 squared**, right?

I just said I had a million observations. How big is 18,000 squared compared to a million?

**It's a lot bigger.**

So I got a bit of a problem. I'm gonna have to regularize the crap out of this. Right? Use a lot of ridge penalty. Do something on it. For this small dataset... But the principle is still there, which is that I can fit a regression that says: how much should this movie... how much should movie $j$ tell me about movie $i$?

Right? $j$ is the movie that's similar, $i$ is the target movie. And this is a... not a similarity in the matrix reconstruction sense, but a similarity in the sense of how much weight should I give when building this model.

Yeah.

**Student:** What am I predicting?

I'm still predicting the rating:

$$R_{u,i}$$

This is the rating user $u$ gives to movie $i$. I've got a million observations in this dataset. Right? A million pairs of a person rating a movie? That's a million... $n$ = 1 million.

Most... I got 99 million I don't have, but I can't put a loss function over the ones I don't have. Of the million I have, I can now take and say, hey, that's equal to some constant plus weights summed over the $k$ most similar movies. Right? I know these ratings—these are the $k$ ones that are similar movies that I've given ratings to—and this is the weight of how much weight I give to each of those, because some movies are maybe more informative, probably because they're more similar. But I've dropped the similarity—it's just how much does it predict?

Yeah.

**Student:** Where's the 1 million?

The 1 million was the number of non-zero ratings I have, ratings that aren't missing. Whereas the 18,000? That's the number of movies.

So, $w_{i,j}$ is a weight of how much movie $j$ predicts movie $i$, in terms of liking. And that's gonna be the square of the number of movies.

**Student:** $j$ is the k-nearest, but across all million observations...

I'll end up with a whole bunch of movies. So in the end of the day, right, when I take all the ratings I have for all the users and all the movies, I need to estimate something that's of order the number of movies squared.

Well, I need to find the $k$ nearest neighbors, but the weights... I can't fit the weights on one person. If I only have Lyle's rating of Avatar, I can then go and say, how much did Lyle like the $k$ most similar movies that he's rated to Avatar? I can't fit the weights on that—that's one observation.

I need a whole bunch of observations, like a million of them, to have a hope of fitting the weights of how much each movie tells someone about Avatar.

Okay, I'm gonna push on, because I want to cover a whole different set of approaches there. I'm going to skip some of the pieces there.

## Matrix Factorization Approach

A whole second approach—right? This was k-nearest neighbors, soft k-nearest neighbors, sort of regression—is to view this as a **matrix factorization problem**.

And I now want to say, how do you do something PCA-like, but where it's not PCA, because there's missing data?

### Basic Matrix Factorization

So I've got a rating matrix $\mathbf{R}$. That's my rating matrix $\mathbf{R}$. And I want to factor it.

This is a surprisingly popular problem in machine learning. And the idea of factoring it is super simple. We're gonna break up matrix $\mathbf{R}$ into the product of two matrices:

$$\mathbf{R} \approx \mathbf{P} \mathbf{Q}^T$$

Where we're gonna say that $\mathbf{R}$ is equal to the product of some matrix $\mathbf{P}$ and matrix $\mathbf{Q}$, which we usually write in the transpose form.

And the idea is that $\mathbf{R}$ is gonna be $U \times M$, right? Users by movies? Let's do $U$ and $M$. $\mathbf{R}$ is users by movies. $\mathbf{P}$ is going to be users by $k$—it's going to be a **reduced rank factorization**—and $\mathbf{Q}^T$ is going to be $k$ by movies.

This should look really familiar, and you can write it either in the matrix form or in the vector form. And typically for movies of the size I had, turns out that **60 is a good number**, just to give you a flavor, in the real world.

So, very much reduced dimension, right? I've got 18,000 movies, I'm projecting them into a 60-dimensional space. Right? And similarly, I've got half a million users. I'm projecting them into a 60-dimensional space.

Make sense? So I've got something that was high dimension—half a million by 18,000—and I'm now going to represent it with a lower-dimensional representation.

### Connection to PCA

And these should look a lot like PCA. So far, there's nothing that says this couldn't be PCA. Right? Could be principal components.

But we're going to do a version of it, right? So $\mathbf{P}$... If you think about these, $\mathbf{P}$ would be called the **principal component scores**, and $\mathbf{Q}$ would be the **loadings** on the principal components.

Right? Remember, scores and loadings? Well, principal components are coefficients. I like to think of them in terms of an expansion.

Right? So I've taken: how will I rate things, how will anybody rate things, and I broke it up in sort of two pieces. And it looks almost like an SVD. Right? There's sort of a left side and a right side, but we've got no principal component values that we're actually gonna compute here. We're not gonna do anything eigenvector-y.

So far, so good?

### Loss Function

Now the question is, what's the loss function we're optimizing?

And in general, we're going to optimize the same thing we always optimize, which is **reconstruction error**:

$$\mathcal{L} = \sum_{(u,i) \in \text{observed}} (R_{u,i} - \mathbf{P}_u \cdot \mathbf{Q}_i^T)^2 + \lambda_P \|\mathbf{P}\|_F^2 + \lambda_Q \|\mathbf{Q}\|_F^2$$

I want to find $\mathbf{P}$ and $\mathbf{Q}$ such that, subject to $k$ (my hyperparameter, which is the number of components), I do as good a job as possible of reconstructing the ratings that I have.

Now, note that before in PCA, it was a summation over all movies and all users, but now I'm missing 99% of them, so it's just the reconstruction error over the things I have.

Make sense?

And now, like I always do in machine learning, I'm vastly concerned about overfitting, so I'm gonna put on **regularization penalties**. Right? And the standard penalties that people tend to use, all things being equal, would be $L_2$. You could also use $L_1$ penalties if you wanted, right? But I want to make the... both the scores and the loadings be small.

Make sense?

### Optimization: Alternating Least Squares

Now, let's think about solving... so it makes sense, this is a loss function? It's an $L_2$-penalized reconstruction loss. I want to make it small.

Now, the question is, is this a convex optimization? It's sort of tricky.

First of all, is it quadratic? Ish! It's actually not really quadratic, because that's the quadratic of the product of $\mathbf{P}$ and $\mathbf{Q}$. I don't know $\mathbf{P}$ or $\mathbf{Q}$.

Right? So, in fact, I'm going to have to... I get to introduce a new optimization technique. This is an **alternating gradients optimization technique**. And we're gonna see next week the **EM algorithm**, which is by far the most popular alternating sort of method, which will be an alternating gradients for likelihood.

But, if you know $\mathbf{P}$, this is quadratic in $\mathbf{Q}$. Right? So you can do a closed-form solution.

If you know $\mathbf{Q}$, this is quadratic in $\mathbf{P}$. So you can do a closed-form solution.

Each of these moves will decrease the loss. This actually is convex—it will keep going down. So the way you optimize problems like this is **alternating**:

1. You guess $\mathbf{Q}$
2. Solve for the best $\mathbf{P}$, or if you wanted, if it were nonlinear, you do a gradient descent in $\mathbf{P}$

Right? Eventually, today we'll come and plug a neural net in here, but for now, this is just sheer quadratic. So it's bi-quadratic, if you will, right? It's quadratic in $\mathbf{P}$ given $\mathbf{Q}$, it's quadratic in $\mathbf{Q}$ given $\mathbf{P}$.

You can solve for one given the other, solve for the... go back and forth. Each time you go forth, you're going to provably be reducing the error—the penalized error—and therefore, if you keep going down, and the whole thing is nice and non-negative, you will, in fact, converge to an optimum.

Does that make sense? It's sort of a fun piece here. Yeah, I don't know, I like these methods.

Cool.

Right? So, given $\mathbf{P}$, $\mathbf{Q}$ is just a standard ridge regression type problem. And vice versa. That's it. You go back and forth: boom, boom, boom, boom, boom. Loss goes down, you get an optimal reconstruction error subject to the ridge penalty. Life is super clean.

## Non-Negative Matrix Factorization (NNMF)

There is also another class of algorithms which I would be remiss—I'd be bad if I didn't tell you about—which is, people often like to use something called **non-negative matrix factorization**, NNMF to its friends.

And we can make another assumption about $\mathbf{P}$ and $\mathbf{Q}$: that they are **non-negative**. All the entries in them are non-negative.

And if we think about what we're starting with in this sort of rating thing, and in a lot of problems, your matrices are strictly non-negative. Right? Ratings are from 1 to 5, or watches—one, not watches—zero. Those are both non-negative matrices, in the... not in the eigenvalue sense, but in the entry-level sense. Make sense?

So some things in life, but not everything—some things are strictly non-negative. Right? There's no possible way to get a negative rating. You click 1 through 5 on a little selection box.

Make sense?

### NNMF Formulation

Now what I can do is say I want to take that non-negative matrix $\mathbf{R}$ and factor it in terms of something which is constrained—not just any $\mathbf{P}$ and $\mathbf{Q}$ which, like you'd get for orthogonal ones like PCA—but I want the entries of $\mathbf{P}$ and $\mathbf{Q}$ themselves to be guaranteed to be non-negative:

$$\mathbf{R} \approx \mathbf{P} \mathbf{Q}^T \quad \text{subject to} \quad \mathbf{P} \geq 0, \mathbf{Q} \geq 0$$

**Student:** More sparse, less sparse?

**Less sparse?**

In general, if you decompose things—I should, I'll try and bring them... If you take a face image, if you break it up into something which is the eigen-faces, if you do a PCA on the faces, do you have a basis function? The first principal component of a face is just a round oval, the second face is maybe, are you stretched up or not? The third one might be, you know, hair or beard or something, right?

If you do that, you get a very distributed feature set.

If you force your face image to be only non-negative, so you can never have something positive minus something negative, you'll get a very sparse representation. You'll get one piece which is the head, one which is the eyes and nose. Because you can add the eyes, plus the nose, plus the ears. If you can't have negative things to cancel, it forces it to be much more sparse.

So, in general, non-negative matrix factorization makes things **more sparse**. It's a constraint. Right? It's almost like a regularization. It's gonna prevent you from having some solutions. It's blocking any solution that's got any element of $\mathbf{P}$ or $\mathbf{Q}$ being negative.

Right? So it's saying you only get positive pieces here, and you get only positive weights on them. So it says, what things contribute? And when you have only positive contributions, in PCA, some are positive, some are negative, things cancel out.

### NNMF vs. PCA

Gives a better fit, right? PCA is the best basis for the fewest bases for $L_2$ reconstruction. You can't do better than PCA, so a non-negative matrix factorization will give you more components, or can't give you less—generally gives you more, but they will tend to be sparse, and they'll tend to be interpretable, because these are the things that contribute to the higher rating. There's no "some positive and some negative."

That make sense?

And it turns out that if things are guaranteed to be non-negative, then there are nice alternating methods that you can do—multiplicative updates and other optimization approaches—but that's much less important than the core notion here.

So, one of the preferred ways of solving the classic matrix factorization is to do a non-negative matrix factorization.

I could put in all sorts of things. I could have my $L_2$ regularization on $\mathbf{P}$ and $\mathbf{Q}$, I can force $\mathbf{P}$ and $\mathbf{Q}$ to be non-negative, I can put a similarity weight, I can put whatever else I want in here and do some sort of approach there.

But I think the main piece is to think of reconstruction error, ridge penalty—I could put different penalties on $\mathbf{P}$ and $\mathbf{Q}$ if I wanted—and to think about, maybe I want to force all the elements to be non-negative.

### Parameter Count and Regularization

If I'm worried about overfitting, which I sort of am, how many parameters are in this model?

It's... I said there were, what, 60 for $k$? So it's $60 \times$ half a million plus $60 \times 18,000$:

$$\text{Parameters} = k \times U + k \times M = 60 \times 500{,}000 + 60 \times 18{,}000$$

Make sense?

Again, how many parameters is that compared to my observations? I only had a million observations. This has got a lot more parameters in it. Right? Because I got half a million people times 60.

So, I gotta regularize really heavily. Regularize $L_2$, regularize maybe by forcing it to be non-negative. That empirically worked well.

Cool!

So that sort of wraps up the matrix factorization part. I want to take the last 10 minutes and, A, think about what's going on in the real world, and B, talk about extending this to the neural net world.

## Modern Approaches and Real-World Considerations

### New User Problem

What happens if you get a new user in this model?

**You're out of luck.**

Right? Out of sample here was a new movie for a given user. It's just a matrix completion problem. Categorical users, categorical movies—you can't generalize this.

Okay, that's sort of a bummer. We'll get to there.

### Two-Tower Neural Network Approach

How do people do this in the more modern era? I actually do not now know how Netflix does it today, because it's a proprietary algorithm, but I can tell you how a lot of people do it.

Sometimes called **two-tower retrieval**, but the idea is super simple. I'm gonna take a neural net. I'm going to take each user and embed them. And for a user, I will take both a categorical user and whatever else I have—your zip code, your whatever.

So I'm gonna take whatever information about the user, and I'm going to have a neural net that gives me a $\mathbf{p}$-vector:

$$\mathbf{p}_u = f_{\text{user}}(\text{user features})$$

For every movie, I'm gonna have a different neural net that takes the movie, and if you want the actor and the year of release, and the number of Oscars it won, and, you know, whatever it was, viewership, right? All that data I have. And I'm gonna embed that:

$$\mathbf{q}_i = f_{\text{movie}}(\text{movie features})$$

So I've now got two different functions, one embedding users and one embedding movies. Right? So this is the functional form—make these just be a standard multilayer perceptron, right? There's no CNNs here. Or make it a transformer, I don't care.

Make sense?

### Neural Network Loss Function

Now I'm gonna have to estimate: how do you embed users? How do you embed movies?

And what am I going to do? I can say in the simplest version:

$$\hat{R}_{u,i} = \mathbf{p}_u \cdot \mathbf{q}_i$$

The rating of user $u$ for movie $i$ will be $\mathbf{p} \times \mathbf{q}$.

So, now I've got a bunch of ratings I can sum over for the ones I have. And I want the predicted rating, right? Which is a function of the embedding of the user and the embedding of the movie.

Make sense? I want that predicted rating to be as close as possible, say, in an $L_2$ sense, to the actual rating:

$$\mathcal{L} = \sum_{(u,i) \in \text{observed}} (R_{u,i} - \mathbf{p}_u \cdot \mathbf{q}_i)^2$$

I can now do gradient descent. So I'm learning the embedding of the user and the embedding of the movie using gradient descent to make the ratings as accurate as possible, over the ratings that I have.

So it looks a lot like I did before. I'm minimizing the reconstruction error, except before I minimized reconstruction error, assuming that there was a very simple form—a PCA-like sort of form. And now I'm saying, all I care about is the embedding, and it's gonna be nonlinear, and I'll just set up some model.

So this gives me now a general way to learn a model that in some sense says a good model embeds the users and embeds the movies in whatever way most accurately predicts the rating. Just gradient descent it. And now I don't care about alternating, because I'll do gradient on both the things simultaneously.

Yes.

### Why Use Dot Product?

**Student:** Yeah, so there's an interesting choice here, which you didn't have to do. I could have put $R$ being some neural net function of $\mathbf{p}$ and $\mathbf{q}$.

And instead, I picked a very particular function. And this is... So you could put something more complex, but it's very common in the recommender systems to particularly embed by forcing $\mathbf{p}$ and $\mathbf{q}$ to map to the same space.

Right? So, this happens inside attention mechanisms, inside transformers. There's a lot of times where you might want to make the functional form be fairly specific.

If you have enough training data, you don't need to use a dot product—just make it a neural net function of $\mathbf{p}$ and $\mathbf{q}$.

If you don't have enough data, which is often the case, because you've got a lot of users and a lot of products, then you make a fairly restrictive assumption. Right? So this is a very restrictive assumption—it's sort of arbitrary. Just use the dot product. But it's a way to make the two embeddings capture the same sort of information.

Make sense? And again, with enough data, I would say just throw a neural net—call that $h(\mathbf{p}, \mathbf{q})$—but in fact, often, even a big company like Netflix doesn't have enough data. Why not? Because there's lots of users and lots of movies, and if the user embedding is rich enough, then you just don't have enough data to fit an arbitrary functional form.

So it's a really a trade-off: how much data do you have? Great question.

## What Netflix Actually Optimizes

Cool. What does Netflix actually want to optimize? Not predicting your rating, but...

**Student:** Your watch time.

That's not a bad proxy for what they care about.

**Student:** Revenue!

They want to maximize revenue. Well, that's close. They want to maximize something close to revenue. They want to maximize **profit**, which is, in fact, revenue minus cost.

Do all movies cost the same amount to Netflix? No, they're paying different amounts for different movies.

What would they rather show you? A movie that cost them more money, or one that cost them less? All things being equal?

The cheaper one, right? And even there are some implicit costs—you might want ones that are cached. There's a transmission cost for some things where you want things that are popular and widely cached.

So there's a bunch of functions, so in the end of the day, there are lots of things you might want to optimize.

And if you think of it, a lot of it is sort of TikTok-y: keep me watching stuff, maximize engagement, because the biggest problem is if you don't actually renew your subscription to Netflix.

And, of course, these days with ads on Netflix, which pisses me off—I'm paying a subscription fee and they're still showing ads. But okay, apart from my... I hate it. But nonetheless, the more I watch, the more ads they show, the more revenue, so the amount of time I spend watching stuff is, in fact, directly correlated with money.

### Real Loss Function

So in some sense, they actually don't care about my ratings at all. What they care about is the revenue. And so now if you look back at sort of this approach here, instead of predicting rating, how about I take an embedding of a user—embed Lyle and his viewing history—and embed some movies, and try and predict for that movie and that user: how much expected profit will I make?

$$\mathcal{L}_{\text{real}} = \sum_{(u,i)} \text{Profit}(u,i) = \sum_{(u,i)} [\text{Revenue}(u,i) - \text{Cost}(u,i)]$$

Make sense? Because once I'm in gradient descent land, and once I've gotten the embeddings, I can throw in all sorts of complicated factors. Right? The loss function can be very complicated. It doesn't matter as long as I regularize enough that I'm not overfitting.

So they want revenue, and that includes things like:
- **Novelty** - Don't tell me things that are completely obvious I've already seen
- **Engagement** - How long do I watch, how much do I like it?
- **Risk** - How likely is it to piss me off so I don't come back again, because it was such a terrible movie?
- **Appropriateness** - Am I going to show things that piss parents off that I'm showing to their kids?

I mean, lots of factors come in. And you can throw them all into the loss function and just gradient descent it.

So in a modern company, they're maximizing revenue as the loss function, but they're still using the same sort of embeddings of things like movies and people to try and maximize that loss function. And still, somewhere under the hood, it is doing a pretty good job of recommending movies that I'm likely to like, because the ones that I probably like mostly correlate with a lot of these other factors.

Right, so happily, it's not all... In the ideal capitalist world, they both increase my likelihood of enjoying the movie, and they also make money. But realize that the loss function trades off both of those.

### Alternative: Predicting Watch Probability

The other piece you could do is you could try and directly predict: will I watch it?

$$P(\text{watch} | \text{movie}, \text{user history})$$

Given the movie and the history—so that's a sort of a version of the user history.

### Exploration vs. Exploitation

The other thing that one could do is rather than being given the data, if you're running Netflix or TikTok, you could not just try and optimize the expected utility, but you could also combine some loss function for learning.

Try showing something and see what that helps me learn about the user. Show me something, see how much I like it—maybe I can find something I hadn't tried out and learn more about me.

And when we come back in reinforcement learning, we'll talk about the trade-off between **exploration** and **exploitation**.

**Exploration:** Tell me the movie that most changes the parameters in the model.

**Exploitation:** Show me the movie that makes the most revenue expected right now.

Make sense? So an ongoing organization like Netflix, you're both doing exploration—you're trying to... it's like ID3 decision trees. What's the question that's gonna give me the most information? What movie will tell me most about you? Not a movie you hate, not a movie you love, a movie that's sort of off a little bit to the side that might be just a little bit weird, that might give me a whole new set of things I could show you.

Make sense? So every company at the Netflix scale is both optimizing their revenue and trying to do a little bit of "try stuff out, see things, see what works."

## Summary and Key Takeaways

So, everything we showed in the first half of the course—regression, matrix factorization with missing data—all of it generalizes. You should look up non-negative matrix factorization on small data sets, subtract off the baseline.

### The Netflix Prize Story

And finally, there's an interesting story behind the Netflix Prize. There were actually two teams—roughly at the same time—that crossed the finish line. They got more than 10% improvement over the baseline that was done by the Netflix engineers. They got the million dollars for the one that actually did best on the validation set.

And Netflix didn't use their algorithm. Although the deal was, I will give you the million dollars because you won the prize, and you give me the algorithm for licensing—for the million bucks. That was the deal. They got it—big team, big ensemble method won—and Netflix didn't use it.

**Why?** Too complicated. The winning one was, like, 40 or 50 different things all averaged. Wasn't worth it for the extra couple of points. Million bucks—good investment. They got more than a million bucks of research, really top teams working on it.

### Privacy Issues

And they thought about doing a second version of the prize, but didn't run it again because of **privacy problems**.

They had anonymized the data. All they gave was random digits associated to the users, so half a million people replaced with a unique ID. And the actual movies were shown.

It turned out that some clever computer scientists who studied privacy showed that that, in fact, is major information leakage.

How do I learn about you if you're one of those half a million people, when your name is nowhere in here? All I have is half a million people, and which movies they watched.

Anybody know **differential privacy**? Nothing, yeah?

You compare what movies you watched with some other website which has your name on it and shows the movies that you watched on that website. You can, for a non-trivial number of people, do a high probability match. If she watched all these movies in common with that person whose name I know, it's probably her. And that then shows a bunch of other movies you watched—that was a data leakage. There was a lawsuit.

And Netflix decided, on second thought, what I thought was really safe private data—just purely anonymized movies—in fact, leaked information.

### Privacy Laws

The U.S. has very strong protection laws for sharing movie watching, partly because 30 years ago, some reporter in Washington, D.C. went to a local video rental store, and this will appall you—I apologize. It turned out that some of our politicians were renting pornography.

And because this was then leaked to the press, the politicians in Washington passed a law saying, hey, you can't release people's movie watching to protect our privacy.

So anyway, there's a privacy issue in sharing these data sets, but to this day, Netflix does recommend stuff using neural net versions, and it works pretty well.

## Conclusion

We'll see you next week. Again, after class, I'll be here about 20 minutes for questions, and then if you want to visit my office hours, email me—I'll be there again, and we're seeing people.

Thank you!

---

# CIS 5200: Machine Learning - Lecture 18: K-Means Clustering and Gaussian Mixture Models

## Introduction to Unsupervised Learning and Clustering

And what I want to do today is really quickly cover the K-means algorithm, which I assume everybody has seen, but we'll do it anyway to remind you of, again, this duality between an $L_2$ loss function and an MLE estimation function, now in unsupervised learning.

Clustering is nice. We saw K-means implicitly for doing radial basis functions, right? We found the centers of the kernels, the radial basis functions, using the K-means algorithm, so it's a nice way to find centers or approximations. We'll see that it's a different reconstruction error, a different type of loss function.

But mostly this week, I want to focus on the **EM algorithm** (the Expectation-Maximization algorithm), which is second to gradient descent as the most widely used optimization algorithm for MLE-type problems. It's an example of an alternating descent in likelihood, and it's the sort of idea that just turns out to be useful for everything in terms of K-means (which will have a Bayesian interpretation), to filling in missing data, or imputation, as statisticians call it.

## Lecture Roadmap

With no further ado, we'll talk about clustering. We'll talk about K-means as a loss function algorithm, and then mostly I'm going to talk this week (and the slides are up—I'm not going to finish today's slides, they'll spill over to Wednesday) about the EM algorithm and Gaussian mixture models.

## K-Means Algorithm

### Basic Intuition

So the idea of K-means is super simple. You've got a bunch of points—here I've shown them in a two-dimensional space, but again, think of a high-dimensional space in the embedding space, right?

If I have a whole bunch of points, I randomly pick $K$ of them as my initialization. And I now say I'm going to iteratively do two things:

1. If I know the centers of the clusters (which are these points), I can now assign every point to a cluster.
2. If I know for every point what cluster it belongs to, I can find the center of the cluster.

Make sense? So I've got here three points. Given three points, I can divide the plane. There's going to be a line that separates this point from that point—all the points over here are clustering closer to this point, all the points over here are closest to this point, all the ones over here are closer to that point.

### Distance Metrics and Voronoi Tessellation

I've got to define "closer," right? A distance metric? K-means uses $L_2$ distance, but you could use any measure you wanted.

Oh, and the other jargon: these lines, this separation line, is called—somewhat confusingly—a **Voronoi tessellation**. Nice Russian guy. We divide up the world into hyperplanes, right? And again, I'm only drawing in two dimensions, but picture this as a $(P-1)$-dimensional hyperplane, right? And it's going such that if there's a line between these two things, this is exactly orthogonal to the line between them. It's the hyperplane defined by the line between these two points.

That makes sense geometrically? They should look really familiar.

### Hard vs. Soft Clustering

Good? This is a **hard clustering algorithm**. Hard meaning that every point is 100% or 0% in the cluster. We're going to shift pretty soon to soft clustering algorithms. In soft clustering algorithms, a point can be distributed across all the different clusters, but now we're doing hard clustering.

Okay, and again, jargon terms you need to know: **hard versus soft**. One-hot encoding is hard, right?

### The Iterative Process

Once I have assigned all these points to the three clusters here, I can now compute the **centroid** of those clusters. And that will then give me new points. Now I get new centroids. These are the centroids of these clusters here. I have the new centroids. The new centroids give a different tessellation. I keep repeating.

And we will show in a little bit that this is an **alternating gradient descent**, which means it is basically guaranteed to converge to a local minimum.

What's the magic keyword I said in that thing? Converge to a **local minimum**.

### Convexity and Complexity

Convex, non-convex?

**Non-convex**. Hard or not hard?

It's basically NP-hard, it's an integer assignment problem, it's an approximation. Now in practice, these things work really well, but realize if you're a mathematician, unless the clusters are well-separated (which I'm not going to talk about because that's a stupid limit), then in fact, this is an NP-hard problem.

But it will converge to an optimum. It's a gradient descent algorithm. And if you keep going, you get something that looks like this, and it's beautiful! Awesome.

### K-Means Algorithm Summary

So, big picture:
- Pick $K$ clusters, centroids at random
- Alternate:
  - Assign every point to the closest centroid
  - Set the centroid to the mean of the examples in it
- Repeat until convergence

And I guarantee you that for a finite number of points, it will converge in a finite number of steps. We'll see in a second why.

## Formal Loss Function for K-Means

So let's try and be a little more formal about this, because I'm going to actually, this week, be pretty mathematical about things. So I'm going to stop hand-waving for a while, just for variety.

### Defining the Loss Function

There's a loss function. Before I've used $L$, now in this world, people like the word $J$, I'm sorry. That $J$'s sort of like an $L$, pretend it's an $L$.

And what I'm going to have is two sets of parameters:

1. **$\mu_k$**: The centroids of each of the $K$ clusters
2. **$r_{ik}$**: The assignment variables

And $r_{ik}$ will just be 1 if point $i$ is in cluster $k$, and 0 if it's not.

Make sense? And of course, there's one hyperparameter: $K$, right? Controls the complexity, looks sort of like K-nearest neighbors in some funny sense of $k$—oh, except this $K$ is the opposite of K-nearest neighbors.

### Digression: Relationship to K-Nearest Neighbors

Let's digress and think about that. If I've got K-nearest neighbors, roughly how many clusters does that correspond to? I've got $n$ points over K-nearest neighbors, that leads to $n/K$ clusters! There are different $K$'s, they're opposite.

People with me? I apologize, the $K$'s are exactly opposite, they're inversely related. The number of clusters is inversely related to the number of points per cluster, right? This is the $K$ in KNN, this is the $K$ in K-means.

They are related, they're not exactly the same, so this is just order of magnitude, but it gives you some intuition in terms of your model complexity, right? We're taking $K$ principal components, now we're taking $K$ clusters. So this $K$ is a measure of complexity, whereas the other K-neighbors is the opposite direction.

### The Loss Function Formula

So does this make sense for a loss function?

$$J(\{r_{ik}\}, \{\mu_k\}) = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \|\mu_k - x_i\|^2$$

The centroid $\mu_k$ is, we'll see, the mean of the points in the cluster. So it will turn out, we have to do the derivation. This is going to be pretty straightforward—I'm going to give you the formal thing there, it's pretty simple. $\mu$ will be the average of the points in the cluster.

### Alternative Distance Metrics

Are there cases when it's not the mean? It's no longer called K-means if you wanted to do something different. You could do **K-medoids**, and instead of computing the mean, you could compute a medoid, like a centroid.

So you can do all sorts of things—you don't have to use an $L_2$ distance. You don't have to assume that... well, see, this follows. This is the dual to a Gaussian mixture model with a particular assumption. So we'll show that for a particular model, this is optimal.

But realize that you can always pick a different distance metric. If you don't want to assume Gaussian noise, if it's optimal, it will provably compute the average. If it's not a Gaussian, if it's a Cauchy or whatever else, there are probably some other functions that you might want to use, right?

So again, just like linear regression, very important point: for linear regression with Gaussian noise, using an $L_2$ error was provably optimal. What we'll see is that for a Gaussian mixture model of a given form, using the $L_2$ distance is provably optimal, and that then provably gives you the mean of the clusters there.

Make sense? So this is one example, but it's the most popular one, because people love $L_2$ distances, they love Gaussians. But you don't have to use it.

### Complexity of the Optimization Problem

Linear, quadratic, more complicated? Is it quadratic in the parameters?

It's not quadratic. It's quadratic in $\mu$ if you know $r$. If you know what point something's assigned to, now it's a quadratic problem, which means you can just take the derivative and solve for it, which will give you the average.

But if you don't know $r$, you've got to find both $r$ and $\mu$ at the same time. The parameters overall are not quadratic.

It's not quadratic. You've got $r$ times $\mu$ squared. It's not quadratic. It's got an $r$ times a $\mu^2$, right?

Isn't $r$ just a 0 or 1? Yes, and if you knew $r$, the problem is trivial! But I don't know $r$.

And the other thing is, $r$ is just a 0 or 1. What does that tell you about discrete binary things? Is it easier to find $\mu$ or $r$?

$\mu$ is real! You're going to take a quadratic thing, you'll solve for it, it'll be trivial. If it's discrete, now I've got to do a search for it. We hate discrete problems, we want continuous problems, because we want to use gradient descent on them, right?

So we'll get to a continuous version of this later, but for now, let's look at this, and we're going to find a reconstruction error.

## K-Means as a Reconstruction Problem

So one way to think about what this is doing is this is a **reconstruction error**. Now, it doesn't look like PCA, but what's the reconstruction error?

For every point $x_i$, I'm going to approximate it by the $\mu_k$, which is the center of the cluster it's in. If it's not in the cluster, it's zero, I don't care. So this is the quadratic reconstruction error, right? It's an $L_2$ squared reconstruction error. It says: approximate every point $x_i$ with the centroid of its cluster. And I want to minimize that reconstruction error.

So this is, in fact, a reconstruction problem. Does that make sense?

In the old days, before I was born, they would call $\mu$ the **codebook**. And in the days of efficient transmission, instead of sending $x$, you would send the $\mu$ as an approximation to it. I don't know, that's before my time, but I'm told that used to be a codebook. And occasionally, you see literature still using that term. But mostly, it's just an approximation: I approximate $x$ with $\mu$.

Cool, so I'm interested in finding both the $\mu$'s (to use for radial basis functions) and the $r$'s (because I can cluster a bunch of things together). Often people group together genes, or RNA, or proteins, or words. I group lots of words together, group words in the same cluster, right? Okay, they're not Gaussians, I'll use a different model, but it's the same idea.

## The K-Means Optimization Algorithm

So, this is the same formula I just wrote. And the algorithm for K-means is the following.

### Step 1: Find Best $r$ Given $\mu$

I'm first going to say: find the best $r$ given $\mu$, right? So assume that I know what the centroids are, and assign every point to the single cluster (hard clustering) that minimizes the reconstruction error.

More formally, this is:

$$\hat{r} = \arg\min_{r} J(r, \mu)$$

And the way that's going to work is I'll set $r_{ik}$ to 1 if $k$ is:

$$k = \arg\min_{k'} \|\mu_{k'} - x_i\|^2$$

It's a bunch of math saying: pick the $r$, the assignments, such that they minimize the reconstruction error assuming you know $\mu$. Make sense? Which is just the closest one.

### Why Not Use Standard Gradient Descent?

Yeah. Why are we looking at this algorithm? Why not just do the whole thing with one single gradient descent? I got a loss function, I could take $\frac{\partial J}{\partial r}$, $\frac{\partial J}{\partial \mu}$, walk down the gradient.

Why not... why are we doing... why are we spending a whole week on an alternating optimization, when in fact, we could just do the whole thing as a single gradient descent, like a good neural net person would?

Good question. Yeah?

**The $r$'s are discrete**, so in some sense, if we do the optimization, we're going to have to do something that moves through an infeasible optimization space. So we have to try $r$'s that are not 0 or 1, and hope we get there somehow, and then there's no way to guarantee that things are nice and clean. In fact, there's no way to guarantee anything, because the problem's NP-hard.

So in fact, it doesn't work really well for a combinatorial problem—something where we're trying to assign things—to use gradient descent.

Why don't you take them out? Throw the $r$'s out, what are you going to do? The whole point is that each point is in exactly one cluster. That's the whole game here: every point gets into one cluster. That's the hard constraint.

And that constraint makes the thing incredibly nonlinear and non-convex and non-optimal in terms of guaranteed solution, and it means we need a different solution method, or at least an alternating one.

So, we first assign each point to a cluster $k$.

### Step 2: Find Best $\mu$ Given $r$

Now, the second step is, given the current estimates $\hat{r}$ (the current assignments of points), now what I want to do is minimize, find the $\mu$ that minimizes $J$:

$$\hat{\mu} = \arg\min_{\mu} J(\hat{r}, \mu)$$

Now, if we think about what that math is going to look like, I'll just sort of sketch it out and not do all the math, but given the $r$'s, I've assigned each point to a cluster. So the only thing that exists is:

$$\sum_{i \in \text{cluster } k} \|\mu_k - x_i\|^2$$

Right, these are vectors, right? $P$-dimensional vectors. How far is $x$ away from $\mu$? I've got each point in the cluster, and if you ask what $\mu$ minimizes the average distance (this summation over the points), the one that minimizes it—if you take the derivative, because now if $r$ is fixed, this is just quadratic—take the derivative with respect to $\mu$:

$$\frac{\partial J}{\partial \mu_k} = \sum_{i: r_{ik}=1} 2(\mu_k - x_i)$$

Setting this to zero gives you something that looks like: pick the $\mu$ to be equal to the average of the $x$'s in that cluster, which is:

$$\mu_k = \frac{\sum_{i: r_{ik}=1} x_i}{\sum_{i: r_{ik}=1} 1} = \frac{1}{n_k} \sum_{i: r_{ik}=1} x_i$$

The summation of the $x$'s in that cluster divided by the number of points in the cluster.

Does that sort of make sense? I mean, geometrically now, I will wave my hands. If I got a bunch of points here, and I say, hey, I want to pick some $\mu$ such that I have the minimum, right, the:

$$\arg\min_{\mu} \sum_{j=1}^{M} \|\mu - x_j\|^2$$

over $\mu$ of the summation of $j$ equals 1 to $M$ of these $M$ points here (these are the ones where $r$ is equal to 1). I want the minimum of $\|\mu - x_j\|^2$.

Then the point that does it is going to be the one that's in the center—the average of them, right? And again, take $\frac{\partial}{\partial \mu}$ of this, you'll get $2(x - \mu)$. When you sum it, you'll get the thing there, and yeah, it works out.

Does this make sense? The average is the one that minimizes the distance between them.

### Clarifying the Notation

Yes? What is $k'$?

Oh, the $k'$? This is the minimum over $k$. $k'$ is a variable, right? The minimum over the $k'$ of $\|\mu_{k'} - x_i\|^2$. So you're checking each of the different capital $K$ $\mu$'s. You're going to try each of them, and you're going to pick the one that is minimum, where $x$ is closest to it. So it assigns every point $x$ to the closest one, yeah.

How do we decide which $r$ equals 1? All this math says is: for each of the different clusters $k$, with $k'$ being the dummy variable that iterates over them, check that centroid of that cluster and say, hey, how far is it away from $x$?

I'm trying to assign $r_{ik}$. I'm trying to assign each point $i$ (observation $x_i$) to a cluster $k$. And the $k$ I'm going to assign it to is the one such that $x_i$ is the minimum distance from that centroid, right?

So again, if I have a bunch of points, and I've got some $\mu_1$, and I've got some $\mu_2$, and I've got some $\mu_3$—if I take some point here, I'm going to say: how far is this point, $x_i$, from $\mu_1$? How far is $x_i$ from $\mu_2$? How far is $x_i$ from $\mu_3$? It's closest to $\mu_1$.

Assign it to that one. So $r_{i1} = 1$, $r_{i2} = 0$, $r_{i3} = 0$. So I've assigned the point $x_i$ to the closest centroid.

### Key Point: Alternating Minimization

And the key point is, this is a minimization problem, right? So step back: I'm minimizing $J$. And to minimize $J$, I get to pick two sets of parameters. I pick the $\mu$'s, and I pick the $r$'s.

I'm going to go back and forth, alternating between picking the $r$'s given the $\mu$'s, and picking the $\mu$'s given the $r$'s. Each of them is a minimization problem.

And this is an **alternating minimization problem**, an alternating gradient descent. Well, it's not really gradient in some weird sense—it's an alternating minimization, but each of these steps is **guaranteed to reduce, or at least not increase, $J$**.

So when I pick $r$, if I change it, if I do a different assignment, it'll reduce $J$. When I pick $\mu$, if I change it, it'll reduce $J$. So each of these reduces $J$ until it stops changing. When it stops changing, it's converged.

And because the assignment is discrete, there's a finite number of steps. You change, you change, you change. At some point, you're guaranteed it won't change again.

So it's very clear that each of these steps is guaranteed to be decreasing $J$, or if it's not decreasing, we've stopped changing assignments to new clusters, and we're done.

So this is like a gradient descent. There's no gradient, but it's a descent.

### Non-Convexity

Yes? Is this convex?

It's **not convex**! This is a super important point! This is not a convex problem. In fact, it's very non-convex.

Because it's a discrete problem with a hard one-to-zero assignment, it's NP-hard. The worst case is to check over—if you had infinite compute, which I don't, because I'm a machine learning person, you would try all possible assignments of points to clusters. And if you tried all possible assignments to clusters, you'd plug each one into the loss function, and by exhaustive search, you could find the best $J$.

But that is too expensive in any world in which I live in.

Make sense? So, super important: **non-convex**, right? Given $r$, finding $\mu$ is quadratic. It's trivial. It's not just convex, it has a closed form.

But finding the $r$'s—how should you assign them? That's a non-trivial question.

### What is NP-Hard?

Yeah. What is NP-hard? The answer is, for this course, don't worry, it just means too expensive to do in the real world. I'm not going to—this is out of scope for this course. For people who like the more mathematical thing, it's a very formal definition of "hard."

But just trust me, what it means is: finding the optimum is too hard. If you want to know how hard, just think of a simple question: **How many possible clusterings are there?**

If I have $n$ points assigned to $K$ clusters, how many assignments are there?

Yeah, if you want... $K^n$, something like that, right? It's going to be ballpark. It's going to be something that's going to be exponentially big.

The reason it's non-convex is because of $r$, because you don't know which cluster each point should be in, absolutely. And that's what makes the alternating approach necessary.

If the clusters are widely separated, then the whole problem is easy. Something like $K^n$ ballpark, yeah. It's big. Too big!

## Convergence Properties and Local Optima

Cool. Now, note this has the property—because it's a descent (you're decreasing the loss function) and it's non-convex—you can get stuck in a crappy place. If you start from a bad guess, right? So here's a different guess: here, here, here.

You can converge, and here is the result, having run the code. Thank you, Andrew Moore, who I stole the slides from, with permission.

What you see is now I've got cluster 1, cluster 2, and cluster 3! This is a **local optimum** to the function $J$ I wrote before, found by converging from a bad start, right?

So if you start badly, you can do things. I get weird stuff. I've started with $K$ clusters and end up with $K-1$ clusters at the end. It's like, whoa! We did this convergence, we lost a cluster along the way somewhere, right?

So the gradient descent guarantees it will converge to something because you're decreasing $J$ every step, but it's non-convex, so you've got no guarantees.

### Can You Lose a Cluster?

Yeah. Is there a bound for this? No, it's hard to bound it, unless things are well-separated, in which case the whole thing's trivial. But the answer is it's still bad.

So if you start badly, you end up with bad clusters. If you start better, you end up with better clusters.

Might you lose a cluster? Yeah, it seems it shouldn't be. I'm going to leave that as an exercise for the reader. Maybe we'll have the recitations do it. It's a weird thought. It doesn't seem it should happen.

But I think the answer is things can converge to degeneracies so that two centroids end up in the same place. Yeah, because you can get degeneracy.

### Practical Solutions

Yeah, so in the real world, there are a couple of ways that work. **scikit-learn** builds in algorithms that either run K-means clustering 10 times and pick the best of the 10. That's not a bad idea, because I got $J$. I got the loss function, I could run it as many times as I want, I pick the lowest loss function, so running it 10 times is popular.

I can try and pick these starting points to be maximally spread out, to make them far away if I want. So there's a bunch of hacks that people have to try and make the thing converge somewhat more reliably.

But again, provably, you don't get guarantees on their performance; you just get better performance. Personally, I like just running the thing 10 times and taking the best solution. Seems to work perfectly fine. And it's 10 times the compute cost, but the compute cost is nothing, so I don't care.

## Application: Image Segmentation

I'm going to move on. So what you can do is—this is not how anybody does segmentations in the modern era; people do segmentation by using deep learning methods—but you could actually say, hey, I've got every point here is a pixel with an RGB and an X-Y location. If I define a distance between them (which is complicated because it's both spatial distance and color distance, so I've got to have some weighting between them), I can now run a clustering algorithm on the pixels, and I get a bunch of clusters. Here I've picked $K = 3$.

And you can see that there are three clusters of pixels, and they sort of get the notion of ground, tree, and background. And you can say, maybe I should have had 4, so I had the house separately.

### Choosing the Number of Clusters

How many clusters is optimal? What's the right number of clusters $K$?

Who knows, right? If you want interpretation, it's a hyperparameter. If you're using this as a first step and then using it in supervised learning, you could check to see how well this preprocessing leads to a final supervised learning result. That's one way. If you want interpretation, you ask the person you're interpreting to: three clusters, 5 clusters, 8 clusters—how many, right?

So there are a bunch of ways to try and look at it. But in general, it turns out to be a somewhat arbitrary decision, often driven by the end user, what decision they want to make. There's some decision maker, and you say, hey, I've got these 3 clusters or 5 or 7—you're the biologist, which one do you want? And they pick the one they like.

Yeah. If it's clearly separated clusters, you'd get, like, the scree plot. Remember last time with PCA, we showed that each component explains less and less variance? In practice, the world is rarely cleanly separated.

### When K is Meaningful

In rare occasions, and we'll talk later about Gaussian mixture models, there are true mixtures of the world. I worked with a statistician once who was interested in various bacterial infections, and people would have 1, 2, 3, or 4 different bacteria growing in them. This is a latent variable, right, so it's a mixture of, say, 3 different bacteria in you. You then observe a single $x$ for each person (a bunch of blood proteins and such), and you're trying to infer or impute the hidden state of how much of each of these three bacteria are present.

Make sense? In that case, $K$ is really meaningful. If you actually sequence the bacteria, you can find them. They're discrete species of bacteria. And they're different, and you got 0, 1, 2, 3, or 4 of them in you, and you only observe one $x$. And so that $x$ is generated by a mixture model of bacteria.

So sometimes they're real mixtures, but mostly K-means is used in a purely fictitious world. There is no real $K$. How many clusters of words are there? If I break all you students into clusters? Well, what's the right similarity or distance metric? And then how many clusters of students are there? Everyone's unique. There could be $K$ clusters, right?

So it is somewhat unclear for a given application. You have to think about the application: how many clusters make sense?

### Distance Metrics for Image Segmentation

Yeah. $L_2$. But realize that the trick that is tricky is, if you've got something that's only in one space—if it's only X and Y, then the clusters (this is clearly not the case)—if you use only X-Y distance, you'd get things that were actually contiguous to each other. If you used only RGB, you can get one thing, so RGB sort of makes sense to use a metric on, and that's pretty good. You can do RGB, it's basically that one.

Ideally, you want some mixture of RGB plus X-Y distance, now you've got a hyperparameter that specifies in your distance metric: how important is it that objects be physically close versus that objects be color close?

Make sense? And what is that? That's going to be application-dependent, right? In this example, the algorithm did pretty well with RGB. Just take the three things, weight them all equally, use Euclidean distance.

## Summary of K-Means

Cool. So for this section, what should you know? That K-means has a clean loss function, it's an $L_2$ reconstruction error. And in that, you need to find both the centroids and the cluster membership.

So, jargon: the $r_{ik}$ is **cluster membership**—is point $i$ in cluster $k$ or not, right?

And usually people start by doing $K$ random points. If you go to **K-means++**, for example, in scikit-learn, it says: choose centers at random from the data points, but weight the points according to their squared distance from the closest center already chosen. So it picks a point, then it finds one that's far away, then it finds one that's far away from all those, finds one that's far away, so it finds a set of starting points that are far from each other.

### Outliers and K-Medoids

Yeah. There's something weird here about outliers. K-means is not going to be super stable for outliers, because everything is in $L_2$ space, right? And so outliers are going to have a big influence.

If I got something that looks like... depending upon—let's do $K = 2$—depending upon my initialization, either I get one cluster here and one over here, but that's highly unlikely. If I pick points at random, I'll pick this point and that point, and then I'll get one cluster here, maybe, and one cluster here. Or maybe it'll go with one cluster there and one there.

But if you've got outliers, the $L_2$ distance is problematic because the $L_2$ distance (which we have not yet shown, but it comes from the Gaussian distribution, but we saw that before for linear regression)—so if you've got outliers, you might not want to do it.

The other algorithm that's worth knowing is **K-medoids**. The $\mu$ is not forced to be a point—right? The $\mu$'s are not in the training set necessarily. I might want to say, instead of picking the centroid (here's $\mu_1$, here's $\mu_2$), I might want to pick the point in the training set that's closest to the center, right?

In that case, $\mu_1$ would be here, and $\mu_2$ would be here, because $\mu_2$ has to be a point in the training set.

Now, sometimes that's nice, because if these are images, the average image is pretty bizarre, right? I've taken this image, this image, this image, and I've averaged them; I got an average image. That's sort of crappy. I hate averaging images.

Either I do a K-medoid and pick one image to be the center, or one movie or something, instead of averaging movies, or...

### K-Means on Images

How would you run K-means on images sensibly?

**Embed them first!** Embeddings give things in a nice space where $L_2$ distance makes sense. So if I'm going to do K-means on images, I'm going to do K-means on their embeddings, not their pixels, right? Distance makes sense in embedding space; distance doesn't make much sense in pixel space.

Yeah. I'm going to pass on that question. I want to keep moving, sorry.

## Transition to Gaussian Mixture Models

Cool. And there's a probabilistic interpretation.

Okay, switching gears entirely, but not at all. For the rest of the week, I'm going to talk about **Gaussian mixture models** and the **EM algorithm** (the Expectation-Maximization algorithm). And we'll see that this is, in some sense—well, we'll see that K-means is a super special example of this much broader algorithm.

### The Idea of a Mixture Model

So the idea of a mixture model is that we're going to generate $x$ with some probability. And the way we're going to generate it is we're going to pick some hidden value $z$, which will look sort of like—we'll see that this piece will probably look like an $r$. We're going to say, hey, with some probability, we'll pick which of the $K$ clusters it's in.

So $z$ is going to look like: what cluster did I assign this to, right? And I'll have a $z$ for every $i$, for every point. Every $x$ will come from one of the clusters, and I'll have some probability distribution over clusters, right?

### The Mixing Coefficients

And the simplest way to write that is with $\pi$. So $\pi_k$ is a $K$-dimensional probability distribution, right? $K$ numbers between 0 and 1 that sum up to 1. And that's the probability of coming from each of the $K$ clusters.

We don't know anything about it, right? It's roughly suggesting what the size of the cluster will be, but it's not a hard constraint, it's a soft constraint, right? And of course, we never know it. This is the truth underlying the world, like the noise in linear regression. You can never know what it is; you estimate it.

Make sense? So I'm going to pick which cluster. I've got $K$ clusters—here they are: cluster 1, cluster 2, cluster 3. Maybe this one has a probability of 0.7, and this has a probability of 0.2, and this has a probability of 0.1. I pick a number using this probability, oh, it's cluster 1.

### Generating Data from a Cluster

Now, once I pick the cluster, now I'm going to generate $x$ with some probability distribution, which is dependent upon that cluster. And the simplest one is to say it's a normal with mean $\mu_k$ and covariance $\Sigma_k$, right?

So I could have: here's cluster 1, here's $\mu_1$, and here's $\Sigma_1$ is this covariance matrix—maybe it's symmetric. Here's cluster 2, here's $\mu_2$. Here's the cluster there. Here's $\mu_3$, here's its center, and here's its—whoa! This one's got a weird covariance matrix, right? Sort of long and narrow and tilted. But that's okay. There's no reason it can't be.

Make sense?

### The Parameters

So my full model here says I'm going to have $K$ numbers that specify the $\pi$'s. Well, it's actually $K-1$ degrees of freedom, right? Because they have to sum up to 1.

And for each of the $K$ clusters, I've got a centroid, and I've got a covariance matrix.

Given this model, I can now generate data. In machine learning, this is called a **generative model**, in the sense that you can generate as many $x$'s as you want from it. In statistics, it's usually called... a **model**, right? But I will tend to use the machine learning terminology, because we're in a machine learning course, not a stats course. I generally call it a generative model.

Make sense?

And what are the parameters? The parameters are $\pi$, $\mu$, and $\Sigma$. And there are $K$ of each of them. And then there's one hyperparameter: big $K$, right? The number of clusters.

### Maximum Likelihood Estimation

So this is a specification of a model. I now want to estimate my parameter set $\{\pi, \mu, \Sigma\}$. I want to build a maximum likelihood estimator for this, which I want to estimate with a sort of gradient descent.

But it will turn out that just like for K-means, the most easy way to estimate it was back and forth between estimating the $r$'s (which looks sort of like the $\pi$'s) and the $\mu$'s and $\Sigma$'s (which look sort of like the $\mu$'s), right?

You already see K-means looks sort of like this, but there's no $\Sigma$ in K-means, right? The K-means assumes that everything is spherically symmetrical, same covariance matrix, right? So K-means is a special case.

So what we're going to do is try and estimate these. We're going to then use an alternating expectation method that will do a gradient descent. And it will look a lot like K-means, but be fancier, and it will take us over an hour to do, so I won't finish it today.

Cool for the roadmap? Hopefully cool.

## Maximum Likelihood for Gaussian Mixture Models

Okay, so let's start it. So the parameters are $\{\pi, \mu, \Sigma\}$ for capital $K$ different clusters.

And what do I want? I want to maximize the log-likelihood of the data. What's $\mathcal{D}$? $\mathcal{D}$ here are the $n$ $x$'s. I'm sorry, the $n$ $x$'s, right? I've got $n$ points. So that's my data—the $n$ points, $x_1$ to $x_n$. They're vectors, right? Everything's vectors.

### The Log-Likelihood Function

And what does $\mathcal{L}(\pi, \mu, \Sigma)$ look like? It's:

$$\mathcal{L}(\pi, \mu, \Sigma) = \sum_{i=1}^{n} \log P(x_i | \pi, \mu, \Sigma)$$

The summation, $i$ goes from 1 to $n$ points, of the log of the probability. What's the probability? It's the same thing I wrote in the first line. The probability is:

$$P(x_i | \pi, \mu, \Sigma) = \sum_{k=1}^{K} P(z_i = k | \pi) \cdot P(x_i | z_i = k, \mu_k, \Sigma_k)$$

So we have a summation over little $k$ goes from 1 to big $K$ of $P$ of $z$ equals $k$. I've done a funny notation thing here. Sometimes people like to put a semicolon $\pi$, sometimes like to put a subscript. So maybe a little easier to read with the subscript. So note that:

$$P(z_i = k | \pi) = \pi_k$$

The probability that point $i$ comes from cluster $k$ is parameterized by $\pi$, right? $\pi$ is the thing I need to know to know how likely that is to come from that cluster, right? $\pi$ is the parameter, and $P$ is the probability.

Right, so again, the notation is sometimes a little confusing. There's a true probability $P$, but we don't know it. We're going to estimate it with a parametric model, and the parameter for that's going to be $\pi$.

$P$ doesn't depend upon $\mu$ and $\Sigma$.

Similarly, the other piece, we're going to say: the probability of $x$ given $z$ equals $k$, right? Probability of $x_i$ given $z$ equals $k$. That's a different probability distribution, right? This normal distribution. And the normal distribution here has parameters $\mu$ and $\Sigma$ in it:

$$P(x_i | z_i = k, \mu_k, \Sigma_k) = \mathcal{N}(x_i | \mu_k, \Sigma_k)$$

And note, critically, that the normal distribution doesn't depend upon what $\pi$ is. It doesn't depend whether a cluster has a lot of points or a few points, and the cluster with a few points or a lot of points doesn't depend upon $\mu$ and $\Sigma$, right?

So it's a **factored model** where the parameters come in two bunches. One bunch is $\pi$ (the cluster membership probabilities), and one bunch is $\{\mu, \Sigma\}$ (the distributions of the points within a given cluster).

### Why Take the Log?

Yeah. Why do we take the log? You don't have to take the log; you could always deal in actual likelihoods. It's always a good idea to take the log. If you're an engineer, if you multiply all the probabilities of your $n$ points, it becomes vanishingly small—a product of a million tiny probabilities.

If you're a mathematician, the probabilities multiply, and the math is sort of ugly, whereas the log probabilities add, and the math is much nicer.

This week, we're mostly being mathematicians, and so what we want to do is put this in a form that's easy to optimize. Maximizing the likelihood is exactly the same as maximizing the log-likelihood. It's mathematically equivalent. But the log-likelihood turns out just to be much nicer in terms of doing the math because I have the summation over $i$ of the logarithm.

### The Challenge: Log of a Sum

Now, hang on, there's going to be a problem that's going to annoy us. The reason it's going to take over an hour instead of 15 minutes is: **can I pass the log through a summation?**

I can't, right?

So this crappy thing is going to force me to do Jensen's inequality and elbows (ELBOs) and all sorts of ugly math, which I don't really care about, but I do want you to have the basic idea. And because I can't pull the log through a summation, I'm going to have to do some sort of hacky, clever, mathematical tricks to pull the log through the summation, which I'll end up bounding it and showing that I have a bound on what's going on. So the math is going to be sort of ugly.

But if I could pull the log through the summation, I'd have the log of the probability of the membership plus the log of the probability of the normal, and that'd be awesome! But unfortunately, it's not going to work out so nicely, because I can't pull the log through the summation.

Which is just crappy, but it does let me show you a cool math trick. I promise more math.

### Understanding the Current Formulation

People good on this piece here? Because I think this is the core. Before we had $J$, which had $r$'s and $\mu$'s. Now we have this log-likelihood, which has $\pi$'s, $\mu$'s, and $\Sigma$'s.

Yeah. What is $z$?

$z$ is, for point $i$, what cluster it's in, right? So before we had $r_{ik}$, which said $r_{ik}$ is 1 if $i$ is in cluster $k$, 0 otherwise. Now I'm going to have $z$—we're going to have a **soft clustering** now instead of a hard clustering.

And so instead of having each point be hard in 1 and 0 in the rest, we'll have a probability distribution over all clusters. And so I need to know, for a given point $i$, what's its probability of being in each of the $K$ clusters?

So for $z_i = 1$, I'll have a probability of that point $i$ being in cluster 1. For $z_i = 2$, I'll have the probability of point $i$ being in cluster 2.

### Expected Value Interpretation

So note what I'm doing is I'm taking a sort of expected value of $P$ here. I'm summing over the $K$ clusters: how much does point $i$ belong to each of the $K$ clusters, right? So this probability $P$ here is a cluster membership or a distribution over the clusters for the point, right?

And it really is an important difference. Before, each point was in one cluster. Now every point is smeared out over all $K$ clusters.

So a given point is going to have $K$ numbers. Before it had $K$ numbers too, but they were one-hot. Now, each point has $K$ numbers in their distribution.

Make sense? So this $z$ is telling me, for this point $i$ (for each of the $n$ points, there'll be a $z_i$), and that $z_i$ is going to be: if it's in cluster 1, here's how much it's in it; here's how much it's in cluster 2; here's how much it's in cluster 3.

Make sense? Great question.

### $z_i$ Takes on Multiple Values

Yeah. If $z_i$ is no longer 0 or 1, $z_i$ is going to take on each of the $K$ values. And what you see here—again, think of this summation over $k$ as something that looks like an expectation.

What I'm doing is I'm weighting this likelihood. This is how likely the $i$-th point is if point $i$ is in cluster $k = 3$. And given the $\mu$ and $\Sigma$ of that cluster, how likely is that point to come? This is the probability of seeing a point if it were in cluster 3.

But I've got to weight that by: how likely is that point to be in cluster 3? So this is the **expected value** over all of the $K$ clusters of this $i$-th point, right?

If you take a point over here, call this $x_{300}$. $x_{300}$ has a certain degree of membership in cluster 1—maybe it's 40% in cluster 1, maybe it's 50% in cluster 3, and okay, if it's 40% and 50%, it's got to be 10% in cluster 2, right? So the probabilities have to sum to 1.

And now, if I say, what's my likelihood, my probability? It's the expected value of: how likely is this point to come from this center times 0.4, plus how likely does it come from this center times 0.5, plus how likely to come from this center times 0.1.

So I'm taking a weighted value, or an expectation, or an average—but maybe average is a weird word because it's weighted by the probability.

So that's what this piece is doing here, right? So instead—again, this is a soft clustering because this point is 40% in this cluster, 50% in this cluster, 10% in that cluster. It's a soft clustering. Points are not hard-assigned to a cluster; they're spread out over clusters.

### Not MAP Estimation

Yeah. This looks more like an MAP. No, this is not an MAP. You can do an MAP version, but then I would have a prior. No, so this is not an MAP. And we're not going to do the MAP version. You could do an MAP version, but we're doing the MLE version. It's the max or argmax, where we're finding the parameters that maximize the likelihood of the data, not the most likely parameters.

Cool. Good. We have plenty of time for questions, by the way.

## Covariance Matrix Choices

Okay. So I now want to go on a very long digression, and then I'll come back to how to estimate this thing. And the digression I want to do is: as a modeler doing Gaussian mixture models, you get to pick what sort of models you want for the covariance matrices.

So you're going to have $K$ clusters (hyperparameter), you're going to have centroids of them (that's a $P$ or $M$-dimensional vector, whatever your feature space size is—you've got no choice over those). But the covariance matrices, you've got a lot of choices.

And they range from having a **full covariance matrix** to being a **shared spherical one**.

So let's sort of look at maybe what some of these look like.

### Spherical Covariance Matrices

So I got a bunch of points. Let's pick $K = 3$. I could say, hey, I'm going to require all three of my clusters to have a covariance matrix—and these are supposed to be exactly spheres, and all the same radius, right? $\sigma$, $\sigma$, $\sigma$.

So I could say the covariance of every single cluster—every cluster—is radial, right? It's got a covariance matrix of $\sigma^2 \mathbf{I}$ (sigma squared times the identity matrix), as we talked about with RBF, right? The radial basis function.

There we said, hey, our basis functions will have whatever centroids, but they're all the same size—that's one hyperparameter, right? A $c$ or a $\sigma$, which says: how big are these, and they're all the same dimension?

Make sense? I could let them be all spherical, but maybe I want one to be big and the other ones to be different sizes, right? If I think some parts of my space are very spread out.

### Counting Parameters

Oh, by the way, how many hyperparameters? If I have a single shared covariance matrix, the number of hyperparameters is... a parameter, sorry? **1**, right? Radial basis function, there's one width for all of them.

If each can have their own width, then there's $K$ parameters to fit.

But maybe I'd like to have them be **diagonal**. You could have them all be diagonal, the same ones, or you could have, say, that one's like that, and this one's looks like that! Now I've got something where I've got $K$ different things, each of which are—well, how big is $\Sigma$?

$\Sigma$ is $P \times P$. Ballpark, how many parameters does it have? $P^2$, if you're going to be sloppy. A little more precise: $P^2/2$, because it's symmetric—covariance matrix is symmetric. If you're being very fancy, then the diagonal only shows up once. But we'll say that this is of order $P^2/2$ parameters, right?

So it's **a lot of parameters** to have a full covariance matrix, right? So people mostly don't do that, but sometimes you do. How much data do you have? How fancy a model, right?

### Range of Covariance Models

So the simplest version is a radial basis function, right? A shared spherical thing where you've got one parameter $\sigma$.

The next one up is sort of from number 5 here: there are $K$ different ones, each are radial.

And the full one over here is basically $K$ clusters times—instead of $P$, it's $M$ (sorry, we're in unsupervised learning)—$M^2/2$ parameters.

And you can do anything in between. I don't care so much—well, I think the important thing to realize is this is **a design decision**, right? How fancy a model do you want to fit?

The math all looks pretty much the same. I'm going to tend to do the super simple shared covariance model, which is the one most people use. But you could write the other ones with a little more matrix math.

### Why $M^2/2$?

Why is it $M^2/2$? What is $M$? Number of features, right? It's an $M \times M$ matrix, and it's—and the important thing is the covariance matrix is **symmetric**. The similarity of any two features is symmetric.

### Diagonal Covariance (Naive Bayes)

Cool. I'm going to mention one more, because we'll sort of mention it briefly in the future. You can say that I want to have every cluster have a **diagonal covariance matrix**, right? So all of the features are uncorrelated with each other, but you could let each feature have its own size, its own variance.

So you could have something that says, hey, if I've got two features, $x_1$ and $x_2$, maybe they look like this. And the covariance matrix looks sort of like that, which looks like the first principal component going up that way, second one this way. And I say, hey, the covariance has a $\sigma_1$—well, $\sigma_1$ is this one, $\sigma_1$ that's this big, and a $\sigma_2$ that's that big.

Make sense? If the features are assumed **uncorrelated**, that—what we'll see end up being called a **naive Bayes assumption**. The "naive" part says uncorrelated.

And that's pretty simple, because I now have $M$ features (or $M$ variances) for each cluster, rather than $M^2/2$. So that's at least plausible.

## Distance Metrics from Covariance Matrices

I think the final thing to note is: in all things, as you look at these covariance matrices, when you look at the Gaussian distribution, if you derive the corresponding distance metric, what's going to show up every single time is the distance is going to have a $\Sigma^{-1}$ in the middle of it.

So we before looked at distances that looked sort of like—I just erased it, right? It looks sort of like:

$$d^2 = \|\mu - x\|^2$$

Or we said, hey, if there's a single $\sigma$, then often these things had a $2\sigma^2$. Remember this sort of thing from before, from the Gaussian? Coming from the norm?

Now what's going to happen is we'll have something that looks, instead of $x - \mu$ times $x - \mu$ transpose:

$$d^2 = (x - \mu)^T \Sigma^{-1} (x - \mu)$$

Sorry, $\mu - x$ might be consistent: $(\mu - x)^T \Sigma^{-1} (\mu - x)$.

And so we'll end up with a new sort of distance metric which says that the distance between $x$ and $\mu$ depends upon the inverse covariance matrix.

### Positive Semi-Definite Property

And the nice thing is, the covariance matrix has what mathematical property? **Positive semi-definite**.

And that means that anything you put as a positive semi-definite matrix in the middle, this whole thing is still a norm, still a distance. So the cool thing is that when you measure distance weighted by a covariance matrix in the middle, **it's still a distance**.

And by distance, I mean exactly the properties I gave for distance before. So the nice thing about these—all these covariance matrices with different structures—is they all give distances, right?

So we have a Gaussian, which is $e^{-d^2/2}$, sort of thing, right? Or there's a half in the Gaussian. Or you have something that says, from the Gaussian, we get a distance.

Does this make sense? I'm waving hands again, because I don't really want to go into the math, but I do want to note that it is important the covariance matrix is positive semi-definite, and it is important that all these different covariance choices use a real covariance—subtract off the mean, get this symmetric piece there, and it's like a squared sort of thing. And it will then give you something that when you plug it into the Gaussian, if you do the distance version, the log of the exponent works out.

We'll work out one example of it to show it cleanly.

## Model Choices Summary

Cool. So there are a bunch of choices you have for the mixtures. And really, there are two things to pick.

### Choice 1: Membership Distribution

One is, **what is your model for the membership distributions?** And the choices are really either:

- Either you say, hey, I'm going to assume that all of the $\pi$'s (all of the clusters) are equally likely to generate a point—a **uniform distribution**.
- Or I'm going to fit $K-1$ parameters.

Am I assuming that every cluster has the same number of points in it? Absolutely not, right? It's not a hard assumption. It's the model. The model is they're each generating points from a uniform distribution. The chance of them all being exactly the same is pretty small. That would be good luck or bad luck, but just would be luck. But they tend to be roughly the same.

Make sense?

### Choice 2: Covariance Matrix Structure

So two choices. One choice is: **what's the covariance matrix?** Right, you always have a $\mu$. What's your covariance matrix?

And the simplest choice is: they're all shared spherical.

The second choice is: **what do you assume about the sizes of the clusters**, the probability of clusters? Either you assume they're all equal probability, or you don't.

## K-Means as a Special Case of GMM

Now we can go back and talk about K-means, because I wanted to argue that K-means is, in some sense, an $L_2$ piece that comes from a probability distribution.

We saw a loss function before, but we didn't see what the assumptions are behind K-means. And I like to know what the assumptions are, because I want to know: should I use K-means or something else?

### Assumptions Leading to K-Means

So if you assume that:
1. The mixture weights are all equal
2. The variances are shared spherical
3. (I think the limit of $\sigma$ goes to zero, but that one's less important)

You get back out K-means!

So K-means is trying, in its loss function, to give you clusters that are all roughly the same size. That's sort of cool, as long as you believe the clusters are all roughly the same size. If you think they're not, it's a stupid model, right?

And it's sort of—if you think really hard about the loss function, you can sort of figure it out. But I find it a little more intuitive to say, hey, let's start with the Gaussian mixture model and sort of see where the K-means comes from, right? But there is a duality between them.

### Deriving K-Means from GMM

Let's take a second and look at K-means, because we're going to do the EM mostly on Wednesday. We still have to do the MLE estimation, the gradient descent, the alternating piece there. Let's get rid of all of this... um, cool!

So what do I want? I want to find the maximum likelihood:

$$\arg\max_{z, \mu} \mathcal{L}(z, \mu)$$

What are my parameters? The parameters are $z$ and $\mu$.

Where did the $\pi$'s go? They're all $1/K$. I don't need to estimate them; I just assumed they're all equal, right? So I'm making a model where I say that the $\Sigma$'s are $\sigma^2 \mathbf{I}$.

And I'm going to drop the $\sigma$ from my argmax, so it should be $z$, $\mu$, and $\sigma$. But it's going to turn out the $\sigma$ is really not going to matter, the same way it drops out of the linear regression problem.

So I'm going to take the argmax over $z$ (which are: how much does each point belong to each cluster—we're still soft so far) and $\mu$ of the log-likelihood.

And I can write this, if you will, as the log of the probability of the data. I'm going to write it now with the $z$'s, which I don't know, because I'm trying to figure out—the $z$'s are hidden, right? They're **hidden variables** or **latent variables**, non-observable variables. That's the assignments of things to points.

$$\mathcal{L} = \log P(\mathcal{D} | z, \pi, \mu, \sigma)$$

Given $\pi$ (which in fact we know) and $\mu$ and $\sigma$ (which I don't know).

And I can write that as—what does this piece look like? It looks like the log of:

$$\arg\max_{z, \mu} \sum_{i=1}^{n} \log P(x_i | z_i, \mu, \sigma)$$

I need the summation over $i$ goes from 1 to $n$, right? So I have:

$$\arg\max_{z, \mu} \sum_{i=1}^{n} \log \left[ P(z_i | \pi) \cdot P(x_i | z_i, \mu, \sigma) \right]$$

The log of the probability of the point being in cluster $z$ times the probability of the point given that it's in cluster $z$.

This makes sense? The log of these two pieces—the first piece, $P(z_i | \pi)$, is what? Probability of point $i$ is in cluster $z$ is $1/K$, right? So this first term is $1/K$, so I have this probability piece, which is going to be the log of $1/K$.

And then I have a second piece: $P(x_i | z_i, \mu, \sigma)$, given $\mu$ and $\sigma$ (which is the same for every cluster now, right? Every cluster has the same $\sigma$—no, $\mu$'s different). Every cluster has its own $\mu$ and the same $\sigma$.

### The Gaussian Distribution

So I've got $P(x_i | z_i)$. Which looks like what? So I have the log of—what is the Gaussian normal distribution? It looks like some constant (which I'm not going to worry about—a normalization constant) times:

$$e^{-\frac{\|\mu_{z_i} - x_i\|^2}{2\sigma^2}}$$

It's the distance between $\mu$ of—the $\mu$ that this point is in (whichever cluster it is, it's $\mu$ of $z_i$, right? It's $\mu$ of the cluster that $i$ is in) minus $x_i$, quantity squared, over $2\sigma^2$, right?

So I've got two pieces:
1. How likely does it belong in the cluster? But that's a constant! When I do the optimization, this is gone, I don't care about it.
2. The log of a normalization constant to make it a probability, which I don't care about—that drops out.

The log of $e$ gives me:

$$-\frac{\|\mu_{z_i} - x_i\|^2}{2\sigma^2}$$

How far away is the point $x_i$ from the centroid, right? All weighted by the same amount.

Make sense?

### Simplification

Now, let's just—yep, yes? Why'd we assume Gaussian? Because that's the model we liked. Why'd we assume they're all equally the same size?

K-means follows if you assume every cluster is equally likely and they're all generated from the same Gaussian with a different center and the same width. That is the assumption we're making, and I'm in the middle of proving that that leads then to the K-means loss function, right?

And if you look at it, what do I have here? I've got this argmin of assigning it to the right point and how far away it is in squared distance divided by $2\sigma^2$. But the $\sigma$ drops out—they're all the same width. So $\sigma$ doesn't matter, the 2 drops out.

And the argmax of that is just the argmax over $z$ (assigning these to these clusters) and $\mu$ of the summation over the $i$ points of the centroid of that point:

$$\arg\min_{z, \mu} \sum_{i=1}^{n} \|\mu_{z_i} - x_i\|^2$$

### Connection to Original K-Means Formulation

And what I'm going to argue is this is basically what I said before. What I said before in slightly different notation (which I like better) was this is:

$$\arg\min_{r, \mu} \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \|\mu_k - x_i\|^2$$

So the $\mu_k$ looks like... $x_i$ squared looks the same. And we've just shifted the notation slightly weirdly, right? And it's sort of a—yeah, almost the same notation, but this one says, hey, summation over the points and then find the best $z$ to put it in. This one says we're going to find the best $z$ to put it in, and we're going to call that $r$.

So it's a notational shift, but it's the same loss function.

### Key Takeaway

So a big takeaway: if you assume that the clusters are equal size, that they're all radial (spherically symmetrical), assume they all have the same covariance matrix, you get out an $L_2$ reconstruction loss function that looks exactly the same as the K-means one.

And the last piece, which I didn't quite show, is that as $\sigma$ goes to zero, you're going to get then a **hard assignment** rather than a **soft assignment**.

So the standard mixture model one ends up with a soft assignment.

### Why $\sigma \to 0$ Gives Hard Assignment

Let's think about why $\sigma$ going to zero gives a hard assignment. So if I have a bunch of points—clusters with their centers—in a soft assignment, this point is 50-50. And this point is almost all in cluster 1. And this point is mostly in cluster 2.

If I shrink the covariance matrix, now, if this covariance matrix gets tiny, then it really—I mean, here's still the 50-50 line of which one it's closer to. But now, if you shrink the covariance matrix to be really small, it ends up being really likely in this one and really unlikely in that one, because I have to sum to 1, remember?

So it's incredibly unlikely for either of them, right? Because it's like, how many standard deviations away? But that doesn't matter. The assignments sum to one.

And it turns out that as you shrink the covariance matrix to be small, then in the limit, it becomes a hard assignment to the closest one.

Yeah. Yes, as the covariance goes to zero, the inverse covariance goes to infinity, and it really slams it into the assignment there.

So K-means is that limit, right? The hardness comes from pushing the covariance matrix to zero. And the rest of it comes from the spherically symmetrical, which makes sense. And the one that bothers me the most is K-means assumes all the clusters are the same size.

Make sense?

## Other Gaussian Mixture Model Examples

We can think about other Gaussian mixture models. I've seen data where my data looks like: I've got a bunch of points here (that's when the machine is working), and I've got a bunch of points out here (that's when the machine is broken).

Make sense?

### Unequal Cluster Sizes

What does this Gaussian mixture model look like? There's one piece here, $\mu_1$, with a lot of points in it and a very small standard deviation. There's another point, maybe the center is $\mu_2$, very close to $\mu_1$, and it's got a HUGE covariance matrix and very little stuff in it.

K-means a good model? **K-means is terrible!** I got 99% of my stuff in one cluster and I got 1% out here.

But do I want a mixture model? I do, because I really want to know for a given point: is it in cluster 1 (call it functioning) or cluster 2 (call it broken)?

Unsupervised learning, right? But nonetheless, I have a strong prior, because I'm a Bayesian modeler, that most things are here and most things are not broken.

So K-means is terrible if you've got some sort of background noise, right? Things that are far away. And it turns out, often you have something where you want to have something like that, with a well-focused piece here and a bunch of stuff out there.

Make sense?

### Zero-Inflated Models

Let's do another mixture model. When I look at distributions of almost anything—how many dollars I get from someone when they visit my website—so this is number of people, and this is number of dollars, what I often see is something that looks like this. There's a ton of people here, and then there's a distribution that looks like that.

This would be called a **zero-inflated model**. It's a mixture of two components. One component is no money—you didn't spend anything on the website. You came and you left. Bummer. This is something which is actually not Gaussian, but if I did log dollars, then the zero-inflated would become 1, but you know, right? But this is a distribution, so I can look at the distribution among the people that spent money.

Make sense? Very popular in the business world. This shows up for citations and lots of other things, right?

So often what you say is there's two components. One component is zero. I know the mean—it's zero. Everything in it is zero. The standard deviation is zero. The mean is zero. It's just zero, but that might be 70% of my observations.

Now, the 30% have some mean and some standard deviation. I'll fit them. So note that this is a mixture model, and I could estimate it with EM.

And you could say, oh, I'm going to first subtract off all the zeros and then do it. Yeah, you could do that too. But it's nice to actually often estimate the two together, because you want to estimate what fraction of the people are zeros, right?

So there's a $P(\text{cluster } 1)$, or $P(\text{cluster } 0)$—probability of spending zero—and there's a probability of spending non-zero.

Make sense? So often you have some knowledge of the world that says that there are potentially two different bacteria in the person, or two different types of spenders.

### Types in Game Theory

People who do game theory talk about **types**. Or if you go to model people shopping on eBay, you might model shoppers on eBay as a mixture of professional shoppers and amateurs. And you don't know for a given person: are they a professional or an amateur?

Make sense? But nonetheless, there's a mixture of them, and they will each have their own model, and you're going to fit the model.

### Mixture Models for Recommendation Systems

Where else? Okay, television. When I first started doing recommendation systems for television, it wasn't like the modern world. When I go to actually use my Netflix, either I sign in, or my daughter does, or my wife does.

But in the old days, you had one account for the whole family. And the preferences were a mixture. My 2-year-old daughter and I had somewhat different movie preferences, right? But Netflix didn't know who was watching what.

So to do a good model of user preference, they should view it as a mixture model. In this case, three components: me, my wife (but we're pretty similar), and my daughter, who is way different.

And you should say, each viewing has a probability of being the daughter or the parents, and each of them then has its own distribution.

## Conclusion and Next Steps

So we're going to come back on Wednesday, and we're going to actually show how to use the EM algorithm to estimate these Gaussian mixtures.

But hopefully I've convinced you that **lots of things in the world really are mixtures**. And with that, I will stop, and we'll see you Wednesday!

---

# CIS 5200: Machine Learning - Lecture 19: Expectation-Maximization Algorithm and Gaussian Mixture Models

## Introduction to Hidden Variables

In this lecture, we'll discuss the concept of hidden variables, which we need to estimate. These are not part of the problem statement itself—we need to discover them.

The process is actually super simple. We're going to say: if we already have an estimate of $\pi$, $\mu$, and $\sigma$, how likely is a point to belong to a particular cluster?

The likelihood of a point belonging to a cluster is the product of two things:

1. **The base rate for that cluster** (denoted by $\pi$): For example, there might be a 30% chance of a point coming from that cluster.
2. **The probability of the point given the cluster centroid**: This depends on how far the point is from the centroid.

Make sense? And we're going to assume in this class, pretty much always, that it's a normal distribution, but it could be any distribution.

### Computing Point Likelihood

If you know the parameters $\pi$, $\mu$, and $\sigma$, you can estimate for every point how likely it is to come from that cluster.

The formula is straightforward: peaks occur for clusters with high $\pi$ values (high base rates), and the likelihood also depends on how far the point is from the centroid.

## The Two-Step EM Process

Now, the second piece: Once we have that likelihood, we now have a membership value indicating how much every point belongs to every cluster.

And now we're going to estimate the parameters $\pi$, $\mu$, and $\sigma$—which, by the way, we'll call $\theta$ when referring to them as a group.

### Estimating the Mean (E-Step)

We will show that the best estimate of the mean and the standard deviation of a cluster is what you'd intuitively think it would be.

**How do you estimate the mean of a cluster?**

Take each of the points, all $n$ of them, and weight them by how much they belong to that cluster. It's a weighted average.

- If a point is 70% in the cluster, then it contributes $0.7 \times x$ to that cluster.
- If a point is 20% in the cluster, it contributes $0.2 \times x$ to that cluster.

Then we sum those up, and since it's an average, we divide by the sum of the probabilities.

$$\mu_k = \frac{\sum_{i=1}^{n} P(z_i = k | x_i) \cdot x_i}{\sum_{i=1}^{n} P(z_i = k | x_i)}$$

So, in some sense, it's trivial. It's just a weighted average of all the points by their membership in the cluster.

### Dimensions and Vectors

Let's be clear about the dimensions:

- $x$ is a **vector** (dimension $D$)
- $\mu$ is a **vector** of the same dimension $D$
- $\pi$ is a **scalar** (or more precisely, $\pi$ is a distribution, so we have different $\pi$ values for different clusters)
- Each $\sigma$ is a $D \times D$ **covariance matrix**

If I have $D$ features, then $\sigma$ is $D \times D$—it's the feature-by-feature correlation matrix.

So in fact, all the $x$'s and $\mu$'s should be in bold (they're vectors). Think of everything—the $x$'s and $\mu$'s—as living in the feature space, and the $\sigma$ sits in the feature-by-feature covariance space.

### Estimating $\pi$ (M-Step)

The $\pi$ values are easy to estimate. For every point, I have a measure of how much it belongs to every cluster.

For any one cluster $k$, the $\pi$ for that cluster is just:

$$\pi_k = \frac{1}{n} \sum_{i=1}^{n} P(z_i = k | x_i)$$

It's the sum over all $n$ data points of how much each of those points belongs to the cluster, divided by $n$. So that's also an average.

### Estimating the Covariance Matrix

Finally, the covariance matrix is computed as:

$$\Sigma_k = \frac{\sum_{i=1}^{n} P(z_i = k | x_i) \cdot (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{n} P(z_i = k | x_i)}$$

This gives us a $D \times D$ matrix. In the general case, I have a covariance matrix for every single cluster, and each one is weighted by how much each point belongs to that cluster.

## Summary of the EM Algorithm

So to summarize:

- **E-Step (Expectation)**: We estimate the **membership** of every point in every cluster (i.e., we estimate $P(z_i = k | x_i)$ for all $i$ and $k$).
- **M-Step (Maximization)**: We estimate the **parameters** $\theta = \{\pi, \mu, \sigma\}$.

It's always this back-and-forth between:
1. Estimating something that looks like the $Z$'s (the hidden cluster assignments)
2. Estimating the parameters

### The Hidden Variable $Z$

One way to think about it: the E-step computes the **expected value of the $Z$'s**—how likely is each point to be in each cluster?

**What's the dimension of $Z$?** If we write $Z$ as a matrix, how big is it?

$$Z \in \mathbb{R}^{n \times k}$$

$Z$ is $n \times k$: for every one of the $n$ data points, we have a probability for each of the $k$ clusters.

You could view it as $Z_{ik}$, where $i$ indexes the data point and $k$ indexes the cluster.

### Connection to K-Means

Last class, I talked about $R_{ik}$, which was the k-means hard assignment of $Z$.

$R_{ik}$ for the cluster membership is **one-hot**: k-means says every point is in exactly one cluster. It's a distribution, but it's a degenerate one, where the point is 100% in one cluster and 0% in the other ones.

So $R$ was an $n \times k$ thing, and $Z$ really should be thought of as an $n \times k$ thing also—for every one of the points, how much is it in each of the $k$ clusters?

## Nomenclature: Parameters vs. Hidden Variables

I want the nomenclature to sink in along with the intuition:

- **$Z$ is a hidden variable**: It's something that's not a parameter in the model.
- **$\pi$, $\mu$, and $\sigma$ are parameters**: You use them to generate data from some distribution.
- **$k$ is a hyperparameter**.

But $Z$ is a **hidden variable**—it's something you can't observe. In reality, we might believe that a point truly comes from one of $k$ clusters. If you generate data, you pick the cluster first, and then you pick the point given the cluster (we did that last time).

So each point really is in a cluster, but we never see what cluster it's in because we're doing unsupervised learning—it's hidden. The best we can do is compute the **expected value** of that $R_{ik}$ matrix, which is my estimated probability of the point coming from each of the clusters.

### Abstract Interpretation

This is fairly abstract, and the same principle applies to missing data and other applications. But the bottom line is:

**We never know for sure what cluster a point came from.** The best we can do is estimate the probability distribution over clusters, and then we'll use that distribution.

## What is EM Doing?

The normal distribution says: given cluster $k$, which has a known $\mu$ and $\sigma$, what's the probability of seeing that $x$?

$$P(x | z=k, \theta) = \mathcal{N}(x; \mu_k, \Sigma_k)$$

**What is EM doing?** We're estimating two sets of things:

1. **The $Z$'s**: The cluster memberships
2. **The parameters**: $\pi$, $\mu$, and $\sigma$

And what I will show is that each of these steps is **increasing the likelihood**. So if we sequentially alternate between these two steps, each of them increases the likelihood. Therefore, you keep moving the likelihood up, and it will converge to **a** maximum likelihood solution.

### Convergence Properties

**Question**: Does it converge to THE maximum likelihood, or A maximum likelihood?

**Answer**: It converges to **a** maximum likelihood, because this is a **non-convex problem** in general. So I have no guarantees about what it will converge to.

But it will keep moving in the same direction (upward in likelihood), and it can be shown formally that it will converge. And it always does in practice.

## Controlling Cluster Sizes with $\pi$

**Question**: $\pi$ represents how large each cluster is, or more precisely, how large we expect each cluster to be. If we have some prior where we expect to see a big cluster and a small cluster, can we control that by setting the initial value of $\pi$?

**Answer**: I'd be careful here. We're doing gradient descent from some initial value, and you have no guarantee the initial value controls where you end up.

And you hope it doesn't! You really hope that no matter what initial value you pick, you'll converge to the same optimum. I've told you I have no guarantees, but ideally that would be the case.

### Prior vs. Initialization

If I want to impose a **prior**, first of all, am I doing MLE (Maximum Likelihood Estimation)?

No, then it's **MAP** (Maximum A Posteriori), and MAP is not controlled by the starting point of gradient descent. **MAP is controlled by putting the prior into the equation.**

So if you're a Bayesian, you may certainly put in a prior for $\pi$.

### Example: Equal-Sized Clusters

If you have a really strong prior (we saw an example on Monday), there's a famous algorithm where the really strong prior is that **all the clusters are equal-sized**. That was **k-means**.

In k-means, we said: forget the gradient descent to estimate $\pi$—we're just going to set all the $\pi$'s to $\frac{1}{k}$.

**Does that guarantee every cluster is equally big?**

No, it does not! That's a very important point—that's the parameter in the model.

### Probabilistic Interpretation

Say I build 2 clusters, each of size 3, by flipping a coin with 50% probability. I tell you the probability is 50%. If I flip the coin 6 times, do I get exactly 3 heads and 3 tails?

Sometimes yes, sometimes no. With some probability that you could compute, I'll get 3 and 3, but the prior (or the model) doesn't guarantee you're going to get clusters of that exact size.

If you run k-means, you're not going to get all the clusters of exactly the same size. But you're highly unlikely to get a cluster where it's 99% of points in one cluster and 1% in another, if you have a 50-50 prior or model. That's incredibly unlikely.

### Summary: Three Concepts

So please try to be super clear between:

1. **Your initialization**: You might want to initialize based on what you expect, but you hope it doesn't affect the answer too much.
2. **Something you bake into the model**: For example, setting $\pi = \frac{1}{k}$ in k-means.
3. **Your outcome/observed variables**: You flip the coins, and you should get roughly an even distribution if the $\pi$'s are even, but they're not going to be exactly the same.

A **prior over $\pi$** (in the MAP setting) might say: "I think the $\sigma$'s are close to diagonal, and I think the $\pi$'s are close to equal." But my **posterior** over $\pi$, $\mu$, and $\sigma$ might be different.

So those are all controlled separately.

### Digression: Neural Network Priors

This sort of annoys me. There was, for a while, a trend in neural networks where people would try to build the prior in by picking the initial weights or something. It didn't work very well.

If you have a prior, **build the prior in** (into the loss function)—don't try to hack it into the initial condition.

It is true that if you train a neural net on one thing and then train it some more later, you'll stay closer to where you used to be. But if you really want one neural net to stay close to another neural net:

**Use a penalty function.** What sort of penalty function?

**KL divergence** between the distribution over their outputs. If you want to regularize one network to be close to another one, put a penalty into the loss function that says you want these distributions to be close.

This is done, by the way, in **distillation**.

Okay, end of digression.

## EM Algorithm Walkthrough Example

I'm going to walk through an example three times today, so hopefully this will make lots of sense.

### Initial Setup

Suppose we start with a 3-mixture model. In general, you could either start by assigning points to clusters, or you can pick the parameters. People usually start with the parameters.

So you pick:
- Some $\pi$ values (maybe you have 3 clusters that are roughly the same size: red, green, and blue)
- Some random centroids (maybe actually data points, like in k-means)
- Some random covariance matrices

And you've started off your algorithm.

### Iteration 0: Initial Parameters

If you look at the initial setup:
- You have some $\mu$'s (centroids)
- There's a green cluster that's very circular—that's a radial basis function, a Gaussian that's symmetric with a centroid here
- You have a red oval-like cluster with a covariance showing a diagonal orientation
- You have a third center for the blue cluster, which is way off to one edge

So I have 3 clusters, each with $\pi = \frac{1}{3}$ (initially).

### Iteration 0: E-Step

Now, first, the **E-step**: For every point, find how much it belongs to each cluster.

You can see that each circle represents a data point:
- The points out on the left are almost 100% green—the probability of being green is really high
- You can see a little bit of red, but essentially no blue
- The points over on the right are almost equal across red, green, and blue—this point is right on the boundary, about one standard deviation away from each center

So you can see the E-step computing these memberships. The likelihood for each point and cluster is:

$$P(z_i = k | x_i) \propto \pi_k \cdot \mathcal{N}(x_i; \mu_k, \Sigma_k)$$

where all three $\pi$'s are initially $\frac{1}{3}$.

### Iteration 0: M-Step

Now we do the **M-step**, which updates the parameters. I'm going to compute:
- The centroid of the greens (weighted by the green probabilities), so it's going to move off to the left here a bit
- The covariance of the greens, so it'll no longer be quite so spherical
- Same thing for the blues and the reds

And whoa! The green centroid moved way over here and got sort of angled. You can see the red and blue also moved. The blue was initially way out on the edge, but if you look at the blue points, they're all centered pretty much in one area, and they're pretty round—much rounder than the initial guess. The blue cluster is almost spherical, not quite.

So that's the M-step completing.

### Iteration 1: E-Step

Then the next E-step: Given these new parameters, where the cluster sizes ($\pi$'s) are pretty similar but have shrunk a little bit (the blue is now like 0.22 instead of 0.333), you can see:
- These points are mostly green
- These points are green-red
- These points are mostly blue

So I've got my E-step that says how much do these points belong to each cluster.

### Iteration 1: M-Step

Given that E-step, I do the M-step, and:
- The green centroid is going to shift a little bit more in its direction
- The blue might shift a little bit more down that way

And you can see—oh, look! These clusters are starting to separate.

**Do they have to separate?**

No, they could still be overlapped. I showed before the case of one little tight cluster that's really concentrated at the centroid, and then one big diffuse cluster.

This happens a lot in the real world. You get this sort of bimodal distribution where:
- Everything working correctly is really similar (tight cluster)
- When it goes wrong, it's way off (diffuse cluster)

Sometimes with other things—like when a device fails or a robot fails—it may shift from one mode to another mode, so it shifts to a different distribution.

### Multiple Failure Modes

In some sense, each **failure mode** gives you a different distribution of $x$'s. You have to pick your hyperparameter (how many different modes you want), but there's no labeling—we're unsupervised. You don't have to say "this was failure type 2, this was failure type 3, this was normal operation," etc.

### Convergence

So we go back and forth between E and M steps, and pretty soon this thing converges.

After 20 iterations, this particular example (for this starting point) converged, and we see we have three nicely separated clusters:
- The red ones are a little bit angled
- The blue ones and green ones are a little more round

But it converged to a nice model!

## Degenerate Cases and Practical Issues

**Question**: What about degenerate cases?

There are plenty of degenerate cases. Mostly what happens is you'll find that a cluster just vanishes away—there's nothing there anymore.

### Single Point Clusters

The other thing to be careful about: if you have a single point in a cluster, **what's the covariance matrix of a single point?**

It's zero! (Or actually, undefined.)

So occasionally things go wrong. If you're truly estimating a covariance matrix and you end up with a single point, you get something with a covariance of 0, which can cause numerical issues.

### Zero-Inflated Models

Sometimes a zero covariance is actually correct! What example did I give where there's a cluster with covariance of zero?

**Zero-inflated models.**

For a lot of things, you get a cluster where everything in the cluster is exactly zero:
- Papers with zero citations
- Customers who have bought zero products
- Days where zero people showed up in the classroom

So often you get one cluster of purely zeros if you're doing things correctly.

### Practical Solution

In that case, you probably want to build it in and hard-code it. Say:
"My model has a cluster of $x = 0$, and the covariance matrix is zero. I'm not going to try to estimate it."

So I often hard-code a cluster at zero if I expect zero-inflation, because when I go to run the code to estimate it, all of a sudden I've got a covariance matrix of 0, and while everything might be fine, you do worry a little bit about numerical issues with zeros.

## K-Means vs. Gaussian Mixture Models

**Question**: Why is k-means terrible for zero-inflated data?

K-means assumes the clusters are equally sized, but I might have 95% of the items be 0 and the other 5% be in other clusters.

Plus, **what else does k-means assume?** What does it assume about the covariance matrix?

**Answer**: Equal and radial (spherical).

The **radial assumption** doesn't bother me if I've scaled things correctly. The **equal covariance assumption** really bothers me.

### When to Use K-Means

So if you're using k-means (this is a great question for the final exam!), it should be the case that you expect:
- The clusters to be roughly equally big (equal $\pi$'s)
- The clusters to have roughly equal covariance (equal $\sigma$'s)

If that's close to true, it's a great algorithm. If it's far from true, you're in bad shape.

### Popularity vs. Generality

Here's the good news and the bad news: I would say **k-means is used about 10 times as much** as Gaussian mixture models (I'm making that number up—it could be 20 or 100 times).

**Why is k-means so much more popular than Gaussian mixture models, even though k-means is provably a special case?**

You can make all the decisions you want: why not just run a general mixture model? The most general approach is where each cluster has its own covariance matrix and its own center—I have no restrictive assumptions.

You can either run k-means, or you can run a full general Gaussian mixture model with no simplifying assumptions. I just said: every cluster has its own centroid and covariance matrix. I've made no assumptions.

**So why do people make simplifying assumptions?**

### Computational Cost?

**Is it more expensive?** Yes, but compute cost is so cheap nowadays.

I worry about:
- The cost of data
- The cost of labor

My data scientists cost a lot (they're mostly working for a couple of years before they get their PhDs or something, but they're still expensive). The cost of GPUs versus $80,000 for a starting data scientist at Penn-level salaries (which I know is way below industry—don't get me going)... The compute power does not bother me.

### The Real Issue: Overfitting

I worry about some other kind of power:

**Statistical power!**

**It's more complex, but why do I care about complexity?**

**Overfitting!**

I don't have enough data. **Data costs me money.** Compute is really cheap (I mean, I know that OpenAI worries about it, but what's a few billion dollars between friends? That's just not the limiting thing for most machine learning problems).

But **statistical power is a problem**, which manifests as overfitting.

### Model Simplicity

**The simpler the model, the less data you need to avoid overfitting.**

I don't care about how well I fit the training data—I fit the data beautifully with a complex model. But do you think this data really has exactly 3 clusters? Maybe, maybe not. Maybe it should have been 2. Is this exactly right? Who knows?

It's unsupervised—maybe it's a first step to preprocess stuff, maybe to explain the data to show to some client.

**The reason people like simpler models is they don't tend to overfit.** They're less likely to converge to something that's weird.

The more you let everything have its own covariance matrix, the higher the chance you're going to converge to some weird solution.

### General Principle

In general, **you want the simplest model you can fit**, mostly because:
1. You don't have enough data
2. You worry that if the optimization is non-convex, who knows what it's going to converge to

You can run it 10 times from different initial conditions and make sure it's pretty stable—that's not a bad check.

But often, you just want something that is, in fact, going to run, and mostly **k-means works quite well out of the box**, as long as the assumptions are more or less true.

### Standardization

Most people **standardize the $x$'s** to make things more like radial basis functions, getting rid of the covariance structure.

And often, you don't really know—I mean, fine, maybe these clusters are all non-spherical, but how non-spherical are they? If I assumed they were spherical, it wouldn't have been horrible. It might have been more realistic (less prone to overfitting).

So you always worry about overfitting and model complexity.

## Mathematical Foundation of EM

Okay, end of intro. Let's now get into the math!

So we're going to derive the EM algorithm in general.

### The Likelihood Objective

The basic notion of what we're doing is that we want to maximize the likelihood of the data, or in this case, the log-likelihood:

$$\log P(D | \theta)$$

**What is $\theta$ in our Gaussian mixture model?**

$$\theta = \{\pi, \mu, \sigma\}$$

That's the set of parameters.

We can write this as:

$$\log P(D | \theta) = \sum_{i=1}^{n} \log P(x_i | \theta)$$

The summation over all the data points $i$ (from 1 to $n$) of the log of the probability of each data point.

### Introducing Hidden Variables

We can break this up further because each data point could come from one of $k$ clusters:

$$\log P(D | \theta) = \sum_{i=1}^{n} \log \left[ \sum_{k=1}^{K} P(x_i, z_i = k | \theta) \right]$$

How likely is data point $i$? That probability depends upon:
1. Which of the clusters is it in?
2. What is its value?

So I have the $x$'s (the observed values), and I'm asking: how likely is this point, given its value and given the cluster it's in?

I'm going to sum that over all the clusters. If it were a hard clustering, only one term would matter (the cluster it's actually in). But in reality, we're not going to know which cluster it's in, and this will end up being an expectation—how likely is it to be in each of the $k$ clusters, times how likely it is to come from that cluster.

### Decomposing the Joint Probability

**Question**: Is the first summation over the points and the second over the clusters?

**Answer**: Yes! The first summation (over $i$) is over the data points, and the second summation (over $k$) is over the clusters.

We can break things up one more step:

$$\log P(D | \theta) = \sum_{i=1}^{n} \log \left[ \sum_{k=1}^{K} P(z_i = k) \cdot P(x_i | z_i = k, \theta) \right]$$

We've taken this joint probability and broken it up into:
- The probability that the point is in cluster $k$: $P(z_i = k) = \pi_k$
- The probability of that point given it's in cluster $k$: $P(x_i | z_i = k, \theta) = \mathcal{N}(x_i; \mu_k, \Sigma_k)$

So this is the main likelihood equation we're going to work with: the summation over the points, of the log of the sum over the clusters, of how likely the point is to be from the cluster times how likely the point is given it's from that cluster.

### Why Not Just Use Gradient Descent?

**Question**: We have a likelihood and some parameters we're optimizing—why not just do gradient descent?

We spent the first half of the course pretty much just gradient descending things, and it mostly worked great, even for neural nets which are incredibly non-convex. And it doesn't find the true optimum, but we didn't care because that was overfit anyway.

**Why not just use gradient descent here?**

**Answer**: You could estimate this with gradient descent in theory. The problem is it's really non-convex, and because the points really come from one cluster rather than from all of them, the convergence doesn't work super well.

### Alternating Optimization

You get **much better convergence** if, instead of gradient descending the whole thing, you **alternate**:
1. First, try to find what clusters each point is in (with some distribution)
2. Then, try to estimate the parameters

If you go back and forth, you have much nicer convergence properties.

We've seen this a couple of times—we saw this with matrix factorization, where we had the scores and the loadings. You go back and forth; each one was a trivial ridge regression, but combined they're complicated.

### Why Alternating Works Better

Particularly for assigning points to clusters, the solution looks like the point really came from one cluster. The probability distribution is going to be pretty much assigning it (when it converges) to one cluster and almost zero across the other clusters.

It turns out that if you **alternate**, you get much better convergence properties.

You could run gradient descent—there's nothing stopping you. You could take derivatives with respect to $\pi$, $\mu$, and $\sigma$, find the gradient, and do a step down it. But it works much better to alternate.

So we're going to go back and forth between the **E-step** and the **M-step**.

## Jensen's Inequality and the ELBO

Now I want to take about 15 minutes to get into the derivation.

### The Problem with Logs and Sums

We start from this equation:

$$\log P(D | \theta) = \sum_{i=1}^{n} \log \left[ \sum_{k=1}^{K} \pi_k \cdot P(x_i | z_i = k, \theta) \right]$$

The problem I need to deal with is that I want to take the log through the summation sign.

Why? Because I want logs of probabilities. This probability has two pieces:
1. The cluster membership ($\pi_k$)
2. The exponential Gaussian ($\mathcal{N}(x_i; \mu_k, \Sigma_k)$)

If I could take the log of this, the log of the Gaussian is beautiful—it becomes the log of $\pi$ plus the log of the Gaussian (which turns the exponential into a quadratic form), and the math is gorgeous.

**But you can't pull a log through a summation.**

Where would we have a problem? Everywhere!

### Jensen's Inequality

To deal with that problem, we're going to use a clever piece of math which in the math world is called **Jensen's inequality**. People in ML, for obscure reasons, like to call it the **ELBO** (Evidence Lower BOund). We'll get back to that in a second.

The intuition behind Jensen's inequality says that if you have a **concave function** like log:

Here's a picture of a concave function (see the blue curve). When we say concave, we mean it's **concave down** (or equivalently, convex up). If I draw a straight line (the red line) from any two points (between $v_1$ and $v_2$), this linear approximation is always going to be underneath the blue curve if the blue line is concave.

Take this as a hand-wavy, axiomatic definition of concavity.

### The Theorem

Here's the theorem we want to look at. If we take:

$$\alpha \cdot f(v_1) + (1 - \alpha) \cdot f(v_2)$$

As $\alpha$ goes from 0 to 1, we move along the red line (since that red line is a linear combination of $f(v_1)$ and $f(v_2)$).

This is **less than or equal to**:

$$f(\alpha v_1 + (1 - \alpha) v_2)$$

which is the function evaluated at a point on the $x$-axis.

So this mathematical statement says: for some point which is a linear interpolation between $v_1$ and $v_2$, if we take the function $f$ of it, the function $f$ is always going to be **greater than or equal to** taking a similar interpolation of the $y$-values (which are $f(v_1)$ and $f(v_2)$).

Formally:

$$f(\alpha v_1 + (1 - \alpha) v_2) \geq \alpha f(v_1) + (1 - \alpha) f(v_2)$$

This is **Jensen's inequality**. It's pretty trivial—the red line (the interpolation) has to lie below the blue line (the function).

### Multi-Dimensional Generalization

Now we can generalize this. Instead of having a single $\alpha$, we're in a fancier space where we have basically $k$ alphas (which are going to be our probabilities, like our $\pi$'s—how much you belong to each of the clusters).

The weighting we're going to have is not just a single $\alpha$, but $k$ of them, and they sum to 1 because these are our probabilities.

What we can say is:

$$f\left(\sum_{k=1}^{K} \alpha_k v_k\right) \geq \sum_{k=1}^{K} \alpha_k f(v_k)$$

where $\sum_{k=1}^{K} \alpha_k = 1$ and $\alpha_k \geq 0$ for all $k$.

This is the **multi-dimensional generalization** of the one-dimensional Jensen's inequality.

Instead of a single $\alpha$ here, I have $K$ of them. Instead of $\alpha$ going from 0 to 1, these alphas sit in a $(K-1)$-dimensional simplex. Remember, these are our probabilities.

The original $\alpha$ was between 0 and 1 as we moved from $v_1$ to $v_2$ ($\alpha$ goes from 0 to 1, and $1 - \alpha$ goes the other direction). They sum to 1.

This is now a multi-dimensional $\alpha$. We're going to have something that's $k$-dimensional, they all sum to 1, they're all between 0 and 1—they're all probabilities (representing point membership in clusters).

The same intuition that worked before works here: I can now pass the function through the summation. This gives us an **upper bound** on the function evaluated at the sum.

The alphas are going to end up being our approximations to $P$. Remember, $P$ was: how likely is point $x$ to be in each of the $K$ different clusters? We're going to approximate $P$ by a $Q$, which will be our approximation to it. That will be the alpha—how much do I think each point belongs in each cluster?

## Deriving the ELBO

Now we're going to do a super cute little trick.

### Multiply Top and Bottom

We're going to take our $P(D | \theta)$ (which was the log of the summation) and multiply both top and bottom by something.

We're going to define a function $\mathcal{F}$ which is:

$$\mathcal{F}(Q, \theta) = \sum_{i=1}^{n} \sum_{k=1}^{K} \text{[something with } Q \text{ and } \theta \text{]}$$

What I want to do is something that has the log inside things. Using Jensen's inequality, we can show that we can take this summation and manipulate it cleverly.

### The Manipulation

Let me define $Q$ as my approximation to the probability. I'm going to have a new function where I multiply and divide by $Q$:

$$P(x_i | \theta) = \sum_{k=1}^{K} Q_{ik} \cdot \frac{P(x_i, z_i = k | \theta)}{Q_{ik}}$$

where $Q_{ik}$ is my estimate of the probability of point $x_i$ being in cluster $k$.

Let me say that more clearly. I'm going to take the formulation where I have an $\alpha$ and a $v$:
- The $\alpha$ is going to be $Q$: my estimate of the probability of point $x_i$ being in cluster $k$
- The $v$ is going to be $\frac{P_\theta}{Q}$
- The function $f$ is going to be the log

**Question**: Why can't I assume $Q$ to be 1?

**Answer**: Because the whole point is that I don't know which cluster every point is in, and the best I can do is to approximate it as a distribution. I can't assume the answer away.

### The Clean Formulation

Let me say it once more. Let:

$$v = \frac{P(x_i, z_i = k | \theta)}{Q_{ik}}$$

So $v$ is this piece. Let $\alpha \cdot v$ be the $Q$ part. So now I have:

$$Q_{ik} \cdot \frac{P(x_i, z_i = k | \theta)}{Q_{ik}}$$

This is a classic proof trick: I just multiplied by $Q$ and divided by $Q$, and I separated it into two pieces, $v$ and $\alpha \cdot v$.

So I now have:

$$\log \left[ \sum_{k=1}^{K} Q_{ik} \cdot \frac{P(x_i, z_i = k | \theta)}{Q_{ik}} \right]$$

That was actually pretty clean!

### Applying Jensen's Inequality

I have this piece, which is:

$$f\left(\sum_{k=1}^{K} \alpha_k v_k\right)$$

And now I can use Jensen's inequality and say: I can pull this function $f$ (the log) through the summation, so I get:

$$\sum_{k=1}^{K} \alpha_k f(v_k) = \sum_{k=1}^{K} Q_{ik} \log \frac{P(x_i, z_i = k | \theta)}{Q_{ik}}$$

So the full expression becomes:

$$\mathcal{F}(Q, \theta) = \sum_{i=1}^{n} \sum_{k=1}^{K} Q_{ik} \log \frac{P(x_i, z_i = k | \theta)}{Q_{ik}}$$

And this whole piece is going to be a **lower bound** on what I started with:

$$\mathcal{F}(Q, \theta) \leq \log P(D | \theta)$$

So this $\mathcal{F}$ is a lower bound on the likelihood. If I can maximize this $\mathcal{F}$, I can push it up as far as I can toward the actual likelihood I want.

### Summary of the Derivation

That was a lot of work. Let me summarize what we did:

We started with log of the summation, and I wanted to pull the log inside the summation, so I get something that's a weighted sum:

$$\sum_{i=1}^{n} \sum_{k=1}^{K} Q_{ik} \log(\text{something})$$

I did that, but I lost the equality—I now only have a **lower bound**, an **estimated lower bound**, an **ELBO**.

I'm more of a statistician and like the old-fashioned "Jensen's inequality," but this is an estimated lower bound—this $\mathcal{F}$.

Now what I need to do is try to make $\mathcal{F}$ as big as possible, because I want to maximize the likelihood. If I can make $\mathcal{F}$ big, the bigger I can make it, the more I can push up toward making the likelihood bigger.

And I get to pick:
1. The $Z$'s (the hidden values)
2. The parameters $\theta$ (the $\pi$, $\mu$, and $\sigma$)

And we'll show that EM does that!

### Historical Note

There's a great paper by Neal and Hinton. I met Hinton decades ago when I was doing this sort of EM work. It turns out this guy Hinton—you've heard his first name? It's **Jeff!** He used to do this sort of work.

He did THE seminal work that said: if you're maximizing likelihood, as long as you keep moving up, you're going to maximize it. Sort of cool!

So he doesn't just do deep learning—but now he does, and he worries about Google (and AI safety).

## Decomposing the ELBO

We have a function:

$$\mathcal{F}(Q, \theta) = \sum_{i=1}^{n} \sum_{k=1}^{K} Q_{ik} \log \frac{P(x_i, z_i = k | \theta)}{Q_{ik}}$$

This is a **lower bound** on the log-likelihood.

I'm going to prove—and I'm going to cheat a little and skip some steps—that you can break this up in a couple of different ways.

### Using ChatGPT for Derivations

I thought this was sort of fun. I didn't want to type a bunch of stuff out, so I went over to ChatGPT. I have a million tabs open (people get stressed out about this).

I pasted my cut version of the formula, which I had on my screen, and asked how to derive it. It said, "Hey, I can derive it. Here's the derivation," and talked for 21 seconds.

First, instead of writing $Q(z_i = k | x_i)$, let's just call it $Q_i(k)$. And instead of $P_\theta(z_i = k | x_i)$, let's just call it $P_i(k)$.

Is that what I want? Yes, sort of.

### Simplifying Notation

There are two pieces here. Let's write these up on the board so we don't lose them.

We have:

$$\mathcal{F}(Q, \theta) = \sum_{i=1}^{n} \sum_{k=1}^{K} Q_{ik} \log \frac{P_\theta(x_i, z_i = k)}{Q_{ik}}$$

Remember, there are two pieces. The first piece is:

$$P(z_i = k | x_i, \theta)$$

This is: how likely is the point to be in that cluster? How much is that point in each of the clusters?

Times:

$$P(x_i | z_i = k, \theta)$$

This is: how likely is the point given the theta for that cluster?

And then we have the $Q$ piece, which is the same $Q$ as before.

So there are two pieces for the probability:
1. Cluster membership: $P(z_i = k | x_i, \theta)$
2. Probability of the point given cluster membership: $P(x_i | z_i = k, \theta)$

### Clean Notation

Now we're going to simplify the notation. We'll call the whole first thing:

$$Q_i(k) = Q_{ik}$$

That's how much, in my current estimate (in the current iteration), do I think this point $i$ belongs to cluster $k$? That's the same as the $Q$ here.

We can call the first term (the cluster membership part):

$$P_i(k) = P(z_i = k | x_i, \theta)$$

So now this whole messy thing becomes:

$$\mathcal{F}(Q, \theta) = \sum_{i=1}^{n} \sum_{k=1}^{K} Q_i(k) \log \frac{P_i(k) \cdot P(x_i | z_i = k, \theta)}{Q_i(k)}$$

Clean, clean, clean!

### Factoring Out Terms

Now we do a little rearrangement. Let's factor this out:

$$\mathcal{F}(Q, \theta) = \sum_{i=1}^{n} \sum_{k=1}^{K} Q_i(k) \left[ \log \frac{P_i(k)}{Q_i(k)} + \log P(x_i | z_i = k, \theta) \right]$$

Which is:

$$= \sum_{i=1}^{n} \sum_{k=1}^{K} Q_i(k) \left[ \log P_i(k) - \log Q_i(k) + \log P(x_i | z_i = k, \theta) \right]$$

So I just took this and broke it up.

### Identifying the KL Divergence

What is $Q \log P - Q \log Q$? We have a name for that:

$$\sum_{k=1}^{K} Q_i(k) \log \frac{P_i(k)}{Q_i(k)} = -D_{KL}(Q_i || P_i)$$

That's the **negative KL divergence**!

The first term is: how different is $P$ from $Q$? So the first term is the KL divergence (well, negative KL divergence).

### The Second Term

The second term is:

$$\sum_{i=1}^{n} \sum_{k=1}^{K} Q_i(k) \log P(x_i | z_i = k, \theta)$$

So here it is:

$$\mathcal{F}(Q, \theta) = -\sum_{i=1}^{n} D_{KL}(Q_i || P_i) + \sum_{i=1}^{n} \sum_{k=1}^{K} Q_i(k) \log P(x_i | z_i = k, \theta)$$

We get:
1. The negative KL divergence of $Q$ and $P$
2. Plus the summation over $k$ of $Q_i(k)$ times $\log P_\theta$

But the summation over $k$ of $Q_i(k)$ is always 1 (remember, $Q$ is my approximation to $P$, and it has to sum to 1 for each point).

### Interpretation

So this ELBO, this estimated lower bound, has two terms:

1. **How similar is $Q$ to $P$?** (The negative KL divergence)
2. **How likely is $x$ to come from this cluster given the parameters?**

This makes sense! What direction should this be? Am I trying to make the ELBO big or small?

**Big!** I'm trying to maximize the likelihood.

Do I want $Q$ to be close to $P$ or far from $P$?

**Close!** I want the KL divergence to be small, so I want to make the negative KL divergence big.

That's reassuring!

The second term is: how likely is $x$ to come from this cluster given the parameters in it?

So this is great! What I want to do is two things:
1. Make $Q$ close to $P$ (my probability estimates close to reality)
2. Make $x$ be likely to come from the parameters of the cluster

Sort of cool!

### Question on Summation

**Question**: Why is it not $k$ times the summation? For a given point $i$, the summation of the membership of $i$ in each of the $K$ different $Z$'s—what is it?

**Answer**: There's still a summation over $i$ in front of everything. This is the summation over all the data points of:
- For that data point, how close is its distribution to the true one?
- For that data point, how likely is it to be generated given the $\pi$, $\mu$, and $\sigma$?

So this summation here is just over the $k$ clusters. Everything is inside the summation over $i$, and all of these are in the summation over $k$.

We broke this up into two pieces:
1. The KL divergence (remember that $k$ indexes the $K$ clusters)
2. The expected log-likelihood term

For each point, this is a KL divergence of its distribution over the $K$ clusters—how close our estimate $Q$ is to reality.

Then the other piece: for each of these, sum the $Q$'s times this other piece, but this piece doesn't depend on $Q$.

### Question on Q vs. P

**Question**: What is $Q$?

**Answer**: $Q$ is our approximation to $P$. We've just shown that we're going to try to make $Q$ look as close to $P$ as possible.

## The E-Step and M-Step

Now that we have this, we can go back to the notes and say: we've broken up this bound into the negative KL divergence plus the log of the probability.

Now I can do two different sorts of optimizations over this estimated lower bound $\mathcal{F}$.

### E-Step: Optimize Q

One is to look at this piece and say: **what $Q$ do you think will maximize this?**

It's sort of trivial: if you set $Q$ equal to $P$, you've maximized it (by making the KL divergence zero).

So for my **E-step**, I'm going to set my $Q$ equal to $P$:

$$Q_i(k) = P(z_i = k | x_i, \theta^{(t)})$$

### M-Step: Optimize θ

The other step says: I can refactor this same thing differently.

I have my same $\mathcal{F}$:

$$\mathcal{F}(Q, \theta) = \sum_{i=1}^{n} \sum_{k=1}^{K} Q_i(k) \log \frac{P(x_i, z_i = k | \theta)}{Q_i(k)}$$

I can break this up into two pieces:

$$= \sum_{i=1}^{n} \sum_{k=1}^{K} Q_i(k) \log \frac{1}{Q_i(k)} + \sum_{i=1}^{n} \sum_{k=1}^{K} Q_i(k) \log P(x_i, z_i = k | \theta)$$

The first term is $Q \log \frac{1}{Q}$, and the second is $Q \log P_\theta$.

$\log \frac{1}{Q}$ is negative $\log Q$. So negative $Q \log Q$ is called **entropy** (denoted $H$).

So this breaks up into two terms:

$$\mathcal{F}(Q, \theta) = H(Q) + \sum_{i=1}^{n} \sum_{k=1}^{K} Q_i(k) \log P(x_i, z_i = k | \theta)$$

Now I want to maximize this by leaving $Q$ fixed (so I'm doing the **M-step** now).

$Q$ is fixed, so I want to maximize this to find the best $\theta$ (the $\theta$ that maximizes this). Does the entropy of $Q$ depend on $\theta$?

**No!** That's just how much I'm currently assigning every point to every cluster.

So that entropy term doesn't depend on $\theta$, and I can now maximize something that looks like an **expected value** of how likely am I to see this $x$ and $z$.

### What Does This Mean?

If you look at what this is doing, it's saying: weight every point by its class membership $Q$. You can write the math between it, but basically, if I'm trying to maximize the log-likelihood here, what does this $P_\theta$ look like?

Remember, that was the Gaussian. If I take the log of the Gaussian:
- There's a constant term in front (which I don't care about)
- Times $e$ to the minus [something]
- The log and the exponent cancel
- I get something that looks like a standard $x^T \Sigma^{-1} x$ quadratic form

So if I maximize this piece (which is the expected value over $Q$ of the log probability of the $x$'s given their clusters), it ends up saying: the way to maximize that is to do exactly what I showed earlier:

1. Find the $Q$-weighted $x$'s for the $\mu$
2. Find the $(x - \mu)^T (x - \mu)$ for the $\Sigma$

## Initializing Q

**Question**: How do we initially approximate $Q$?

We're going to have to initialize to actually run this algorithm. Let's talk about doing this:

1. We're going to have some sort of model (which is an equation form, mostly picking what kind of covariance matrix you want)
2. We need to pick some **initial parameters** $\theta^{(0)}$ (the initial $\pi$, $\mu$, and $\sigma$)
3. We have our data $x_1$ through $x_n$

### The Algorithm

**E-Step**: I'm going to start by picking $Q$ by saying: given the $\theta$'s of my last iteration (which is $\theta^{(0)}$, my initial guess of $\pi$, $\mu$, and $\sigma$), how likely is each $z$?

So I compute that estimate of $P$, which I'm calling $Q$:

$$Q^{(t+1)} = \arg\max_{Q} \mathcal{F}(Q, \theta^{(t)})$$

which just gives me my current estimate of $P$:

$$Q_i^{(t+1)}(k) = P(z_i = k | x_i, \theta^{(t)})$$

**M-Step**: Given that $Q$ (my current estimate of $P$), I'm going to find:

$$\theta^{(t+1)} = \arg\max_{\theta} \mathcal{F}(Q^{(t+1)}, \theta)$$

which is:

$$\arg\max_{\theta} \sum_{i=1}^{n} \sum_{k=1}^{K} Q_i^{(t+1)}(k) \log P(x_i, z_i = k | \theta)$$

My current $Q$ (my current estimate of $P$) times the log of the probability.

### The Back-and-Forth

I'm going back and forth:
1. I started by guessing the parameters
2. Given the parameters, I have an estimate of all the point memberships
3. Given the point memberships, I can estimate the parameters

### Why Do We Need Q?

All of this math looks sort of bizarre the first 4 times you see it. Why do I need a $Q$?

In some sense, what $Q$ is doing is saying: let me be super formal and talk about my current estimate of $P$.

It turns out, for this EM algorithm, the current best estimate of $P$ is just to use $P$ and call it $Q$.

### Advanced Extensions

There are fancier things (I won't cover this year), like **Latent Dirichlet Allocation**, which is a different model with Dirichlet probabilities instead of Gaussian. There are a bunch of other fancy models where you don't actually compute $Q$ exactly as $P$—you just say: I'm going to estimate $P$.

As long as you're moving in the right direction—no matter how complicated it is—if you do a better estimate of the class membership and a better estimate of the parameters, each of them is moving toward moving up your lower bound, and you're always moving toward optimizing.

So the machinery is almost too heavy here because $P$ and $Q$ are basically the same—$Q$ is just the current estimate of $P$.

But in an advanced class (particularly in the stat department—Shane Jensen teaches a Bayesian class), you'd be saying: we can't really get all the way to $P$, so we'll use $Q$ as an approximation to it.

### Summary

It's a lot of math, but it's really just saying: there are two pieces you can optimize, and as long as each of them is moving in the correct direction, your current estimate of the lower bound keeps going up and up and up.

As long as the estimated lower bound keeps going up, you are gradient ascending toward the MLE. Then you can prove pretty trivially that you're guaranteed to converge to **an** MLE (not necessarily **the** global MLE, since this is non-convex).

## Iteration and Convergence

**Question**: How does the iteration work?

It goes back and forth. Each time, I keep track of the $\theta$'s for each iteration. At iteration $t$, you keep updating them.

When I come to the next step, the $Q^{(t)}$ will use the previous parameters. There's no $Q$ in the initialization—

The **E-step** (expectation step) takes in the previous $\theta$'s and estimates $Q$. It takes the parameters and estimates the class memberships given those parameters.

The **M-step** says: given the current estimation of the class memberships, re-estimate the parameters.

Back and forth, back and forth.

### Convergence Criteria

**Question**: What's the convergence point?

At some point—and again, I'm an engineer, I don't care if you truly, truly converge to the exact answer. Think about gradient descent: you're just getting down and down and down.

But what's happening is the likelihood estimate is **monotonically going up**. Every step you do (each update of $Q$, each update of $\theta$) makes the likelihood higher.

**When do you stop?**

Do I know what the true maximum likelihood is? Nope, it's a bummer. So you never know how well you're doing compared to the absolute best you could do.

All you can tell is **how much you're changing each time**.

So your criterion is either:
1. You run it for 30 steps (or some fixed number)
2. You run out of money (computational budget)
3. You run it until the increase per step is less than $10^{-6}$ (you're not changing very much)

### General Problem with Iterative Methods

I have to say, for all the gradient methods we're doing, when do you stop?

If you're training a neural net, do you know how close you are to the optimal model?

The loss keeps going down because you're gradient descending it. The loss always goes down if you take a descent step. But you don't know if you hit the bottom.

You can go down, down, down, down, down, and you think you've plateaued, and then you run another million steps, and it goes down more. Sometimes it does that.

**The curse of all these descent/ascent methods** (I use them interchangeably) is: you don't know when you got there.

All you know is:
- I go down pretty quickly at first
- Then I go down slower and slower
- Then I'm almost hardly changing
- At some point I get bored or run out of money

## Missing Data and Imputation

I want to take the last 5 minutes to talk about a different EM problem that sounds different but is mathematically, abstractly the same: **how do you deal with missing data** if you're a statistician?

### The Missing Data Problem

Suppose you have some matrix (thinking back to our Netflix problem) where you have a bunch of $x$'s, and you have some numbers filled in, but you have some entries that are missing.

You could say: I'd like to estimate what the missing values are.

If you're willing to write down some model of what they look like, then you could say: I can do something where each of these missing values is like a $z$—it's like a class membership, it's a missing thing I don't know.

If I had a model, I could then estimate these. Many of these models (the fancy ones) have a back-and-forth EM view.

### Simple Imputation

Now, the **dumbest missing data model** says: let's assume that these values are all Gaussian.

If I say that $x_1$ is Gaussian and something's missing, **what's the best estimate of it?**

**The mean of all the ones I have.**

The fancy word is **IMPUTATION**: if something were missing, you might impute it and put in the average value.

### Critical Assumption: Missing at Random

Now, this assumes one enormous thing. **When is this a good idea or a bad idea?**

The problem is not going to be the Gaussianness. Feature independence is a question of a fancier version (do I have to have a multivariate Gaussian of everything?). But let's assume feature independence.

**There's one thing that's bigger than this that will mess you up.**

In 99% of machine learning problems, **is imputation a good idea?**

The big question is: **is the data missing at random or not?**

You make an enormous assumption, and people often don't tell you they're doing it. They say, "I just ran imputation, I just replaced all of them with the average."

**That assumes that the missing data are drawn from the same distribution as the ones that are there.**

### Real-World Examples

**This is, in my experience, 99% of the time wrong.**

I remember a Wharton student who ran this great survey and asked, "How many of you hate filling out surveys?" And 98% of the people said, "I'm happy to fill out surveys. It's great."

And I'm going, "Yeah, okay, and there's 2% who fill them out even though they hate them." **The data are almost never missing at random.**

- If I'm looking for your GPA on a job application and it's missing, is it random? **Probably not!**
- If I'm asking where were you born and it's missing, is it random? **Maybe, but probably not.**
- If I'm measuring the chemical density of a compound in my polymer and it's missing, is it missing at random? **No!** Perhaps it's almost zero (too low to measure, so it's missing). So maybe missing means zero. Or maybe missing means it's off the charts high. I don't know.

### Practical Advice

So in general, **things are not missing at random in the real world**, and so you should not be just doing imputation blindly.

If you want to impute the value, then **at least give me something else that says whether it's missing**: yes/no indicator variables.

Usually, **whether something was missing or not conveys information**—that's super important.

### Statistical Theory

There's a whole fancy literature on this, and if I were a statistician, I'd spend a whole lecture on it, but I won't. Most people just do dumb imputation and add in the missingness as a feature (an indicator variable).

So just be careful about imputation.

## Summary and Recap

So we're at time. Let me summarize:

We saw last class:
- Lots of different covariance matrices
- People use super simple ones in practice

You should know:
- **K-means is a limiting case** of Gaussian mixture models
- **The EM steps**: E-step (estimate memberships), M-step (estimate parameters)
- **The term ELBO** (Evidence Lower BOund), because the recitation instructors will be upset if you don't know what ELBO is!

Cool! Have a good recitation, see you all on Monday!

---

# CIS 5200: Machine Learning - Lecture 20: Bayesian Modeling and Generative Models

## Introduction

Okay, welcome, welcome.

Nice to actually hear people talking in the audience, and not just on phones, looking at the phone.

Good, so we're in the sort of middle of a very short segment on Bayesian modeling.

What I want to do today is to just range through a bunch of different, widely used, slightly out-of-fashion Bayesian models. I want to show sort of a similarity between them, and an approach to modeling that still lives on in modern deep learning, mostly in things like Bayesian optimization, where people have priors over what sorts of models or what sorts of hyperparameters they might want to use, and then compute posteriors.

But all of these models follow a similar pattern. What happens is we're going to build some sort of a model. A first principle is: this is the model, this is how the data is generated. And we're going to estimate it using something that usually looks like an EM algorithm. Almost all the models have some version of hidden data (not all of them do). Almost all of them have an MLE and an MAP piece.

And it's nice just to think about the structure. If you're going to build in inductive priors, all of them have extensions into the deep learning world where one looks at those applications. 

And so, we'll do a bunch of these. We talked last time—I'm going to quickly review GMMs (Gaussian Mixture Models)—just to get those back into our heads. We'll talk about Naive Bayes, LDA (Latent Dirichlet Allocation), Hidden Markov Models, and a lot on Belief Nets.

Cool! 

## Review: Gaussian Mixture Models (GMMs)

So, what did we do last time?

We said, hey, we're going to assume the world comes from a mixture of $k$ different components. And each component will have some sort of a base probability $\pi_k$, and given we pick which component we're from, we're going to generate some data—maybe with a Gaussian, but it could be with any other distribution we want.

### The EM Algorithm for GMMs

And we'll go back and forth between saying, hey, given some point, what cluster did it probably come from? Or more precisely, in expectation, how likely is it to come from each of the clusters, right? It's membership across the clusters.

And then given the membership, we said, hey, we could estimate the parameters. The parameters are just an MLE—expected value—and everything gets weighted by how much we think each point belongs to the cluster.

Given the parameters, we can now go back and repeat the process.

Sound good? Pure repeat. Same thing as last time.

Well, there are many other models that have that sort of property.

## Naive Bayes

One of the earliest versions is one that you might call Naive Bayes, used for language analysis in the ancient days of my childhood.

### Model Structure

And here the model is—and I pardon, the picture's a little bit messed up—but what we have is: for each document, we will say it comes from one of $K$ topics.

So I'll have a hidden variable $Z_i$ in the general case that says, I don't know what topic it could be on—economics, could be on politics, could be on law, could have been sports.

And that's going to be hidden for observation point $i$, and then given that model, we will generate a whole bunch of words, where the words are generated with some probability conditional on the topic. 

Make sense? 

So if you're in economics, there's a probability of a bunch of words, like dollar, or yen, or pound. And if you're in a different topic, like sports, there'll be a different probability of dollar, or word, or pound.

Make sense? 

So again, hidden memberships, and then we'll generate maybe $N$ words, independently.

### The Independence Assumption

This, of course, is ridiculous.

Right?

Are words conditionally independent? Is each word independent of the others?

Right? We're assuming here that the probability of the second word being something, given the topic and the first word, is equal to the probability of the second word given just the topic:

$$P(w_2 | \text{topic}, w_1) = P(w_2 | \text{topic})$$

**Wrong!**

Right? If the first word is "THE", the second word is going to be something different than if the first word is "EVERY".

Make sense? 

But why is it a plausible model, or a useful model, right? All models are wrong, but some are more useful. Strong assumption, weak assumption?

Yeah.

### Bag of Words Representation

So this assumes the document's a bag of words—we're just counting them. Nobody does that anymore that I know of. But there certainly are cases where you might want to assume that something is just a bag of words. I just counted how often each word shows up. I've thrown away the order.

When would you want to do that?

Big data set, small data set?

Big dataset?

Small data set?

**Small dataset!** The smaller the data, the simpler the model, the stronger the inductive bias.

If you've got a crappy, tiny amount of data, all you can do is worry about how often does each word show up in the bag?

If you've got big data, if you've got a trillion tokens, of course the order matters. But there's a lot more orderings of words.

### Combinatorics: Why Bag of Words?

If I have a vocabulary of 10,000 words and a document of size 100, how many possible documents are there?

10,000 words, 100 words long?

10,000 possible first words, second words, third words...

Big, small?

**Big! Right? Google big!**

There are a lot of possible documents. On the other hand, if you ask how many numbers do I need to count up all possible counts of all words in the document of 10,000 vocabulary size?

It's a 10,000-dimensional embedding, right? Just 10,000 numbers, most of them zero.

So, one thing to be thinking about always in machine learning is: How much data do you have? How fancy a model can you fit?

And Naive Bayes is really a stupid model, or really a simple model, or really a strong, unrealistic assumption, but it makes it really easy to fit the model.

Cool. Good?

Yeah.

### What Are We Learning?

What are we trying to predict here? So what are we learning?

First of all, what are the parameters of the model?

It's the $\beta$—it's the probability of each of the 10,000 words of vocabulary that you have in each of the, say, 30 topics I have.

What do I often want to know?

I often want to know for a given document what topic is it on. Right? If I'm in a company, should I send it to the accounts payable department or the sales department? Those are different topics. "My product is not here" versus "I want to open an account with you"—different topic.

In the modern era, of course, you do an embedding and do supervised learning model, but this allows you, in some sense, to cluster things together.

And what do I estimate? What does the $Z_i$ tell me? For each document?

It's the expected probability of it being in each of the topics. It allows me to label what topic a document's probably on.

So, for each document, if I have $K$ topics, $Z$ is a $K$-dimensional distribution, right? It tells me, for this document, estimate the probability that it came from each of my $K$ topics.

### Inference and Bayes' Rule

So often, you want to break things up. Here's a bunch of data. Give me some unsupervised method that tells me what are the dominant $k$ topics, how much do they show up. And I can summarize them, and I can count which words there are.

Now, these models give us what? This is giving me the probability of the word given the topic:

$$P(\text{word} | \text{topic})$$

If I want, I can find the probability of the topic given the word:

$$P(\text{topic} | \text{word})$$

How would I do that?

**Bayes' Rule!** Right?

So, we can swap back and forth, right? It's a full model.

I could find, first of all, how likely is this document on these topics, then I can say, how likely is the word given a topic?

But often what I want is—well, this is useful for interpretation. What's the topic about? Look at the most probable words.

Oh, crap, that's probably not useful. What's the most probable word in English?

**"The"** in every topic!

So I might want ones that are more discriminative. But I might want to swap and say, what's the probability of the topic given the word? Which words are most informative?

This is used in sentiment analysis, right? So sentiment analysis: do you like it or not like it? But it's also used when we've got a whole bunch of protest tweets from the Middle East, and we want to know what are people protesting?

One version is I just take all the stuff and I slam it into GPT-5, and I say, summarize all this stuff. And you sort of hope it does a good job.

Or the other version is you take all these tweets, or X's now—and now... still tweets. And then you say, hey, find me 20 topics that explain these well. What are the words in each of the topics?

Cool. So that's the idea.

### Estimation

So what's the estimation piece here? Again, I want to look at this.

Okay? Yeah, so if it's not observed, this is an EM algorithm, right? Back and forth between what are the assignments of topics to—I'm sorry, of documents to topics—and what are the parameters $\beta$?

If you label a bunch of these, and say, hey, this is economics, that one's sports, or this one's cricket, that one's soccer, whatever, then the whole thing is supervised. If I know the label of the topic, then it's trivial.

Right? Probability of the topic: I just count what fraction of the things are on cricket. And probability of word given topic: if I know the topic, I just do the estimation.

### The Zero-Count Problem and MAP

As always, when we do these estimates, one wants to be a little bit careful about what happens when you have zero values. So, if for a given topic, like cricket, I have never seen a word—right? So, say, "Lyle" has never shown up in cricket.

What is the MLE estimate of probability of "Lyle" given cricket?

**Zero.** 

If I now turn around and I have a new document, and the document just says, "Lyle just started playing cricket."

What's the probability of that document being on cricket?

**Zero in the MLE.**

Right? Because we started by saying the model estimates $\beta$ of probability of "Lyle" given cricket at 0. Therefore, we believe that the word "Lyle" never shows up on the topic cricket.

If you flip it around and estimate the probability of cricket given "Lyle", if "Lyle" shows up, it's not on cricket according to the model.

You with me? 

Now, words are really sparse. Most documents don't have most words—they're zero, they're long-tailed, there are a bunch of really obscure words, like "Lyle", that don't show up very much.

And so you want to be very careful in reality if you do these things. You want to do not an MLE, but an **MAP** (Maximum A Posteriori).

And there's a bunch of ways to do it, but in some sense, what you're mostly doing for the MAP is you say, well, imagine for every topic that every word had shown up once, or a half a time, or an eighth of a time.

If you think back to the really early lecture on MLE/MAP, remember we were flipping coins? We said, hey, you can pretend we've seen one head and one tail, or two heads and two tails, or half a head and half a tail, and they all have names like Jeffreys Prior, and Laplace Prior. And I don't care if you know the names, but I do care that you know that for something that's super sparse, like words, if you actually try to estimate the parameters, you probably want an MAP.

Just throw in some sort of pseudo-count. I'm not going to spend time in this class on this, because it's not that popular to use these models anymore. But it is important enough just to remember: **MAP for rare things. Don't put the zeros in**, because if you have something that says "Lyle plays cricket", and you try and use Bayes' rule for the probability of cricket given "Lyle", you'll get zero because we've never seen "Lyle" with cricket.

Okay, end of tirade. Good?

### Summary of Naive Bayes

So, summarizing, Naive Bayes, in the common usage, you see what the variables are, you know the labels, you hand-label. This is whatever, but you could also do an unlabeled version, which would be an EM.

Cool!

Yeah, that's it. I said most of that already.

## Latent Dirichlet Allocation (LDA)

I want to very briefly cover a method which I still do use a fair amount, which is an extension of Naive Bayes called **Latent Dirichlet Allocation**.

Oh, right. I didn't even... did I write it up there? I didn't. LDA, everybody calls it LDA. But it's probably worth writing the whole thing out.

**Latent Dirichlet Allocation, LDA.**

### Generalization of Naive Bayes

So, LDA is a generalization of Naive Bayes. I'm going to cover it briefly to show, sort of, how some of the EM gets generalized. I'm going to hand-wave a little bit, be a little formal, a little hand-wavy.

And here, what I want to say is, before I assumed every document was on a single topic. But in fact, that's not a great model.

A better model says **every document is a mixture over topics**.

Right? It's both on sports AND economics, because it's how much somebody paid to buy some sports team in England, which seems to show up a lot in my feed, even though I don't follow soccer. But for whatever reason, you with me?

So, if you think about it, I'm going to now have $K$ topics. But every document will be a mixture over those $K$ topics. And then once I picked a topic, then I will, in fact, generate a word with some probability.

So now I've got something more complicated hidden.

### Two Perspectives: Generation and Estimation

And I think it's worth thinking through two sides to these. I find students find this a little bit confusing. There's the **generative side**, and then there's the **estimation side**.

Right? So first, assume that you're omniscient. You're God, and you're going to generate topics.

#### The Generative Process

How do you generate a document?

You start some document $D$, and the first thing you're going to generate is you're going to pick a distribution $\theta$ over the topics.

Right? So if I have $K$ topics, then $\theta$ is a distribution over the $K$ topics:

$$\theta \sim \text{Dir}(\alpha)$$

where $\theta$ is a $K$-dimensional probability vector: $\theta = (\theta_1, \theta_2, \ldots, \theta_K)$ with $\sum_{k=1}^K \theta_k = 1$.

Right? And every different document gets its own distribution.

So I've generated a distribution over topics for this document, which says, here's the probability of every word in that document coming from each of the topics.

Make sense? 

To generate this distribution, I need a prior, a distribution over that distribution. And I'm not going to show the math—that's called a **Dirichlet distribution**. A Dirichlet distribution is something that gives distributions over probability distributions.

Make sense? These are $K$ numbers between 0 and 1 that sum to 1. It's a distribution.

Good? So now I've picked the distribution $\theta$ for this document.

Now, given that distribution, I'm going to pick words one after another.

Each time, the first thing I do is I pick a topic:

$$z_n \sim \text{Multinomial}(\theta)$$

And say the first topic is economics. And given economics, I've got some $\beta$, some probability of each word, and I generate a word from economics, which is "buy".

And then I go and I do another thing. I pick the topic, and this topic is going to be sports! And I generate a word with some probability from the sports topic, and it's "kick".

And I pick the topic again and again, it's economics! And it's, you know, "pounds".

And I pick it again, and I get a topic, so I get $N$ words. I'll generate $n$ topics. For each topic, I generate a word:

$$w_n \sim \text{Multinomial}(\beta_{z_n})$$

Make sense as a generative model?

I can generate synthetic bags of words, right? Later on today, we'll get to sequences, but now there's no sequence of words, we're just generating them. We could generate "BUY" again here. Who knows, right? That'd be two "buys" in the bag.

Good? 

Yeah.

**Question:** Is it random?

Well, random, but let's be precise about random as a general word. It's a general distribution, so I've got, for each of the $K$ topics, I've got the probability of every single word in the vocabulary given that topic.

It's a two-layered distribution model. Random is fine, I just don't... it's random numbers from a distribution.

Yeah.

### Why Is This Useful?

How is this useful? It's useful in the following ways: now that I have this model, note that the model has **two sets of hidden parameters** for every document.

It's got a hidden parameter $\theta$ for that document, which is the mixture of topics in that document.

And for every word in that document, it's got a $z_n$, which is the topic that word was generated from.

Right? So I don't know the $\theta$'s, I don't know for a given document what the mixture of topics is. And I don't know for a word what topic it was on.

**All I observe are the words!**

So those are observable.

Now, why is that useful? I can now run an EM algorithm, and I can give in a bunch of documents with words. I can estimate two things:

1. The hidden values (not hidden parameters—that's a bad word). I can estimate the hidden values.
2. I can estimate, for every word, what topic it was from.
3. And for every document, what's the mixture of topics?

I'm going backwards, right? I'm using estimation.

**Generation is something that only mathematicians and deities do, right?**

The real world generates documents, or people generate documents, right? And they generate them as if they were following this process. I don't know the process—I mean, I can't observe the hidden values in the real world. All I can get are bags of words for documents.

But now, given the bags of words for documents, I can run the EM algorithm.

### What We Learn from EM

And at the end of the EM algorithm, I will know for every document: here's the expected value of $\theta$—how much of each topic distribution I think it's on. And for every word in the document, what topic did I think it was on?

I'll also know the parameters, which is for every topic—right, I've got $K$ total topics in the world—so I'm going to have, for each topic, I have the probability of each word given that topic:

$$P(\text{word}_v | \text{topic}_k) = \beta_{vk}$$

So this is the model parameters, right? That's $\beta_{vk}$: probability of each word given each topic.

So that's the key parameter in the model: how likely is each word given each topic?

Which, in some sense, I don't care so much about. Mostly what I want to know is for every document, estimate the hidden value of what's the topic distribution?

This document is 70% economics and 20% sports, and 10% other stuff.

Make sense? 

So this is like Naive Bayes, but Naive Bayes made a really strong assumption. It said every document's on a single topic. Now I've got a sort of a flexible Naive Bayes that says, oh, no, it's not. Every document's a mixture over topics.

Yeah.

**Question:** Are words given hard assignments to topics?

In general, the way this works is that when people run this algorithm, they do a hard assignment. In theory, you could let every word be a distribution over topics, but that turns out to blow things up insanely bigly.

And so the standard algorithms that do this—and you can use either an EM algorithm or something called Gibbs sampling, which is a slightly more, sometimes more efficient version of these estimations (we're not going to cover it)—in general, in practice, people assign each word to a single topic.

Right? So think of that as a hard assignment of the word to the topic. And then, of course, this is an expected value. And the MLE estimator of the $\theta$ is just count up how often each topic showed up in each of the hard assignments.

But you could, in theory, do a soft assignment. It just turns out that the memory gets really big.

Yeah.

**Question:** Distribution per document?

So I have a distribution for each document. I have a different distribution over topics. Each document has a different distribution over the $k$ topics.

So this one is mostly economics and sports. A different document will be mostly sports and losing, I don't know, or computers and coffee.

### The Inference Process

And from the distribution, we draw topics. And from the topics we get words. 

For the estimation, we say, **given the words**, estimate the hidden values of the topics. And from the hidden values of the topics, estimate the hidden values of the topic distribution.

We're throwing it backwards, but of course, it's going to be an EM algorithm that goes back and forth.

So let's try and look at this a little more formally.

Yes?

**Question:** What about "the" and "and"?

So, in a simple model, what's going to happen is every topic's going to have a high probability of generating "and" and "the". And those will just be something where that's a high probability across all the topics. If you swap with Bayes' rule, seeing "the" or "and" doesn't tell you much about the topic. "The" has a high probability given every topic.

In the old days, people liked to use **stop words**, and they would throw out common words like "the" and "and". But stop words are super annoying, because sometimes the words are useful, and every different dataset has different stop words. I was looking at medical records, and there were stop words like "procedure" and "disease", because they're so common, they told you nothing. For all the different diseases, you would have a word "disease" showing up.

So, you could put in stop words, but it's mostly out of fashion. And again, I think the piece to note is that you're mostly not going to use—well, you can use these or not. In fact, if you run a standard LDA topic model—now, there is actually one that's not a great one in scikit-learn. It will, in fact, come with a bunch of stop words, and it'll throw out words like "the" and "for" for you.

Yep.

That's great.

### What We Really Care About

So, like, what we're really doing is what we care about is using—well, we'll cover how EM is the way we're going to do it. We're going to find the parameters in the model, which will let us, most importantly, for every document, say, what's the distribution of topics in this document?

It will also allow us for future documents, given a word, to predict what the distribution of topics is.

And so you can think there are a lot of cases where you might want to build a model of something. Say, a disease: you've got a bunch of proteins in your blood, and they pulled them out. And the proteins are generated by some mixture model that says there's a couple of different latent factors generating proteins. And so you have a distribution instead of over topics and words—you have proteins. 

What you don't know, unfortunately, is what are the 3 or 4 causes of the proteins being there. So you'd like to build a model that says, given the observable proteins (the "words" in your blood), give me an estimate—the most likely, the MLE estimate—of what are the different causes, the diseases behind it?

Right? And so, in the end, I don't care about the generative model, but I can't estimate it without writing down a model.

**The model is my assumption about the structure of the world.**

And I have some sort of a prior over how the distributions look. And I have some sort of model of how words are generated. Words are not Gaussian.

Right? The words are—each word is there or not there. I'm going to have a multinomial. There's a probability of each word.

Right? If I've got 10,000 words, there's a 10,000-dimensional vector here that says here's the probability of each word given each topic.

Make sense? So $\beta$ is: number of words by number of topics—that's a matrix.

That's a model I assume. And if it's a real value, then it might be a Gaussian distribution. If it's words, it might be—each one is sort of almost binomial, but it's not binomial, because it's not just one word.

Well, you can generate words separately. I mean, pick your model. Other things will have Poisson distributions for arrival times. 

### The General Philosophy

So the game here is you say, **this is what the model of the world looks like. These are what the probabilities look like**, and a well-specified distribution, by definition, is a generative distribution. It's one you could generate data from—that's complete.

Then, given that, I'll use EM and estimate the hidden values and the parameters, and that will allow me to, for both the training data and future data, tell me: what are the probable labels? What are the probable topics? 

Does that make sense as a big philosophy? 

And now we've gotten much more into, sort of, you've got to pick what the distribution looks like. And you've got to build, then, an estimator to estimate it. And there's lots of software packages—we're not going to talk about them—for estimating these things. This is not something you do in scikit-learn. But there are libraries that let you build these things and let you estimate them.

Yeah.

Sorry?

**Question:** How does it look for the words?

The words are super easy. All we're doing for words is, given a topic—each of 10 or 100 topics, whatever—how likely is each of my 10,000 words?

Right? It's just 100 vectors of size 10,000.

Right? That's the model we have here. You could pick a different one, but that's a perfectly fine one. And that's one that's used in LDA.

Yeah.

**Question:** Does this allow the same word to be in multiple topics?

**Absolutely!**

Right?

Think about it, I've got each of my topics: econ, and politics, and sports!

And then I've got each word. And if I've got the word "alligator", I could have it in econ be 0.001 and in politics be 0.01, and in sports be 0.1, because I think that the team in Florida must be named the Alligators or something.

And I have the word "duck". And the word "duck" in econ is 0.002, but in politics, it's 0.03, and in sports, it's 0.001. I don't know.

Right? So this says that every topic can generate every word. You could have zeros if you want, but be careful about the zeros—you probably want to put a 0.0001.

Right? Something in there to do an MAP. But every topic can generate every word. But some words are going to be more likely based on the topic, right? Economics will talk more about euros than sports. But of course, politics is going to talk about euros sometimes.

Good? 

### Running the EM Algorithm

Okay, so let's run the EM.

The **E-step** is going to compute—and again, this is reinforcing Monday's basic idea—we have hidden variables. Here, the hidden variables are:

- $\theta$ for each document: how likely is each topic?
- $z$ for every word in every document: what topic is it on?

Right? So I'm doing a soft estimate of the $\theta$—it's a distribution. I'm actually doing, in this case, a hard estimate of the $z$'s.

And as you pointed out, it doesn't have to, in theory, be hard, but in practice, the standard way is to make $z$ be a hard estimate, a hard cluster. We assign every word to a topic.

**Question:** If I have the same word twice in the same document, does it have to be in the same topic each time?

**Absolutely not!** There's no reason it should be. And if it shows up 20 times, I'll get a pretty good estimate of the different probabilities of the topic it's from.

Right? Some of the "pounds" are econ, and some of the "pounds" are sports. You know, "he pounded the guy, then got penalized 30 pounds," I don't know, you know.

So I've got the **E-step** where I estimate these hidden variables given the parameters in the model.

And then, in the **M-step**, given that I now have an estimate of $\theta$ and the $z$'s, and I know for every word what topic I think it was from—right? A provisional assignment—now I can estimate the parameters. And the main parameter is the same one we had for Naive Bayes: it's the probability that each word in the vocabulary comes from each of the $K$ topics:

$$\beta_{vk} = P(\text{word}_v | \text{topic}_k)$$

And then there's also another parameter I'm not going to worry too much about, which is the Dirichlet prior, which is the prior over the $\theta$'s, has a parameter in it, which you can just call a hyperparameter, not worry about it.

Yeah.

**Question:** What are we doing?

This is, in some sense, the super advanced extension of k-means, right? K-means, we went back and forth between hard assignments of points to clusters, and the centroids of the clusters.

Now we're going back and forth between the $\theta$ and $z$, which are like assigning points to clusters. In fact, every point IS assigned to a topic. That's almost like a k-means, but I also have the $\theta$ here.

And once I have the assignments, when I know for every point what topic it's in, now I can go back and estimate—and I know the distributions—I can estimate the $\beta$'s: how likely each word is to come from the topic. 

So, parameters and hidden values. EM is always: **E is the hidden values** (often the cluster assignments), and **M is the MLE estimate of the parameters**.

**Question:** What about the words?

Well, the words are always—we're not generating the words. The words are given, the words are the data. I get a whole bunch of documents, I count up the words in documents. If I've got 10,000 words of vocabulary, I got a 10,000-dimensional vector for every document. So the words are given to me—that's the data.

The labels for the words I'm trying to estimate. And based on the labels, I estimate the probability weights.

And we'll start with some sort of random assignment, right? So all these things, right? How do we do all the EM last time? You've got to pick some initialization—often you randomly assign words to topics and start the process.

Right? So you've got to start somewhere with initialization.

**Question:** What is $\alpha$ and $\beta$?

$\beta$ is the probability of the word given the topic. And for this class, don't worry about $\alpha$.

We're estimating $\beta$ on the M-step, and we're estimating $\theta$ and $z$ on the E-step. And those things I do want you guys to all understand.

### The Math Gets Harder

Okay, there's one other subtle point, which I'm not going to do fully, but last time, when we did the EM for the Gaussian mixture model, when we went for the E-step, we showed that we had $Q$ approximating $P$ with a KL divergence, and we said, "Duh, just set $Q$ equal to $P$." It's so trivial, why am I going through all this math?

If you do the same math for this, which I won't, what you're going to find is this thing has got all this complexity in it, and when you do the KL divergence there, you can't actually solve the thing in closed form. All you can do is a **gradient step** to make it closer.

So, as you'll find for LDA, it looks really similar in the math, but you can't just say, "Well, duh, KL divergence of $P$ equals $Q$, set $Q$ equal to $P$." Now all you can do is say, "I got these two things that show up in the middle of it, but I can step closer to it."

And so that's why all that "stupid math" on Monday was nice, because when you look at something that's got two sets of hidden things, now, all of a sudden, doing the E-step is not so clean.

And the paper by Neal and Hinton—yes, that Hinton—was one of the early ones showing how beautifully you could gradient descent these two pieces, where you say, as long as the E-step moves toward getting a closer approximation, then you'll be improving the likelihood, and you go back and forth.

**Question:** Convex, non-convex?

**Non-convex**, and it turns out, if you ever run these things, there's a whole bunch of different algorithms out there—variational methods, Gibbs sampling methods. They're incredibly different in terms of how well they can converge to a good solution.

So this thing is incredibly non-linear because you're assigning stuff, right? It's an NP-hard problem—you're assigning stuff to clusters. So, just be careful if you run stuff that a lot of the algorithms for LDA look nice, but converge to super crappy solutions.

So, unlike Gaussian mixture models, which are mostly fairly stable, the fancier the model, like this, the more unstable the convergence algorithms are.

Yeah.

Yep.

**Question:** How do we set this up that says I need to make these estimates closer?

The answer is if you write out all the math, it's super straightforward. I can point you—I'll put some references—I have all this stuff written out, but the answer, in some sense, is I really don't want to get people distracted. That's a whole separate course. 

What I do want is just to reinforce this notion of EM over and over, that there are hidden parameters, that if you've got multiple of them, now the E-step becomes non-trivial. It was trivial before, now it's non-trivial. And the M-step is, in fact, still trivial, because you have the weightings of everything.

So I'm not answering your question, for which the class will mostly be thankful. Some people will be annoyed.

Cool! That was LDA.

### Modern Alternatives

Super fast—something you... well, it's still used a moderate amount in papers. Often people now replace it with versions that say, just take the document, embed it, and now run a k-means clustering on the embedding.

But realize, again, that you always have some inductive prior. Something either you assume: each document's on a single topic, or you assume a document's a mixture of topics.

The trendy, current versions say: take the document, embed it, and now do a k-means hard clustering, assuming that there's a single center of the document.

Make sense? That works—it's state-of-the-art. But it wouldn't surprise me if in 3 years, some clever person says, "Wait! Why do you think that each document comes from a single cluster? Isn't that ridiculous? I mean, the embeddings make the cluster centers more flexible, but why don't we have each document be a mixture of centers? We could have 3 or 4 centers for each document and cluster better!"

Right? So people go back and forth on estimating them. So at the moment, LDA is sort of out of fashion, and BERTopic or things that do embedding and k-means clustering are in fashion, but many things are mixtures.

Right? And partly, I think, as you think about something like my Netflix account, which is a mixture model of the three people that use it, where Netflix doesn't know necessarily who the three people are—think about the mixture model, and think that almost always you have a mixture model. You have an unknown distribution like $\theta$.

Right? What's the distribution over people in my Netflix account? And then, for each person, what's the probability of each movie?

Does that make sense? And you're going to pick the model form that makes sense for your world.

Good?

Okay, onward.

## Hidden Markov Models (HMMs)

Let's do an entirely different type of model that also fits the same general process as a generative model. And those are HMMs, or—didn't write it out, eh?

Oh! **Hidden Markov Model.**

And here, again, we're going to have a generative model, which is how the data would be, in theory, generated. But I don't care about that, except I have to assume it. Then I'm going to run an EM algorithm to estimate some hidden values and some parameters.

### The Generative Process

So how do I generate stuff?

I'm going to start in some hidden state—one of $K$ states—so this will be a hard assignment. You have $k$ possible states.

And I will have some **transition matrix**, where I go from state 1 to state 2, which is, I will say, there's some probability of state at time $t+1$ given state at time $t$:

$$P(s_{t+1} | s_t)$$

So that's a $K \times K$ matrix of probabilities.

Right? All the things that are between 0 and 1.

And this tells me, for each of the $k$ states I could be in—happy, sad, sleeping, awake, whatever—what's the probability of transitioning to each possible next state?

Make sense? 

Then, for each state, I will have what's called an **emission probability**. I will generate some observable $X$, and I will say, what's the probability of $X$ given the state:

$$P(x_t | s_t)$$

### What's Hidden?

So, what's hidden?

What's hidden are the states. You don't know what state you're in.

Right? Are you broken? Are you working? Are you open, are you closed? Whatever the states are, they are hidden.

And you have emissions—you do observe the emissions, right?

And what are the parameters? The parameters are:

1. The Markov transition matrix: how likely you are to move to the next state from the current one
2. The emission probabilities: the probability of each of the possible emissions

If this is discrete, this will look like we saw before. The emission is a word—it'd be a probability of a word given the state.

If this is real-valued, you'd have a probability of an $X$ given the state, which would look like, say, a Gaussian model.

Right? For each of $k$ states, I could have a Gaussian distribution over $X$'s.

Right? So an HMM could either be discrete emissions—right, one of 10,000 words—or it could be something which is a Gaussian model, but it's a Gaussian model for each state.

Make sense?

### The Inference Problem

Yes, so again, we're going to go through a sequence, a real sequence of things. I'm going to give you a sequence of observations:

$$X_1, X_2, X_3, \ldots, X_T$$

So these are observed $X$'s, either discrete ones like words, or each $X$ could be a real vector.

And based on this, or a whole bunch of these sequences, I want to estimate for each $X$: what state was it in?

One of $K$ states: $S_1, S_2, S_3, S_4$. What state was it in?

I want an MLE estimate of the state, the most likely state. We'll do this often as a hard estimation. What state was it in?

And I want to know the parameters, and the parameters are, again:

1. The state transition matrix: how likely is each successive state given the previous one?
2. The emission matrix: how likely is $X$ given state?

The emission looks very much like the Naive Bayes model. But note that what we now have is **time**—discrete time.

### Causal Models

If this were a neural net, like a transformer, we would call this a so-called **causal model**. Causal in the sense that to predict the next thing, we know the previous thing. We're predicting the future given the past, right?

Or put differently, the model generates emissions sequentially in discrete time.

Right? There's no bag of words here. Each word is successive, and the next word depends upon the—well, in theory, it depends upon all the preceding words.

### The Markov Property

But here's the **magic of the model**. I need more space! The magic of the model is going from $S_1$ to $S_2$ to $S_3$, and this then generates $X_1$, and this generates $X_2$, and this generates $X_3$.

**If I know $S_3$, then knowing the previous $X$'s or states tells me nothing more about $X_3$.**

So let's be very formal about that.

The probability of $X_3$ given $S_3$, or any $X$ given the state it came from, is the same as the probability of $X_3$ given $S_3$ and whatever else you want—$S_2$, $X_2$, $X_1$, all the other stuff!

$$P(X_3 | S_3) = P(X_3 | S_3, S_2, X_2, X_1, \ldots)$$

Right? $X_3$ is **conditionally independent** of all of the preceding stuff, given $S_3$, its parent.

$X_3$ is conditionally independent of all the earlier history, given $S_3$.

### Is the Markov Assumption True?

Shouldn't there be some dependence? **This is the Markov assumption.**

Right? The Markov assumption.

Is the Markov assumption true? Does it describe the world accurately?

It's like every modeling assumption in machine learning: is the world linear? Is it logistic?

**No, they're all wrong!**

But they're reasonable assumptions, and you make the most complex assumption you can afford.

Right? Either you're very wealthy, and you say, "Hey, I'm going to predict the $k$-th word given ALL the words beforehand." Right? Put them all in a big context window, embed them all, do something.

Or, you say, "Hey, I'm not that wealthy, I'm going to map all of these to a state, and I'm going to then use that state to predict the next state, and predict the next word."

Which one is better?

**Depends on how much data and how much compute you have.**

The standard assumptions that are used in most large language models are not Markovian. They put in the whole context window, and look at all of it.

But there are plenty of **state space models**—there are whole startups in Silicon Valley that are arguing, "No, our state space models are much more efficient, they're much faster, they're cheaper, they need less training data, they're better than your transformer models."

So far they're this big, and the transformers are this big, so I'm not sure they're going to win. But there's still people trying to figure out, maybe it would be better to use a state-space model.

### Latent States as Embeddings

And you can view these latent states in some sense as an embedding. Once I have figured out this state, if that tells me everything I need to know about the past, that's super efficient.

What do I want to know? I want to predict the next word. Think about how LLMs work. They take a sequence of words: "I can't believe I ate the whole..."

And then it predicts the next word, probably "thing", maybe "hamburger", you know, maybe "tarantula", but that's low probability.

And predicting $X_3$ in a Markov world says, **once you have this embedding, this state space, this latent variable**—I'm using them all pretty interchangeably—**this is a latent variable, it's a hidden state, it's an unmeasurable thing you can't see**. All I see is the words: "I can't believe I ate." Those are observable. Those are the emissions. They each correspond to some state, which is a latent variable, right, a vector that describes things.

And the model here says, look, **once you know this latent state, it tells you all there is you can know about the next word**. All that other history provides no marginal information.

That's conditional independence.

Yeah.

### Connection to LSTMs

**LSTMs are definitely a state-space model just like this.** The hidden Markov model says, let's do a hard assignment where this is a one-hot encoding of state.

An LSTM, for those that don't know—they're a bunch of recurrent neural networks—uses a **vector embedding** for the state.

The hidden Markov model says there's just a probability matrix from here to here. The LSTM says, "Oh, build a big neural net to go from this real vector to that real vector." But they're fundamentally the same idea.

**An LSTM looks just like the hidden Markov model made non-linear.**

Right? It's very much a state-space model. They're out of fashion this year because the unrolling's more expensive than doing things in parallel, but it's good to know these things, because they come back in and out. So, yes.

LSTM looks a lot like this. Replace one-hot with a vector, replace a linear transition matrix with a nonlinear function. Same idea. 

You can do the same thing, by the way, with linear models. It's common to use Kalman filters and state-space models. Instead of having this be one-hot, you could make this thing be real-valued, and still have a linear transition of some sort, or a Gaussian transition. So there are lots of models you could use.

Yeah.

Yep.

### The Hyperparameter $K$

**Question:** How do you choose $K$?

You need always—all of these models have a hyperparameter, which I've been calling $K$, which is the **number of states**.

Right? If you think about everything I've said, pretty much, they all say there's $K$ topics, there's $K$ states, there's $k$ centroids, there's $k$ clusters. You always have a hyperparameter.

You also choose: are the states one-hot, which they are in a Markov model? Or are the states real-valued embeddings? So that's a choice you make. So you've got to tell me, as an engineer, what $K$ is.

What's the right number?

Who knows? If it's too complicated, you're going to overfit. If it's too simple, you're going to underfit.

Right? How much can you afford? How big is your GPU? How much is your training set? But yes, critically, **all these models have a $K$-dimensional hidden space**.

Right? There are $K$ topics for the LDA, there are $K$ hidden states here. You've got to tell me that. Not my problem.

Yep.

**Clarification:** We're talking about the hidden space, we're talking about topics and not words. 

Yeah, here, the words are down here. The words are emissions—they're things you see. These could be words or numbers.

This thing, you can think of it as $K$ topics, or more generally, it's $K$ states. Maybe they're rainy days, and some days are sunny days, and some days are cloudy days, and some are snowy. You got 4 states, and if you can't observe them, then you don't know what the weather is, but the weather's going to determine something like your sales—how many people show up, how much energy is used?

Right? So, if you could measure these, these would be great. You'd say, "Hey, I can label, we're on a snowy day, so of course we're going to have more people showing up late."

But if you can't observe it, then you have to estimate something there.

And again, there's going to be an EM method, which we're not going to cover, that goes back and forth. If you know the hidden states, it's easy to find the probability of each $X$ given the state and the probability transitions—the parameters are easy, given the states.

If the hidden states are known—and if you know the parameters, then you can estimate a probability distribution over the states.

Yeah.

### The Reverse Process

**Question:** So this is the reverse of generation?

**Yes.** Yes, all these EM algorithms have the property that they're reverse, right? They're an anti-generative process. There's a hypothetical generative process, which you can't observe.

And what you have is a bunch of outputs. And now what you want to say is: **what are the most likely sequence of states that we went through that would have given this observed output?**

And we also, along the way, learned the parameters—what's the model—which allows us for any future sequence to say what it did. 

And so you could, for example, say, "Hey, this ocean water is a mixture of a whole bunch of different organisms, and each organism has some sequence of states which I can't observe. And the sequence of states gives me the different nucleotides in the DNA. And so if I have DNA sequences, I can have a model of DNA sequences that say, hey, they're generated by some sequence of introns and exons and hidden states."

And it doesn't—you're going to have to do the modeling for your own problem, but all of them: you've got something observed, you've got a bunch of sequences, and you're going back and saying, **these sequences were generated from some model with some parameters** $\beta$ and some transition matrix, either called $M$ for Markov or $G$ for transition, probability of $S$ given previous $S$.

And that then lets you say, for this sequence of things I observed, what are the likely states behind it?

### Embeddings for Sequences

And for language models, this is used a lot. If you do this, and you say, "Okay, give me this embedding here, this state," **that's a summary of the information of all the preceding stuff that came**.

Make sense? So a standard way to generate an embedding for some sequence of language is you take a set of words, you pop it into a GPT, it's predicting the next word, the next word, the next word. As it's predicted the last word, it's got some sort of embedding that it's using to predict the next word. **Take that last embedding.**

So the same way we said we could take an image and go through a bunch of layers before we do predictions, we can take something near the end of the neural net before we see the labels, as an embedding of the image. And that makes a much nicer way to say, "Are these two images similar?"

I can take sentences, and take a bunch of sentences, and each sentence is a sequence of words, and I take the embedding here. And if I want to measure how close two sentences are, **I use the $L_2$ distance of their state spaces at the end, of their embeddings at the end of the sentence**.

So the same thing that's useful to predict what comes next is, in some sense, **a summary of everything that went before**.

And if the world is Markovian, then this thing captures all the information of what was before it.

Yeah, I'm going to move on. Yep.

### The Distributions in Detail

**Question:** How does the distribution of $X$ given $S$ look like?

If this is discrete, it's exactly like the $\beta$ we saw before. It's a probability of each of the $X$'s, each of the words given the state.

And how does the distribution of $S_{t+1}$ given $S_t$ look like?

It's a transition matrix—it's a $K \times K$ matrix, where for each of the states here, there are $k$ values, which is the probability distribution over the next possible states.

It's for each of the $K$ states here, you get a distribution over each of the $k$ states here.

So you've got $K$ possible things you're coming from at $S_t$, and you've got $K$ possible things you're going to at $S_{t+1}$.

**Question:** Where do we find them?

That's the EM algorithm—it's back and forth. We say, what's most likely given the other stuff?

Question? Yeah, and I'll move on.

Sorry?

### Current Usage of HMMs

**Question:** Where are HMMs currently used?

The answer is currently they're mostly not used. If you went 15 years ago, all the language models and gene sequencing models were HMMs. Starting around 8 to 10 years ago, the HMMs have largely been replaced—well, they were replaced with LSTMs and recurrent neural nets, which are, instead of being linear and discrete states, they're continuous states and non-linear transformations. Now they're largely being replaced by transformers.

So, in some sense, people really don't use hidden Markov models in the contemporary world.

**And why am I talking about them?**

Because there is a minority of people who think that a lot of the neural nets should be shifting to versions that are non-linear versions of this.

So make $S$ be a real-valued embedding, make this transition matrix be a neural net function, and make this be a nonlinear emission. Now you get something that's actually still moderately widely used, where this thing could be a state emitting an image and a transition to the next state.

If you're trying to build movies...

So there are plenty of **non-linear neural net versions that have a state space flavor to them**. That make sense?

So, the HMM is currently—I don't know anyone currently commercially using HMM. There was a period I remember when all the speech recognition systems did; now none of the speech recognition systems do. But the same idea is really common if you look at a modern speech recognition system.

It assumes that there's some sort of a real-valued state vector. It then emits a vector with a Gaussian mixture model of cepstral coefficients, which capture the frequencies, and you have a set of neural net-driven transitions between states.

So, **some versions of state-space models are still fairly popular in the neural net world**. So I want you to be used to that notion of thinking about the world as state vectors that transition, which in the modern world is a deep learning transition, and given a state that has an emission, which in the modern world is not a Gaussian—well, might be a Gaussian mixture model for some of the commercial speech recognition, or might be something that's a more fancy nonlinear neural net.

Does that make sense, that this structure is still widely used? It's just we've replaced trivial matrices with fancy neural nets, and we've mostly replaced one-hot with real-valued embeddings.

Sorry?

**Question:** Action spaces?

Yep, and we'll see action spaces. We're going to come back and talk about reinforcement learning in the last module of the class. And in reinforcement learning, we'll have action spaces added to these.

So, but again, we'll have in RL, we will see again state-space models showing up a lot.

Okay, I'm going to move on, because I want to cover one more topic. There's an EM thing. I want to talk—yep, quickly—about **Bayesian networks**.

## Bayesian Networks (Belief Nets)

Which is a formal structure of writing semantics of conditional independence, and in fact, all of these versions I've shown before are special cases of Bayesian networks.

The idea of Bayesian networks or belief nets is a simple **graphical notation that captures conditional independence**, which of course, I'm going to make sure we formally define, and a way to specify a full distribution over a full probability space that's in a much more compact form.

So, hidden Markov models are one form that says there's some probability distribution over all the $X$'s, but here's a particular form it takes, which assumes some conditional independence, right? Given the current state, the current emission and all the future is conditionally independent of the past. The past doesn't matter. So we'll write that out in this form.

### The Structure

So the idea is we're going to have a bunch of **nodes**—the world will have nodes or random variables, or things that you can measure. So you'll have some nodes, and we're going to have **links** between the nodes.

And the nodes will take on random variables that can be discrete or real, but we'll do discrete today.

We've got a set of **directed edges** that says this causes or influences that. It's an **acyclic graph** (a DAG—Directed Acyclic Graph).

And what we're going to have is some probability distribution at the top, a probability of something, and for each of the nodes here, we'll have the probability conditional on each of the parents, right? So parents, children, probability graph. 

So, if this is some $X_i$, then we have the probability of $X_i$ given each of the two parents:

$$P(X_i | \text{parents of } X_i)$$

And we're going to do discrete ones—we'll get **conditional probability tables** (CPTs).

### The Classic Example: Burglar-Earthquake

And I'm going to do this—how many people have seen this example, the burglar-earthquake?

Yeah, like, half the people have, half haven't, okay. Let's just quickly go through it and think about how to specify it. If I've got something that says I've got a burglary and an earthquake—we're in California now—we have an alarm going off, we have John calling, we have Mary calling.

To give a full probability distribution over these 5 variables, I could list all possible combinations of them. Each is binary, so that's $2^5$ possible combinations, minus 1 (because they have to sum to 1).

But in fact, all I need to do to fully specify the distribution is say:

**What is the probability of burglary?**

$$P(B) = 0.001$$

One in a thousand in the nice Stanford neighborhood we're talking about here. 

**What's the probability of an earthquake?**

$$P(E) = 0.002$$

This is 2 in 1,000 in the same nice Stanford neighborhood.

What's the probability of no burglary?

$$P(\neg B) = 0.999$$

Right? They have to sum to 1, so I don't specify things that are already specified.

**Probability of the alarm:** you can now see that there are four possible conditions, right? The burglary and the earthquake can either be true-true, true-false, false-true, false-false. Each of those has a probability:

$$P(A | B, E)$$

So that fully specifies all possible probabilities of the parents gives this.

Then I have the **probability of John calling**, which only directly depends upon the probability of the alarm:

$$P(J | A)$$

Which is either true or false.

So, it looks sort of like a Markov model, and it has a Markov property.

### The Markov Property and Conditional Independence

Right? And the Markov property here is that the probability of John given the alarm is going to be equal to the probability of John, given the alarm and the burglary:

$$P(J | A) = P(J | A, B)$$

We're going to say that the probability of John is **conditionally independent of the burglary, given the alarm**.

Right? Which people often write as:

$$J \perp B \mid A$$

The probability of John is conditionally independent of the burglary, given the alarm. 

Make sense? So, once I know the alarm went off, I don't learn anything more if I learn there was a burglary, or if I learned there was an earthquake.

**Does the probability of a burglary increase the probability of John calling?**

Sure, right? Burglary happens—I heard there was a burglary—causes an alarm, the alarm causes John to call.

**Does the probability of earthquake going off increase the probability of John calling?**

Sure!

**Does the probability of John calling increase the probability of a burglary?**

It's sort of weird. These things are statistical—they're capturing conditional independence. In the ideal world, they would be causal. But there's nothing in the math that says they have to be causal.

It turns out empirically, if you make them causal, they're more elegant, they're simpler, they're shorter. So, your fantasy, your hope is they're causal, but I haven't promised anything in the semantics that says they're causal.

### Causality vs. Diagnosis

In a causal one, the causes flow down. The reverse direction is called **diagnosis**.

And so it IS the case that if John calls, it's more likely there was a burglary.

Right? And if you think about how things go, causality and probability flows down, so the probability of John calling given the alarm—and I can have the probability of alarm given the burglary.

If I want to go the other direction, how do I reverse causality?

This is the probability of alarm given burglary and earthquake. How do I reverse a causality?

**Bayes' rule**, right?

These are going one direction. If you want to flip the other direction, it's just Bayes' rule.

Question?

Sorry, a little bit louder, there's air conditioning.

**Question:** Is John's calling conditionally independent of burglary given the alarm, by assumption in this model?

**Yes.** In the real world, probably false, but the approximation assumed here is that John calling is conditionally independent of the burglary, given the alarm.

Right? And again:

$$P(J | A) = P(J | A, B)$$

So that is what the diagram means. What the diagram means is that this is a conditional independence. This is a sufficient representation to capture the entire distribution.

So we've got something that looks like $2^5$ possible labels here, and we can tell the probability of all of them just given this diagram.

So, **conditional independence reduces the number of parameters in the model**, right? All of these probabilities are parameters in the model.

And the conditional independence means I don't have to tell you every possible thing—I've reduced the model size down, the number of parameters.

Cool.

### Explaining Away

The interesting, weird thing is to think about what these semantics tell you and don't tell you.

So, assume there was a burglary and an alarm. Let's try and ask: is the probability of an earthquake given a burglary and the alarm equal to the probability of an earthquake given the alarm?

$$P(E | B, A) \stackrel{?}{=} P(E | A)$$

I got a yes? Say that intuitively.

If the alarm goes off, how likely is an earthquake?

In some sense, one way to view it is either there's a burglary or an earthquake, or both. But what's the probability of both a burglary and earthquake in this model?

$$P(B, E) = P(B) \cdot P(E) = 0.001 \times 0.002 = 0.000002$$

They are assumed in this model to be independent. Now, I can tell you this is wrong. There should be an arrow from earthquake to burglary. When there are earthquakes, there are more burglaries. And burglaries don't cause earthquakes, but earthquakes do cause burglaries. The model's wrong, but okay, work with me.

Russell and Norvig have a nice model here.

So, under this model, burglary and earthquake are independent, and they each have a low probability.

If I see an alarm, what does that tell me about the burglary and the earthquake?

Probably one of them went off.

Did probably two of them go off?

**Unlikely.** Could be, but it's a thousand-fold less likely, ballpark.

So now, if I ask, what's the probability of an earthquake given alarm?

Oh, it's, you know, two-thirds or whatever. I could do the Bayesian rule and find it out.

What about an earthquake given a burglary and an alarm?

If I know there was a burglary, as well as the alarm, **earthquakes are less likely**.

The technical term is **the burglary "explains away" the alarm**.

Right? It's a plausible cause for the alarm, and therefore, given I have one plausible cause, the other one is less likely.

I want to note, this is all my talking intuitively. At the end of the day, **all you have are probabilities**. If you're going down the causal direction, you just multiply probabilities and marginalize. If you're going reverse, you use Bayes' rule and flip everything.

So I'm not saying anything that you can't trivially find by just using Bayes' rule and marginalizing, summing over things. It's trivial math.

But it is nice to get the intuition as to what's going on, because it is the case that in some sense, the burglary and the earthquake can both cause the alarm. The alarm causes John and Mary to call.

### D-Separation

By the way, the other way around, if I have John—let's do it, write it out.

If I ask, is the probability of John calling given the alarm equal to the probability of John calling given the alarm and Mary calling?

$$P(J | A) \stackrel{?}{=} P(J | A, M)$$

Does Mary calling tell me anything more about John calling?

**No.**

These two things are different. This direction, they really are conditionally independent.

Right? So the alarm **d-separates** (a fancy word, which we'll explain on the side)—d-separates John from the rest of the world. Once you know the alarm went off, the burglary, the earthquake, Mary tells you nothing more about John.

There's no active path there.

And so if the alarm went off, it's going to cause John to call with some probability, it's going to cause Mary to call with some different probability. But if I know the alarm, Mary calling says nothing about John. 

If I don't know the alarm, does Mary give me information about John?

**Yeah**, because if Mary called, I will use Bayes' rule and find the probability of the alarm, and the alarm will then tell me something about John.

Yeah.

So, note there's this sort of **weird information flow**. Alarm blocks information from flowing from John to Mary. Alarm blocks information from burglary to John. But alarm does not block information flowing from the burglary to the earthquake.

Now, the blocking word is called **d-separates**. These things that link are called **active paths**.

The alarm d-separates John and Mary. So I'm saying that the semantics, the meaning of this picture, right, which is a formal specification of conditional independence, says that John is conditionally independent from Mary given the alarm, right? I would write that:

$$J \perp M \mid A$$

John is conditionally independent from Mary calling, given the alarm.

If Mary calls and we did not know the alarm, now we learn something about the alarm, which tells us something about John.

If Mary calls and we already knew the alarm went off, Mary calling tells me nothing more. I already knew the alarm went off! Probability of 1. You just told me the alarm went off. If I know the alarm went off, Mary calling tells me nothing about the alarm—I know it already went off.

Yeah. 

### The Local Markov Assumption

So let me just sort of quickly walk through the pieces here, and I want to sort of highlight two things. The same Markov property of Markov models—the definition of a belief net, a Bayesian net like this, is a **local Markov assumption**, which says that a variable $X$ is independent of its **non-descendants**, given its parents.

So, your parents—$A$'s parents are $B$ and $E$. Its descendants are $J$ and $M$. The variable $X$ is independent of its non-descendants, given its parents.

That's the assumption, and I'm not going to walk you through it, but if you look at it, you can break this up, and you can say the **full joint distribution** over everything is—you could write it out, I will walk you through it. It's:

$$P(E, B, A, J, M) = P(E) \cdot P(B | E) \cdot P(A | B, E) \cdot P(J | A, B, E) \cdot P(M | A, B, E, J)$$

This is just a definition—it's always true by the chain rule of probability.

And then I can apply the conditional independence assumptions and say, well, what's the probability of the burglary given earthquake? It's just the probability of the burglary—they're independent:

$$P(B | E) = P(B)$$

What's the probability of $A$ given $B$ and $E$? That stays as is:

$$P(A | B, E)$$

What's the probability of John calling given the alarm, burglary and earthquake? It's just John given the alarm:

$$P(J | A, B, E) = P(J | A)$$

That's the local Markov assumption.

And so you can take this full probability distribution, and you can always separate it out into the pieces, which are the ones that show up in the conditional probability tables. 

Yep.

**Question:** Do you have to do it in a sorted manner?

You've got to do it in a topologically sorted manner, as long as it's a probability of a descendant given a parent. You can write the whole thing out.

If you ever get a problem that says, give me the probability of a parent given a descendant, then you will use Bayes Rule and flip it.

### Learning the Probabilities

**Question:** How do we get the probabilities?

In the simple world, you assume everything's measurable, and you just count them up. Again, if all the variables are measurable, if there's no hidden variables in this, then you just count up.

I had a thousand days, and I got one burglary. The MLE is one in a thousand:

$$P(B) = \frac{1}{1000}$$

So, if everything is observable, which is how these are usually used, it's trivial. If they're not observable, it's ugly, but you're going to have to use an EM method to find the non-observable ones.

Bummer. 

### Building the Structure

And the other piece is, to wrap up—there are slides, but I'm not going to cover them—that usually people just write these down from theory. They interview experts and say, "Hey, how do things relate?"

If you don't write them down by experts, then you have to do a search to find them. It's a very big search space of possible graphs, but you can do randomized search algorithms and find ones that are more likely.

Most people just build them. They do not need to be causal, but if they're causal, they're more elegant.

## Summary and Takeaways

So the takeaways from this, and I'm coming up to time, and I'm going to try and end, is to say that:

- **Conditional independence** has a bunch of math properties, which I won't test you on. But you should be able to go through and think about which things are conditionally independent of what.

- I talked about **d-separation**, where if you know one thing, other things don't affect it. That's basically a Markov property.

- I talked about building them—there's a bunch of things.

**Belief nets give you an elegant specification of a joint probability distribution, making some assumptions about what's conditionally independent.**

That includes a hidden Markov model is a form of a belief net.

And when we get to reinforcement learning, we'll be able to add in **action nodes**, where you actually do something which changes the world, and it's used a lot in the RL world.

If you can get a **causal network**, it's going to be more sparse in general, and it works nicer, and it's more interpretable.

In general, you really want to know what the structure is. If you have to search for the structure—where all the nodes are and how they link—that's a terrible search space. I've done it. But you never quite know what you're getting.

Cool! 

## Closing Remarks

We are done with this for now.

You should have all heard from your TAs and have feedback on your proposals. If you don't, post on Ed and I will go bang on them.

Yeah, hang on.

---

**End of Lecture**

---

# CIS 5200: Machine Learning - Lecture 21: Generative Modeling

## Introduction

Hi, Lyle.

There's no balance. Okay, um.

Assuming we're all good, uh, I think we can start. So, my name's Alok, I'm one of your TAs, um, I'm obviously not Lyle, but… Today, um, he asked me to step in to talk to you guys about generative modeling.

Just a short disclaimer: I only have an hour and a half, which means I can't cover these things in their full, extensive detail, but if you're interested, there's a full course offered every fall, um, I think it's called ESE or, like, CIS 645. Um… But yeah, today, the idea is… yeah, I will wow.

He told me to do it 5 after, but… I can do now. Um… Yeah, so the whole point is, I guess, I mean, if this class is to sort of give you guys a taste and a flavor of, you know, where machine learning is today.

And by doing this, um… I mean, sort of understanding the principles behind generative modeling, because they aren't too far away from what exactly you've already learned in class, with things like parameter estimation, maximum likelihood, posteriors, et cetera, et cetera. And, you know, a hint of deep learning.

So… Yeah, my goal today will just be twofold. So, it's not for you to sort of leave here with a… let's say… ready to go out in the wild, harnessing, you know, 4 nodes of GPUs, but rather to understand the principles of generative modeling.

## Lecture Outline

So what I mean by this… We'll start by going over just a brief, sort of, primer on what exactly generative modeling is, and, like, write down some objectives.

So, I'll call this, like, a high-level overview. Then, we'll discuss, like, maybe the most naive or obvious thing you can do, and how that's manifested into a very rich line of work, um, known as **Variational Autoencoders (VAE)**.

And third, if we have the time, I'll sort of explain the primitives of what we know today as **Diffusion Models** or **DDPM**. Um, this sort of line of work really is, uh… it's developing by the week. It, you know, we've seen many, many other models since Diffusion that are, you know, variants, but share the same underlying principles, so I think… It's maybe less important to learn, like, the actual equations of the models themselves, and maybe more important to understand the underlying intuition behind the models and why they work. And once you do that, then you'll never forget. You know, the exact sort of model formulations and everything else will seem sort of manageable, so…

Um, again, like, I'm not covering these in exhaustive detail, but hopefully the idea is that you get a taste of what exactly these things are, and how they work.

Okay, um, it's 48… So, I guess I have to take attendance?

Um… I believe it was close?

Now I just start this. How long does he typically leave this up?

I think so. Okay, several minutes. Do I just, like, keep going while this is up?

Okay. I mean, I'm not gonna use the computer screen, so… That's… it's fine. Um… Oh, yep, sorry, sorry.

Oh, I also have to share my Zoom screen, yes.

Probably… helpful. Cool. Alright, hopefully everyone's good now. They can see.

It's good, though.

Am I good? Hello?

Yes, all good. You can talk away.

Okay, cool, cool, cool. Um, sweet.

## Part 1: Primer on Generative Modeling

### What is Generative Modeling?

So, the first thing that I promised you is a primer, and… This just means starting at the very basics, right? So, first off, what is, like, generative modeling? Like, very, very precisely. And we see, like, ChatGPT, we see diffusion, we see, like, you know, DALL-E, etc, etc. Like, what does this actually mean?

Well, let's start with, sort of, the task at hand, which is, we have some data, $X$.

And what we're trying to do is **learn the distribution of this data**. So… Learn $p_{\text{data}}(X)$.

Okay? And… What's the most naive way that I can, sort of, learn this probability distribution?

I think I might have already said it, but… Kind of something.

Yeah, you can sample from it, sure, but I'm just talking at, like, a high level, like, I can do no better than this.

### Maximum Likelihood Estimation (MLE)

Okay, well, what if I start writing something that looks to be effective?

$$\arg\max_{\theta} \mathbb{E}_{X \sim p_{\text{data}}} [\log p_{\theta}(X)]$$

I choose some parameters $\theta$ for a distribution of my choice. And what I'd like to do is look at all the data that I have $X$ under $p_{\text{data}}$. And what would I like to do with it?

Well, I would like to… maximize my probability of observing said data.

What is this sort of called? Sort of. It has, like, a name, like, this method for obtaining an estimate of a probability distribution.

**Maximum Likelihood Estimation**, very good. Maximum likelihood estimation, right? And the canonical form for this involves, like, some expectation over the log probability of observing my data $X$, right? 

$$\arg\max_{\theta} \mathbb{E}_{X \sim p_{\text{data}}} [\log p_{\theta}(X)]$$

Um, you can find this in the slides, I'm not gonna go ahead deriving it for you, but… Ideally, like, the way this usually goes is I have, like, say, $N$ $X$s from a training distribution, I assume that they're all IID, which means I can separate this thing out into a product. And then I take the log of the products, which turns into a sum, and then I conveniently divide the sum by $N$ and hide it behind the expectation.

This is, like, usually how this looks. And then to get the argmax, I just set the derivative equal to 0 and solve, right?

Does everyone, like, understand and is okay with this approach?

Yeah. Why are we reviewing this? Like, this is the best thing you could possibly do, right? This is, like, optimal.

So, you're trying to… the $\theta$ you get is maximizing the probability of you observing $X$ given some parameters that you're trying to figure out, right?

And so… Yeah, like, the idea, you know, coin flipping is usually what people use. You can choose a Bernoulli, you compute the log likelihood, you take the derivative, you set it equal to 0, and then you solve for your parameters $\theta$.

Okay, cool.

### Why MLE Doesn't Work at Scale

Um, why does this not work at scale?

So, the spoiler is, like, we can't use this.

And the reason we can't use this is… It's not too complex, but… Just think about it from, like, the point of view that I have a really large dataset of images. What would that entail?

So why can't we use maximum likelihood estimation when the size of my dataset is really large? Or the ambient state space is also really large?

You could, you could, but still, I'm thinking on the orders of, like, millions or billions.

Think, like, if I had a dataset of images, each image has 3 channels, and each of those channels has its own value, right?

So, if I'm estimating, like, a high-resolution image, 4K by 4K by 3, then I'm solving a linear system on the order of, like, 16,000 squared, right?

And so this basically just gets really, really large. And so, taking the derivative of that and setting it equal to zero is, like, intractable. And if I don't choose a good prior distribution, then I'm, like, completely lost.

### The Solution: Neural Networks

So, the idea behind generative modeling, at its core, is how can I approximate this $p_{\text{data}}$ with a probability distribution $p_{\theta}$, so with a family that I choose.

Um, hint, hint, wink, wink. We use deep learning a lot, and it turns out that these parameters, $\theta$s, are just the weights of my deep learning network.

So, I approximate the probability distribution, or the maximum likelihood estimate, with a neural network, and that is how I, like, subsume the whole scaling thing.

Okay. Does this make sense, like, as a high-level heuristic?

Okay, so… MLE is intractable. What I do is use a neural network instead. Do some clever sort of math tricks to work around this being completely intractable. And then, I just want to optimize it, right? I optimize it.

So it becomes practical because instead of solving it in closed form all at once, which makes all of these things… they all need to know the form that this takes, but when it gets really big, solving for it exactly is impossible.

How does having a neural network help us? Because we iteratively approximate the $\theta$s instead of trying to solve for them directly, using, like, gradient descent.

Okay. Is everyone clear? Like… Does that make sense?

So rather than finding the solution at once, you've got to go with something iterative. Yeah, we could use a neural network to approximate, right? It's the same, like, intuition for why you can't solve a linear system in high enough dimensions if you just try to invert it exactly. It's just cheaper to approximate it.

Um, okay.

### The Two-Stage Process

So, what's the next thing that I can do instead of doing this?

So, first off. The whole, like, generative modeling thing proceeds in two stages.

**Step 1** is to estimate $p_{\text{data}}$. Estimate data and compute this sort of approximate probability distribution.

And **Step 2** is to actually sample from it.

And this is not something you necessarily do in, like, supervised learning, but it's important, because once we choose a probability distribution, it's not going to be useful to us at all unless we can sample from it.

And so the idea, if you think about, like, a generative model trained on, you know, MNIST, right, which is your classic deep learning dataset.

What happens is I have… the MNIST dataset of images, and I encode the MNIST set of images into some, you know, lower dimensional space. Then, in this lower dimensional space, I know how to sample from the distribution. And then I take that, you know, sample that I did in the lower dimensional space, and I move it back over into the space of images. 

And that is, like, the… It's called an **autoencoder schema**, but that is, like, the main idea here. 

So… When I say sample, I mean, we're trying to estimate the parameters $\theta$ that'll bring us from some space $Z$ over into the space $X$.

And… The way we do this is by learning a map that transports you over to $X$ and back to $Z$.

Okay, this is all, like, high-level vibes, so I'm gonna formalize this with math.

Um… does anyone have any questions about the high-level, like, roadmap of the lecture?

Yes. When you say estimate $\theta$, are you doing that as what is the set of functions that will convert it?

Yeah, the $\theta$s are going to dictate how this map looks, right? Yeah, okay. Do you guys know which mapping exactly? Is it for the $Z$ that you're talking about?

So $\theta$, in this case, there's two sets of parameters, but I'll go over this, like, in excruciating detail. Give me, like, 5 minutes. Like, once you convert it, you want to see how effective...

Yeah, exactly. Okay.

## Part 2: Variational Autoencoders (VAE)

### Deriving the ELBO

Um, okay, so… Maximum likelihood estimation, so what's, like, the quickest trick we can use to subvert, like, this being intractable?

Well, we have the log likelihood, right, of $p_{\theta}$, and I said I wanted to introduce a variable $Z$.

How do I compute or sample from, how do I write this distribution in terms of $Z$ and $X$?

It's like… So… I can write $p_{\theta}(X)$.

And this could be equal to, like, the integral, or the sum, of this joint distribution with respect to $Z$, right? 

$$p_{\theta}(X) = \sum_{Z} p_{\theta}(X, Z)$$

It's just, like, the law of total probability.

Okay, cool. Thanks. What is this?

This is just, like, this is probability theory. $p(X, Z)$ is the joint distribution over $X$ and $Z$.

Okay. Cool. So, I have an integral. I'll write it as a sum, maybe. It'll be easier, um, to sort of conceptualize. Great. And I take the log likelihood of this thing, right?

So… really, I have:

$$\log p_{\theta}(X) = \log \sum_{Z} p_{\theta}(X, Z)$$

Right?

Now, I'm gonna play one more trick, which is, I'm going to introduce a probability distribution over $Z$.

Right? And… Specifically, I'm not just interested in how $Z$s are distributed, but I'm interested in how $Z$s are distributed with respect to $X$, right? Given a sample $X$, I'd like to understand what the $Z$s will be.

Okay? And so, the easy way to do this is just to say a probability distribution exists, we'll get more concrete about what this means.

$$\log p_{\theta}(X) = \log \sum_{Z} \frac{q(Z|X)}{q(Z|X)} p_{\theta}(X, Z)$$

$q(Z|X)$ over $q(Z|X)$.

And all I've done here is just multiply by 1 inside the sum.

Are we happy with this? Okay, cool.

### Jensen's Inequality

So, here is, like, the sort of magic. And… We'll just… okay, so…

What I'd like to do is lower bound this quantity. If I can lower bound this quantity and maximize it, then I've effectively maximized my objective.

So how can I do this? Well, I'm going to try to pull the sum out of the log. There's a convenient inequality we can use here. Um, you know that the log function is concave.

So it looks like this, right? Which means if I take any two points on the logarithm and I draw a line between them, it means the function must be above that line at all times.

Anyone know what this is called? It's usually shown in the case of convexity.

Yeah, it's called **Jensen's inequality**. And there's a convenient form that extends this to beyond just two variables, but it basically says that if I take the sum of weighted… So, this would be like your equation for a line.

I take this over $i$. Then this sum, so this line, must be below the function for a concave $f$, if $f$ is concave.

And it looks like this:

$$f\left(\sum_{i} \alpha_i x_i\right) \geq \sum_{i} \alpha_i f(x_i)$$

So, sum over all $i$s, $\alpha_i x_i$, and the only constraint here is that the $\alpha_i$s must sum to 1, right?

Okay. This is, like, called the definition. So, what am I going to do here? Well, what else sums to one that we know of?

Probability distributions, right? So, I can easily just replace these sums with expectations.

Which is literally what's going on over here. And what this allows me to do is pull out the sum.

And now, I can rewrite, so… I can rewrite this. I have to pull out one of these $q(Z)$s as well.

$$\log p_{\theta}(X) \geq \mathbb{E}_{Z \sim q(Z|X)} \left[\log \frac{p_{\theta}(X, Z)}{q(Z|X)}\right]$$

$q(Z|X)$, log of $Z$… Sorry, $q(Z)$ given $X$… Over $q$… Or $p(X, Z)$, rather, $p(X, Z)$ over $q(Z|X)$.

Great. And hopefully one more, sort of, like, I can do a bunch of algebra, and what this allows me to do is basically write it like this, or I have the expectation of the log probability… oh, okay, wait, there's one more step.

I can rewrite the joint probability $p(X, Z)$ using Bayes' rule, which is just $p(X|Z) \times p(Z)$.

$$p(X, Z) = p(X|Z) p(Z)$$

Okay. Now, I can just… do some algebra, I can break up this sum, and what it'll look like is basically this:

$$\log p_{\theta}(X) \geq \mathbb{E}_{Z \sim q(Z|X)} [\log p_{\theta}(X|Z)] - \mathbb{E}_{Z \sim q(Z|X)} \left[\log \frac{q(Z|X)}{p(Z)}\right]$$

The $Z$s are drawn from this $q$ because we define it, right? And I'm interested in the log likelihood, which is this, of $p(X|Z)$.

And I subtract out, because… I want to separate the terms.

This $\log \frac{q(Z|X)}{p(Z)}$.

Does anyone know what this formula is? If I just pretend like this expectation is not here, um, what is this, uh, what is this quantity?

Entropy's close, very close. KL divergence, very good. So, I can write this as:

$$\text{KL}(q(Z|X) \| p(Z))$$

Between $q(Z|X)$ and $p(Z)$.

### The ELBO and Its Components

Okay, so here is, like, hopefully going to be the first aha moment of the lecture.

Which is… Okay.

I'm going to now… so this is called the **ELBO**.

ELBO, it stands for **Evidence Lower BOund**. And what it does is it places a lower bound on the log likelihood.

And so what this allows us to do is if we maximize this ELBO, which may be more tractable than maximizing the log likelihood as a whole, then effectively we've accomplished the same task. We've still learned some sort of approximation to the real probability distribution.

Okay. Now, I'm going to write some Greek letters below these probability distributions.

So, for $p$, I'll use $\theta$, and for $q$, I'll use $\phi$.

$$\text{ELBO} = \mathbb{E}_{Z \sim q_{\phi}(Z|X)} [\log p_{\theta}(X|Z)] - \text{KL}(q_{\phi}(Z|X) \| p(Z))$$

And this should just signify to you that I'm parameterizing these distributions by neural networks.

Okay, so… What does this exactly mean? What's going on?

Well, to sort of assign some, like, notation to all of this, or to give it some meaning...

Uh, we call $Z$ a **latent variable**, so if you've heard of, like, a latent variable before, it's like a hidden variable that sort of controls what our probability distribution looks like.

And this $p(Z)$ is a **prior** over our latent distribution. We oftentimes choose this to be zero-mean unit-variance Gaussian, because Gaussians are easy to work with.

Then, I have two of these probability distributions. So, $p_{\theta}$, and I have $q_{\phi}$.

$p_{\theta}$ takes us from $Z$ to $X$, and we call this, um, we call this a **decoder**.

And $q_{\phi}$ takes us from $X$ to $Z$, and we call that an **encoder**.

And so, what's happening here is this first term is going to be maximizing the log probability of our observation $X$ given the latent $Z$. And so, we call this term, like, **reconstruction**, because… we want this to be big, right? We would like the probability to be high under this estimate.

Yup. And we call this term, like, **regularization**. Again, $\phi$ here is... $\phi$? Yeah. $\phi$ is just the parameters of the neural network that we'll be using in $q$.

Right, and then we call this term, like, the regularization term. What this allows us to do is keep $q$ very close to $p(Z)$.

So, $p(Z)$ is zero mean Gaussian, and we'd like $q$ also to look like a zero-mean Gaussian.

### The Intuition Behind Regularization

And the intuition for this is all gonna come back to this idea of sampling.

Which is, once I learn this reconstruction term, um… and regularization… Once I have learned this reconstruction term and assuming the regularization term looks good, what I can do is choose an arbitrary latent $Z$, which I know how to sample from, because it's a Gaussian.

And then, I can feed it through my decoder, $q$, or $p(X|Z)$, right?

And that'll transport me all the way over back into $X$ space.

And so, if you want to, like, assign a sort of visual to it, or an example, think of, like, an image, right? Think of MNIST, like, handwritten digits. I can train an autoencoder that sends those handwritten digits to a zero-mean Gaussian latent space.

Then I can sample from that zero-mean unit Gaussian, and ideally, I'll get new digits.

That's, like, the whole power of the generative model. It's exactly what's going on with, I mean, literally all of your favorite models, they should probably...

Um… But yeah, does this make sense to everyone?

Yeah. Can you explain how this… converts into the image of...?

Yeah, okay. So… the image, right? We'd like to sample… we'd like to sample… Right? $Z$ needs to be something we know how to sample from.

So, let's call it a zero-mean Gaussian. Um, we would like $q(Z|X)$ to be close to the zero mean Gaussian, because that's how we train this encoder, right? Um, and so, the idea is that if it's close to a zero-mean Gaussian, then we know how to sample from it faithfully. If we don't know how to sample from it faithfully, then we could replace this $q(Z)$ with, like, a $p(Z)$, right?

And that's practically what we'd be doing in test time, just using $p(Z)$ instead of $q(Z|X)$. Because we know how to sample from $p$, but not $q$ directly.

Okay, yeah. Why is it that we want the distribution of $q(Z|X)$ to be the same as $p(Z)$?

What I'm trying to reconstruct, my goal is just to get the next part, right? So, how do you sample from $q$?

$q(Z|X)$. But my doubt is like, why do the distributions have to be the same?

We want them to be similar because we don't know how to sample from $q$, we know how to sample from $p(Z)$.

$p(Z)$ is a prior, it's a Gaussian.

Okay. Okay.

### Training a VAE

Cool. So, uh, how do we actually train this?

Okay. I've basically given you a loss. How do we train this? Anytime I give you a loss, and you want to use a neural network...

Gradient descent? Very good, very good. Okay, so, effectively, the, like, meta-algorithm for VAE is pretty much: I have some points, I send them through the encoder to the latent space, I reconstruct them through the decoder.

And then I compute this ELBO loss, backpropagate through it.

And then, ideally, I just learn some sort of structure that, uh, yeah, I can then sample from and, like, life is amazing.

### The Reparameterization Trick

There's a slight problem. Uh, I've sort of been, like... Oh. I've hidden something intentionally.

Uh, which is… does anyone… does anyone see it?

All the reasons. Am I good to erase this?

Sweet.

Okay, what's the problem? Does anyone know? You want to compute the gradients, which means you have to get a gradient of both of these terms with respect to both $\phi$ and $\theta$.

Um… Does anyone know how to take the derivative of this?

$$\nabla_{\phi, \theta} \left[ \mathbb{E}_{Z \sim q_{\phi}(Z|X)} [\log p_{\theta}(X|Z)] - \text{KL}(q_{\phi}(Z|X) \| p(Z)) \right]$$

Okay, you probably, like, there's a formula, right? If I just model things as Gaussians, uh, I'm… I'm fine. This has a closed form, I can take the gradient.

Uh, okay, how about this? Right? Like, how do I take the derivative of this term?

$$\nabla_{\phi} \mathbb{E}_{Z \sim q_{\phi}(Z|X)} [\log p_{\theta}(X|Z)]$$

This is hard, right? Because… The $Z$s are what, like... they're parameterized by $\phi$, I draw $Z$ from $q$, and then I feed it into $p_{\theta}$, right?

This could be an integral, this could be a sum. Um, it's basically intractable. I can't just drag the derivative in and, like, call it a day, because these things, like the $Z$s, rely on $\phi$.

Okay. So, there's a clever trick we can use to skirt this.

Um… Yeah, there's a clever trick we can use to get around this, so if I let $p_{\phi}$ and $p_{\theta}$ equal... so I'm just gonna let both the encoder and decoder just be Gaussians.

And I can learn their means and variances. So, this is:

$$q_{\phi}(Z|X) = \mathcal{N}(Z; \mu_{\phi}(X), \sigma_{\phi}^2(X))$$

$$p_{\theta}(X|Z) = \mathcal{N}(X; \mu_{\theta}(Z), \sigma_{\theta}^2(Z))$$

You know, mean and variance for the encoder… Okay, and this… oh, sorry, this $\theta$.

If you're in my recitation, you know I mess up a lot, so just, uh, correct me if I'm wrong at any point.

Uh, when I say something, if it sounds like utter nonsense, because, well, chances are, it probably is.

Okay. To do this… The trick that I can use…

So… I've written this suggestively, right? Suggestively in the sense that if I call $p(Z)$ a Gaussian, and I call $q(Z|X)$ also a Gaussian, then what does this mean about, like, each $Z$ that I draw?

Like, it's also, like, Gaussian, right? So, what I can do instead is instead of sampling from $q$ of, you know, $Z$ given $X$, I can write $Z$ as just the mean of this… plus, you know, this, uh…

I can write it as, like, $\sigma$ squared times... Why do I use…

Yeah, plus $\sigma$ times $\epsilon$, where $\epsilon$ is just a zero-mean unit Gaussian.

$$Z = \mu_{\phi}(X) + \sigma_{\phi}(X) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

So, mathematically, sampling from this distribution and sampling from $\epsilon$ and computing this are the exact same thing, because Gaussians are, like, invariant under linear combinations.

And now, I can write this term as instead of taking the expectation over $Z$, I can take the expectation over $\epsilon$.

$$\mathbb{E}_{Z \sim q_{\phi}(Z|X)} [\log p_{\theta}(X|Z)] = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} [\log p_{\theta}(X|\mu_{\phi}(X) + \sigma_{\phi}(X) \cdot \epsilon)]$$

Right? Now, there's no reliance on $\phi$… there's no reliance on the sampling of $Z$ in this expectation in a way that blocks gradients.

And so this is how I practically train a VAE, right? I… first, I compute, you know, the encoder, the decoder. That's easy enough to do. I compute the reconstruction loss, I feed all this through. Whenever I want to backpropagate, I use this $\epsilon$, this, like, zero mean noise, instead of $Z$s drawn directly from $\phi$ in a way that I can't differentiate through.

And everything works out fine, because I can backprop through these things, but I can't backprop if I just have a random sample.

Okay, this is like a subtlety, but it's, like, a very good interview question. Like, people ask about reparameterization tricks, people use it all over the place.

Um, and I think also, like, the ELBO derivation is very, like, a good interview question as well, I've been asked this.

Um, it takes many different forms, by the way, so… There's, like, 7 different forms of the ELBO, um, I don't know, as long as you can get to, like, I would say the second form that we derived here, you could just massage this into, like, various different forms that, uh, each sort of highlight different aspects of what you're doing.

Okay, so… This is the VAE. Uh, does this, like, make sense at a high level?

Yes, no, I see, like, nodding heads. Okay, cool.

### Does VAE Actually Work?

Alright, so, uh… Does the VAE actually work?

Like, do you guys use this in practice? Have you seen this in practice?

I mean, the chances are, like, in 2024, like, no.

Um, there are many, sort of, different reasons why this is the case.

Uh, first is that, you know, depending on how you tune this exact thing, you might have this reconstruction term dominate versus this regularization term dominate, um, but I think, like, a nice… sort of… like, broad strokes, like, high-level answer, is that:

Concern number one, okay, is: **Is one $Z$ sufficient to learn $p(X)$?** Right? What if the distribution of $p(X)$ is so complicated that I cannot just draw a map from, like, a single variable Gaussian, to, like, my data distribution? 

Uh, this is often the case with real-world data. The data is not nice at all, it looks terrible, um… So what's, like, a… what's a fix that I can do?

Instead of using one latent, what if I use, like… multiple, right?

### Hierarchical VAE (HVAE)

Um, so, this is a variance of the VAE called the **HVAE** (Hierarchical VAE).

You've probably not heard of it before, but it provides very good intuition for, like, what we're going to discuss for the rest of the lecture, so I'm going to mention it anyways.

Um, HVAE. And what this does is it uses, basically, $L$ latents.

You don't have to write any of this down.

Um, but basically, each latent has its own, sort of, prior, and then I can draw each of these latents with respect to… the previous one, right, and… 

$$Z_L \sim p(Z_L)$$
$$Z_{\ell} \sim p(Z_{\ell}|Z_{\ell+1}), \quad \ell = L-1, \ldots, 1$$
$$X \sim p_{\theta}(X|Z_1)$$

Yes, $Z_L$ can just be drawn from this zero-mean Gaussian, which is simple.

Okay, and intuitively, what's going on is… okay, we start over in the space of $X$s. Uh, I go to, like, $Z_1$. I go to $Z_2$. I go all the way over to $Z_L$. 

And… gradually, like, as I go from latent to latent, I'm learning more structure about my data as I gradually add noise at each level, and then to train it, I just do the ELBO, but in parallel. 

This latent and this latent is its own ELBO, this and this is its own ELBO, this and this is its own ELBO.

I won't, like, write out the math, but basically, these terms telescope quite nicely, and you can get, like, a nice compact objective to write down.

A good exercise, um, the math is not that important.

Uh, but what is important is the idea of, like, inserting multiple latents, and trying to learn, like, sequential structure about my data. 

Uh, it turns out in practice, whenever you, like, train these models, they tend to learn components or structure about the data in order of its importance or variance.

So, high variance components tend to get learned first. So, like, edges in an image, for example, or, like, you know, big, like, structures, like faces and, like, bodies.

Um, and then more fine-grained components are learned later in the hierarchy.

And this is because of how the variance propagates as well.

This is, again, like intuition, you can sort of do the math and see why this is the case, but… Uh, yeah.

I'm gonna… like, this is going to basically conclude our discussion of VAE.

Does this, like… at an intuitive level, make sense?

So, with the VAE, with HVAE, you can just rewrite the ELBO and basically do all the same stuff.

But, uh, keep the idea of using multiple latents, and gradually moving from latent to latent in your head, because, uh, it comes up later.

## Part 3: Score Matching and Diffusion Models

So, the next thing I want to discuss is, uh, orthogonal to sort of… this idea of the ELBO, or… MLE… Maximum likelihood estimation. Well, actually, it has some connection… But it's just another method.

So, the… let me erase this.

So, I'm gonna write this… It's called **Score Matching**.

### The Problem with MLE

And… let's see… So, again, suppose I'm in the maximum likelihood estimation regime and, like, the objective that I care about is:

$$\max_{\theta} \mathbb{E}_{X \sim p_{\text{data}}} [\log p_{\theta}(X)]$$

Max, you know, $\theta$. Uh… Okay, um… 

Now, I want you, just for the sake of assumption and illustration, to assume that, uh… $p_{\theta}$ takes the form of what's called, like, an **exponential family**.

Again, you don't have to know what this means at all, I'm just trying to illustrate a point here.

Which is that if this, like, distribution looks like this, where I have, um… 

$$p_{\theta}(X) = \frac{\exp(f_{\theta}(X))}{Z(\theta)}$$

where $f_{\theta}(X)$ is some function of $X$. And this has to sum to 1, so what I do is I normalize by… this thing down here, which we'll call the **partition function**:

$$Z(\theta) = \int \exp(f_{\theta}(X')) dX'$$

Okay. So, lots of probability distributions sort of fit this regime.

Um, there's, like, very, very rich statistics literature discussing all sorts of things you can do with these sorts of parametric families.

But the thing I want to draw attention to is this denominator.

So, it's the integral of the exponential of $f_{\theta}(X')$ over all of $X'$. Right? This is what the integral does.

And the point that I want to draw attention to, so we can just use $Z(\theta)$ to denote this quantity. It's like a fixed scalar, but it's also **intractable to compute**, right? This is, like, half the reason why MLE is really hard.

### Introduction to the Score Function

There's a trick, so… Suppose instead that, uh… Okay. So, I can't do MLE, right? What's another thing I can do?

Let me introduce this concept of the **score**. And… the score can be written like this.

I think you use, like, $s_{\theta}(X)$ is equal to the gradient with respect to $X$ of the log probability:

$$s_{\theta}(X) = \nabla_X \log p_{\theta}(X)$$

Sorry, $\theta$... heating up.

And so, if $p_{\theta}$ is an exponential family, just bear with me, I know this is, like, a lot of jargon, I can rewrite like this. And I have this, like, $Z(\theta)$ in the denominator.

$$\log p_{\theta}(X) = f_{\theta}(X) - \log Z(\theta)$$

Right? And I'm trying to get the gradient of this objective.

So, I can just take the gradient, because I'm taking the log, and then the gradient:

$$s_{\theta}(X) = \nabla_X f_{\theta}(X) - \nabla_X \log Z(\theta)$$

And I end up with just the gradient of $f_{\theta}$ minus the gradient of the log of the partition function with respect to $X$. 

Um… $Z(\theta)$ is already a constant that I've integrated over all $X$, it's a number, which means if I want to differentiate it with respect to $X$, it's just 0. 

$$s_{\theta}(X) = \nabla_X f_{\theta}(X)$$

And so, what the score does is it subsumes, like, this problem in MLE of estimating this partition function. If you're a physicist, this is like a partition function, right?

Um, it's like the normalization constant at the bottom of softmax.

Um, so yeah. The whole point of using the score is that I can avoid computing this partition function.

Okay, so this is, like, thing one, and this is, like, gradually leading you up to diffusion models. So, like, just bear with me.

### Geometric Interpretation of the Score

Um… Okay. So… I have the score, and… Just to give, like, a geometric interpretation, because there is one.

Um, does anyone want to, like, take a stab at what exactly the score tells us? So, just look at this thing, right?

$$s_{\theta}(X) = \nabla_X \log p_{\theta}(X)$$

I have a log probability, and I'm taking the derivative of it with respect to $X$, my data.

So, what should, like, in words, can someone describe this for me?

What do we know about gradients in general? Like, they give us instantaneous rates of change.

This tells us instantaneous rates of change with respect to… as the data changes, how does the probability of us observing that data change?

Yes, very good, that's exactly correct. The score, if you can think about it, is like a compass, right? It tells you how the distribution of the data changes as you make microscopic changes to $X$. 

And so, intuitively, if you can learn the score, then you can effectively drag yourself to, um, points in $X$ that are closer to high-probability regions, and all, like...

I'll get more precise about what I mean by drag yourself, but effectively it has to do with sampling.

Um, and it looks very, very similar to gradients.

But intuitively, like, if you can learn the score, there's something called, like, the **manifold hypothesis**, which says that, uh, all the high-density regions of your data happen on a low-dimensional manifold.

The score is basically, like, giving you access to that manifold. Uh, it tells you how exactly that probability changes with respect to $X$. You just query it, it gives you, like, a gradient direction.

And so, if you want to maximize that score, then you're, by, you know, by proxy, maximizing the likelihood. I've done the same thing, right, as maximum likelihood.

If you can take my word for it, these things are sort of analogous.

Then, it suffices to just learn the score instead of learning the actual probability distribution.

So that's exactly what you can do.

### Score Matching Objective

Um… So, like, score matching naively looks like this, where you want to minimize:

$$\mathbb{E}_{X \sim p_{\text{data}}} \left[ \| s_{\theta}(X) - \nabla_X \log p_{\text{data}}(X) \|^2 \right]$$

The expectation over data, um… of... the squared norm of...

$$\| s_{\theta}(X) - \nabla_X \log p_{\text{data}}(X) \|^2$$

That's minus the gradient of $\log p_{\text{data}}(X)$.

Okay. This is score matching. So, I learn the score of the predictor that I'm using. I take it from the real, true data distribution, and I just perform regression on the score.

Score matching. Uh, there's a problem here.

Does anyone know what the problem is?

Taking derivatives is fine, but I'll give you a hint. I can't take the derivative of something because I don't know that something.

Yeah, right? Like, this assumes I have access to $p_{\text{data}}$.

Okay, this is, like, not really… it's not ML.

Um, but basically, there's a very nice paper by this, like, Finnish statistician, Aapo Hyvärinen, in the 1990s, but he shows that you can rewrite this objective by doing integration by parts, it's, like, very simple.

Um, and the resulting distribution looks like this. Where I have… so I won't derive it.

But you should do it at one point in your life.

Uh, and it looks basically:

$$\mathbb{E}_{X \sim p_{\text{data}}} \left[ \| s_{\theta}(X) \|^2 + 2 \cdot \text{tr}(\nabla_X s_{\theta}(X)) \right] + \text{const}$$

Yes, $\|s_{\theta}(X)\|^2$ plus the divergence… Well, the trace of, like, the Jacobian of $s_{\theta}$:

$$\text{tr}(\nabla_X s_{\theta}(X))$$

Um, plus some constant term that does not depend on $\theta$, so we can basically ignore it.

Uh, what you'll notice about this expression is that it does not actually rely on $p_{\text{data}}$, so if you want to score match, you just use this.

Um, and this is, like, a very, very pretty trick.

This is called **denoising score matching**. Uh, there's still a problem. So, like, even if I can do score matching using this formula, um, it turns out this doesn't work remarkably well in practice.

Uh, and there's another reason for this. But it's very subtle.

### The Low-Density Problem

So, if you want to learn a probability distribution, you have to learn both the high-density regions and you have to learn the low-density regions, right? 

And whenever I do the derivative, like, I take the derivative equal to 0 trick, and I have this expectation, I usually, like, substitute this expectation for a Monte Carlo estimate, or, like, the sample average, right? This is, like, a classic trick we play.

It's like, I use the sample average of this thing to approximate its expectation because of the law of large numbers, which says the two converge in the data limit.

Uh, okay.

So, suppose I'm using, like, a Monte Carlo estimate. And I'm just sampling randomly to get samples.

What if I would like to learn something from a low-density region?

Uh, the probability that I actually draw that thing in the first place is low.

So whenever I go to the empirical sampling of this, even if it is in the low-density region, when I go to backpropagate through this, I'm not actually… it's gonna get overshadowed by the terms that occur more often. It's like class imbalance.

Okay, so the key thing… So, like, okay, also this divergence term is expensive to compute.

But the key thing is that, like, if I want to learn low-density regions of the probability distribution, I'm not going to do it well with score matching.

And you can, like, draw this out in sort of, like, code it up, it's not too hard. Play around with it on your computer, you'll see the exact sort of thing I'm talking about.

### Denoising Score Matching (DSM)

Uh, and so there is… there's a fix. There's a fix.

Um, but it's, like, very… How do we say, like… it's simple, but it's, like, insightful.

And the fix… is pretty much that instead of score matching with respect to $X$, I can score match with respect to $\tilde{X}$.

Which I'll call $X$ plus some noise:

$$\tilde{X} = X + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I)$$

And this noise is normally distributed.

You can sort of think about this as, like, ridge regression, whenever you add, you know, $\lambda I$ to your Gram matrix, so it becomes invertible. Um, if you don't know what that means, it's fine. 

Uh, but the idea is that now, if I have a low-density region, and I add zero-mean Gaussian noise, I have, like, artificially inflated the density of that region, so it might show up whenever I'm learning it more often.

So now the question is, how can I match the score when I have this noisy thing?

Um, this technique is called **denoising score matching**.

And it is quite literally the core of what's happening in diffusion.

Um… DSM.

Okay. Uh… Let me see…

We'll give it a little bit of space. Okay, so I assume $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$.

Just add arbitrary noise. $\sigma$ just controls how much noise we add to the data.

Uh, okay.

Let me… Do we want to do…

Actually, yeah, it's probably clean to do this derivation. Okay, so I'm gonna do a little bit of math. It's not, like, too… like, the math itself is not too important.

Um, what is important is, like, the final form that you end up with.

Because it literally just gives you the, like, the diffusion objective on a silver platter.

Um, so, let me just do it, and… if you don't understand, that's fine, but I would like everyone to understand the final result.

### Deriving the DSM Objective

So… here's what I'm given:

$$s_{\sigma}(\tilde{X}) = \nabla_{\tilde{X}} \log p_{\sigma}(\tilde{X})$$

And we'll use, like, $\sigma$ to parameterize the distribution.

Can everyone, like, agree with me? This is… by score matching with $\tilde{X}$, or with $\tilde{X}$, like, I want to learn this score.

So, there are two things that I can… there are really 3 things I know.

So, what do I know? 

**Fact 1:** This is a very common trick, but the derivative of the log probability, or the gradient of the log probability, can just be written as: 

$$\nabla_{\tilde{X}} \log p(\tilde{X}) = \frac{\nabla_{\tilde{X}} p(\tilde{X})}{p(\tilde{X})}$$

This… log rule. It's equal to one over the thing inside times the gradient of that thing:

$$\nabla \log f(x) = \frac{1}{f(x)} \nabla f(x)$$

And this just comes from taking the derivative of the thing on the inside, and then using the log rule, it's 1 over that.

Um, good trick, especially when you get to reinforcement learning next week, this is, like, all the… this is, like, the policy gradient thing, it just abuses this.

Um, okay. **The second thing that I know:**

Okay, so what is, like, this… distribution $p_{\sigma}(\tilde{X})$, right?

Well, I can write it as the following integral. I'm adding two random variables:

$$p_{\sigma}(\tilde{X}) = \int p_{\text{data}}(X) \mathcal{N}(\tilde{X}; X, \sigma^2 I) dX$$

So what does, like, Stat 430 or probability class tell you?

This is just a **convolution**.

Okay? **And the third thing that I can do** is write out the conditional:

$$p(X|\tilde{X}) = \frac{p(\tilde{X}|X) p_{\text{data}}(X)}{p_{\sigma}(\tilde{X})}$$

As the following. $p(\tilde{X}|X)$ times $p_{\text{data}}(X)$ over… $p_{\sigma}(\tilde{X})$. 

What does this equal?

This equals:

$$p(X|\tilde{X}) = \frac{\mathcal{N}(\tilde{X}; X, \sigma^2 I) \cdot p_{\text{data}}(X)}{p_{\sigma}(\tilde{X})}$$

This is normal times $p_{\text{data}}(X)$ over $p_{\sigma}(\tilde{X})$.

Cool. So, I know these 3 things. Now I have to go about computing the score.

Okay? So… if I want to compute this score, then I know this formula from Fact 1.

If I know this, I have the formula for this thing already, the denominator, which is just… this convolution, and… the remaining calculation means I need to get the gradient of the probability.

Okay, I'll get it. So, the gradient of the probability distribution with respect to $\tilde{X}$:

$$\nabla_{\tilde{X}} p_{\sigma}(\tilde{X}) = \nabla_{\tilde{X}} \int p_{\text{data}}(X) \mathcal{N}(\tilde{X}; X, \sigma^2 I) dX$$

I can take this integral, this… right. What does this look like? Well, I know that I can just substitute this in.

And this is just going to be:

$$= \int p_{\text{data}}(X) \nabla_{\tilde{X}} \mathcal{N}(\tilde{X}; X, \sigma^2 I) dX$$

Okay? Now, I can use, like, there are certain conditions where I can't drag the gradient inside the integral.

But this setting is one of them where I can.

And so I just drag this thing in, and the only thing that's reliant on $\tilde{X}$ is this normal.

So this is called the **dominated convergence theorem**, for those of you who want to know. If not, it doesn't matter. Unless you want to do pure math the rest of your life, it's not critical.

Now, I have the normal, right? And I'm taking the derivative of it.

Well, a normal PDF is just an exponential function.

I could write it out, but in the interest of time, I'm just going to tell you what it is.

Uh, it's just $(\tilde{X} - X)$ over $\sigma^2$ times the normal:

$$\nabla_{\tilde{X}} \mathcal{N}(\tilde{X}; X, \sigma^2 I) = -\frac{\tilde{X} - X}{\sigma^2} \mathcal{N}(\tilde{X}; X, \sigma^2 I)$$

And I still have another normal multiplied, because it's just an exponential.

Um… You can, like, finish the math on your own if you want.

Okay? And… I play one more trick, which is multiplying and dividing by $p_{\sigma}(\tilde{X})$.

Uh, $p_{\sigma}(\tilde{X})$. So they give... I'll pull one of these out.

And what I get… So this is where this third fact comes in.

I can substitute in $p(X|\tilde{X})$ by just swapping these two using Bayes' rule.

And effectively, when I put it on the bottom, the normal and the $p_{\text{data}}$ cancel out. And I'm left with:

$$\nabla_{\tilde{X}} \log p_{\sigma}(\tilde{X}) = -\frac{1}{\sigma^2} \int (\tilde{X} - X) p(X|\tilde{X}) dX$$

Over $\sigma^2$. And then it's just $\tilde{X}$ minus... $X$ times $p(X|\tilde{X})$.

Okay. Great. We're almost there, so please tune in again.

Um, but… I can recognize this, so if I… assign $p(X|\tilde{X})$...

Okay, so… what's happening here?

There's a nice conditional expectation formula for normals that tells you, basically, that this entire thing is going to be equal to… the expectation… of $X$ given $\tilde{X}$ minus $\tilde{X}$, because I just distribute this probability in:

$$\nabla_{\tilde{X}} \log p_{\sigma}(\tilde{X}) = -\frac{1}{\sigma^2} (\tilde{X} - \mathbb{E}[X|\tilde{X}])$$

All over $\sigma^2$. I think this is where… I'm… I'm confident about this.

Okay, uh, please trust me… Great. Now… So this, this, like, form is what's important.

So, I know that there's a $p_{\sigma}$ here and a $p_{\sigma}$ in the denominator of the score formula, so really, they just cancel, and on the left I have that my score:

### The Key Result of DSM

So the final form that is important, uh, pay attention again, is… this:

$$s_{\sigma}(\tilde{X}) = -\frac{1}{\sigma^2} (\tilde{X} - \mathbb{E}[X|\tilde{X}])$$

Okay, what does this tell us? Well, first off, this is my score, so when I want to go back and do score matching, I just plug this in along with my estimate $s_{\theta}$.

And, yeah, things will actually reduce quite nicely. I won't, like, write out the entire thing.

But effectively, like, the score matching loss… should look something like… yes.

Should look like an expectation over data… um… so let's just call this the expectation over data. Uh… it will look like the expectation over $\sigma$, over $\epsilon$, of:

$$\mathcal{L}_{\text{DSM}} = \mathbb{E}_{X \sim p_{\text{data}}} \mathbb{E}_{\sigma} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2 I)} \left[ \left\| s_{\theta}(X + \epsilon) + \frac{\epsilon}{\sigma^2} \right\|^2 \right]$$

Something like this... times some weighting...

Okay. Great. So, here's, like, the insightful part of all this math.

What I've shown is pretty much that… if I augment the thing that I want to score match by just some zero mean Gaussian noise, then I can write the… I can write the closed form objective.

And I can write the score like this. And what does this mean? Well, it says that given an $\tilde{X}$, which is just a noisy version of $X$, I would like to minimize the reconstruction error with respect to the original $X$.

Like, that's the important takeaway. We call this process **denoising**.

So, I train on real data and substitute, like, an empirical expectation as a proxy of this expectation, right? And the way I train it is just by taking samples, adding noise by some amount of noise, and then predicting what that original $X$ is, given just the noisy sample.

This is how we do it. And so this suffices.

So, a broad takeaway here is that this actually suffices to learn the score.

And if I can learn the score, then I can learn the probability distribution.

And so that's, like, the overarching story. Which is… if I add noise to my sample, and I do denoising score matching, and I train to minimize the reconstruction error, then I've learned the score, which means I've learned the probability distribution.

Does that, like, overall sort of track? Basically, **denoising is akin to learning the score function, which is akin to learning the distribution that you wanted**.

Okay. This is, like, literally all diffusion is doing.

Um, so there's… So there's a point, so… again… right? So this is DSM, which is a single step of denoising score matching for some fixed $\sigma$ that you choose.

Uh, it turns out that, so…

I will formalize this soon.

**Learning the score is akin to denoising, which is akin to learning $p_{\text{data}}$.**

Why? Because of that, like, entire, like, argument that I put over here, and… Okay. So, if you believe me that this is the case.

Uh, then the question is… given the score, how do I sample, right? How do I go from an arbitrary point, like, basically, how can I use this to do generative modeling, right?

So there are two things. The first is, given a score, I can sample from it using... which method do I want to introduce?

Okay. The question I want to answer first is, how can I use DSM for generative modeling?

Finally, does anyone have any guesses? Uh, my hint is… um, remember HVAE?

Yeah, yeah, do it multiple times for different noise levels, right?

So what I can do is, instead of just denoising one $X$ with noise level $\sigma$, or $\tilde{X}$ to, like, an $X$, I can just denoise, like, an arbitrary number of times at different noise levels.

## Part 4: Diffusion Models (DDPM)

So, that formulation should look something like… should look something like this, where I have… like, $X_0$.

Alright, so this is, like, the… these are gonna be the DDPM, or, like, the diffusion equations.

But… I basically define $X_t$ given $X_{t-1}$ as the following normal:

$$X_t | X_{t-1} \sim \mathcal{N}(\sqrt{1 - \beta_t} X_{t-1}, \beta_t I)$$

$\sqrt{1 - \beta_t} \cdot X_{t-1}$, um… plus $\beta_t I$.

And these $\beta_t$s are going to decrease over time. This looks quite arbitrary. Let's think about it for a second.

### Understanding the Noise Schedule

If $\beta_0$ is equal to… if $\beta_0$ is equal to 1, right? Then… $X_1$ given $X_0$ is just gonna look like... or, like, $X_t$ given $X_{t-1}$ is just gonna be this, like, normal, like… 

So, $X_1$ given $X_0$, if $\beta_0$ is 0, then this $X_t$ given $X_{t-1}$ is just going to be deterministically drawn from $X_{t-1}$ because $\sqrt{1 - 0} = 1$ times $X_{t-1}$, and $\beta_t$ is 0 times the noise, so it's just 0.

So, deterministically, I'm just drawing it from $X_{t-1}$.

Okay. And when $\beta_t$ equals 1, so, $\beta_t$ equals 1, then, what am I doing? I have $X_t$ given $X_{t-1}$ is now going to be drawn from:

$$X_t \sim \mathcal{N}(0, I)$$

Next, because $\sqrt{1-1} = 0$, which means this mean term is 0 mode, and $\beta_t = 1$, which means the variance is $I$, so it's just standard normal.

And so I just choose an arbitrary noise schedule between 0 and 1.

And intuitively, what happens… is I transport from my image at time 0, so, like, $X$ space, to $X_1$… to $X_t$.

Where this is, like, you know, a nice image of a human, right? And then I slowly add noise, I slowly add noise, I just follow the schedule. And then I end up all the way over at $X_T$, which is just going to be a zero-mean unit Gaussian.

Right. Okay. So, that's the first part of it.

So, why do I actually choose this $\sqrt{1 - \beta_t}$ form?

The answer is that if I define $\alpha_t$ as $1 - \beta_t$:

$$\alpha_t = 1 - \beta_t$$

And I write $\bar{\alpha}_t$ as the product of all the $\alpha_i$s up to time $t$:

$$\bar{\alpha}_t = \prod_{i=1}^{t} \alpha_i$$

You can basically verify to yourself that this relation holds.

So the reason it takes this form: one, is because at time step 0, I want it to look like my image space, and at time step, like, you know, at the end of this, like, noising process, I want it to look like pure noise. 

The second reason is that whenever I take these products and I roll them out sequentially, the actual generative process looks like this:

$$X_t | X_0 \sim \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1 - \bar{\alpha}_t) I)$$

And, uh, this is discrete time, meaning that, like, each of these $t$s is, like, a fixed integer. If it's continuous time, this thing actually looks like a differential equation, and researchers have really, like, exploited that, and many, many follow-up papers to diffusion do continuous time, and this is, like, the basis for more generative models.

Okay. This, uh… this is the diffusion equation for the forward process.

The reason I have this square root is literally because I want to make sure that my variance doesn't blow up.

Um, and yeah. Sometimes this is called, like, **Ornstein-Uhlenbeck process**, if you remember that.

### The Reverse Process

Okay, so… the last thing that I want to do is figure out: Okay, so I've told you how I can go from image space to noise, right? But that's not what we care about. What we care about is going from noise space to image.

So, we go from image space to noise by just gradually doing this forward diffusion.

And we haven't used denoising score matching yet, but that's exactly what we do.

At each point, we just… okay, we just use DSM, right? DSM… I have $X_1$, recover $X_0$. I have $X_2$, recover $X_1$, because each of these things are just adding noise.

We do DSM here, we do DSM here, we do DSM here.

And so, at test time, what I'd like to do is draw a sample from the zero-mean unit Gaussian, because I know how to do that.

And when I draw that sample, I want to transport it back to the image space.

Right? How do I do this? I have to reverse this equation.

Right? I have to go the opposite way. I no longer want to go from $X_0$ to $X_T$, I want to go from $X_T$ to $X_0$.

And it turns out that because I've chosen all of these things as Gaussians, and we know that Gaussians are closed under, like, posteriors, right?

And so, because I've cleverly chosen this to be a Gaussian, and these constants being the way they are, I can write the equations that take me from noise to image space in closed form.

And so, like, that's the whole magic of diffusion.

The way I do that, I think it's, like… each successive update, $X_{t-1}$ given $X_t$, or... $X_{t-1}$ given $X_t$ and $X_0$, looks like:

$$X_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( X_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} s_{\theta}(X_t, t) \right) + \sigma_t z$$

Where $z \sim \mathcal{N}(0, I)$.

More precisely:

$$X_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( X_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_{\theta}(X_t, t) \right) + \sigma_t z$$

Using, again, some, like, zero-mean noise sampled fresh.

Okay. Does this, like, make sense at a high level to everyone?

Like, it took me, like, many hours and hours to, like, actually finally understand what was going on in diffusion.

I just want everyone to understand, like, at a high level.

Like, it's just… using this denoising score matching trick at multiple steps to do… to basically learn the score function, and when I learn the score function, I've learned the probability distribution.

And then I can sample from it using this reverse process.

I actually want to talk more about sampling, but we'll stop here, like, tentatively, just for questions about the high-level intuition.

Okay. Yeah, what's up? 

**Student:** So, uh, at a high level, when we want to generate some data using the diffusion model, we usually give it some prompt, like we want to generate an image or something. So, how does the prompt… take us to this...

Yeah, so I haven't told you how exactly to do that yet. This is just unconditional generation. The method that you use to, like, condition on some other information that you tell it, like, "oh, generate the image of a human" or "generate the image of a flower," it's called **guidance**.

And effectively, I just add a second conditional variable into all the math that I just did. And I'll derive, like, analogous equations. There's actually two ways to do it, but… um, I don't have time to talk about that here.

It's like a whole... it deserves its own separate piece.

But yes, the way that, like, you typically use diffusion models in practice is you tell it what you want, and we condition it on that, we do, like, classifier-free guidance by adding a conditional term and training appropriately.

Correct.

## Part 5: Sampling from the Score Function

So the last thing that I want to talk about is… this idea of sampling, just a little bit, like, more closely.

I sort of skimmed over it, but if you recall back to the, like, the diffusion reverse process... There's a question, like, you know: suppose I would like to... so I have access to the score function.

$$s_{\theta}(X) \approx \nabla_X \log p_{\text{data}}(X)$$

Given the score function, how do I generate samples?

This is the question. It's like… Yeah. This is statistics, it's called, like, score-based sampling.

Um, but… suffice it to say that… if I'm given an arbitrary point $X_0$, which is supposedly some noise sample that I was given from the diffusion process...

So, you just initialize this. I can iteratively do the following:

$$X_{k} = X_{k-1} + \epsilon s_{\theta}(X_{k-1}) + \sqrt{2\epsilon} z_k$$

$X_k$ is equal to $X_{k-1}$ plus some step size $\epsilon$ times the score, plus $\sqrt{2\epsilon}$ times some noise $z_k$.

Okay. And so what this will do… so let's, let's, let's revert back to the geometric interpretation we were talking about earlier. What is the score?

The score is... what was it?

Yes. Yeah, gradient of the log likelihood with respect to the data.

And so, intuitively, what this is doing, right, if you were to write out the maximum likelihood problem, this is just performing **gradient ascent** on the log likelihood.

And so, if I give it a point $X$, it's going to drag me to the point, like, the nearest mode, okay?

I'm going to converge to the nearest, sort of, high density region according to the score, because that's what the score tells you.

It gives me the direction to move to bring this towards the regions of high probability in my training data.

### The Problem with Naive Gradient Ascent

There's a small problem with what I've written up here, in that doing naive gradient ascent... If I were to, like, draw what this looks like, I have, like, you know, a probability distribution with multiple modes that looks like this… Like… 

Doing raw gradient ascent on, you know, the score function is just going to drag me to the nearest mode for every point that it's closest to, right? So, like, this point's gonna go here, this point's gonna go here, this point's gonna go here.

But really, like, there could be another region that, you know, is better to sample from, but because these are all local gradients, it's just gonna end up at this local mode even though this might be a higher-probability mode over here.

Right? The same idea with gradient descent on non-convex functions, it's just not okay. Gradient ascent is not guaranteed to converge to a global maximizer.

### Langevin Dynamics

Alright, so… what can I do?

What do we do in stochastic gradient descent? We use, like, a noisy version of our gradient.

So, stochastic gradient descent uses a noisy version of our gradient, but if we just add noise to gradient ascent, right? If we add some, you know, some zero mean noise, now, maybe I can escape this, like, local mode, and converge to the global one, right? 

Like, I'm gonna add noise, this… helps you escape. And so, the whole point… so this is called, like, **Langevin dynamics**, or, like, the **Langevin equation**, if you've heard of it.

Um, physicists came up with it a while ago.

Uh, it's like the canonical equation to sample from something, um, assuming you have access to the score function.

And I add noise because I don't want to converge to a local maximizer, I want a new sample, right? I want a fresh new sample every time I run the initialization.

And so, that's the whole thing. The intuition for why this works is the same reason why stochastic gradient descent works on, like, non-convex objectives. I have some noise, it helps me escape what would otherwise be local minima.

## Conclusion

Okay. That is… all I had to talk about, we have, like, 15 minutes left, so feel free to ask me really any questions about any of this.

You can also... I can write pseudocode out, like, for the algorithm, if that would help. Um… but yeah, otherwise, any questions?

[Questions and answers continue...]

I think... I mean, you can... I mean, you can accelerate sampling the same way you accelerate gradient descent...

---

## Summary

In this lecture, we covered:

1. **Primer on Generative Modeling**: Starting from Maximum Likelihood Estimation and why it's intractable at scale
2. **Variational Autoencoders (VAE)**: Deriving the ELBO, understanding the encoder-decoder architecture, the reparameterization trick, and hierarchical VAEs
3. **Score Matching**: Understanding the score function, denoising score matching, and how it relates to learning probability distributions
4. **Diffusion Models (DDPM)**: The forward and reverse diffusion processes, and how denoising score matching at multiple noise levels enables high-quality generative modeling
5. **Sampling with Langevin Dynamics**: How to sample from learned score functions using noisy gradient ascent

The key insight connecting everything: **learning to denoise is equivalent to learning the score function, which is equivalent to learning the data distribution**.

---

# CIS 5200: Machine Learning - Lecture 22: Reinforcement Learning

## Introduction and Course Logistics

Okay, awesome!

We've now finished supervised learning and unsupervised learning, and we have an echo.

We have no more echo?

And now we get 3 lectures before Thanksgiving to cover reinforcement learning, which is a fun field. It was trendy in the 80s, and then it went out of fashion, and nobody did it for 15 years, and now it's become really popular.

I want to give you both an overview of the broad sets of methods and to dig into one piece of math, which is Bellman's equation—a great recurrence relationship. Why is this not showing up on the screen? That's sort of annoying.

Hold on, was the check-in slide also not up? No one told me, oh dear!

Okay, let's put the check-in slide up. Well, I gotta tell... no, it's still not up. I can't see behind me, you gotta tell me when things don't show up. Let's see, this is super annoying.

Ha! Okay, I'm here, you're here. Yeah, things don't show up—yell and scream. I can't turn around behind me.

### Announcements

So, next week is Thanksgiving. As posted in the syllabus, there's no class on Wednesday. There's no recitations over Thanksgiving break, and the TAs will mostly be away, but we'll try to answer questions about your projects if you have them.

Also, while I think about it—the slides I posted for today will take us all the way through Wednesday. And then we'll come back next week and look at policy gradients, and maybe probably DPO (Direct Preference Optimization) and PPO (Proximal Policy Optimization).

How many people have done RL before? Yeah, like five of you. You can all go to sleep—no, you can all think about how this fits together as a review, seeing it done super quickly the second time.

## What is Reinforcement Learning?

### Core Concepts

Awesome! So, what we'll see for reinforcement learning is that there are several different big paradigms within the whole area. So this is gonna feel super fast.

The core idea—and I'm going to put this up here so I can refer to it again—is that you are building (I love this word is now back in fashion again) an **agent**. That's your AI. That's your machine-learned model.

The machine-learned model is going to produce an **action**. Mostly, we'll be doing discrete action spaces, where you pick one of $k$ possible actions, but it could also be a real-valued action, a real-valued vector.

The agent sends that action to the **world**, which is often called an **environment**. The world could be a user at the other side of ChatGPT, it could be a robot, could be anything, right?

And the world then sends you back two things:

1. **Reward**: This is how many dollars, or utils, or points you get. I will always talk about reward—realize rewards can be positive, zero, or negative. More is better. That's the thing we'll be optimizing.

2. **State**: The agent tells you back, generally, some sort of a state—a description of where the world is.

### The Learning Goal

Make sense? So your goal as a machine learning agent is to learn some mapping from state to action. We're going to learn a **policy**:

$$\pi: S \rightarrow A$$

which, in general, is a mapping from state to action that says, "Hey, this is the best thing you should do."

And of course, "best" is going to have to be quantified by **maximizing** some sort of reward. Oh, and it's complicated—it's a whole sequence of rewards, right? I take an action, the world does something, I take an action, the world does something, we keep repeating.

### Concrete Example: Tic-Tac-Toe

Framework good? I'll be very concrete about this a couple times. Let's start by being super concrete.

Imagine you're playing tic-tac-toe—sort of a stupid game, which if you don't know it, don't worry about it. But typical of games, we will start with some board position.

We're in the initial position, we will take some action, which is to make an X or an O somewhere. It's a game where we have a board with 9 positions, we take turns, and I move first, I put an X.

The environment moves next, and it places an O. That's the state. I move next, I place an X. The environment moves somewhere, places an O, I move an X, I go, "Hey, I won! I got 3 in a row, yay!" I got +1, I got a plus one reward.

So note that there's a sequence of **states** and **actions**. And I followed some sort of a policy, but you couldn't tell what it was because it's invisible in my head. But that will be the model that we'll learn.

And I got a whole bunch of actions with no reward, right? I played once—no reward, once—no reward, once—then I got a reward.

Make sense?

### Key Differences from Supervised Learning

**Question**: Shouldn't there be intermediate states that give some sort of reward?

The formal semantics of this is: every turn, I take an action, and the environment then gives me some reward. For many, many, many problems like tic-tac-toe, or chess, or Go, you don't get an intermediate reward.

The way that tic-tac-toe works—the way chess or Go or most games work—is: no points, no points, no points, and at some point, you've either won or lost or tied.

**Question**: How does it learn a sequence? 

That's what's gonna take us three lectures. That's the problem, right?

**Why is this really different from supervised learning?**

There's a lot of things that people call reinforcement learning, called imitation learning, that makes it supervised. But the way I posed it here, that's not supervised learning, because **we don't know yet**. There are **no labels** for the correct action.

You take an action. You say, "Robot, go left 3 meters!" It does something, it goes left 2.7 meters, whatever it does, and you get a reward. Zero—you didn't crash into a wall, but you know, hey, you didn't get the cheese or whatever you're looking for.

There's no one who ever tells you what you should have done in reinforcement learning. There is no $y$ label that says this was the correct action.

If you do have labels, that's nice—that's often called imitation learning, which is really easy. Here's the state, here's the right action. That's supervised learning. That's easy. But here, you don't get told what you should have done.

### The Counterfactual Problem

In fact, do you see what happens? If my action is "go right 3 meters," what do I learn about going left?

**Nothing**, right? You never see the action that you don't take.

**Question**: Why would you use reinforcement learning?

You answered your own question. If I know that this was the correct action to take, why would I use reinforcement learning? I wouldn't!

The problem is, often in the world, you don't know what the right action to take is. I'm gonna show you an advertisement or a movie, and I can see whether you watched it. I might even be able to ask you, "Did you like it?" But who's gonna tell me what movie I should have shown you that would have been better?

So the world—we're now moving to a different set of assumptions. Before we had $X$'s and $Y$'s, now we have states and actions. The states look a lot like $X$'s, but the actions really don't quite look like $Y$'s. You're taking an action in the world, and you only get to see the consequences of the action you take. You cannot, in general, see the consequence of the actions you don't take.

### Model-Based RL: The Exception

Now, the one counter-example we'll see, which is sort of nice, is if you have a **model of the world**. Then, in fact, you could ask it and simulate "what would happen if..." And then we can actually do learning that's easier, or at least different, if we have a model of the environment.

The environment is a function:

$$P(s_{t+1} | s_t, a_t)$$

that takes in a state and an action, and produces a new state. If you know that, life is very different.

### Exploration vs. Exploitation

The other piece that we're going to worry about a lot in the background as we go through this is to say there's always a choice in learning in this sort of world between:

**Exploration**: Try an action you haven't taken before in this state and see what happens. Try new stuff out—order a meal at the restaurant you've never tried.

**Exploitation**: Try and take what you currently believe is the best action—order the meal you liked last time.

And again, we're running an optimization, but it's an optimization that's unfortunately doing two things at once. We're trying to optimize the sequence of future rewards. And to optimize the sequence of future rewards, sometimes it's best to try new actions you've never tried before—even if they're crappy, because you learn they're crappy (but you might learn they're good). And sometimes it's best to focus in on the things that have worked in the past.

Make sense? So it's a very different sort of optimization. As always in machine learning, we're gonna have a model, an inductive bias, a loss function, and an optimization. But the optimizations will be a little bit different because we're optimizing over a whole future set of lots of actions, and the actions include taking things that might tell you about the world.

## Applications of Reinforcement Learning

### Historical Context and Modern Applications

Classic game theory. Just to give a timeline on these things, starting back around 2017 (so we're talking now almost a decade ago), the modern version of... I mean, RL's been around since forever.

In fact, funny thing, the classic textbook is Sutton and Barto. Sutton, the first author on the RL textbook—these are the guys who invented RL—Sutton was an undergrad exactly in my year. I knew him back when we were undergrads together at Stanford in the... I'm embarrassed to say, 70s.

So this dates back to the 80s when he did his PhD thesis.

But what we figured out we could do more recently, a decade ago, is: have a game, start playing—computer starts playing. As it plays the game a lot of times, in something like 24 hours of very old GPUs, CPUs (2017), it could learn to play Go, or chess, or a dozen other games. Learn to play just by trial and error.

Sort of cool. RL is used in:

- **AlphaGo era**: Self-guided helicopters, games
- **Bidding**: If you want to go on eBay, what's an optimal bidding policy to try and bid to get items at the lowest price? You see the consequence of the bid you gave, but you don't see the consequence of the bid you didn't give. At the end of the auction, either you bought the object for some amount, or you failed to buy it. It's a sequence of actions over time.
- **Advertisement**: Showing advertisements. I show you an ad, I don't know what happens if I had shown you the other ad. And I care not about just the immediate effect of showing you the ad, but about how much money I'm going to make from you over the next year. It's a sequence of actions.
- **Chemical plants** (I used to do this)
- **Robotics**: Used a lot now. Think of robots like at Penn—they're mostly drones trying to find an optimal policy for the robot to fly.

### Chatbots and LLMs

**Chatbots**: ChatGPT-4o, Claude, or Gemini—they're all pre-trained with supervised learning. Given a sequence of tokens, predict the next token. And they're all **post-trained** with reinforcement learning.

Pretty much any commercial chatbot you'll use has RL in it.

**Why is RL important for chatbots?**

They used to be initially language models—supervised learning, predict the next word. Why is RL so important?

**What's the reward in a chatbot?**

**Answer**: User satisfaction. Can I observe user intentions? I never see user intentions directly.

**Student**: Retention.

You can try to optimize for retention. Most of them don't, actually. I mean, that's sort of what they want, maybe. But they're not actually rewarded that way. I'm gonna have to get a reward function. We're still in machine learning here in chatbot land.

I've trained up a model that predicts the next token because of the proper distribution over tokens using cross-entropy. I find a trillion tokens, and then I'm gonna post-train it. I give it a prompt, have it give an answer, put some randomness in. Do the users like the response?

And in fact, I'm typically going to pay a whole bunch of people—mostly lower-paid Kenyans or Nigerians (Kenyans mostly), and a few high-paid computer scientists—to look at two possible answers that this LLM might generate (maybe by putting some randomness into it, a higher temperature) and ask: "Which one do you like better?"

Think about thumbs up, thumbs down. And if you pay attention, often your chatbot has a little function at the bottom like a thumbs up, thumbs down on that response. The thumbs up, thumbs down? That's reinforcement.

Or they'll give two versions. I see this on ChatGPT periodically. "Here's two versions. Which one did you like better?"

We're gonna cycle eventually back around to that and show that that is, in fact, a kind of policy learning, where the policy is: given the state of the LLM, the words so far, the context, predict the next word. And if you say, "I like this one better or worse," or you give a thumbs up or a thumbs down, you're giving reinforcement. And that reinforcement is the signal that will be used for the gradient descent of the neural net.

Does that make sense? We're gonna come back and do this in gory detail a week from today.

**Question**: Does it follow supervised learning, or does it follow RL?

The standard current paradigm is you **pre-train with supervised learning**—predict the next token given the last context—and you **post-train with RL**.

So all of the standard commercial models have that two-stage training: supervised pre-training to learn a model to predict the next token (which just models language), and then post-hoc training, instruction fine-tuning, RLHF (Reinforcement Learning from Human Feedback)—we'll talk about reinforcement learning from human feedback: "Yes, no, like this one better"—use that to gradient descent it.

Cool, we'll come back and do that.

## Challenges in Reinforcement Learning

This is all really, so far, just setting things up.

**Why are these hard?** Again, you should be noting that in general, you only take one action out of a discrete action space. If it's producing the next token, it may be one out of, you know, 40,000 tokens, but it's still a discrete action space. Or for a continuous action robot, you only see the direction you send your robot.

You're never getting the supervised feedback of "this would have been the best action."

And in general, RL takes a **sequence of actions**. You're playing a game: I play, the machine plays, I play, the machine plays, I play, the machine plays, and after 50, 100 moves, I win or lose.

So every time you take an action, you're not getting immediate feedback. You only know at the end of the game how well you did. And that's true for lots of things.

## Contextual Bandits: A Special Case

There is one special case of RL which has a jargon name that you should learn: **contextual bandit**.

A lot of the companies I talk to say they're doing RL. But when they actually look to see what kind of RL they're doing, what they're doing is a special limiting case of RL, which is a contextual bandit.

### What's the difference?

In a **contextual bandit**, I've got a state. I'm in some state. I then take some action. I get some reward, and the reward can be a function of the action I take, and it could be a function of what state I'm in or what state I end up in.

You think about: I'm selling products, the action might be showing you an ad, and the reward is how much money I make or lose when you buy it. That's **one shot**—in the sense that you do it, and I don't worry about how this will change the future.

Whereas in a **full RL** (what I like to call RL, but hey, people call contextual bandits RL also), you take an action, you're in a state, you take an action, the action **changes the state** for the future.

I show you an ad, and not only do you either buy it or not buy it, you're more or less likely to come back tomorrow. Most actions change the state of the world.

If I'm in a robot, and I say (again, think of a robot as a drone that flies around), "I'm a robot, and I say go this way," I don't just get a reward—I also change the state. I move it to a different position. Oh, and I use up some of the battery or whatever.

So in general, as you take actions, you are changing the future state, and these things often have a whole sequence of actions. If I want the robot to go and empty my trash can, it's gotta do a whole bunch of actions: go over and get it and come back again. And only if... well, maybe it crashes into the wall and I get a negative reward, but only at the end when it empties the trash does it get a big reward—"Good job, good robot!" Go plug it in, have some electricity, have some energy, recharge yourself.

### One-Shot vs. Sequential

Are people clear on this distinction between a one-shot "do it, pick the best action" (which is relatively easy) versus "you're gonna do it, it's gonna change the state, and we're gonna cycle around and around and around this loop"?

**Question**: What are some one-shot ones?

**Answer**: Games are typically not one-shot. Ads are good one-shot examples. It's another good one-shot example.

You asked me, okay, think about chatbot. Is the chatbot where all I care about is: you're gonna ask a question, the chatbot is going to answer it, and you're gonna give it a reward—good or bad answer? That's a contextual bandit.

Or is it the case that based on what answer the chatbot gives, it changes the next question you ask, which changes the whole future conversation? So it's very different.

Most chatbots are now optimized to give the best immediate response. They're contextual bandits in the sense that they're giving you the answer that's the best answer (in some sense—the best will come in a second, now in an hour, two hours).

But in fact, they should be optimized not on a single answer, but on the whole conversation you're having. How satisfying was the entire conversation at the end of the conversation? And obviously, what the chatbot tells you for your first prompt will influence your second question, which will influence what you answer for the next one. The whole conversation is a chain.

And so, as the chatbot has a conversation with you, it's changing your state. You're the environment, and it's changing the context of what's been talked about so far.

That make sense? So it's really very different to do what most companies do today, which is: "Here's a prompt, here's an answer. What's the best answer?" versus "Let's have a whole conversation and let's develop it."

Think about it: if you were doing a one-shot answer, would it be longer or shorter than if you're setting up a whole conversation?

**One shot is longer.**

Does GPT tend to give long answers or short answers compared to what your friends do?

**Long answers.**

It's optimized to give you long answers—not necessarily because long answers are good for a full conversation, but because the RL that's used today is optimizing for "Do you like this answer or that answer better?" It's a contextual bandit. If you only get one shot, tell me the whole answer now. If it's the start of a conversation, give me something and then let's talk more about it.

So it's different to optimize a whole conversation and get RL at the end of it (more expensive) than to do a one-shot "Here's your answer. How much do you like it?"

## Two Classes of Reinforcement Learning

Cool. Okay, so we're going to distinguish two big classes of reinforcement learning:

### 1. Model-Based RL

In a **model-based** system, we're gonna actually learn (hey, we're in machine learning) a **model of the environment**.

The environment is generally modeled as something which says, "Hey, the environment says I'm gonna give you a probability of the state at time $t+1$, given the state at time $t$ and the action at time $t$":

$$P(s_{t+1} | s_t, a_t)$$

The environment or the world is a mapping from state and action to next state. Make sense? And it also gives you a reward based on those.

If you're in the Bayesian model, this is gonna be a **Markov Decision Process** (MDP), and I will walk through that in a second. If you're in the neural net world, this'll be a neural net.

We're gonna then use the model of the environment to eventually find ourselves a **policy**:

$$\pi: s_t \rightarrow a_t$$

or more generally:

$$\pi(a_t | s_t) = P(a_{t+1} | s_t)$$

A policy is really a probability of action at time $t+1$ given state at time $t$. I've written it two ways. It can be a probability distribution (the general form), or it could be deterministic—the policy could always give you the same action given the same state.

### 2. Model-Free RL (Value-Based)

The other option is to say, "Hey, it's too complicated to learn a model of the world." Instead, what I'm going to do is try and give some estimate that tells me how good every state is—how good in the sense of "what's the expected discounted return of all the future rewards?"

We will call that the **value of a state**:

$$V(s)$$

So every state will have some associated value of the state, which will be the expected sum of future rewards with appropriate discounting factor. We don't know this, of course, but we're going to try and estimate it—we're trying to learn this function.

Or we will learn something else that is peculiar and weird to reinforcement learning: we will learn the **Q-value**, which is the value of being in a state **and** taking some action:

$$Q(s, a)$$

One version says: "I'm in a state. How good is it?" And you should think always when you see "value of a state," it's subscripted implicitly with "under some policy":

$$V^\pi(s)$$

What's the value of the robot being here now? Depends what policy it's following. If it takes different actions, different places are better.

Whereas the Q-value says: "I'm in some state now. I'm going to take whatever action—go right, go left, go up, go down, turn the power off—and then, for the future, I will then follow some policy after that."

$$Q^\pi(s, a)$$

So both of these have a little tiny subscript for the policy.

**Question**: So the only difference is with Q, you're also considering what next action?

**Answer**: Q, you can take one action that can be potentially **off-policy**. We're going to talk about on-policy (following the policy) and off-policy. So Q says we're gonna take one action that maybe is not the one that's our general policy, and then we're going to follow a policy in the future. And this lets us do **exploration**.

### Q-Function Details

The first thing to say is: this action in Q is **any action**. I've got $K$ possible actions. Q is a function from the state (which could be some state vector that describes my current state) plus any of the possible actions:

$$Q: S \times A \rightarrow \mathbb{R}$$

Go up, go left, go right—could be stupid, could be good, but this is going to allow us to do exploration. This will allow us to say, "What happens if I take this action which I've never tried before, haven't tried often?"

**Question**: Well, this action is any action whatsoever. So it's not part of my policy?

**Answer**: In general, a policy says, "Hey, I'm an agent, and I'm gonna follow some policy." That policy is some function that maps from a state to a probability distribution over actions. You can think of it as a neural net, for example, that gives you a probability distribution over actions. You pick one from the probability.

But what I want to do for Q-learning (which we'll get to on Wednesday) is I want to say, "Hey, I'm gonna start in this state, wherever I am. I will take some action, whatever it is—but not chosen from my policy, necessarily. And then I end up in some new state (based on the environment), and going forward, then I'll follow my policy."

So this lets me try an action that is NOT taken from my policy. My policy may have a probability of zero of taking this action. But hey, I can still hypothetically try it, or in the real world, I could try it. I could make my robot flip upside down once and then follow my policy and see what happens. In expectation, we'll see how well that works. Maybe cool, everybody applauds and goes "yay" and loves it, and it safely lands. I don't know.

**Question**: Does Q... yeah, so Q, in some sense, is: take the action now, and that'll take you to a new state, and that new state will have a value?

**Answer**: So you'll see as we go to the Bellman equations, which are recurrence relations, you can see that if you're in some state and take some action, you'll end up in some new state, and the value will be updated recursively.

Make some sense? So yes, you're exactly right.

**Question**: If you sum Q over... actually, V, it's the sum, but it has to be taken with the expectation—it's an expected value?

**Answer**: So you have to take each of these actions and weight them with the probability of the policy:

$$V^\pi(s) = \mathbb{E}_{a \sim \pi}[Q^\pi(s,a)] = \sum_a \pi(a|s) Q^\pi(s,a)$$

It's a sum, but it's a weighted sum. The policy says, "Here's your probability of action." And particularly if your policy is deterministic, it gives a probability of 1 to one action and 0 to all the rest of them. So in that case, you're gonna take your Q-value and only plug in the one which is the action you're taking.

You with me? Because the expected value is Q weighted by $P$.

**Question**: You're getting an expected value out of that. But when we're in Q, we are not following any policy?

**Answer**: No, no, Q does have a policy. Again, what is Q? First, take one action, then follow the policy in the future. So Q has a policy. Both V and Q are under some policy.

The first action is not random—it's whatever I pick. This is a function. Think of all of these things I have up here as equations—they're functions.

$V(s)$ is a function which maps from the state space to the real numbers:

$$V: S \rightarrow \mathbb{R}$$

$Q(s,a)$ is a function that maps the state space cross the action space to the real numbers:

$$Q: S \times A \rightarrow \mathbb{R}$$

These are functions. And they're functions that we're going to estimate. We're gonna learn them! We're gonna learn Q.

And eventually, we'd like to learn an optimal policy while learning V and Q.

For the fancier methods, it'll be a little bit annoying because given a policy, we can estimate V (how good is every state?). And given V, we can pick what a good policy is.

In fact, Q makes it really obvious. For some policy, I learned Q. Now, what's an optimal—a better policy?

$$\pi'(s) = \arg\max_a Q^\pi(s,a)$$

Pick the argmax over the actions of Q. Argmax of this says: pick the action at the moment that gives you the highest expected return.

But that's still assuming the policy, so they're gonna go back and forth.

## Summary of Concepts

So this is all the setup. We're gonna do this all in great detail. All these things can be discrete or real. All these things can be simple functions or neural nets.

Cool! So we've now done the introduction.

I'm gonna say a little bit more about what RL is. We're gonna talk about:
- Model-based RL and Markov Decision Processes (MDPs—to their friends)
- POMDPs (Partially Observable MDPs) will be the hidden ones
- Dynamic programming, which is a nice recurrence relationship
- Model-free methods
- Temporal difference methods
- Q-learning, which uses the Q
- On and off policy
- Monte Carlo search

We'll do so many things! That'll be so fun. And then Monday, next Monday, we'll do deep reinforcement learning.

And we'll probably do policy methods—which you don't need to know because they're all letters that will make sense on Monday, next week.

## The Big Roadmap

Cool! So the big roadmap, which we're gonna go back and forth on, and this will all make much more sense by the end of Wednesday, is:

In your RL, you have two classes of options. Across the top, two are things where you have a **model of the world**—you're learning to estimate the probability of the next state given the previous state and the action:

$$P(s_{t+1} | s_t, a_t)$$

If you have a model, you can either then do an exhaustive search and say, "What if I did this? What if I did that?" because I have a full model of the world (which nobody does because it's too expensive).

Or I could do something which is **dynamic programming**, which we're gonna cover, which says, "Hey, look at all of my first actions I could take, see how well they end up, and then use that to recursively update my model and my policy."

Then we'll switch and talk about what most of the companies that I know are using, and certainly what is used in the language world (which is my world): **model-free methods**, which don't try and learn a model of the world.

They either use a value function or a Q function and try and learn a policy. Either we'll do **temporal difference** (which I'm going to show you in a second, a simple example of, before we come back and do it a second time), or they do **Monte Carlo**—the ones that play chess or Go randomly try a whole bunch of plays through the game and simulate what's gonna happen.

So this is a super quick overview which doesn't need to make sense yet, but you'll see this slide three more or four more times in the next 2 days.

## TD(0) Example: Temporal Difference Learning

Let's start with just a simple TD(0) (temporal difference zero) example. I want to work it in full to show the magic of how a value function works, which is sort of simple and sort of amazing that it works.

### Problem Setup

What we want to do is: given some policy $\pi$ (which says, given a state, here's the action you should take, or the distribution over actions you should take), find **the value of each state**.

There's actually not really any learning here yet. We're just simply trying to do this. But what we'll do is we will go back and forth:
- Given a policy, we will learn how good every state is under that policy
- Given how good every state is under the policy, we'll use that to learn a better policy

So this is a back-and-forth iteration—one of these dual optimizations. It's not EM at all, but it's another back-and-forth dual optimization, and it's gonna be, in fact, a gradient descent in value.

Under some simple conditions (which have fancy math names like **ergodicity**, which says you will visit all states an infinite number of times eventually), this will converge to the optimum—actually, the **global optimum**, which is sort of cool and sort of weird.

Of course, why do I hate algorithms that are guaranteed to converge to the global optimum in a case like this?

Why is this a horrible thing?

**Answer**: Could be overfitting, but in fact, I can regularize, and that's not a huge problem in these things. Amazingly enough, these 64 billion parameter models don't overfit terribly if you put a little L2 regularization. So overfitting actually is not the big problem.

But anything which is guaranteed to solve an incredibly hard problem? I can give you an algorithm that's guaranteed to solve an NP-hard problem. What's the downside?

**Answer**: Convergence! It's gonna take forever!

So there are all these great theorems. Anytime you have a theorem that says, "Look, this is guaranteed to give you an optimal solution to a really, really hard problem," you should go back and say, "Yeah, that's gonna take really, really long."

Which is why people do lots of hacks and approximations (like the contextual bandits) instead of solving the full problem, because it can be very expensive. But okay, in simpler problems, this works nice.

### The Algorithm

We're moving toward an iterative algorithm that says:
- Given a policy, find the value function
- Given the value function, find the policy

Or the same thing with Q—they're almost exactly all parallel. And I'm just gonna walk you through a super trivial example.

### Example: Mouse in the Maze

This is the classic "mouse in the maze" example. My version of it.

I have a discrete set of states. Each state here has a square, it has a label on it: A, B, C, Food, D, E. So I've got a set of states.

Each trial (so this is going to be discrete time—some RL runs forever, but lots of RLs are like playing a game where you have a fixed thing), we're gonna start the mouse (the robot) on a square, like square A.

The action space is to move to any adjacent square. This is the dumbest robot example I can do. We're in discrete space here.

If you end up going in the square labeled "Food," you get a Froot Loop. I was surprised—I talked to Barbara Sahakian here; she's a world leader on mice. Mice love Froot Loops, apparently. She feeds her mice with Froot Loops. Go figure.

And if they get in the "Shock" square, they get shocked and you end the trial. So it's like playing a game: you win with a Froot Loop, or you lose with a shock.

Make sense?

Initially, you got no idea what's good or bad, and we're gonna do a temporal difference model-free RL.

We're going to **initialize everything to zero**. The value $V$ of every state is zero. You can see here that we got a bunch of states (4 + 3 + 4, so we got 11 states), and they all have value 0.

I'm going to pop the mouse down somewhere, move through the maze until I exit, and I update the values of the squares as you leave them. We have to do a bunch of trials.

This is model-free—we're gonna have no model of the world. There is some sort of model: you're in some state, you take some action, there's a probability of ending up in the square you tried to move to (which could be 1 or could be not 1, because, hey, robots are imperfect, mice are unpredictable).

We good?

**Question**: We're gonna... this is an observable process, so we know what state we're in, and we pick the action?

**Answer**: We are the robot, the mouse, yep. So in this case, we're obviously doing a V version.

### The Update Equation

Let me do it by example, and then let's look at it. Here's the equation, which I will come back to, but let's just look at it in detail.

The equation that we're going to use for the update is: we're going to be in some state, call it $S$. Our current state has some value. This is our estimate of value—we never know the true value. We're in machine learning land, so we don't know the true value, but we have a function which is estimating it.

In this case, the function is just a table lookup. In a fancier version, the function of state would be a vector, and this would be a neural net.

But we have the value of the current state: $V(S)$. We're gonna take some action (go left). We will end up in some new state, which will have some value estimate $V(S')$.

We're going to look at the difference between the value of the new state minus the value of the current state:

$$V(S') - V(S)$$

Plus, we might have gotten a reward. We got a reward, we add in the reward $R$.

We're gonna say, "Hey, this is going to tell us how much to change the value of the preceding state."

And on top of that, we're gonna have a learning rate (because, hey, we're doing a gradient descent-y sort of thing, so there's always an arbitrary learning rate).

So the change in the value of my old state is gonna be equal to this:

$$\Delta V(S) = \alpha \left[ R + V(S') - V(S) \right]$$

Let's just do it before I take any questions and look at the actual case here.

### Walkthrough Example

Let's say we do a trial, and we start at A, and our policy takes us... I didn't even tell you what the policy was, but we end up in B, then we end up in C, then we end up in Food.

What are we going to look like?

**If we start in A and we move to B (from A to B):**
- The value of A was 0
- The value of B was 0
- The reward in A was 0
- There's **no change**

Reinforcement learning is based (and most learning is based) on **surprise**, in the sense of getting something you didn't expect. We thought, "Hey, the value was zero." And we took a step to B, we ended up in B, the value was 0, the reward was 0—no new information, no update.

With me?

**We go from B to C**: Same thing. No change.

**We go from C to Food**: Still no change. Value of Food: zero. Value of C: zero. Reward: zero. No change.

**Now, for the Food state**: We exit, we go back to our cage, we get a Froot Loop.

So now, what happens?

$$\Delta V(\text{Food}) = \alpha \left[ R + V(\text{Exit}) - V(\text{Food}) \right]$$

- The value of Food: 0
- Minus the value of the cage (Exit): 0
- No change so far, but we got **one Froot Loop of reward**: $R = 1$
- My learning rate is 0.5: $\alpha = 0.5$

$$\Delta V(\text{Food}) = 0.5 \times [1 + 0 - 0] = 0.5$$

I've now changed Food to 0.5. That's it.

The 0.5 says there's a learning rate $\alpha$ of 0.5, because, hey, I picked a learning rate of a half because it was just an easy number. So I got a learning rate:

$$0.5 \times (0 - 0 + 1) = 0.5$$

**Question**: Why do we get a reward for the Food step, not when we were at C? Because it feels like you're moving from C to get the reward?

**Answer**: You can do it however you want. The way I've set things up here is that you only get the reward when you move from the Food state to the outside, and you get the reward in the cage when you get it.

So you can set it up however you want. But very often what happens in most games or most shopping experiences is you only get the reward when everything is done.

In general, for this sort of reinforcement learning, it's very common (for discrete sequences) that the reward is **at the very end**, after everything is done.

Make sense? If it's an infinite time (which we'll cover in a little bit), you'll get rewards all the time as you go along. But in general, you get the reward for the preceding state, or maybe you have to pay a cost for the action—the action might cost you time or energy or something.

But in general, think of it as—it's arbitrary, but for this course, think of the reward as always being given **afterwards**, just as a cleaner way.

**Question**: So I only changed the value for a state $S$ after I've left $S$, gotten the reward for $S$, arrived in $S+1$?

**Answer**: Yeah. If, in fact, you take an action and you move from one state to another, this is an algorithm. You do what the algorithm says. It says you're in a state, you follow the policy. The policy is to take some action (which I didn't show you). The action then leads to the environment putting you in a new state (which I did show you). You were in A, you took some action, the action put you—you look up, "Where am I?" Maybe I moved to the right, and I'm in B. I look up what's in my database for B. Ask the neural net, "What's the value of B?" It gives me the value. I check to see what the environment rewarded me. That's all we're doing.

### Iteration and Backpropagation

The nice thing about this is sort of amazing: I can **repeat** this. I've now gone through one cycle. I've updated—at the end of this whole sequence, the last state is worth 0.5, all of them are zero.

If I do the same thing again, I follow the same policy, I walk through the same thing, I go boom boom, I'm gonna go from A to B—no learning. I'm gonna go from B to C—no learning. I'm gonna go from C to Food.

**Ah! I'm in C.**
- The value is 0
- I move to Food, the value is 0.5

$$\Delta V(C) = 0.5 \times [0 + 0.5 - 0] = 0.25$$

So $0.5 - 0 + 0$ (no reward) times 0.5—C is now worth **0.25**.

**I moved from C to Food, then from Food to Exit:**
- Food was 0.5
- Exit is 0
- Plus a reward of 1

$$\Delta V(\text{Food}) = 0.5 \times [1 + 0 - 0.5] = 0.25$$

So $1 - 0.5 = 0.5$ times $0.5$ (learning rate)—I add that in, so Food becomes $0.5 + 0.25 = 0.75$.

**So I'm learning gradually that the Food state is good.**

If I do this an infinite number of times, what's gonna happen?

**Answer**: 1, 1, 1, 1, 1, right?

So note that although this RL—the TD(0) is sort of misleading, but that's what it's called—it's a one-step (zero-based indexing). TD(0) is the smallest, most immediate reward you could do, and the one that's typically used.

Although I've only taken the reward for one step ahead, if I do this enough, I gradually learn—it sort of backpropagates. The information flows from the end state gradually back.

My robot does all this stuff, I get the reward at the end, and it's gonna take a long time because it only moves one node back, and it only moves gradually. But as I do this over and over and over again, it's gonna learn it.

If a certain fraction of the time I take the same policy and it ends up going into the Shock? Well, so be it. I will learn the thing isn't as good.

So we're learning things very much **in expectation**. Note that the world here could be **stochastic**. Most worlds are, if you're doing robots, or you're selling ads to people, or buying stuff—the world, the environment is generally stochastic.

But if you run it enough times, you're computing an **expected value**. You're summing over things, and if you keep doing it, it will converge to the expected value of this square under this policy.

And this is all for a given policy. It's sort of weird because I didn't show you the policy. But note that I'm not learning the policy—I'm learning the value given the policy.

If I run this a bunch of times, starting from a bunch of places, I might converge to something that looks like this:

- The places that are up here are pretty good because you're likely to end up with Food
- The ones that are down here are sort of bad because if you're at I, you have a non-trivial chance of ending in Shock, but hopefully your policy is to walk around the edge and go up here and sneak in to get the Food
- But if you might fall into the Shock by mistake (because you're an imperfect robot or a rat), this square is not as good as that one

### Key Point About Value

One question before I continue: Every time you look at the value of the state, it's the value **what**? What's implicit?

**Answer**: Under a policy!

It doesn't make any sense to ask "How good is the state?" It depends how good your policy is, and it depends what the environment is. But if you change the policy, you will change the value.

### Questions

**Question**: Earlier, when you calculated the value for Food, what's the value for the exit? Is it always 0 or 1?

**Answer**: In this game, I've said every time you exit, it gives a value of 0. The exit value is 0. There's no reason it has to be—you could give Froot Loops a third of the time randomly. In that case, the value would be 0.333.

If it got Froot Loops a third of the time, we're computing expected values with this algorithm.

**Question**: But then, like, when doing the very first example, wouldn't we be doing $0.5 \times$ exit value, which was just...

**Answer**: It was 0 minus V(Food), initially 0, and plus reward 1, that becomes 1. $0 - 0 + 1$ times the learning rate. The exit value is zero. The reward was 1.

So I've assumed that the exit state had a value of 0 and that the reward of that state you were leaving was 1. And again, I don't care which—I mean, pick whatever model you want. What I want is the algorithm.

Once you have the algorithm, then you estimate the pieces here. It's a gradient descent-y sort of algorithm. It could equivalently have said the exit has a value of 1 and there's a reward of 0—would have given the same answer.

**Question**: Why do we always call it TD(0)?

**Answer**: It's TD(0) because you're looking only at the immediate one-thing-back.

Shouldn't it be TD(1)? I can't help you. Ask Sutton or Barto, but Barto's retired. It's the nomenclature. This is—I'm giving you the jargon. You should know it. You don't have to like it, but you do need to know that this is TD(0).

TD(0) is: you update it immediately after the next step.

**Question**: Were you involved with prior... yeah, you could... there are lots of variations.

**Answer**: Once I have the value functions, then I could say, "Given these value functions, what's a good policy?"

### Finding an Optimal Policy from Values

If I want to be greedy (do exploitation—exploitation and greedy are equivalent, exploitation's a greedy strategy):

If you're in state A, what's the best action? You only get to see neighbors—all you see is B and C. It's a super local thing. What's the optimal policy in A?

**Go to B** (because B has higher value than C).

If you're in B, what's the optimal policy? **Go to C**.

If you're in C, what's the optimal policy? **Go to Food**.

I can create a policy given the values. If I'm able to look at the value of my neighbor, I can then tell you an optimal policy.

Now, is that the same policy it was trained on? **Not necessarily.**

If I follow that policy, will it give me the same values I have here? **Not necessarily.**

So note this **alternating optimization**:
1. I give a policy, I use that to slowly estimate the values
2. Given the values, I can find an optimal policy under those values
3. Follow that policy, and then I get a new set of values

And I'm going to state without proof that that will, in fact, under fairly reasonable conditions, converge to the optimal policy and the optimal values.

**Question**: Why would we update it?

**Answer**: Given the values from one policy, if you change the policy, in general, will the values change? **Yeah!**

So it is iterative. You gotta go back and forth: policies to estimate value, values to estimate policy.

**Question**: If the values are conforming with the policy...

**Answer**: Well, because the policy that I used here may not be the optimal policy. I started with some policy. We just picked some crappy policy initially to start the thing. I never told you my policy. I just made up some crappy policy. And then I followed it and found the values.

Because remember, this is a gradient descent-y sort of thing where I started all the values at 0, and I picked some sort of policy (whatever it was). The policy could have been "walk randomly"—that's a fine policy.

One policy says: flip a coin, and of all the possible actions (there are mostly two actions, some are three, whatever, four actions), pick one at random. That's a policy.

That policy leads to a value. And that value then suggests a better policy.

**Question about convergence**:

**Answer**: The convergence is somewhat surprisingly—as long as you're doing adequate exploration (which has funny math names like **ergodicity**, visiting everything often enough)—then, in fact, it will converge to an optimal.

### The Pseudocode and Generalizations

The pseudocode here uses $\alpha$ instead of $C$ for the learning rate (who cares about notation).

There is a possibility, which we'll come to later, which says that often you want to have a **discounted cash flow** or a **discounted value**, so getting a dollar next year is not as good as a dollar today. We'll offer a factor like 0.9 that says that a reward in the future is worth less.

In general, RL falls into either:
- **Discrete trials**: You run your robot for a period, and you come back and it ends and you get the reward
- **Things that run forever**: You're running a power center or a GPU farm. In the GPU farm, you're running the thing forever and ever. You want to optimize it using RL, but the value of the cost of energy and the value of not going down is different a year from now than now.

In the more general form, we will have a **discount factor** $\gamma$, which says that the reward one time step in the future is 0.99 of what it is now.

In the discrete world, it's standard to make $\gamma = 1$ (which is the one I did). If you're playing chess or Go, you mostly really don't care how long it runs—you just care about whether you win or lose. Whereas if you're running a data center, you really do care about this month more than a year from now. Those GPUs are all worth half as much anyway—I'll be onto a new job!

Good?

### TD(0) Equation

Cool. Okay, so that was TD(0). This is the equation, again, the same one that we just saw:

$$V(s) \leftarrow V(s) + \alpha \left[ r + \gamma V(s') - V(s) \right]$$

Which is TD(0), which is to iterate the value given a policy. It will then learn the value of every state in expectation under that policy.

Cool! We will later learn a bunch of ways to do that. Cool! So that was **temporal difference learning**—one step ahead, response to one action, model-free.

Good, so we got one little box there. That's good. We're fine on—I mean, I really am going to finish most of these slides on Wednesday, so this is totally fine.

## Markov Decision Processes (MDPs)

Now we're going to shift gears, and we're going to talk about **model-based** RL. I'm going to start with the Bayesian version, which is, I think, cleaner and easier to understand in discrete space, with the understanding that in the modern era, most people replace the Markov Decision Process with a neural net. But the idea is going to look really similar.

### Review: Markov Models

We need to remember what a Markov model is, which I covered really quickly in the Bayesian world. Then we'll do a Markov Decision Process.

In a **Markov model**, you start in some state. You have some probability function that gives you **emissions** $X$'s (things that you observe). If it's discrete, you're in one of $K$ states here, and you've got $V$ possible emissions (because I'm a language guy, so it's vocabulary—but whatever it is).

Then there is a probability of each of the $V$ tokens being emitted given each of the $K$ states:

$$P(x_t | s_t)$$

Or in general, this could be a function. It could be a Gaussian function if it's real. So given a state, we have a probability of emitting something that you see.

We have a **transition function** that says, given each of the $K$ states I could be in, how likely am I to be in each of the $K$ next states:

$$P(s_{t+1} | s_t)$$

And again, in a neural net world, this would be a real-valued vector and a real-valued neural net function that takes you to the next state.

**Question**: Whatever the outputs are, like, why do they need to be specific? Why are they specified for S1? Why does B always need to be the same across everything? Why does B need to be the same?

**Answer**: The assumption (and this assumption comes in almost all these models) is: **the model doesn't change over time**.

You're learning a model—here's a model of the world, a model of the environment. And the model of the world is the same. The state changes, but if you got your robot and it's going from one state to the next state, it only depends upon the current state and secondly on the action you take.

The model doesn't change over time. The emissions don't change over time. You're gonna learn a model of the world.

The world is **stationary** in this technical sense—stationary in the technical sense that the transition functions are the same always.

And it's also **Markovian**, in the sense that (at least by assumption of our model), if I'm in state 3 and I want to know what I'm going to see and where I'm going to end up next, it doesn't depend upon state 2 or state 1:

$$s_4 \perp s_1, s_2 | s_3$$

$s_4$ is conditionally independent of $s_2$ and $s_1$, given $s_3$. The world is Markovian.

It's the model representation. Now we're going to modify the Markov model.

Remember, if it's a **Hidden Markov Model** (HMM), you don't see the state. But for the moment, assume it's a Markov model, so we do see the state.

### Markov Decision Process (MDP)

With a **Markov Decision Process**, in each state, we're gonna pick an action. We're in a state, we're gonna pick an action. Think in discrete space for the moment, which means now if I have 10 actions, I will have **10 different Markov transition probability matrices**.

For each possible action, for each possible state, I will have transitions. So this looks like a **tensor**—it's $K \times K \times |A|$ (where $|A|$ is the action space).

For each action, you have a transition matrix:

$$P(s_{t+1} | s_t, a_t)$$

If I'm in a state and I go right, I've got a probability distribution (hopefully I mostly go right, but sometimes I don't). If I'm in the same state and I go left, I've got a different probability distribution over things. If I'm in the state and I say "hover, standing still," different transition—hopefully has a high probability of staying still, but there could be wind, could be drifting a little bit.

So the **Markov transition matrix** generalizes to the Markov Decision Process where you now have added in an action.

If we're going to learn a model of the world, we're going to need to learn these functions—these Markov transition matrices and the emissions.

**Question**: K is the dimension of the state space?

**Answer**: Yes. The hidden state or the latent variable. I will use "hidden state" and "latent variable" fully interchangeably. So $K$ is the dimension of the hidden state, the embedding of the hidden state. $V$ is the dimension of the discrete emissions, of the emission state.

I picked $V$ just because I think of it in terms of vocabulary. My world tends to emit tokens, so $V$ is the number of possible tokens you could emit.

But remember that when you're emitting something, the hidden state is entirely separate dimension from the emission state.

Make sense? One says, "What do you use to describe your robot in terms of what it knows about the state?" (which is the latent state and the position of it, and the altitude, and whatever the rotation). And the emission state is whatever information it conveys to you that shows up on your sensors.

**Question**: Is emission strictly dependent on action?

**Answer**: Do emissions depend upon actions? No, in general, the way this model works is the emissions are only a function of the state:

$$P(x_t | s_t)$$

And the action changes the transition here:

$$P(s_{t+1} | s_t, a_t)$$

So if you look at it, the state transition matrix (how you go from one state to the next one) depends upon the action. But the emissions of what comes out do not—they depend just upon the state.

**Question**: What are the emissions?

**Answer**: The emissions are the outputs—they're the things you can see. In my world, it's what GPT types out to me—that's the emission. The hidden states are the hidden embeddings.

It emits, at the end of the day, some token.

### Hidden vs. Observed

Cool. You can also have these be **hidden**. If you have a Markov model where you don't see the state, it's called a **Hidden Markov Model** (HMM). And a piece of jargon: if you can't see the state of a Markov Decision Model, it's called a **Partially Observable Markov Decision Process**, or a **POMDP**.

Which I love—POMDP sounds great!

So, fine. You could either observe the state, or you don't.

### MDP Applied to the Maze

Cool! So what does this look like in the maze world we have here? The state is where you are: A, B, C, D, E. You have actions—maybe you go up, down, left, right.

Of course, if I'm in A and go up, where am I likely to transition to? Probably back to A with high probability, right? But hey, you know, maybe the action space says "invalid action"—it just says "bump," can't do it, leaves you in A.

You might exclude them and only have the action space limited to possible actions, so you have different actions possible in A than in B. B can go left and right. A can go right and down.

You have a transition where you go there, and we have rewards. So we can write this environment as a Markov Decision Process where, based on what state I am in and what action I take, I have a probability of ending up in each of the neighboring states.

If I'm in B and go right, what's the chance I end up in H? **Zero!**

In general, these things are super sparse. In general, given most states you're in, lots of discrete ones are just not accessible. It's fine—it could be zero.

### Formal Definition of MDP

Cool. So now we can get super mathy, but it's really... I'm just gonna remind you what the math is.

The **Markov Decision Process** is a 5-tuple:

$$\text{MDP} = (S, A, P, R, \gamma)$$

- $S$: A set of states ($K$ possible states)
- $A$: A finite set of actions
- $P$: Probability of the next state given the current state and action: $P(s_{t+1} | s_t, a_t)$ (the world model)
- $R$: A reward, which in the most general case could depend upon the current state and the action and the next state: $R(s_t, a_t, s_{t+1})$. But mostly, we just make it depend upon the current state: $R(s_t)$
- $\gamma$: A discount factor between 0 and 1, which says that the next reward is worth $\gamma$ times what it would be now if I had it now

### The Return (Long-Term Reward)

The final thing I need to do, which is the new one, is to say: **What actually is the long-term return?**

In general, the thing that I'm trying to optimize for is: I want to find some policy $\pi$ (a policy is a mapping from state to action, or state to a distribution over actions):

$$\pi: S \rightarrow P(A)$$

such that I maximize **the return**.

What's the return? The return is how much reward I get at the next time, plus $\gamma$ times how much reward I get two times in the future, plus $\gamma^2$ times how much return I get three times in the future, plus... out to infinity:

$$G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}$$

This is what I would call the **discounted cash flow** if I were at Wharton. But I'm not—I'm in reinforcement learning, so this is **the return**.

Make sense?

**Question**: Why do I put a discount?

**Answer**: If I had $\gamma = 1$, this whole thing goes sort of bad because now a reward infinitely far in the future is just as valuable as a reward today.

If you think about it, imagine I were Google, and I'm showing you an ad. I don't want to maximize the return I get on the ad right now (which is, you know, 1.2 cents—you're worth 80 bucks to me, or 100 bucks, or something). I want to maximize the future—all the future clicks I get from you. But not for all eternity. You'll die someday, and to be honest, a click 5 years in the future to Google is not worth a penny today. It's worth less. How much less? That's the $\gamma$.

So, again, think of either something which is an infinite series (in which case, to make it stable and bounded, it's gotta decay exponentially: $\gamma, \gamma^2, \gamma^3$), or if something is only 100 steps long, you're gonna make $\gamma = 1$ for the first 100 and then 0 for all future time.

If you're playing a discrete game, then fine, make all the gammas 1—it's gonna end in a finite time.

If you're playing an infinite game, which has nice mathematical properties, and next class we're gonna actually use recurrence relationships, the recurrence relationships I'm going to prove next class are going to rely upon the relationship between $G_t$ and $G_{t+1}$.

Because we're going to be at time $t$, we're going to take an action, it'll be time $t+1$, and I need to be able to do a recurrence relationship. This particular equation will form—not just make sense if you're an economist, but actually makes the math really beautiful for taking the difference between $G_t$ and $G_{t+1}$.

So bear with me, but you really should understand: discount factors are the right thing to do in any infinite game. That click you're gonna get in 100 years is just not worth that much.

**Question**: What do you choose for the discount factor?

**Answer**: Totally depends upon your game. If you're looking over years, what's the current discount factor? What's the rate of inflation in the US right now? If it were 5% inflation right now, what would a reasonable discount factor per year be? **0.95**.

But it depends upon—and we're always in discrete time—in RL, the standard methods are all discrete time. We're having tokens, we're doing discrete decisions. But it depends on your problem.

### Summary of Components

Okay, so we've got:
- A state
- A value in a state
- An action, which is a function of a policy $\pi$
- A discount factor $\gamma$
- A reward $R$, which is simply the reward of the state (usually)
- The expected discounted reward, also called **the return** $G$

And then finally, we have (or may have or may not have) a **model of the environment**, which is the Markov transition matrix in the Markov Decision Process: $P(s_{t+1} | s_t, a_t)$.

Great!

### Generalization to Continuous Spaces

Let's just note that I've done things with little mice and mazes, but I could make the state be a vector—a real-valued vector. I could make the action be a real-valued vector if I'm controlling a robot (it would be commands I send to my robot).

I can make the value of the state be a mapping, a function that takes the state vector and produces a scalar:

$$V: \mathbb{R}^n \rightarrow \mathbb{R}$$

The value is always a number, a real number.

And I could make a probabilistic model. In the neural net world, people mostly learn a deterministic model that says: given a state and an action (which could both be real vectors), map them to the next state. That's a neural net:

$$s_{t+1} = f_\theta(s_t, a_t)$$

So all these things work in neural net land. I don't really care—I'm gonna do discrete for most of the week until next week. But they could be neural nets.

## Value Functions and Q-Functions: Formal Definitions

Cool! Finally, because we're moving toward the end of today, you have the policy. The **value of a state**—and if I think about the value of a state, the value of a state should really be subscripted with the policy, because value makes no sense without knowing what policy it's under—is formally equal to:

$$V^\pi(s) = \mathbb{E}_\pi \left[ G_t | s_t = s \right]$$

The expected value (the expectation is over following the policy) of the return, given that I'm in the state $s$.

This is pretty much what I said before, but I'm just being super formal about it.

### Q-Value Definition

And let me now just say the same thing for the Q-value.

The **Q-value**, also under a policy, of a state and an action (because Q is a function of state and action):

$$Q^\pi(s, a) = \mathbb{E}_\pi \left[ G_t | s_t = s, a_t = a \right]$$

is equal to the expected value under that policy of the $G$ value starting at that time, given I'm in the state and I take some action.

So it's a function of $G$ given a state and an action.

I've written this here a little bit more formally. Statisticians like to have random variables that take on values. Computer scientists tend to just write it as a function of state and action. But if I were a statistician, this is how I'd write it.

And I've just plugged in—but I won't write it on the board—$G$ is the discounted future reward:

$$G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}$$

Going forward, $G$ is the return. So we now have a formal definition of $V$ and $Q$ under a policy.

We have it for the infinite time with a discount factor.

### Optimal Policies and Values

We can also then say, **what's the optimal policy?** (which I'm going to use a star on):

$$\pi^*(s) = \arg\max_a Q^*(s, a)$$

So $\pi^*$, the optimal policy, is: given the optimal Q-value, pick the best action—argmax over $a$ of $Q^*$.

And I can certainly define what an optimal value and an optimal Q is.

The best $V^*$ is: pick the best policy that maximizes $V$ given the policy:

$$V^*(s) = \max_\pi V^\pi(s)$$

So all these things we're trying to maximize. $Q$, and the star says: if you got the right—the best Q-value—pick the best action under that.

$V$ says: what's the best value? It's the one that uses the best policy.

What's the best policy? Pick the best action if you have the optimal Q-value.

What's the optimal Q-value? Well, find the best policy for the Q-value.

This is all sort of vacuous, but it's very cleanly formal.

### Next Class Preview

And it will allow us on Wednesday to derive the **Bellman recurrence relationship** that provides not just clean math on this, but in fact, a contraction mapping optimization that allows us to optimize all these.

And on that happy note, I will stop and I will see you all on Wednesday.

---

## Post-Lecture Q&A

I will put up the... [trying to pull up slides]

**Question**: Is there one for the policy and one for...?

**Answer**: Yeah, so you get the... Oh, what? We're not waiting for the slide showing? Oh, yeah, yeah, yeah. One section, sorry. Anybody who wants to look at the slide there, then I'm gonna go back and get the optimal section.

So let me stop and show the optimal policies section.

**Question about optimal V and optimal policy**:

**Answer**: One version is we've known the optimal $V^*$ is a fact. And so what I was... Yep, yes, no...

**Student**: What is the thought? Your argmax...

**Answer**: Yeah, sort of like... Sounds good. The best $V$ is the one that gives you the best policy. And the best policy is the one that gives you the best $V$.

What's gonna happen? I'm gonna go next class, I'm gonna show the whole thing as an iterative fixed-point mapping. It's an iterative algorithm. It goes back and forth, making the value better given the policy, or the policy better given the policy.

**Question**: Assuming you reach a point where the return for changing the policy is so small that you're like, "Okay, at this point..." Is it still guaranteed to converge?

**Answer**: It's a gradient descent, and at some point, there will be no more changes. The slope will go to zero. And there will be no more changes. It happens exponentially, but asymptotically, you'll come to a point where there's no way to make the policy better.

---

**End of Lecture**

---

# CIS 5200: Machine Learning - Lecture 23: Reinforcement Learning: Bellman Equations and Q-Learning

## Introduction to Key Concepts

And we have the return...

Again, this is a technical term you should know: the **return**, which is the expected discounted reward from the future.

And we sometimes have an explicit world model, right? A probability of the next state, given the current state and the current action.

And we said before, and we're going to now move toward the math behind this, that in general, the value of a state $S$...

The value is always the value under some policy $\pi$, right? It makes no sense to ask "what's the value of a state?" without specifying a policy. A state only has a value with respect to a policy.

### Value Function Definition

The value function is defined as:

$$V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$$

where the expected value is taken under policy $\pi$, following the policy.

The return is the expected return given that you're in the current state, right? And the return, again, is:

$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

Right? So at each discrete time $k$, we multiply the reward $R$ by $\gamma$ raised to the power of that time step.

Are we good on nomenclature?

### Q-Value Function

Right? And the Q-value is the same thing, but instead of having it be just the return based on following the policy from the current state, what you do is assume you're going to take a specific first action, and then follow the policy afterward.

$$Q^\pi(s,a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]$$

Cool! Go away, my phone is trying to talk to me.

## Example: Understanding V and Q Values

Um, good! So just to check: what is $V(A)$? What number up here? 0.812!

$R(A)$ is what? Zero! There's no reward in that state.

$Q(A, \text{move to D})$ - we're in state $A$ and we try to move to $D$. What does that say? It says we're in state $A$, the action is "move to D." What's our expected return, then?

0.76, right?

Make sense? $Q$ says you're in one state, you take a specified action. Now, if in fact the action really does take you to $D$, okay, so I've made a little assumption here. I can't actually tell you the exact value of being in $A$ and taking the action "move to D" because the action "move to D" doesn't necessarily take you to $D$.

Make sense?

The action is something you choose, or your agent chooses, your AI chooses. The actual state you end up in is, in fact, determined by the environment, which I haven't told you here.

But, roughly, $Q(A, \text{move to D})$ is going to be 0.76.

Um, what's the optimal policy to follow under $A$?

### Clarifying Q-Values

Can I explain? $Q(A, \text{move to D})$... what's the question?

**Question:** Is $Q$ a difference or a value?

**Answer:** It's a value. $Q$ is not a difference between values. $Q$ is a value. It's an expected return. It's the expected return if you take an action "move to D", and then afterwards follow a policy.

$Q$ is sort of like the next $V$, right? We'll see the math a little bit.

What's $\pi^*$? What's the optimum policy under $A$?

Go to $B$.

### Why Isn't V(A) Equal to 1?

Why might $V(A)$ be less than 1 in this picture?

I mean, it looks like I can follow a nice policy: go to $B$, go to $C$, go to food. Why isn't the value 1 after convergence? This is converged.

**Student:** Yep.

**Instructor:** No, it could be in general because we discount future rewards, but the model we did here is discrete time with no discounting. So this one, this model has no discounting in it.

There are more possible routes, but if my optimal policy is... well, hold it. Did I tell you what the policy is? I didn't tell you the policy. The policy could be, you know, go right with probability 0.9, go down with probability 0.1. So it could be the case that the policy is not the optimal policy, right? There's some policy here, but I didn't say what the policy was, or...

**Student:** Even if I follow the optimum policy?

**Instructor:** Well, not the reward state, no, but if I go from $A$ to $B$ to $C$ to food... If it were a deterministic policy and a deterministic outcome, it would always get 1, the value would be 1.

### Stochasticity in the Environment

You don't know if the action actually does the thing you want it to do. You tell the robot, "go 1 meter left," and it goes 0.95 meters left. Whatever, right? There's no reason to think that the world is deterministic.

There are lots of reasons why worlds are stochastic. And most worlds are stochastic. You don't know what a human will do for sure, and even your robot's approximation of the action has uncertainty.

Cool! Okay.

## Bellman's Equation

End of intro.

What I want to do for the first main piece of today is to walk you through Bellman's equation. This was developed by this clever guy at Berkeley in the '60s, who then died young, very sad.

Bellman's equation is a recurrence relationship, so we're going to walk you through the totally trivial math, which is all up here, and then we're going to use this actually as an update rule.

### Derivation of Bellman's Equation

So, Bellman's observation is that the value of a state $S$ under a policy is equal, by actual definition, to:

$$V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$$

And we can write it as the expected value under that policy of the return $G_t$, given that we're in state $S$, right? Formally, $S_t = s$.

And we just write it as the expected value. I'll write two steps here at once.

The expected value under $\pi$ is the sum over the possible actions of the probability of the action given the state, right? So that's the expected value part. We're taking the expectation over outcomes.

$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a) \left[ \cdots \right]$$

And we also need the expectation - a summation over next states and rewards of how likely it is we'll end up in some new state $s'$ and we'll get some reward $r$, given that we're in the state and we take the action.

So now we have a whole... This is summing over what actions we might choose to take and where we might end up and how much reward we might get.

And then we're going to sum up... what are we going to sum up? The return. What's the return? The return is:

$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

Right? So we're going to sum up what is the return, how much do we expect in the future?

### The Key Trick: Splitting the Summation

So let's write that... which piece do I want to write up here? I'm sort of trying to not copy all the lines at once.

Let's just look at this line before I write it out. This is $G$, right? $G$ is just the summation of the future returns.

And now the trick that we're going to pull to do the recurrence is we're going to break up the summation from time 0 to infinity, right? From right now to infinity, into two pieces.

The first piece is the return we get right now at time $t+1$, the next time step. And the rest of it is going to be all the future returns.

Make sense? The initial one was starting at $t+k+1$, right? So before we had the return starting at time $t$, which is the reward at $t+1$, $t+2$, $t+3$...

Now we're going to start the summation. Here's the one that's the reward at $t+1$ times $\gamma^0$, plus $\gamma^1$ times the future return.

$$G_t = R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+2}$$

Everybody got this trick? Because this is the core of the recurrence. And the core is to say, the expected return is... That's the whole trick of the Bellman equation.

Make sense? And we're going to do this in a second.

### Completing the Derivation

Okay? Um, whoop. So, if you look at this, yeah, look at this one, right? This looks like $G_{t+1}$ given the current state, but it's not quite right. So we're going to now pull this apart. We have the expected value here of...

This is the expectation part, now I'm putting this part here. We get the reward at $t+1$, which is $R_{t+1}$, right? We got a reward, this is the $R$ here, so this $R$ here is the reward at $t+1$.

Plus $\gamma$, plus $\gamma$ times the summation... But here it is. We can write it out. The summation from $k=0$ to infinity...

Ah, but what do we have here? So we have this piece, we have a $\gamma$ in front, right? A $\gamma$ in front, we have the... let me drop that. We have the expected value over $\pi$, right?

$$V^\pi(s) = \mathbb{E}_\pi\left[R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+2} \Big| S_t = s\right]$$

$R$ plus $\gamma$ times the expected value over $\pi$ of summation $k$ equals 0 to infinity of $\gamma^k$ times $R_{t+k+2}$, given that $S_{t+1} = s'$, right?

Given $S_{t+1} = s'$.

So, let's look at it up here where it's easy to read. This piece here says I've got the immediate reward on the next turn, $\gamma$ times the expected value of the future. The future has $\gamma$ starting at zero now with the future rewards. And the state starts at the next state, not the current state.

### Understanding the Reward Function

**Student:** Yeah.

**Instructor:** The reward function we're considering here in this sort of general formulation is: given a state and an action, there is... and you could break these up and decouple them. There's a probability of the next state given the state and the action, which is the world model, and there's the probability of the reward given the state and the action, which is the reward function.

Right? So in this case, in this generality, the reward is a function of the state and of the action you take.

Well, I'd be... I'd be saying the... You say the same reward in the end, I'm not sure. There's the return... reward... use "reward" for a one-time shot. Reward is: I'm in the state, I take this action, here's the reward.

Return is the expected discounted reward, right? So let's try and keep the nomenclature clear. You might think of return as reward, but I think that's a confusing nomenclature. It's not wrong, but think of return as the expected discounted stream of rewards, and think of reward as what you get at one time point, right?

**Student:** Yeah.

**Instructor:** The small $r$ here is the reward I'm getting at time $t+1$, right? And this is two things combined into one equation here, which you might want to break apart. One is the world model: $p(s'|s,a)$ - the probability of the next state given current state and action. How does the world transition? And the other one is the reward I get, which could be probabilistic in general, which is a function of the current state.

It could be a reward for the state, and there could be a reward for the action. And again, most actions in my world have a cost. They cost you energy or time or money.

$\pi(a|s)$ - what's $\pi$? It's the policy. It's the probability distribution over actions given state. That is your policy. It's how likely are you, your agent, your AI, to take each of these actions, given the state you're in?

### Clarifying the Recursion

**Student:** The current step is not based on probably the next state. This says probably the next state given the current state.

**Instructor:** So you start with... we're in state $S$, we're going to transition to a state $s'$ with some probability, which depends upon our action.

And our return can be written recursively as two pieces, and let's look at it. There's the expectation, right, the expectation over $a$, over the next state, over the reward, of the reward you get right now plus $\gamma$ times...

Oh, look at this term, this is $V^\pi(s')$!

### The Core Insight

So hang on one second, I want to say this twice, because this is the core piece. I've written the reward of the current state as an expected value that is a function of the reward of the next state I end up in, right?

And this is pure math. In the sense this is a tautology, in the sense that all math is tautology. It's just an equation that says you can, because of the way the exponential discounting works, you can write the value of the current state as equal to a certain expectation of a current reward plus $\gamma$ times the value of the next state you're in.

$$V^\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s]$$

And we're going to use this practically. This is the basis for pretty much all of RL.

But this is, so far, not an update rule. It's a math statement.

**Student:** Well, I would say what I said. We're saying that the value of a current state under a policy is identically equal to the expected value of the reward you get plus $\gamma$ times the value of the next state you end up in.

**Instructor:** That's what I'm saying.

And what I'm going to then... yeah, yeah. This is a recursion, but note at the moment, all this is is a mathematical identity.

Now what we're going to do in a second is we're going to replace the equal sign with a colon-equal, a GETS sign ($:=$). And that will then be a recurrence relationship, and will, in fact, be a contraction mapping and sort of a gradient descent.

### Identity vs. Algorithm

So at the moment, this is an identity, right? This is just... when I say it's just math, what I mean is it's not an algorithm. It doesn't tell you... there's nothing up there that says $V(s)$ gets this value.

At the moment, it's just an equation. That make sense? These things are equal.

But what we're going to do is we're going to replace the equal signs with something that is now an iterative algorithm. So this equality will be something that we will converge to. And when we converge to it, it will be true. Well, it's always true, right? But it will be something that we're going to be converging to.

That make sense? I'm just trying to be formal between the equality and the convergence.

**Student:** Yep?

**Instructor:** Yes, because there are... here is... Look at where this $R$ came from. This $R$ came from: I'm at time $s$, I'm at time $t$, I take an action, I get an $R$, right? So this is the $R$ at time $t+1$.

And I haven't bothered to put a $t+1$ on it, because it's a dummy variable that's summed over here. But the $R$ is, in fact, the reward that we're getting at time $t+1$.

So if you like... in fact, here it is. Up here, I called it $R_{t+1}$. Make sense? So this $R$ is the value of the reward you get. You're in state $S$, you take action $A$, you transition. As you're transitioning, it gives you a reward.

Make sense? So that is the reward you get leaving state $S$ and taking action $A$.

Cool! Um...

### Q-Value Bellman Equation

It's an identity for every policy $\pi$, except for the $\pi$. You can write the same thing for $Q$. If you think about what happens under $V$, $V$ has an expectation over possible actions over the policy.

$Q$ says, drop the policy for one second and just pick the $a$. Tell me the $a$ you're doing.

And so there's a special case of this equation, which is $Q^\pi(s,a)$. I say, don't follow the policy, just plug in $a$. So now I get the exact same equation here, down here, which is... I got rid of the expectation over the policy, plug in $a$.

$$Q^\pi(s,a) = \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s, A_t = a]$$

Right? So that's $Q$ over $\pi$. It's the same thing, but take $a$ rather than following the policy. And then in the future, of course, I'm going to follow $V_\pi$.

It's a special case. You can also trivially say that if it holds for this, it will hold for some optimal policy if you happen to know the optimal policy.

### Optimal Policy

And what's the optimal policy? Right? What's the $\pi$ that's optimal? The optimal policy is: pick the best action.

And the formal way to say "optimal policy, pick the max action," is the maximum over $a$ of possible actions you could take of this.

$$V^*(s) = \max_a Q^*(s,a)$$

**Student:** Sorry?

**Instructor:** No, because I've replaced... if I follow the... there are 3 options. Option 1: I follow the policy, I pick $a$ with some probability based on $s$.

Option 2: I do a Q-value, you tell me the $a$, I'll do whatever $a$ you want.

Option 3: I'm going to be optimal, and optimal means I pick the argmax over the policy, what's the best action to take, right?

$$\pi^*(s) = \argmax_a Q^*(s,a)$$

So they're all different things. Follow policy... and all of them, after the first step, you do, right? Follow policy, pick any action, pick the best action, and keep picking the best action.

**Student:** Yeah.

**Instructor:** The recursion goes on forever. So this model, to make the math clean, is an infinite series because I've got to shift it by one, and the nice thing is infinity stays equally close when I shift by 1, right? So the whole thing relies on this exponential discounting that lets you shift it by one time point.

Cool! So these are just sort of variations that are not so important, um...

## Temporal Difference Learning (TD0)

But the piece I want to pull back to is I talked about TD0 last time. And TD0 is that algorithm, that update rule, which says, hey, replace your value at some state with the old value of the state, and there should be a... If I were nice, I'd have an $S_{t+1}$ and $S_t$ there, right, to make it prettier, but I didn't here.

$$V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$$

With a learning rate $\alpha$ times the reward, plus $\gamma$ times the value of the next state, minus the current state.

If you squint a little bit hard, but we're going to try and do this a little more formally, this looks like an empirical version of the other one.

### Bellman vs. TD0

So what's happening in Bellman's equation? In Bellman's equation, we're summing mathematically over the expected value of following the policy.

When we do an iterative algorithm, we're sampling the real world following the policy, right? So an expectation, grabbing discrete... I keep trying the policy, I follow it a bunch of times. Overall, I'm going to end up following that policy. Overall the world is going to follow the state transition matrix and estimate the $s'$s and $r$s, so I'm sampling from the world.

And as this converges, what is this going to look like? It's going to look like the value of the next... of a state is equal to... Well, what's it equal to? It's equal to the reward you get after you take an action in that state, plus $\gamma$ times the value of the next state you end up in, right?

So it looks a lot like it's doing the same sort of thing. Here's the reward $R$ here, here's the $\gamma$ here, here's the $V_\pi$, this is $V_\pi$. I didn't put the $\pi$, but there's a $\pi$ under it of $s'$.

This is, in fact, your future return, and this here is your future return. And so what we're doing here is this TD0 is a sort of recurrence relationship that is trying to move us toward something that satisfies this equality.

So, when this thing converges, we will have something that is, in fact, satisfying Bellman's equation.

That's a little hand-wavy, but I'm going to actually show Bellman's equation again and give a little more detail.

### Understanding Convergence

Does that make sense? That there's... that the key here is, in both cases, the value at the state is equal to either the theoretical expected value, computed using $\pi$ and the world model, or the empirical distribution by just sampling from the world and actions and the policy.

Expected value of the current reward you get immediately, plus $\gamma$ times whatever the future reward is you're going to get.

**Student:** Yeah.

**Instructor:** This is one instance, and if you do it a bunch of times, you will be, in fact, computing this expectation. Now, they're not quite the same, because this is obviously sort of gradient descending with a convergence rate, and I've written it here with an equality. But I'm going to replace this equality in just a second with a little arrow, so it's going to make more of the same.

**Student:** Sorry?

**Instructor:** This is, in some sense, this is saying if this thing converges, this whole part should go to zero, right? One way to think of it is once this TD0 is converged, $V(S_{t+1})$ equals $V(S_t)$. And it's converged when this part is zero. And what is this part saying equals zero? In expectation, $V(S)$ should equal $R + \gamma V(S')$.

And this is saying, hey, $V(S)$ should equal, in expectation, $R + \gamma V(S')$, right? So, driving this term to zero is what we're doing with our gradient descent. We're learning to make this term small. Drive it to zero, and eventually the learning thing converges, and this term goes to zero.

And that's... this thing is saying this term should be equal to that term in expectation. They're both doing the same thing.

Monday, we're going to go and we're going to replace this term with a deep learning model, and we're directly going to gradient descent it, and we'll do a Q-learning version of it. So all these things are trying to make $V$ or $Q$ satisfy this equation.

Cool.

## Dynamic Programming

So, what I want to now do is to say, how do we start to use this in an iterative algorithm? And this is dynamic programming. Bellman's equation is dynamic programming.

And to actually use this in a model-based world here, remember there were two models: model-based or model-free, and I'm going to take the next half hour and be model-based.

### Model-Based vs. Model-Free

Bellman's equation is showing you the model-based version. Although, notice it's sort of funny. This is model-based - there's the model. This is model-free - there's no model, right? TD0 is model-free.

Bellman's equation is model-based. If I have the model, I can take the expectation over it. If I don't have the model, I take the expectation by just sampling from the world, and the world provides the distribution of the model.

Does that make sense?

**Student:** Yeah.

**Instructor:** What's the model? $p(s'|s,a)$ is a model. By definition, that's a model of the world. Given a current state and action, what state am I in?

There is no $p$ over here. There's no model of the world. The world serves as its own model; it generates the $s'$s for you.

So I want you to walk away from this 3-day session saying, hey, there's a world where you choose to have a model explicitly, and there's a world where you let the actual physical environment provide the model. They look really similar, and they all get solved by a recurrence equation that looks like Bellman's equation.

### Policy Iteration

Okay, I'm going to push forward. So, to do dynamic programming in the model-based world, I'm going to iterate back and forth between two different kinds of gradient descent.

One is, I'm going to try and find $V$ given $\pi$, which is what we've just been talking about. And the other one is, given $V$, I'm going to try and improve $\pi$, right?

If I can do something that is guaranteed to give an increasingly better estimate of $V$ given $\pi$, and something that's guaranteed to give a better $\pi$ when it increases return, given $V$, then the whole thing, I go back and forth, is going to converge.

Subject to some exploration, you try enough stuff, and you look long enough. Invoke ergodicity. Okay, so...

### Value Iteration Update Rule

Too many words here, but what we showed before was something that looked like Bellman's equation, but I've now replaced it by just putting in a dummy variable that says that I've got $V_k$, so I'm doing discrete time iteration. I'm learning $V$. I have an initial $V$, and at each iteration I'm going to subscript $V$ with $k$, I'm going to be updating $V$.

So, I've got $V_k$, and then I'm going to update $V$ to get $V_{k+1}$, right? If you want, I could put a little arrow here to get... But I've now been more precise. This is exactly my Bellman's equation from before.

$$V_{k+1}(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V_k(s')]$$

And what I'm saying is, hey, rather than saying this is true, I'm going to say, let's use this as a recursion to update it.

So what do I do? What I do is I'm going to start with a function $V$ that says, here's the value of every state. Or if it's a continuous state, here's a neural net that maps from a state vector to a value $V$.

I then can... if I have a model and a policy, I can now compute the expected value over the policy and the expected value of the returns over the world of $r + \gamma$ times what my current model says the next state will be valued at. And that gives me a new value function for my $V$, right? So I've updated the function $V$. I've updated my value function.

### Convergence Properties

And I'm not going to do the proof, but Bellman's equation I just showed you says that $V_k = V^\pi$ is, in fact, a fixed point of this update rule. I think that's trivial, and it will converge to it, which is maybe not trivial. But it does, right?

So, this is an update rule. Instead of doing an empirical sampling like TD0 - try an action from the probability distribution of the policy, see what happens, do a little gradient update, repeat, repeat - this says, compute over all possible next actions, compute over all possible next states, and then take that expectation, one step at a time.

**Student:** Yeah.

**Instructor:** There's no actual gradient here. This is not a gradient algorithm, this is a contraction mapping in a fixed space. It actually sits in a Banach space, not a Hilbert space, which you don't need to worry about. But, so it's not technically a gradient.

But what it is doing is a monotone improvement. So it's going to take you monotonically... it's "gradient" in the sense... there's no gradient. But it's "gradient" only in the sense that it's moving you monotonically in the right direction.

And so the proof, which I'm not showing you, says if you always move in the right direction, and there's a finite distance to get there, you always get to the fixed point, right? So he did prove there's a fixed point. There's some point which is the definition of the same, and then I'm claiming that if you keep moving toward it, you'll end up at it.

Make sense? So "gradient" is... I'm being a little bit metaphorical, thank you.

**Student:** Yeah.

**Instructor:** How is this different from...

### Bellman Equation vs. Update Rule

Bellman's equation says that $V^\pi$ is equal to $V^\pi$. It says the two $V$s are equal at some fixed point.

This is an algorithm, not an equality, right? It puts in... this says, take your old guess of $V_k$, and use that to compute the new one, right? So I would write this with a colon-equals.

$$V_{k+1}(s) := \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V_k(s')]$$

Make sense? $V_{k+1}$ gets this other piece here.

Make sense?

So, Bellman's is the... What I'm saying is, if you take this iterative algorithm... makes sense as iterative. Given a $V$, this gives you a new $V$. If you keep running this long enough, it will converge to Bellman's equation.

Make sense? Bellman's equation is the limiting... like, fancy word is a fixed point of a recur... this is a recurrence relationship. A recurrence relationship is one that takes $V_{k+1}$ as a function of $V_k$. It's a recurrence relationship.

This recurrence relationship is, in fact, I'm not proving it, a contraction mapping. It will move, and it will converge to Bellman's equation, which is a fixed point, right? At that point, Bellman's equation has no recurrence. It's the exact same thing on the right and the left.

Make sense? So this is, in some sense, a much cooler thing, which says, this is how to get there. Bellman's is where you end up. This says, take this thing and you'll get there, right?

And again, the intuition is, if you have a model of the world and an explicit... well, I always have a policy, because the policy is what we're learning, mostly. But if you have a model of the world, you can sum over all the possible policy actions and over all of the possible things the world could do, and that averages a whole bunch of things. In some way, that's much more efficient than grabbing the world and actually trying to see what happens.

Now, unfortunately, in most of the world, you don't have the model of the world. Neither in my language models do I have a model of the world, nor in my robot models. Your robot models, we have an exact model, but if you did have a model... if you're doing a pre-training on your robot for RL, and you had an approximate model of the robot in the world, now you can learn much more efficiently an optimal policy, given that model.

**Student:** Yeah.

**Instructor:** Oh, well, there's no policy... I changed the policy on this page?

The policy on this page is fixed. Remember, there's two steps. This is estimate $V$ given $\pi$. Then the next page is going to be estimate $\pi$ given $V$. I'm going to go back to this page and iterate: update $V$ given the new $\pi$, then I'm going to go to the other page, update $\pi$ given the new $V$.

**Student:** Yeah.

### Alternating Optimization

**Instructor:** It's like EM. It's a class... EM is one of many dual gradient methods, where you've got two things you want to change, and it's relatively easy to estimate the one given the other, and the other given the one.

We saw that in... In fact, matrix factorization, we saw that in EM. So this is a whole class of alternating optimization methods, right? And in some sense, this is not an EM at all, because, hey, there's no hidden values here, and there's no... But in some sense, it does have this feeling.

It is an alternating optimization. It's a big trick of advanced machine learning. There are an awful lot of things you can solve by optimizing one thing given the other, and optimizing the other given the one, and as long as they're both moving in the right direction, then you're moving in the right direction.

And that's a very abstract, tricky process, and EM is sort of the poster child - the first one dating back from, I don't know, '50s, '60s, before my time, right? Of these super useful alternating optimizations, right? It's very different from just the single gradient descent. That's sort of the entry-level optimization.

**Student:** Yeah.

**Instructor:** Mmm, it's $V_k$, no, in some sense, what we're trying to do is to drive the difference between $V_{k+1}$ and $V$ down to 0. So it's a contraction mapping in the sense that we're trying to get it so that we move it so that we're not changing them, so that we're converging to Bellman's equation.

At convergence, we reach Bellman's equation. And then the right and the left-hand side match perfectly. And so we're trying to converge that. So if you will, and we'll see this, we'll do it explicitly in a neural net land on Monday, where we'll see that in some sense, what you'd like to say is that $V(S_t)$ should be equal to the reward you get after taking the state and action plus $\gamma$ times $V(S_{t+1})$, right?

$$V(S_t) = R_{t+1} + \gamma V(S_{t+1})$$

So that's what I want. And if I were a neural net guy on Monday, I'd say take the difference between that, take the square of it to make it an L2 thing, and minimize it.

$$(V(S_t) - [R_{t+1} + \gamma V(S_{t+1})])^2$$

And so in some sense, that's what we're minimizing. We want $V(S_t)$ to be equal to... there's a minus here, sorry. We want $V(T)$ to be equal to $R + \gamma V(S_{t+1})$. That should be equal. And that's what we're trying to converge to.

And then abstractly, the big world, there's two ways to do it. One is, write the whole thing as a neural net. And we'll... it'll be a little technical, but... and then just gradient descent it, or do this sort of iteration.

## Policy Improvement

But I'm going to move on, because we... at this point, we're doing this all assuming we know the policy, but in fact, given $V$, we have to update the policy.

So, in general, what we could do is greedily update the policy. So we're going to start with some initial policy, say random, just pick actions at random, like I initialize somehow, all actions equally likely.

And now what I'm going to say is, given my $V$ function that says I've got a point estimate, I'm going to say my new policy is going to be equal to the best action in expectation.

$$\pi'(s) = \argmax_a \sum_{s',r} p(s',r|s,a)[r + \gamma V^\pi(s')]$$

Which is what? Pick the best action, the action that maximizes... Here's my world model, how likely am I to end up in the new state and get the reward? That's how likely I end up at an $s'$. And how likely I am to get $r$, the expected value of the reward I get, plus $\gamma$ times... follow the policy from the next state I get to, right?

So this is the same sort of recurrence thing. And this is guaranteed, and here's the gradient descent. There's no gradient. But this is "gradient" in the following sense, that it's going downhill. That if you pick this policy, it will be either strictly better than the other one, or in the limit equal to it. But it's never going to be worse than it.

Make sense? So this is sort of a one-step greedy. Follow the best action now, rather than my policy. And the one that gives the highest expected value, keep doing that.

So, given a value, I can find a better policy. And then given the policy, I'll go back and find a more accurate value for that one. But I've got a good initial guess, because I got the policy; it was nearby, so I'm making small steps in the policy. Small steps in the value function, I'm always making the policy somewhat better in the sense of increasing the expected return, and I'm always making the value function catch up with the policy.

I don't even have to converge them both all the way. As long as they're both moving in the right direction, that will converge to an optimal policy with the right V-value for it.

**Student:** Yeah.

**Instructor:** Find the best action that maximizes the return for the...

You always want to maximize the return, not the reward. Reward is the immediate thing. You want to maximize the return, right? So I'm not truly maximizing the whole return; we're only doing a little step to improve it.

Maximizing the return is hopelessly hard because that's an infinite sequence of future rewards over future actions. So again, think back to all of RL: what one little thing can I change looking only one step ahead that will move $V$ in the direction of increasing the return?

### Greedy Action Selection

This maximizes the immediate return, or reward $r$, plus the expected value of the future expected return on the state we go to, right? So you'll take the action. What action should you take? This is greedy because I'm not going to plan 5 steps ahead in all possible things; that's too expensive.

I'm going to say, what's the action I want to take now? I'm going to take the action that maximizes the expected return in the sense that it's the reward I get from the action right now... I'll be in some new state, and I have an expected return from that state discounted by $\gamma$.

So I'm taking... it's a greedy selection. I'm not looking, planning: if I do this, she does this, I do this, she does this... ah, it's too expensive. All I'm saying is, hey, I'm going to take the best action in terms of whatever I would know one step in the future. This should look a lot like TD0, right? TD0, instead of actually computing the expectation, we just took one step and picked the best one, but it's the same sort of style.

**Student:** Well, value is the expected return if you start in that state.

**Instructor:** So, this says $V^\pi(s')$ says if you start in state $s'$, which, in this case, happens to be $t+1$, right? And you follow policy $\pi$? That's your return, right.

And in a sense, reward plus discounted return IS return, right? That's the Bellman recurrence relationship.

I'll say it again. The rewards you get, plus the discount factor, times the return from the next step is your current return. And then you have to take an expectation over what actions you might take and where the world might take you.

**Student:** Yeah.

### Recursive Nature

**Instructor:** This is, in fact, recursive, right? We're looking one step, and all of reinforcement learning, right? It's the same thing I do with the mouse in the maze. The whole thing magically works that says, all I do is look one step ahead, and... before with the mouse, I picked one step and just saw what happened. With some probability $\pi$, I ended up taking my action, and with some probability $p$, I end up in a new state.

In that case, I just had to do it a whole bunch of times, see what happened. And this time, I'm summing over them. But it's the same idea. By looking one step ahead and optimizing that, I can learn a policy that's globally optimal. Magic!

**Student:** Yeah.

## Training in RL

**Instructor:** There are no training sets or testing sets. Yes, the crappy thing about reinforcement learning is the only way to do it is either to explicitly have a model of the world and the reward, or to be able to play it in the world. So if you do RL in a simulator, they have playgrounds, which are models of the world. If you do RL in a robot, you gotta run the darn robot. So either you got a model of the robot, right? $p(s',r|s,a)$ - that's a model of your robot environment. Or you got a physical one, but that's it. There are no $X$s and $Y$s in the real world that you can look up and train.

### Overfitting in RL

**Student:** Is overfitting still a thing?

**Instructor:** Well, the theorems basically say that as long as you have enough data and explore the world often enough, you're guaranteed not to overfit. But that's sort of saying, as long as you have infinite data to train your model, it's very good. So the problem is not so much overfitting here as it is, if it were the case that there were some cases where you hadn't explored certain actions or certain regions of the state space... if you've never seen part of the state space, you've got no idea what a good action is. Now, you might call that underfitting.

If you've only been there once or twice, then you might overfit because the one time I tried that, I got a great result, but unless I've tried it 20 times, 100 times, I don't know that it's reliably a good result.

So, overfitting here means underexploring, right? So before we had a sample size $N$, now we have how long do you run this? How much do you explore the world, right?

And I've been avoiding it except to note that these things are very powerful algorithms, which means it takes a lot of data to get them to converge.

Bummer! We're good.

Okay, that's the equation which I'm not going to do.

## Model-Based vs. Model-Free Summary

Okay, so let's summarize where we are. We said, hey, we could do temporal difference, which is model-free. And we just take one action, see where we end up, see what happens. We said we could do dynamic programming, which we just did, which was model-based. We're in a state, and we take the summation over all possible actions, the expectation over all possible actions, and the expectation of all possible next states we can end up in, right?

So, the dynamic programming we just did shows us: hey, if you can model the full probability action space, $\pi$, and you can model the model of the world, you can sum up the expected value over all actions I take, for all states I could end up in... how good is it?

In some sense, that's much nicer than TD0 because you get to sum over a whole bunch of things at once, but it requires having a model of the world, which I mostly don't have.

Make sense?

### Monte Carlo Methods

The other option, which we're not going to work on, if you have a model-based one, you could, in theory, not just do one step ahead, but just play out the whole game forever. That's computationally completely intractable, so it's uninteresting. Um, but the other one that is used, particularly for game-playing, is to say, I want to evaluate how good something is.

What I'm going to do in my model-free version is play out through the world a whole bunch of paths through the world, so I'm going to do a Monte Carlo sampling of a bunch of different paths through the world. How good is this game state? I need an evaluation for the $V$ of the state, play the game a hundred times out, and if I win 75 times and lose 25, it's a 75 minus 25 good.

Make sense? So it's an estimate of how good things are by actually playing it. Used heavily in the game world, uh, most... well, most of my world sits over here, but without models. Cool!

## Model-Free RL: On-Policy vs. Off-Policy

So we talked about model-based RL, but I want to come back and go into more depth on model-free RL, right? That was the TD0 one. And there are basically two categories of model-free learning.

One is you're learning **on-policy**. You've got some policy $\pi$, and you're always following that policy all the time, and that sounds sort of nice, but is usually not feasible.

The other one is that we will be learning **off-policy**. We're going to try some actions which are not following our policy.

Make sense? If you're on-policy, everything sort of sits in V-space. If you're off-policy, you're in Q-space because what does $Q$ say? $Q$ says, take some action, any action, which can be off-policy, right? And then follow the policy.

Make sense? And that's going to allow us to do exploration. And the key to learning stuff is trying out stuff, right? How good is an action? If you've never taken the action, you'll never know how good it is. If your policy is always go straight, that's maybe a good policy, but you're never going to learn what happens going left.

So if your policy is usually to go straight, you'd like to something occasionally with some probability $\epsilon$, try something else. Make sense? And even if you think going straight's the best thing to do, with probability $\epsilon$, try going left, see what happens, maybe it's better.

Make sense?

**Student:** Yeah.

**Instructor:** Good student, uh, an action that's... an action's always an action in the action space. You can't take an action that's not in the action space. So yes, it's always the immediate action I'm going to take now in the action space.

**Student:** Yep.

### Offline Learning

**Instructor:** You can try and design a policy that enables exploration. That's true sometimes. The other thing which is increasingly the case is often you have records of, in my case, previous conversations talking in the chatbot, or in a robotic sense, previous robot runs.

So I'd like to learn how to learn a good policy when I can't actually try the world. Well, I can't control the world. I've got a historical set of robot runs, drones flying around, or conversations. Make sense? They were collected under one policy, but I want to learn a better policy.

And I really want this because it's expensive to collect conversations, it's expensive to collect robot runs. So now I want to learn a policy better than, and hence by definition, different from the policy that was followed when I was flying the robot, or having the conversation. So, anything that's offline learning, learning from data is off-policy.

Make sense? So off-policy learning is super attractive for almost all modern policies, because we're always limited, whether you're a robot guy or a language guy, you're limited by not having enough... It's very expensive to collect lots of conversations and try them with you and see which ones work better and keep updating the policy. So I really want to be able to be off-policy.

That make sense?

**Student:** Actually, yes.

**Instructor:** Exactly.

### Exploration-Exploitation Trade-off

Okay, and again, there's this exploration-exploitation trade-off. Um, the classic thing is called a **multi-armed bandit**, based on Dr. Bush playing slot machines, where you're going to pick each of the possible actions with some probability.

And you always have this trade-off between, do I pick what I think is the best action? The argmax, the best action, be greedy? Or do I try other arms? I'm using the word "arm" because that's a technical term in the bandit world - different arms are different actions.

Make sense? And there's an optimal piece there. If I have infinite time, try everything and see what happens. If I've got a very short time to learn, be pretty greedy. And in between, there's some sort of a trade-off of how much I want to explore versus exploit.

Cool.

## Formal Definition: Target vs. Behavioral Policy

So, let's be formal about on and off policy. We're going to have two policies in general: a **target policy**. This is the policy we're updating. We're learning the policy, or computing an optimal policy.

And there's a potentially different policy which I'll call $\mu$, which is the **behavioral policy** - the policy that we're following, right?

So it's two different things. $\pi$ is the policy I'm learning, $\mu$ is the policy I'm following. If they're the same, then it's on-policy. But we'll see that they're often different, right?

So, **on-policy**: take my policy I'm learning, use that to pick data. Whatever policy I'm currently learning, use it for learning. Keep... every time I change my policy, I change how I collect data. Sounds good, but in practice, it's mostly not used.

Mostly people use **off-policy**. I'm going to pick a policy $\mu$ to collect my data, and I'm going to use that to update $\pi$, the policy I'm eventually learning.

Um, one reason you might want to do it is you might want to follow a policy $\mu$ that says, don't do these things, they're too dangerous. Even though your $\pi$ may predict them, so, don't tell people to kill themselves, don't crash your robots into the walls, there are lots of bad behaviors you'd like to prohibit. Even if your policy $\pi$ includes those in some fashion.

### Epsilon-Greedy Policies

But the biggest one that I want you to learn is the concept of **epsilon-greedy policies**, which are the ones that are widely used. And the idea is really simple.

I'm going to have a hyperparameter $\epsilon$, and with probability $\epsilon$, I'm going to pick a random policy action, or in some cases do a... pick among the top $K$ best ones a random one, if you have too many of them. And with probability $1 - \epsilon$, I'm going to pick whatever looks like the best action, or the on-policy action.

$$
\mu(a|s) = \begin{cases}
\epsilon/|A| & \text{with probability } \epsilon \\
\pi(a|s) & \text{with probability } 1-\epsilon
\end{cases}
$$

And so, what we can do is you... update the policy with respect to some Q to improve it.

### Annealing Schedule

And typically, people use some sort of an **annealing schedule**. The annealing idea says that you start with $\epsilon$ being sort of big, lots of exploration, and as you learn more and more, you make $\epsilon$ smaller, so you can emerge eventually to an on-policy, optimal policy.

This is true of a lot of things that... you know, start taking big steps. As you take time, take the step size... take the noise to zero, take the variance and epsilon to zero, right? Initially, and the more you've explored over time, time is a discrete number of steps, the less you want to explore. So you'll converge to something. Otherwise, you're always putting noise in.

Cool. Um, we're going to skip SARSA online, and it will not be on the exam.

## Q-Learning

To take the other one, and I'm going to focus instead, in the interest of time, on Q-learning, um, which we're going to do Q-learning today, and we'll do deep Q-learning on Monday.

And the idea is I want to come back and look again at temporal difference, TD0. We're only doing TD0 in this class. We'll use, and... Remember that what TD0 looked like was an iterative algorithm that says the value function at a given state will be updated to be the previous value, plus a learning rate $\alpha$ times the immediate reward, plus the discount $\gamma$ times the value of the next step we end up in, minus $V(S)$, right? We saw that five times. That's the TD0.

$$V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$$

### Q-Learning Update Rule

We can do the same thing now with a $Q$ function. So now let me rewrite... Because I want to be able to go off-policy. If I'm in the $V$ space, I got no actions off-policy. This is all on-policy. But I want to be off-policy. So I'm going to do the same thing with $Q$, and here's the same equivalent version.

The $Q$ at some state taking some action will be updated to be the previous value of $Q$ in that state and action, plus the learning rate $\alpha$, plus, now instead of $V$ here, I have $Q$ of the state I end up in, and I follow the policy going forward from that state, minus the current $V$, which is... the current state, taking some off-policy, potentially, action.

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma V^\pi(S_{t+1}) - Q(S_t, A_t)]$$

Or equivalently:

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma \sum_{a'} \pi(a'|S_{t+1})Q(S_{t+1}, a') - Q(S_t, A_t)]$$

Right? So I've broken things up into two pieces. The immediate value, but this is not the... the $Q$ over here is not the $V$ function because this is off-policy, potentially, right? This is the value of the current state if I take whatever action I chose for my $Q$ value, right? Which is... one of my examples on time, it's on-policy. Epsilon times, it's whatever I happen to pick randomly, right?

And then the $V$ of the next state, this really is $V(S_{t+1})$, right? Because it's $Q$ of $S$ following the policy from that state. So, this is $V^\pi(S_{t+1})$. But it's all expressed in terms of Qs. I don't need any $V$s.

Make sense? So instead of doing TD0 with $V$ functions, I can do TD0-style things with $Q$ functions. But now I get to pick whatever $A$ I want.

**Student:** Yeah.

### Understanding Action Selection in Q-Learning

**Instructor:** Always in Q-Learning, the immediate time step I take action $A$, and for all future time steps, I follow the policy. The first action is taken by whatever I want, right? Some behavioral policy $\mu$, which is often epsilon-greedy, so the first action I pick based on my choice of how much exploration. And then I assume that I will follow for all future time my policy $\pi$, right?

And again, the action $A$ is picked from a different distribution than the policy $\pi$. Everybody else good on the... that distinction of pick the first action from $\pi$ with probability $1 - \epsilon$, pick it randomly with probability $\epsilon$. So that... the initial $A$ here is picked from a different piece here.

$R_t$ is the reward you got if you were in state $t$ and took action $A$. So yep, it's whatever reward you get.

**Student:** Yep.

**Instructor:** If the $A_t$ want to go action?

**Student:** Yep.

**Instructor:** You want, just like a reward.

Now, $\pi$ is what... remember that any $V$ or any $Q$ is defined under a policy, right? So this is another recurrence relationship. And now we're no longer in Bellman model, where we actually have these explicit expectations, right, over policy, or explicit expectation over model. We're just picking one instance, one realization of the... of where the world takes me.

But I think the way to think of it is this is update: in state $S$, I take some action $A$, update with the learning rate times the reward I got when I took this, plus $\gamma$ times $V^\pi(S_{t+1})$, which is exactly the equation I had before, right? This is $V(S_{t+1})$.

But instead of $V(S_t)$, what I have there is the $Q(S, A)$, because this is an action that doesn't have to be under the policy.

### Q-Learning Interpretation

So again, right now at time $t$, I'm going to take some action. And then at $t$... I'll end up in some state $S_{t+1}$, and from that time on, I'm going to follow $\pi$.

Make sense? So Q-learning, all these learnings, off-policy is: take one action that may be anything to explore, and then assume in terms of how good the next state is, assume that you will follow the policy from all future time.

How do we... how do we actually do this in practice?

If it's a discrete state, you do exactly this. If it's a neural net, we'll cover it Monday, but now $Q$ will be a mapping from a state embedding and an action embedding to a real number, so $Q$ will be a neural net, and then we will gradient descent it using an L2 loss, right? So deep Q learning will eventually do a function here. Um, but I'm not going to cover it today.

But in the discrete world, you can think of this as: if states are discrete and actions are discrete, $Q$ is just a number of states by number of actions collection of matrix of numbers. And this tells you how to update it. You're in state $S$ and you take $A$, and you're asking, how good did I think it was beforehand? This is how good, and then you say, hey, how good I think it was after. This is... I take one action, that action, this is how good I think it was, immediate reward plus future return.

That's my new estimate. Move the old estimate toward the new estimate, right? And that's the gradient descent-y sort of thing, right? There's an $\alpha$ learning rate, where we're moving my estimate of how good is it to take this action in the state closer to whatever I guessed it was after I took the action and found out where I ended up, right?

### The Q-Learning Process

It's always the same game. Take an action, right? I play a move on the board, the environment, you play a word back to me, I look at the board and say, what's my chance of winning now? If it was better chance of my winning than before, I update my old Q function higher. If it was worse than it was, I lower it. I haven't changed my estimate, I've learned nothing, I leave it the same.

So it's trying, each time you take a step, you say, hey, I tried this action. I took this action. I spent this method of studying for my exam, and then I... maybe not take the final exam, I do the homework, I see how well did I do on the homework, better, what's my new estimate of how well I'll do on the exam?

So what you're using is always your estimate of winning at the end, the return, based on one step. And the whole thing relies on what's usually the case. If you take an action, you get a new state, you have, hopefully, a better estimate of how good it is compared to before you took the action. Because either the action leads to the world being nice to you, maybe an immediate reward, it gives you Froot Loops, or maybe it gives you immediate penalty and you get a shock. Or maybe you see, oh, I see the Froot Loops, right?

But you're getting some notion that you're getting closer.

**Student:** Yeah.

### Updates Always Happen

**Instructor:** You never check to see whether the exploration is right or wrong. It's a gradient descent sort of thing. All I'm doing is saying I took the action, look at my next estimate of how good things are in terms of return, update based on that.

If I take this action, and all of a sudden my Q-value says, hey, I'm going to lose the game, what do I do? I do an update. Do I not update if, in fact, $Q$ is lower than I thought it was going to be?

It's a number, it's always updated. Either the reward plus the discount plus my future value, this is how good I think my state is. This is how good over here I thought it was before. If this is bigger than that, increase it. If this is smaller than that, decrease it. If this is the same as that, my model was perfect, don't learn anything, right?

So it really is, this is how good I think $Q$ taking $(S, A)$ is. Taking action $A$ and state... this is before, after I take it, this is how good I think it is. Check the difference.

That's all we're doing in all these things, is taking the difference. And again, it's either an explicit expectation, like dynamic programming Bellman, sum over probabilities, or it's a sample over probabilities, and each time take it one by one, but they're all the same. It's all the same algorithm over and over.

### Convergence Guarantees

Always updates no matter what. And, in fact, the convergence of this algorithm is guaranteed, at least for discrete state and action space, where it's easy to show it. As long as the learning rate is reasonable, and reasonable means if you want to be very formal about it, you need to have the learning rate be high enough that the summation of all... think of $\alpha$ as typically between 0 and 1, right? The sum of all the $\alpha$s has got to be infinite. You've got to explore enough.

$$\sum_{t=0}^{\infty} \alpha_t = \infty$$

And the sum of all the $\alpha$ squares have gotta be not infinite. You can't explore too much.

$$\sum_{t=0}^{\infty} \alpha_t^2 < \infty$$

And remember how we had annealing? Annealing says you can make $\alpha$, the learning rate, get smaller and smaller over time, and this gives you some bounds, at least, on how much smaller you want to make it. And so as long as you have infinite exploration in some sense, and non-infinite exploration in a different sense, in terms of learning, then this will sort of guarantee you that you'll eventually visit all the state-action pairs. Which means that it's not cheap. But in fact, it's unusual that you actually get a real convergence guarantee.

And again, I'm not doing the math.

**Student:** Yes?

### Annealing Discussion

**Instructor:** We... well, there's two annealings here, right? What I said before was we anneal the $\epsilon$, in the sense of how much do you randomly try things? And here I said something which is technically not annealing, but similar. We also tend to decrease the learning rate.

**Student:** Yeah.

**Instructor:** More and more greedy, yep.

**Student:** Yeah.

**Instructor:** Correct. Correct.

**Student:** Sorry?

**Instructor:** Well, so here's the deal. If you have $\epsilon$, if you anneal slowly enough, then you're guaranteed to visit everything, and you're fine. If you anneal the $\epsilon$ too slowly, then you never converge and you keep bouncing around for too long.

But I think the answer, in some sense, is... and these are all sitting in the math world. In practice, you're almost always limited by how much data you can collect. So I don't really care about the math. But I think the idea is that if $\epsilon$ is annealed slowly enough, then you are guaranteed to explore enough to visit everything often, and if you visit everything often enough, you will learn it, right?

So if you anneal too quickly, you converged prematurely, you've underfit, or whatever, you've memorized something, you haven't explored enough. If you take $\epsilon$ too slowly, in some sense, it's no problem, but it means that at any finite time, you're still exploring, and you haven't done a good job. You should have stopped exploring and just exploited.

### General Principles for Annealing

So I think the big takeaway is every time you're doing something that requires exploration-exploitation, you should, in general be exploring a lot early on and exploring less later on. If you're doing a gradient descent, you're usually moving big steps initially, and smaller steps later.

Some things in deep learning, there are nice algorithms we've seen, the Adam, whatever, that will automatically adjust the gradient step size for you in a good way. Other things like RL, you're stuck there choosing some schedule of how you're going to change the learning rate. And it's sort of less obvious.

**Student:** Yeah.

**Instructor:** Yeah.

**Student:** Yep.

**Instructor:** Well, you're... to do these things, you're going to have to run it from the beginning. If it's a discrete one, it runs to the goal a whole bunch of times. So you're going to run them so many times that whether you explore more early or late in the process of going to the goal, isn't going to make that much difference.

So you're going to make a whole bunch... if you're playing games, you're going to play a billion games of Go or chess to get this thing to converge. So, if you're in a discrete world, like a game, you're really literally playing, you know, hundreds of millions or billions of games.

### Continuous vs. Discrete Worlds

If you're in a continuous world, there is no goal, right? If you're controlling, uh... data center, Google allegedly uses these things for data center optimization. You're trying to control energy usage, uptime, downtime. There you get a reward every second. How much energy did I use this second? How many customers got kicked off this second? You gotta have some metric, some loss function per second. You get a reward every second, and you have some future discounting $\gamma$, because you don't care about customers a year from now as much as you do customers right now, plus your world's going to change. I mean, a year from now, you've got a different set of GPUs that you got the new Nvidia chips, so Tensor unit, sorry. TensorFlow chip, the TPUs, you know.

So, in that case, you're always in a continuous steady state. You're always exploring the world. Your world, in fact, never converges because the world is shifting, the demand patterns of customers are shifting, the TPUs you have are shifting, energy costs are shifting. Make sense?

So in the real world, the continuous world, you're always updating, you're always doing some exploration because whatever your old policy was, it's optimal for last month's world. But the world is drifting.

**Student:** Yep.

**Instructor:** From point A to point B, you're going to run it enough times that people don't worry about learning or exploring more early or more late. You could be clever about it. There's lots of sensible things you might try.

### Practical Exploration Strategies

The one that people actually do in the real world is only the following one. In the real world, often the action space is very big. There's hundreds of possible actions, and so in general, in a real epsilon-greedy industrial application, you say, pick the best option with probability $1 - \epsilon$, and pick one of the top $K$ with some $\epsilon$, or something that weights them based on how good you think they are, right?

So in general, you don't really want to randomly explore all possible options, because most options are crappy, right? And in general, we'll see that you often optimize your policy, not something random, but by following a human operator.

### Imitation Learning

So if you want to start your robot, you're going to start with imitation learning. You will train a policy up that copies a human flying the bot, and that will be your initialization, and then you'll do exploration gradient descending around that sort of thing. Make sense?

And then you have some idea of what's good and what's bad, and you're going to try things that are close to good. Things that are high in your probability of action given state space. But you don't want to always pick the best one. Picking the best action in the state space is not going to give us enough exploration.

Good.

**Student:** Yeah.

**Instructor:** Your training model is a good starting point for something different, right? And it's either... Yeah, it's often the case, either you're moving to a slightly different factory, or the world is changing slightly over time.

Cool. Um...

And there's the thing written down in detail, but I'm not going to go through it again. I just summarized this, so I'm not going to say it again.

## Monte Carlo Tree Search (Brief Mention)

I want to, in 4 minutes, 3 minutes, say one other version that we're not going to worry much about is that... I'll just wave my hands at this. Instead of taking one path through the world, you evaluate how good is this current state by simulating a whole bunch of trajectories to the end of the game, and taking the expected value. And that's all the piece there.

## Summary and Key Takeaways

So, what you should have at this point is you should certainly be conversant with all the jargon pieces here. You should know we talked about an MDP, a POMDP, you should be clear on model-based, model-free. You should have Bellman's equation as a recurrence relation, this notion that we're mostly off-policy, Q-learning looking like TD, and I didn't talk much about the search one, so I'm not going to talk about that. That will not be on the final, and then we'll come back Monday, and we will look at deep Q learning.

Which is sort of the modern way that lots of these things work, and we'll do a couple more policy ways to learn things in pure policy space.

Have a great weekend, work on the project, move forward!

Some questions...

---

# CIS 5200: Machine Learning - Lecture 24: Reinforcement Learning Lecture

## Course Logistics

### Upcoming Schedule

It's almost Thanksgiving, and then Christmas is approaching. Today's reinforcement learning lecture should be fun. We've got two main topics to cover:
1. Deep Q-learning
2. Policy gradients

These are two of my favorite topics in RL.

### Administrative Details

- **Next Week**: Random odds and ends for fun
- **Project Presentations**: In recitations
- **December 8th**: Last day of class
- **December 9th**: Extra review session (room to be posted)
- **Final Exam**: Locations will be posted soon

Feel free to either take Thanksgiving off or work on your projects. Meet with TAs for any other details you need.

---

## Deep Q-Learning

### Review: Standard Q-Learning Update Formula

Last time we discussed that the standard Q-learning update formula says that you take the Q-function $Q(s, a)$ for some state $s$ and action $a$, and you add a learning rate $\alpha$ times something which is basically how much you change your expectations about how good it is to take this action in this state.

The change in your belief is:

$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]$$

Where:
- $r$ is the reward you get for taking this action in the state
- $\gamma$ is the discount factor (gamma)
- $s'$ is the state you go to
- The term $\max_{a'} Q(s', a')$ represents following the behavioral policy going forward

Remember we talked about off-policy learning where you might have one target policy you're learning and one behavioral policy you're following. You follow your behavioral policy, so this is how good your new update is for the state taking the action minus how you thought it was before.

We're going to try and iteratively learn that.

### Formal Definition

To be more formal, the value of $Q$ in some state following some behavioral action is:

$$V^\mu(s) = \max_a Q^\mu(s, a)$$

Take the best action in that state. There should be a subscript $\mu$ there because the Q-value or the value has a policy. It's the same sort of thing: take this action, then follow the policy in the future.

We're updating with a learning rate times this error term. In some sense, this is an error term - it says "this is how good we thought this action $a$ in state $s$ was" versus "this is how good we think it is after we take the action." It's an error, and we're hoping this will converge to 0. Bellman promises, under some reasonable conditions, that it will.

### Quick Review: SARSA (On-Policy Learning)

Let me quickly review one thing I skipped last time. There was a slide I didn't cover on **on-policy learning**, which is called **SARSA**.

SARSA says: be $\epsilon$-greedy for the current action AND $\epsilon$-greedy for the future time. That was sometimes popular. But in fact, what we're going to mostly do is **off-policy** where we're $\epsilon$-greedy for the current action (with probability $1-\epsilon$ take the best action, with probability $\epsilon$ pick a random action), and for all future time we're going to be greedy (pick the best action given the Q-function).

#### On-Policy vs Off-Policy

- **On-policy** just means you're consistent with your action now and your actions in the future
- A perfectly fine policy is: now and for all future time, pick the best action with probability $1-\epsilon$, pick random with probability $\epsilon$
- That's on-policy. That's the policy. Policies are stochastic.

#### Why Not SARSA?

Why am I not spending much time on on-policy? Why do I not like it particularly?

The issue is: we're always taking a random action with probability $\epsilon$. SARSA says learn to always, probably with probability $\epsilon$, take a dumb action, a non-optimal action.

I don't love that. I'd like to converge to something that I think is optimal, not to converge to something I think is $1-\epsilon$ optimal.

SARSA is on-policy and it's important to understand that jargon term - the same policy now and in the future. But it's not an optimal policy. It says I'm always with probability $\epsilon$ going to take a random thing to keep exploring. Usually I don't want to keep exploring forever. Maybe I do, but often not.

---

## Deep Q-Learning: Gradient Descent Approach

### Recasting as Gradient Descent

Now, what we're going to do is find an alternate way to make this error term go to 0. This is an iterative recurrence relationship. But we can recast the whole thing in something that's going to give us gradient descent.

The idea is, we take this recurrence relationship where we can put $Q$ as a neural net. Take state as an embedding, take the action as an embedding, and predict a real number.

We can say: what I want to make small is this term here. So if I make my loss function be:

$$L = \left( r + \gamma \max_{a'} Q_\theta(s', a') - Q_\theta(s, a) \right)^2$$

Think of this as stochastic gradient descent. My loss function is the immediate reward I get now plus the discount factor times the future rewards I'm going to get, minus my $Q$.

I can now say: I could learn this $Q$ by gradient descent. It's a quadratic loss function. Fine. Take the derivative, I'll get twice this plus then I'll get the full gradient on the $Q$s.

### Double Q-Learning

It's a little weird because $Q$ is showing up twice in this equation - showing up both for the target function I'm learning now (the one I'm really learning) and $Q$ is showing up for estimating the future. In a second we'll break those two $Q$s into different things. We'll do **double Q-learning**.

Note that this is perfectly fine. There's a subscript $\theta$ on these neural nets. I can take the derivative in gradient descent.

Once I have $Q$, then I can get a policy, right? Because $Q$ says state cross action - just pick the argmax, pick the best action:

$$\pi(s) = \arg\max_a Q_\theta(s, a)$$

But I have to search over actions. If it's a small space, it's easy. If actions are real-valued, I can do a reverse gradient descent to find the action that maximizes the output of the Q-function.

So I never learn a policy here. Everything is learning a Q-function, and I use that then to compute a policy.

In the second half of the class, we're going to do the opposite - we're going to throw away Q-functions and only learn policies, which was actually what NLP people do.

### Comparing the Two Formulations

**Question**: Are these very similar?

**Answer**: Yes, they are very similar. This one (the recurrence relationship) and this one (the quadratic loss) are very similar.

The sort of nice thing about this is: I'm now back in standard PyTorch land, and I can use Adam for my optimizer, and I can use all the machinery that I'm used to.

The first one worked fine - it's sort of an earlier method that people used before. In some sense it's simpler because it just has iteration. But they're both doing the same thing.

What I'm trying to show is: if I optimize this with gradient descent, this really would be optimized with literally gradient descent. This one is a recurrence relationship also doing a gradient descent sort of thing. So they're really very similar.

They're getting at the same idea, which is: what I'm trying to learn is to update my Q-function to be similar to what I learn after I take a step.

In both cases, I'm trying to sort of gradient descent it. This is not technically a gradient - there's no gradient here, it's a recurrence relationship. This one we're going to literally solve by taking the gradient. They're incredibly similar, and that's sort of my point.

### Why Just One Step?

**Question**: Why are we doing just one step? Couldn't we do something where we're going to evaluate all future steps?

**Answer**: That would be, for example, a **Monte Carlo tree search**. We would play out the game 100 times and see what wins. That's a fine thing to do.

If you have a game which is cheap to play, like Go or chess or any game, give a simulator for your robot, run it out 100 times and see what happens.

On the other hand, if you're playing in the real world where you have a real robot or a real power plant or a real truck doing stuff, then in fact you can't play out things 1000 times - it's expensive. What you do is:
1. Take one step
2. See whether I'm doing better or worse than I thought
3. Update my function
4. Take another step

This is a **TD(0)** (Temporal Difference) style approach.

**Which one's right?**

Well, if you're playing games, you'll use a Monte Carlo tree search because you'll play it out a bunch of times. If you're in the physical world, you're not going to play it out a bunch of times because robots are incredibly annoying and slow to run experiments on. You can play a billion games of chess. You can't run your drone a billion times. Even Bezos can't afford that.

### Gradient Descent Implementation

**Question**: How are we doing this iteratively?

**Answer**: What we're going to do is:
1. Take the action
2. See what happens
3. Take one gradient descent step on the $\theta$s for the Q-function
4. Take another step

So it's a thing of stochastic gradient descent, or you could mini-batch it. But stochastic gradient descent is sort of the cleanest way to think of it.

---

## Double Q-Learning

### The Stability Problem

One technical detail: if you run these things with training both copies of the network at the same time, they're a little bit unstable.

So in practice, the way people do this Q-learning is as **double Q-learning**, where you:
1. Take one older copy of $Q$
2. Use that to estimate the future
3. Take the one you're learning and update it

The Q-function here - I want to update this Q-function based on what I think the future outcome is going to be.

### The Double Q-Learning Approach

This is double Q-learning because we're:
- Fixing our way to estimate the future for a while
- Then we update $Q$ by gradient descending one of them

So we don't have them converging towards each other doing something unstable.

Every 1000 times, 10,000 times, we go and update the one that we're actually learning. So:
- Update one network
- Fix the other
- Then copy it over

You get more stable convergence.

So every now and then you'll change the one that you're using to estimate the future. We're updating this term every gradient descent step, and the second one we're using to estimate the future.

In fact, even in the early days when compute was expensive, people would use a cheap model - they would distill the big model down to a small model and use a cheap model here (if this were expensive, though it's actually sort of silly for this Q case).

### Understanding the Two Networks

Think about what we're doing: we take a step, we're trying to learn this one here, and we're fixing the one that's used to estimate the future.

Our $Q$ for the future is not something we want to learn every time. That's an estimate of the future. We're learning our Q-function for now.

So we're learning this one. I should have really said - I call this "target" which is a bad word. It's better to think of this one as the target, the one we're actually updating. The other one is fixed.

Again, to distinguish: I'm learning my Q-function by taking one step and estimating how good the future is, and that one is going to be quasi-fixed.

### Action Selection

**Question**: How do we select actions?

**Answer**: In general, if it's a finite set, you'll just try each action. If there's a small set (less than 100), just try each one and plug it into the network and see what's best.

If it's real-valued, you can either sample, or it turns out you can do gradient descent in the input rather than the parameters. We talked about this at one point in class. With a neural net function, you can say: instead of doing what we normally do (which is take the derivative back to the weights of the error), you can ask: take the derivative of the output with respect to the input and ask gradient descent the input to make the input get better and better Q-value output.

So in that case, you're doing a search in the action space to find the best action.

### Update Frequency

**Question**: How often do we update the fixed network?

**Answer**: We're going to update this function 10,000 times, say, and always use this guy to estimate how good the next step is. Then, after we've done this 10,000 times, we'll take a copy of this and make this other $Q$ here be the copy of it, so that's also converging.

So it's two time scales:
- A faster convergence of our actual Q-function we're learning
- A slower (10,000 times slower) convergence of how you estimate the future

Eventually they'll converge to the same thing.

The empirical thing is: if you update this one every single time, the whole thing gets a little unstable because you're trying to make the difference between this and that small. But you don't really want to make them small because you don't want to mess up your future prediction by making it small. So you don't want this one to change too fast. If you change this too fast, the whole thing goes unstable because you really want to learn this one.

If you update it too slow, it's going to take you too much time. So there's no math guarantee, but that's sort of the trade-off. If you update this every single time, it might work okay, or it might just get really wacko because you're not updating your Q-function by learning - you're updating your prediction of the future, which you really shouldn't update. If you do it too slow, it just takes too long.

So there's no good answer, like so many things in deep learning. It's like a learning rate - another hyperparameter.

### The Algorithm

There's the algorithm. I'm not going to write it out. You can use this for all sorts of stuff. Even back a long time ago in 2016, people were training games just from playing Atari and other things. It works pretty well.

---

## History: AlphaGo and the Evolution of Game-Playing AI

I want to do a quick history, switching gears for my second review topic, and look at the history of playing Go, going back to 2016, because it sort of illustrates the steps that early things often do and how they change.

### AlphaGo (2016)

The original AlphaGo is a super complicated set of things. The first thing they did was take a big collection of games of Go that were played by experts and learn a policy, which is a mapping from a board description to where you play your stone.

For those who don't know, Go is played on a 19×19 board where you take turns placing white and black stones, putting down a stone (white or black).

Note that:
- Playing is executing a policy
- One way to initialize your neural net is to watch hundreds of thousands of humans play the game

This turns out to be pretty easy. I still have from this era hundreds of thousands of games because there are Go servers where people go online and play with each other. So you have lots of human-human games, and like all these games, everybody's got a ranking, so you know who's good and who's not good.

#### Supervised Learning / Imitation Learning

This is often how you initialize: by **supervised learning** or **imitation learning** or **behavior cloning** (for this course, it's all the same idea). But the important thing is: there's this dumb form of reinforcement learning that provides... if you put a whole bunch of people together...

**Can you get something that's superhuman?**

Sort of. Bridging over all the people - humans tend to have random things, they get tired, they make mistakes. So supervised learning in almost everything, whether it's recognizing characters or whatever, the AI is usually just a tiny, tiny bit superhuman when it's supervised because it has lower noise than people do. But not very superhuman.

#### Self-Play and Improvement

Once you have this policy (you've trained a network to play Go), it can now play against itself. So you can now have it start to play against itself with this initialization, and it can now learn. It can learn a value network, and it can do Monte Carlo search.

Instead of having a Q-function, what you could use (these guys in fact use a value function) but play out 100 times. It can play with itself and get better and better in gradient descent itself.

#### Feature Engineering

The early versions had engineers - in this case, Go engineers - look at it. They took a description of the board and learned where to play. They did a bunch of things that were features of the board you might compute by hand, which if you know Go is obvious, and if you don't, it's not obvious (but it doesn't matter).

The point is that this was **engineered features** way back in 2016, when engineers would actually try and design features because you learn faster if you put the right features in.

They did a whole bunch of games - they had small, cheap models that were distilled - and it still needed a lot of compute for its era. A few weeks of compute, so not cheap.

### AlphaGo Zero (2017)

Then a year later, it's like "We have a little bit faster computers. Let's just skip all this fanciness."

They decided to:
- Use a single network
- Not even bother training on human games
- Start from random initialization (I'm guessing initially)
- Use the raw board position with no feature engineering
- Still use Monte Carlo search
- Learn a value function and a policy together

And after - I love this - **72 hours**, or 5000 × 72 TPU hours... okay, it was old slow TPUs, this is a decade ago, right? But the point is:

When they first started, they were building all the features and copying humans for initialization. When you have a little more compute power: forget it. Just put in the raw board position, no features, no self-play copying humans, just gradient descent the whole thing with self-play.

I see that a lot in robotics and other things: you start by cloning people, and then if you have enough compute power and good simulators, just learn it.

---

## Monte Carlo Tree Search (MCTS)

We should cover very quickly Monte Carlo tree search.

### Basic Idea

The idea is: I'm in a state, I take an action. I want to know: what's the Q-function for that state and that action?

The answer is: **play out the game 100 times and see, on average, how well you did.**

It's called Monte Carlo because you can do it somewhat randomly. The policy is random or semi-random. So every time you play it out 100 times, you'll get a slightly different game. If you win 75% of the time, it's a 0.75 good thing. If you win 25%, it's a 0.25 good thing. You have an estimate of your Q-function, not by what I just did before ($r + \gamma \max_{a'} Q(s', a')$), but in fact by playing out the world.

If you've got a good simulator, a fast simulator... is that expensive? Yes, it costs compute time. But what's 10,000 TPU hours to you? I mean, sure, it's going to cost you a couple million dollars worth of energy. This is for rich boys and girls to play. Don't try this at home. But this was Google back before it was Alphabet.

### Modified Epsilon-Greedy

In fact, picking an action in the real world is not ever truly epsilon-greedy because there are too many actions. There are roughly $19^2$ actions, and most of them are incredibly stupid.

What you want to do is a **modified epsilon-greedy**:
- With probability $1-\epsilon$, pick the best action
- With probability $\epsilon$, pick from among the top $k$ actions, maybe weighted by their probability in the policy

This makes sense: explore plausibly good actions rather than all of them. You get less exploration, but why do I not want to take all $19^2$ possible actions?

Because it's expensive. Instead of a million dollars, if I'm trying 200 things instead of 20, then it's taking you 10 times as much, which is now 10 million bucks instead of a million, or 100 billion instead of 10 million. Computation gets expensive after a while.

### What You Get from MCTS

What you get from the Monte Carlo tree search is an estimate, which I will call $z$, of the value of the state you are in when you start playing forward.

We're trying to learn two things really in this model:
1. **The policy**: What's the probability of taking an action (playing a stone somewhere) as a function of your state (your 19×19 white/black/empty board)
2. **The value**: How good is each of the states

These are all neural nets - both neural nets.

**Note**: A policy is a probability distribution over moves. We're not just learning the value; we're trying to learn two things at once.

---

## AlphaGo Zero: Learning Both Policy and Value

### The Neural Network Architecture

What we're going to learn, in fact, is a **single neural net** which takes in the state, is parameterized by some weights $\theta$, and learns two things:

$$f_\theta(s) \rightarrow (p, v)$$

Where:
- $p$ is the policy (probability distribution over actions)
- $v$ is a scalar which is the value of the state

### The Loss Function

The loss function that they're using has **three components**. I love this because this should look really familiar:

$$L = \|v - z\|^2 - \pi \log p + \lambda \|\theta\|^2$$

Let me break this down:

#### Component 1: Value Function Loss
$$\|v - z\|^2$$

I want to make $v$ (my output that says how good the state is) close to $z$, which is "I played the game 100 times and saw how many times I won." So I want to make the value function look like my estimated value function by playing the game 100 times.

#### Component 2: Policy Loss
$$-\pi \log p$$

I want to make the policy that I'm learning as an output be close to the actual Monte Carlo tree search probabilities that I've played.

This looks like what? $-\pi \log p$ should look sort of like a **KL divergence**. We're wanting our policy that comes out to be close to when we've been playing.

#### Component 3: Regularization
$$\lambda \|\theta\|^2$$

Finally, we just put an $L_2$ penalty on the weights because we don't want to overfit stuff too much.

### Understanding the Components

**Question**: What is $z$?

**Answer**: $z$ was the observed outcome of playing the game 100 times. We're trying to drive the output $v$, which comes as an output of the neural net, to the observed one.

Now, the observed one is a noisy estimate of the world because we only played it 100 times. But this is stochastic gradient descent. We're fine as long as the observed one is, in some sense, good in expectation on average. It's good, or we're gradient descending toward the true value.

**Question**: About the policy selection formula...

**Answer**: Rather than being epsilon-greedy, what I want to do is pick something that's mostly greedy but somewhat not greedy.

The trade-offs of picking a move $a$ are:
1. **Exploration**: You'd like to visit some action that you haven't tried before
2. **On-policy**: You'd like to do something that has a high probability under your policy because that's what you think is good
3. **Value-based**: You'd like to have one that you think will actually give you a high value

So there are three things that make a desirable action to take:
- Exploration
- Being on policy (pick things that are likely with your policy - that's sort of a greedy version)
- Then you can actually try and take your estimate of the value coming out from the neural net

Don't over-fixate on those too much, but note that the idea is that always, as you're picking an action during training, you need some mixture of:
- **Exploration**: Try actions that you haven't tried before
- But don't try things in the real robot world that are stupid (might bang into walls or break your robot)
- And don't try things that are really low under your probability distribution because they're likely to be stupid

The big piece is: last class I said "with probability $\epsilon$, pick a move totally at random." I'm going, "well, in the real world, that's an incredibly stupid policy." To be truly epsilon-greedy is dumb because almost all moves you can take are incredibly stupid. So at least focus on ones that are somewhat plausible, and then you've got to tune the trade-off of how much exploration vs. how much exploitation. I don't know - that's an engineering thing.

### Why Learn Both Value and Policy?

**Question**: Why are we learning both?

**Answer**: We have a neural net. We have a policy which we've learned so far. We're going to pick something that's high probability under the policy.

A bunch of these models - and if I had a whole course (I'm very excited, why don't we have a whole course on reinforcement learning?), we would do a whole bunch of different architectures where people mix and match Q's and V's and $\pi$'s in various weird ways.

For here, the main thing to note is: you can have both a policy and a $V$, or a policy and a $Q$, or you could just have the $Q$, or you can just have the $\pi$ (just have the policy).

The AlphaGo Zero world has something where it says we're going to actually learn both the value function and the policy. Then we're going to use both the policy and the value function to decide how to do our exploration.

This is sort of redundant, right? You shouldn't need both a policy and a value. But it's learning both of them because:
- We observe the value by Monte Carlo tree search (by playing stuff a bunch)
- We use that to derive what the policy is
- But then we can actually keep a copy and learn the policy as well

They're sort of redundant. If you really have the value function or Q-function, you don't need to learn a policy. If you have a policy, you don't need a Q-function.

But a lot of these systems, particularly from this era (8 years ago, 7 years ago), learned both of them. I'm covering it because there are still systems, particularly in robotics, that use both of them.

### Why Probabilistic Policies?

**Question**: Why not deterministic?

**Answer**: In general, if you're doing exploration, you have to do something that's somewhat probabilistic. You're picking actions somewhat at random. If you're always being greedy, maybe the world has enough noise to give you stuff, and maybe it doesn't.

But in general, people don't like things in deep learning, in deep RL, that have no noise added, because if you're taking a deterministic policy, it's easy to get stuck. Especially if you're playing yourself - if you don't explore somewhat, you can just get stuck in some weird self-play thing.

---

## Policy Gradients

I'm going to shift gears entirely. The final topic of reinforcement learning I want to talk about is **policy optimization**, which is actually the one that I mostly use.

### What is a Policy?

I want to remind you that a policy is:
- If it's deterministic: a mapping from a state to an action
- More generally: a mapping from a state to a distribution over actions

$$\pi: S \rightarrow \text{Distribution}(A)$$

This is widely used in:
- **Robotics** (which mostly means drones at Penn)
- **Large language models** (think GPT-5)

### Supervised Learning of Policies

If you have a bunch of observed states and actions, this is just **supervised learning**.

#### Example: Robotics
- **State**: All the joint angles
- **Action**: Where you want to move it (if you have a low-level controller), or at a micro level, direct commands to the joints

You can do high-level controller or low-level controller.

#### Example: Large Language Models
This is weirder but critical.

- **State**: The context (all the tokens so far)
- **Action**: The next word

**A large language model is a policy.** It takes as input a large vector of context, which it embeds, and it produces as output actions selected from a probability distribution over tokens.

That's critical for the next half hour because I'm going to actually talk about how to train actual LLMs.

An LLM can be viewed formally as a policy - a mapping from state to action. The action is probabilistic. There's some temperature that controls it in the classic ones. I can't find the temperature knob on GPT-5, but at least GPT-4 had a temperature knob. The more temperature, the more likely it was to pick something that was not just the argmax (most likely), but it picks from some distribution over the top tokens.

### Imitation Learning

If you want to train these things - thinking again, I'm talking about language - I've got a bunch of tokens and I've got the next token. I can just train a supervisor on that to predict it.

I'm calling it RL because I'm now calling it a policy, and we'll use that fact that we think of it as a policy in a second. But realize that at this point, for **imitation learning**, it's just supervised learning of a policy.

### Beyond Imitation Learning

In practice, anything you're using - Claude, Gemini, GPT-5 - none of those are pure supervised imitation learning. They don't just take that. What they all do is some version of RL on top of that.

They start with that pre-trained model, and then they do something to it.

Probably the most famous thing they do to it is **Reinforcement Learning from Human Feedback (RLHF)**.

---

## Reinforcement Learning from Human Feedback (RLHF)

I can actually cover it today because it has RL in the thing there.

This is a slightly hacky system, but I'll walk you through it anyway because you should have seen it if you're going to be doing job interviews from AI companies I've talked to.

### Stage 1: Learning the Reward Model

The first thing I want to do is learn a **reward model**.

To train an RL system, I have to have a reward. Normally, you can actually just show things. I'm going to show someone: "Here are two possible outputs from the neural net - which one do you like better?"

I hire a bunch of people (hypothetically, often in Kenya or somewhere where the salary is low), and we say "which one do you like better?"

For example:
- Output A: "Let me explore this answer..."
- Output B: "Let me delve more into your question..."

They might say "I like the delve one." (I'm told "delve" is popular in Kenyan English - I don't know, I've never been there.)

That's one approach, but that's actually expensive. I'm paying these people - I'm making this up - 2 bucks an hour or whatever (probably 4 by the time it's loaded).

#### Why a Reward Model?

To save money (because everything's expensive), we want to train up a **reward model** which will approximate the humans. We'll then use that reward model for our reinforcement learning.

So there's a **two-stage procedure**:
1. First, learn a reward model
2. Then use the reward model to do RL

### Training the Reward Model

The first part's not RL. We're going to:
1. Take a context (some prompt)
2. Run it through a policy (that's an LLM - your trained LLM)
3. Generate multiple possible outputs (in the original version they did 4 possible outputs; a lot of the modern ones just do 2 outputs)

Imagine instead of 4, we do 2 outputs. We will send them to a human labeler and say "which one do you like better, A or B?" for the answer.

Johnny gets two answers. It's forced choice - you'll like one, you don't like the other one. That's a label.

Now what I can do is compare that to my **reward model**, which is just a standard supervised learner. It's a little itty-bitty neural net that's going to:
- Take in the context and the output
- Predict how much you like it
- Output: is it a good win or is it a loss?

We just repeat that reward model training, and at some point we've learned a model that, given a context and an output (a prompt and a response), gives you some number between, say, 0 and 1, where 1 is "I like it better" and 0 is "I like it worse."

Now I've got a cheap neural net which is way cheaper than paying human beings. I can fire all my human labelers (I'll put them on a different task), and I can now use that to do RL on my big model.

### Stage 2: Using the Reward Model for RL

Now what you do is:
1. We have the context (the prompt)
2. We have the policy that then gives a response (the output)
3. We feed that embedding of that into our reward model
4. It then tells me the reward
5. Which now is a loss which I can use for policy training

Remember, I'm doing RL of this type:
- I've got a policy that was pre-trained by copying, say, the internet (a trillion tokens that predict the next token)
- I've learned my policy
- But now I want to improve that policy by instruction fine-tuning or RLHF to make the policy shift in a direction more consistent with this model of what humans prefer

### Understanding the Two Networks

**Question**: How are we using the human labeler?

**Answer**: We've used the human labeler to train up the reward model. Once I've got the reward model, I don't need humans anymore.

**Question**: Isn't there a reward model on the policy?

**Answer**: No. The reward is: you're in this state, you take this action, you get a reward. The policy is: in your state, give me a probability distribution of actions. They're completely different.

**Question**: Is this self-supervised?

**Answer**: It's really not self-supervised because there are two entirely different neural nets:
1. A **little neural net** which learns the reward model
2. A **big neural net** which learns the policy

The big one is the large language model - that's the GPT-5. The reward model is a little model that says "given a prompt and an answer, how good is the answer?"

What we're going to do is use the reward model (which is purely supervised - "tell me how good this answer is to this question") as the reward (literally the reward) in RL.

We'll spend the rest of the time doing a policy gradient, so I'll have a nice formal mathematical way to actually gradient ascend the policy in a way that increases the reward.

**Super important**: Distinguish these two things - they're totally different:
1. How do I get the reward? (Could be by showing it to you, or they've trained up a reward model to approximate what you would say)
2. How do I optimize my policy? (That's the RL part)

### Why Does the Reward Model Need Context?

**Question**: Why does the reward model need to know the context? Can't you evaluate the quality?

**Answer**: You need the state. I can't evaluate how good an answer is without knowing what the question is. The quality of an answer is a function of the answer AND the question.

Without the prompt, how would I know if it's a good answer?

For example:
- Answer: "3.71"
- How good was that? Nice, right? It's a good answer.
- But what was the question? "What's π?" 
- Okay, not so good. Ballpark, but not quite there.

So the only thing you can sensibly evaluate (either as a human or a neural net) is: given a question and an answer, how good is the answer? It doesn't make any sense to ask how good the answer is in isolation.

---

## Policy Gradient Mathematics

### Initial Setup

Before we start RL, the policy was trained by:
- Pre-training and imitation learning
- Given a trillion tokens of text, predict the next token

So **Stage 1** is: train a supervised model - given tokens, predict the next token, and just run that for enough times to get an answer.

It's not an n-gram, no. Think of what is the standard LLM like:
1. Takes in text: "I went to..."
2. Embeds it into some input
3. Puts that into some neural net
4. Gives you a probability distribution over all tokens

If I say the correct answer was "school," we compute the cross-entropy and gradient descent this.

So given any sequence of tokens, it's a mapping from:
- **Context** (the state)
- To a **probability distribution** over emissions/outputs (the action)

So the initial policy is: no RL, straight supervised - predict the next token.

### The Reward Function

Super important: we've now learned a reward. The reward is a mapping from:

$$r_\phi: (x, y) \rightarrow \mathbb{R}$$

Where:
- $x$ is the input (context/prompt)
- $y$ is the output (answer/response)
- $r_\phi(x, y)$ outputs a real number: how good was the answer to this question?

### Formal Policy Gradient Setup

Formally, we start with a dataset of a bunch of samples $(x, y)$:
- $x$ is the context
- $y$ is the model response

We learn a **policy** $\pi_\theta$, which I will call an LLM. Our policy is a mapping from context to a probability distribution over actions (next tokens).

We're then going to train up a **reward model** $r_\phi$. $r_\phi$ is a mapping from $(x, y)$ to a number, trained by having humans give preferences on $(x, y)$ pairs - they give you "good" or "bad."

### Loss Function for Reward Model

The loss function they often use is a slightly weird one:

$$L = -\log(\sigma(r_\phi(x, y^+) - r_\phi(x, y^-)))$$

Where:
- $\sigma$ is the sigmoid function
- $y^+$ is the preferred answer
- $y^-$ is the less preferred answer

This is the negative log of the sigmoid of the difference between how good someone liked the better of the two answers vs. how much someone liked the worse answer.

So this is a model - $r_\phi$ is our reward model with parameters $\phi$. What I want is: there's a difference between how much I think the better of the two answers is compared to the worse one. I want to make that difference bigger.

Then, for reasons I don't worry about too much, they do a bunch of transformations of it, all of which are monotone.

### Training the Policy with the Reward

Now we've got $r_\phi$, and we want to train $\pi$ (our policy).

Remember:
- This is our initial version of $\pi$ that we initialize with a large language model trained on supervised data
- But now I want to train this policy $\pi_\theta$ so that every time I take a step and generate a new sentence, I'll get a new reward
- I will try to gradient ascend to increase the reward (or more precisely, the return)

### The Loss Function for Policy Training

The loss function we're going to use for $\pi$ has **two pieces**:

$$L = \mathbb{E}[r_\phi(x, y)] - \beta \cdot D_{KL}(\pi_\theta \| \pi_{\text{ref}})$$

#### Piece 1: Maximize the Reward
$$\mathbb{E}[r_\phi(x, y)]$$

This is just the reward. Not so far so good, but...

#### Piece 2: Regularization
$$-\beta \cdot D_{KL}(\pi_\theta \| \pi_{\text{ref}})$$

It turns out that learning in reinforcement learning is a little unstable. These things tend to not converge as nicely as supervised learning. So there are a bunch of hacks (or regularizations) to try and make them a little more stable.

The first of them we'll see (the main one) is: what we'd like is to have the policy that we're learning be close to some **reference policy**, which is like an older version or the one before we did any training.

This says: **don't change the policy too much** from the other policy.

Instead of writing this as $\log \pi_\theta - \log \pi_{\text{ref}}$, which is just to say it's $\log(\pi_\theta / \pi_{\text{ref}})$, you can look at the difference being small or you can look at the ratio.

But in any case, what you're saying is: what I want to know is, compare how does my model I'm learning compare to the reference model?

### Why Regularize to a Reference Policy?

So we're doing two things here:
1. **Maximize the reward**: How good do we think this output is?
2. **Keep the current policy from not being too far away from the reference one**

This is a regularization that tries to pull the policy back to the one we had before and not change it too much.

It's very common in the real world that you've got some initial policy (maybe a policy for scheduling airlines). As you optimize it, you don't want to change it too much. It's very distressing for everybody (the pilots) if you keep massively shifting the schedules.

Or if you're ordering equipment or whatever you're doing, it's nice to have the policy not change too much.

So you want the policy both to:
- **Get better** (increase the probability of getting a reward, increase the discounted return)
- **Not change too much**

And:
- (a) It's nice for humans to have the policy be relatively stable and close to something that made sense originally
- (b) It means the thing's less likely to converge to some wacko place in the space

---

## Policy Gradient Derivation

Now we can get to the really messy equation, which will take the next 20 minutes.

### What We Want

What I want to do is look at how we do **policy gradients**.

The idea of the policy gradient is that I want to try and take the derivative of the **return**.

Remember, in general, I'm going to have:

$$J(\theta) = \mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^\infty \gamma^t r_t\right]$$

This is the expected return - the expected value under some policy of the summation from time 0 to infinity of the discount factor times the immediate reward.

That is the **return**. This is the same as when we talked about last class.

Now what we want to do is do gradient descent (actually gradient ascent):

$$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$$

Update the weights of the neural net $\theta$ by adding a learning rate times the gradient with respect to $\theta$ of the return.

### The Policy Gradient Theorem

How do we find the gradient of the return? Now it gets sort of cool and messy.

The answer is you can write this piece here as:

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) \cdot Q^{\pi_\theta}(s, a)\right]$$

The expected value of:
- The gradient of the log of the policy
- Times the Q-function under that policy of the state and the action

Let me write it out more clearly:

$$\nabla_\theta J(\theta) = \mathbb{E}_{s,a \sim \pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot Q^{\pi_\theta}(s, a)\right]$$

### The Log-Derivative Trick

It sort of makes sense, and it sort of doesn't. But there's a weird piece of math because I wanted something that was the gradient of the return, and somehow I have the gradient of the log of the policy (which looks great, like log probabilities), but it's not obvious.

So I asked ChatGPT how to do that, and it didn't really do it nicely. So let me do it myself.

There's this clever identity:

$$\nabla_\theta \pi_\theta = \pi_\theta \cdot \nabla_\theta \log \pi_\theta$$

What? But what's the gradient of the log of the policy? Use the chain rule:

$$\nabla_\theta \log \pi_\theta = \frac{1}{\pi_\theta} \nabla_\theta \pi_\theta$$

So:
$$\nabla_\theta \pi_\theta = \pi_\theta \cdot \frac{1}{\pi_\theta} \nabla_\theta \pi_\theta$$

We've just proved this identity. It's a weird hack, but I'll say it again:

The gradient of the log of the policy is, by the chain rule, $1/\pi_\theta$ (that cancels out the $\pi_\theta$) times the gradient of the policy.

This is an **identity**, and that then lets us write this weird piece.

### Understanding the Return Gradient

What I really want for this piece is something that's going to give me the gradient of the policy. If you think about what's the gradient of the loss function...

The loss function looks like:

$$J(\theta) = \mathbb{E}_{\pi_\theta}[Q^{\pi_\theta}(s, a)]$$

The return, which is the probability of getting at times how much you get. $Q$ is the expected reward.

What you want to do is: $J$ is going to be sort of the expected value under the policy of:
- How likely you are to do something
- Times how much reward you get if you take it

The expectation is how often you take the policy. If you take the policy, you take that action with the policy - why is it probably an action?

This is the value of that action. Let me write these out again:

$$J(\theta) = \mathbb{E}_{s,a \sim \pi_\theta}[\pi_\theta(a|s) \cdot Q^{\pi_\theta}(s, a)]$$

This says:
- The return is how likely you are to take some action (expected value over that)
- Times how good is that action (what's my return given that action?)

So I've got this for $J$. Then I take the gradient with respect to $\theta$:

$$\nabla_\theta J = \nabla_\theta \mathbb{E}[\pi_\theta \cdot Q]$$

The $\nabla_\theta$ of $\pi_\theta$ ends up giving me (using that funky identity) $\nabla_\theta \log \pi_\theta$.

This makes life easier in the sense that I end up not having the derivative of the policy itself in the middle, so it makes the math cleaner.

This is maybe more than you want mathematically.

### Summary So Far

Let me sort of pop up again and summarize. We start with the return. We said we want to take the gradient of the return with respect to $\theta$. That makes sense.

We then said we do some magic math which says that this is equal to:

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot Q^{\pi_\theta}(s, a)\right]$$

The expected value over the policy of:
- The gradient of the log of the policy
- Times the Q-function (which tells you how good things are)

Then there are simple ways to approximate that where you plug in something else, but now we're off topic.

---

## Advantage Function

There's one more piece here. Let me write out the gradient:

$$\nabla_\theta J = \mathbb{E}\left[\nabla_\theta \log \pi_\theta \cdot Q^{\pi_\theta}(s, a)\right]$$

The gradient with respect to $\theta$ of the log of the policy, times - before I said the Q-function or the reward.

In fact, what most people use is something slightly different, which is the **advantage function**:

$$A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)$$

You can think of it as: how good is the Q-function compared to a baseline, a reference?

The standard reference would be: **how good the state is in general**.

### Understanding the Advantage Function

Let's think about what we're doing for this advantage function. The advantage function says: I know how good this action is in the state.

Rather than taking the absolute "how good it is" (which is the Q-function), let me compare: how good is this action in the state compared to my baseline reference of how good the state is?

$$A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)$$

This takes off all sorts of noise because if you're in a good state or a bad state, you'll have a higher Q-value or lower Q-value.

But this says: I want to know just **how good is this action compared to what I'm typically taking in this state** (what's the value in the state).

So it takes off all the noise of "good state, bad state" and says: a good action is one that works better than I thought the state was.

### Why Use the Advantage Function?

This is a standard **stabilization hack** that takes out lots of the noise.

Again, if you run reinforcement learning in the real world, it's very unstable. So these sort of hacks - and there's a million of them, each version has different versions - but mostly they do this sort of piece here.

**Question**: How is the baseline defined?

**Answer**: The baseline here is just $V(s, t)$ - it's the value function. How good do I think this state is?

**Question**: Are we updating the baseline again every time?

**Answer**: In reinforcement learning, there's a baseline. Think of it the same as a reference. We've done this now three times today. Let me remind you because it's a super important concept.

#### Three Times We've Used References Today:

1. **Double Q-learning**: You're learning a Q-function, compare it, use an old Q-function to play out the games

2. **Policy learning**: If you're learning a policy, make that policy be close in log space to the old policy

3. **Advantage function**: If you want to know how good an action is in a given state, compare how your old model says the state is

In all three cases, we're taking something that's old (maybe update it every 10,000 times or something), and we're saying: how good is my current estimate (the thing I'm trying to learn, my real target) - how good is this state actually - compared to how I sort of think this state is?

That model doesn't have to be good or new, but it has to be **stable** because you're not going to update it too much.

It has to be something that at least captures "good state, bad state." If the robot is really far away from the dock, a good action is just one that gets you a little bit better, even though it's still a crappy state to be in because you're running out of electricity and you're far away.

### The Common Pattern

All three of these methods have this notion of:
- Store an old copy
- Use it to compare against it
- Often regularize against it

This is a staple of actual real-world RL learning: you're almost always pushing the policy to be compared against an old policy, or comparing the Q-value against an old Q-value. In this case, comparing the Q-value against an old value.

They're all slightly different, but they're all asking: **I don't care in absolute terms how good this is. I want to know how good it is compared to basically whatever I was doing before.**

---

## What Are We Optimizing?

**Question**: What are we getting out of this?

**Answer**: What we're getting out of this is: we have a return. We want to maximize the return.

$$J(\theta) = \mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^\infty \gamma^t r_t\right]$$

The policy $\pi$ is a function of the weights in the policy network. So we take the gradient of the thing I'm trying to maximize (the return) with respect to the weights of the policy.

That's what I'm trying to learn. So I'm going to learn a policy.

### The Process

Let me step back. What are we doing?
1. Take some action with our policy
2. See what happens
3. Take the derivative of the return, which is the policy times the reward
4. Take that derivative
5. We're trying to gradient ascend

Actually, in this case, you want to **maximize** the return, not minimize it. We're trying to maximize the discounted expected reward. That's reinforcement learning: maximize the discounted expected reward.

In this case, the only thing we're really learning is the **policy**. So we got one neural net:
- Trained it on language
- Now we're gradient ascent-ing it

### Gradient Ascent vs. Gradient Descent

Good point: in supervised learning, we're always trying to **minimize** an error function. In reinforcement learning, we're always trying to **maximize** a reward.

Everything is exactly backwards. But it's gradient ascent rather than gradient descent.

### What Are We Updating?

It's the policy that we're updating. As we take a gradient step, we're changing the weights $\theta$ in the policy. This is the policy - it's a neural net with the weights $\theta$ in it. We're taking $\theta$ steps. We're changing the weights of the policy such that we will gradient ascend the reward.

The reward is basically:

$$J = \mathbb{E}_{\pi_\theta}[\text{reward}]$$

The expected value over the policy. What's your reward? It's how likely you take the action and what reward you get if you take that action.

This should make complete sense. The reward is the expected value over the actions you might take of how good or bad each action is.

Now I want to gradient ascend that. All of this policy gradient stuff is just: doing a gradient descent on this thing directly is annoying. So this other math (which I don't want to get too bogged down in) says: **here's a way to reformulate this so that the gradient of this thing is in fact easier to compute**.

Then the log transformation pulls things out in a way that makes it a little bit cleaner.

---

## Key Takeaways

### What's Being Updated

We're updating the policy. The key point is: **the policy is being updated in a way that maximizes the expected discounted return**.

I don't want to get too upset about the policy gradient details, but try and take home the main pieces:

1. You've got a return
2. You've got a policy in it
3. You initialize it by pre-training it
4. For RLHF, you learned an approximation of the reward (you can skip that step if you want)
5. For RLHF, we learn that
6. Then you can use this reward where we have humans no longer in the loop because we know the reward
7. Then we can take that and use that to do gradient ascent on the policy

That then gives you something that's **instruction fine-tuned**.

I'm going to skip the PPO (Proximal Policy Optimization) details there. I will walk through this quickly, but I just want you to not read all these words. If you want them later...

---

## Three Stages of RLHF

### Stage 1: Supervised Fine-Tuning
Train up your neural net. Given a bunch of tokens, predict the next token.

### Stage 2: Train a Reward Model
Based on humans saying "I like this answer better than that answer."

### Stage 3: Use the Reward Model to Optimize the Return
That's the RLHF tuning. This whole thing is RLHF.

Versions of that are used in all the language models today.

---

## Summary

### Deep Q Networks (DQN)

If you do the deep Q networks (the first thing I showed you), you can actually get something that's obviously superhuman play on any game you can think of, pretty much just by playing itself.

But it is worth noting as a side point: they're still mostly, once they learn the model, doing some search. They learn a model that then searches ahead of it.

But it still gives massively superhuman results, and at the moment, annoyingly, every time you get a new game, you train up a separate model. Humans are really good at generalizing from one game to another game. Play a game and you change it slightly - you have strategies.

These things don't learn a world model that corresponds to how games work, how people think. They just learned how to play the game really well. So **generalization is an open problem**.

There are lots of different versions where people learn different functions - Q-functions, policies.

### Starting Points

Often in hard problems like robotics, people start with **behavioral cloning** or **imitation learning**.

With natural language processing, people start with some model of language.

You might start with robotics with a simulator, clean up imitation learning on a simulator with a controller, then go and optimize it, and then fine-tune it on the real world.

### Regularization

Lots of regularization. I've said three things (I won't repeat them), but this notion that as you learn your policy, don't move too far away from the old policy:
- It keeps it stable
- It keeps it understandable
- It keeps it sensible
- It doesn't explore vast amounts of stupid policy space

### Contemporary Versions

Finally, sort of the contemporary versions - the things I've talked about have a pretty simple state space and action space:
- For language, it's just the embedding of the text and the embedding of the next token
- For Go, it was just the board game

But often you might learn a fancier embedding to learn some sort of state representation. But again, that's an advanced course.

---

## Policy Gradients Summary

The second piece: **policy gradients**.

These work really well for what is called **alignment** (a jargon term you've probably heard but should know).

**Alignment** means: you want to take your policy and align it with human preferences:
- Don't say racist things
- Don't tell people to kill themselves
- Don't give answers that are nasty or snarky
- Be sycophantic and nice and warm and agreeable
- Whatever you decide

You align it with: "You're such a great person! I love that! It was such a brilliant answer!" So great! Whatever you want, it aligns with.

You can use things like **reinforcement learning from human feedback (RLHF)** for alignment by giving that feedback there.

### Direct Policy Optimization (DPO)

Finally, mostly these days when I do things, I don't do RLHF (which is train up a separate model). We'll do something that is **direct policy optimization**.

Once I have the positive and negative labeled outputs, you can throw those directly into your loss function and say: rather than learning an intermediate reward model $r_\phi$, just put in "here's my loss function, optimize this loss function directly."

So there are a bunch of things with names like **DPO** (Direct Policy Optimization) that give maybe slightly cleaner or slightly dirtier (depending on how you look at it) ways of doing these optimizations.

### Common Theme

All these **alignment methods** have the same notion:
- Some humans say "good" or "bad"
- Somehow that feeds into your loss function, either:
  - Indirectly with RLHF, or
  - Directly with DPO
- Then you gradient descend things to not go too far from the original language model, but to be more respectful of whatever people said was better or worse

There's classic RL alignment. Anthropic also has "tell the things in the prompt what to do," but that's not machine learning, so we don't talk about that today.

---

## Closing

That is it! It's time for Thanksgiving. Go back and have a turkey, or a tofurky, or a vegetable, or whatever your kind of Thanksgiving is.

I will see you all next week. I did post some sample exams, but just take a couple of days off and relax, and then study.

I will see you on Monday!

---

# CIS 5200: Machine Learning - Lecture 25: Active Learning and Experimental Design

## Administrative Announcements

### Final Project and Presentations

The major grading is on the report. The presentation is a chance to make sure you have the story clear and to get a couple of questions answered. It's an opportunity to get your recitation leaders to see your work and to get some feedback, which you have the weekend to finish the writing and the last bits of everything.

The presentations are on Monday. We do want you to share the code. Most people have a Git repo or something to link to. The code does not need to be beautiful and documented. It's more, in my case, an audit trail. If we're trying to figure out what you did in the work, it's nice to be able to go back and double-check the code to say, what did they actually really do?

### Final Exam Details

I have scheduled an optional review session to take whatever questions people still have in this room on Tuesday. The final exam is a week from Friday at 3 o'clock, as I hope you all know, in person. I will post on Ed, but we now have a classroom as well as a time: Chemistry Building 102.

**Important technical details:**
- The midterm was one two-sided sheet
- Final: **two** two-sided sheets allowed
- Twice as much material
- Still closed book, closed notes
- No phones
- Still multiple choice

The final will be about **one-third on the first half** of the course material, and **two-thirds on the second half**.

---

## Lecture Overview

This week, I want to do two things:

1. Talk about what computer scientists call **active learning** and what statisticians call **experimental design** - how do you systematically choose which points to label? (Most of today)

2. Spend the end of today and all of Wednesday talking about **explainable AI**, interpretation, and things that are nice to be able to do.

It's worth noting that this is a course on machine learning, but if you're actually in the real world, an insane amount of machine learning is **collecting data and labeling the data**.

What I want to talk about today is a chance to review some of the material we've had. Things like KL divergence will show up again. The question is: can we take the same sort of formal methods that we've used for learning models and then apply them to the question of, "I've got a crapload of data - which ones should I label?"

### Real-World Example

Think about it: Google used to drive around and take photographs of every street sign and every street address. They still do. You'd like to have a small number of them labeled with humans going through and saying, "That's 321 South 20th Street, label the 321." Or people label things as cancer or non-cancer. Labeling is expensive, and you'd like to optimize, in the same sense we do optimization elsewhere, which items should be labeled.

---

## Introduction: Intuition Behind Active Learning

### Simple Example: Linear Regression

**Labels are expensive.** Let's start with a simple example. We're going to do just a trivial linear regression with univariate $X$ between $-1$ and $1$.

So I've got my line, I've got some $X$'s that range from $-1$, $0$, up to $1$.

**Question:** You get to pick **two** $X$'s to label to fit a linear regression. What two points do you want to pick? Are any two points all equally good?

**Scenario:** What if I pick point $-0.01$ and point $+0.01$? I've got the two $Y$'s, and I fit my linear regression through them. How much do you trust that regression?

**Answer:** Not very much. Why not?

**Response:** Not a lot of variance - specifically, not enough range in the $X$'s. Remember, the $Y$'s are always noisy. We're sitting in linear regression land where:

$$Y = \alpha X + \epsilon$$

This is our model behind linear regression. We're back to the first week of class again.

If you choose points too close together, you could just as well have gotten those two points and fit a completely different line! It's better to spread them out more.

**Question:** How much would I like to spread them out?

**Answer:** As much as possible! If I put one point over here (at $-1$) and one over there (at $+1$), now the line I fit is going to be a more accurate estimate.

### Bias vs. Variance

**Question:** Is the estimate from points at $0.01$ and $-0.01$ a biased estimator?

**Answer:** No, it's unbiased. It's linear regression. I'm reviewing the course - we're at the end. But it's much **higher variance**. The noise, if you're on the far sides, is going to have way less effect on the slope than if you have things that are very close together.

**Key Principle:** In the univariate world, and we'll see that in the multivariate world life's more complicated, but the idea is always the same: **Spread the points you're labeling far apart** if you trust your model, to reduce the effective noise.

---

## Support Vector Machines Example

Now let's do a different example. We're going to do the same sort of game, but instead of actually doing a linear regression, we're going to learn a **support vector machine** (SVM) with hinge loss to try and classify things.

It's going to be hard to do a very good job with two points. Think about what happens with an SVM when we get 2 points:

- Either they're the same label, in which case the hyperplane's got to be somewhere else
- Or they're opposite labels, in which case the hyperplane's going to be right halfway between them

### Sequential Labeling Strategy

**Question:** How would you go about labeling 4 points for an SVM?

**Answer:** Maybe pick one at $-1/3$, one at $+1/3$, and maybe this one is $+1$ and this one is $-1$. If that's the case, then I've narrowed things down, and I know that the hyperplane has got to be somewhere between here and here.

Or if they're both $+1$, now I can say, "Hey, well, try over here, see if you can find a $-1$ over here. Oh, I got a $+1$. Try over here - better be a $-1$?" And now I get a hyperplane that'll be at $2/3$.

**Note:** By doing things **sequentially**, I can do a better job. In some sense, the goal is always to repeatedly split things in half. It's almost like... what was the algorithm when we split things in half?

**Answer:** Tree search! ID3, the decision tree search idea - ask the question that splits things in half.

So in some sense, I want to do something that sounds like tree search, but I'm not searching on features right now. I'm searching on **observations**.

If you recursively split things in half, then you exponentially quickly divide things to a smaller region.

---

## Collaborative Filtering Example

Let's think about something different: collaborative filtering and recommender systems.

Usually, users only want to rate a few movies. I get someone, I can get them to rate 5 movies.

**Question:** What sort of movies should I pick?

I worked for a startup company once (they went bankrupt, because they didn't do this right) - a New York City startup. Someone comes in, you go, "Welcome to my movie recommendation system. I'm going to have you rate 5 movies to get started so I can make you good recommendations."

What sort of movies should I pick?

### Bad Strategies

**Random movies?** If I pick a random movie and ask you, "How well do you like 'My Dinner with Andre'?" - you don't even know that movie! There's tens of thousands of movies, and most people have never seen them. Picking a random movie is a terrible idea because you have no clue what it is.

**Popular movies?** It's biased, but everything's biased. The question is, is it a good bias or a bad bias? You could just show popular movies.

Maybe I tried that when I was selling CDs, and it turned out that showing popular CDs is a terrible thing because people already have them. Showing popular movies? They've already seen them.

But more importantly: **How much information do I get by showing a popular movie to you?**

Most people like popular movies - that's why they're popular. That's not a good **information gain** to show someone a movie that everybody likes.

If my prior is that you have a 99% chance of liking this movie, and I show it to you, guess what? The expected information gain is small. You're not going to learn a lot.

### Better Strategy

You need to have some version of selecting movies that:
1. People know something about
2. Are, in fact, somewhat informative
3. Where some people like them and some people don't like them

A probably better version is: "Tell me 5 movies you like that aren't the most popular movies out there."

The key is looking for something that is going to **convey information**. Popular movies don't convey information because, by definition, they're ones that everybody likes. You want movies that half the people like and half don't like - that's a good movie to use for learning preferences.

Interestingly, someone tried this, and showing popular movies is no better than showing random ones. They both work pretty badly. The popular movies don't tell you about the person doing the rating; they just tell you that the movies are popular, which you knew already.

---

## Active Learning Framework

Now I want to shift from the intro into the first of three sets of methods:

1. **Active learning** (two methods)
2. **Optimal experimental design** (statistician's version, gets a little mathy)
3. **Response surface modeling** (yet another version)

### Core Concept

The idea is really simple. We have some training set. We're in supervised learning again for this world. We have $X$'s and $Y$'s.

**Assumptions:**
- We assume we have lots of $X$'s that are cheap
- Do we usually have lots of $X$'s that are cheap? Yes! Images, videos, brain images...
- Usually, the $X$'s are cheap and the $Y$'s are expensive

Unless you're doing self-supervised learning like "predict the next token," in general, that's the case. I want to find the most useful labels and output a model. This is called **active learning** in the sense that if you ask questions, you can do a better job than if you just sit there passively accepting whatever data comes your way.

### Active Learning Process

1. You pick which $X$ to label
2. You take the $X$ and give it to an **oracle** (technical term - typically some laborer in Kenya or a worker at a company like Scale AI, which was acquired by Meta for billions of dollars, that pays people to label data)
3. Given that new label, you **update the weights** in your model (think stochastic gradient descent)
4. Repeat the process

This works super well, and you can mini-batch it in the real world for efficiency.

**Goal:** You'd like to do the query that's going to optimize something. Pick the best $X$.

And this is machine learning, so you have to quantify what we mean by "best."

---

## Uncertainty Sampling

### Method 1: Most Uncertain Labels

A popular approach is to pick things with the most uncertain label.

- Plug the $X$ into your model, and you get a probability out
- Pick the probability that's closest to 0.5

**Question:** Good idea or bad idea?

**Discussion:** How is it biased? People keep always using the word "biases." Bias is something that's either good or bad. There's always a bias - every machine learning algorithm has what's called an **inductive bias**.

You assume that things are translationally invariant, you assume that things are smooth. You always have to have a bias. The only question is: is it a good bias, or is it a bad bias?

### Understanding the Bias

**What's the bias here?**

In general, it is the case that for most of our models, if it gives you a probability of 0.99, it's more likely to be true than if it gives you a probability of 0.7. So in fact, there is a bias that says **your model is not crap**.

Which it is if you start with no data, but usually you bootstrap active learning with some labeled data. So usually it is the case that 0.99 is more likely to be true than 0.6, and 0.5 does, in fact, typically mean you really are clueless about what the label is.

So that bias is sort of true. But is it a **good** bias?

**Question:** What causes a machine learning model to be uncertain about a label?

Say we're labeling an image: "Is this a rhinoceros or is this an elephant?" What makes it uncertain?

### Reasons for Uncertainty

**1. Rare or unseen examples**

Never been seen before, so one thing that's more likely is that stuff that's rare will tend to be more uncertain. That's **good**! That's pushing **exploration** (to use the RL term for it). We want to find things that we haven't seen before, and if we haven't seen it, the output is more likely to be 0.5.

This could be because:
- We haven't seen a lot of rhinoceroses
- The rhinoceros is at an unusual angle
- It's in an unusual environment

**2. Mislabeled or ambiguous data**

What if it's neither a rhinoceros nor an elephant? That's a problem. We probably made a mistake. We should have had a category "other," which most machine learning practitioners forget.

LLMs tend to be trained to give either "yes" or "no," and people forget to train them that the answer could be "I don't know."

So yeah, it could be something else, in which case trying to label it as rhinoceros or elephant is going to be a terrible idea. That might just show errors in your dataset.

**3. Poor quality data**

The other thing that happens with images? Often they're just **blurry**. If it's a bad image, maybe it's a good thing to label because you're learning to get better at bad images. Or maybe it's just too hard and you'll never get it right.

### Assessment

So **uncertainty is not a terrible surrogate**. It tends to find things that are unusual, that you haven't seen much. But it also means that things that are just intrinsically impossible get labeled at 0.5, whether they're the wrong category or they're blurry. In that case, labeling them's not going to help.

So that's a **heuristic** example, very widely used.

---

## Uncertainty Sampling Variants

One probably wants something that's more principled. Some variants of uncertainty sampling:

### Variant 1: Least Confident Label
Give me something that has the least confident label (probability closest to 0.5).

### Variant 2: Close to Decision Boundary
If you're doing an SVM, give me something as close to the decision boundary as possible. Don't label things that are really far from the boundary - label ones that are close. This is pretty close to saying probability about 0.5 in score space.

### Variant 3: Far Away in Embedding Space
Or you could say: give me things that are just far away in embedding space, things I haven't seen before.

But the faraway ones could be informative, or they could be ones that are just wacko outliers.

---

## When Uncertainty Sampling Works

We talked about when it might fail. When it works, it works quite nicely.

If you label examples at random, you get better and better at finding things gradually. If you label things **actively** (near the margin, things that are 0.5 probability), you can learn a lot faster.

There's no point in labeling things that the AI, that the machine learning model, is 99.9% sure about. Most things are 99% sure pretty soon. So you really do want to focus on labeling stuff you don't know. It makes a big difference.

---

## Information Gain via KL Divergence

A nicer way to do active learning is to come back to my favorite piece of mathematics: the **KL divergence**.

### The Idea

I want to ask about the effect of adding one more label to the model. What's going to happen?

**Before I label the point:**
- I will have some weight matrix (some parameter matrix $\theta$)
- I will have some sort of outputs (think of the outputs as a distribution over probabilities for each training point)

**After I label it:**
- I take one learning step (or I do a mini-batch - label 10,000 of them)
- I will have a new distribution over the labels

Either on the training set or on a different test set, but in either case: labeling data → retrain the model → I get a new distribution over how likely I think each of these $Y$'s are given $X$'s.

**Now I can ask:** How much did I change them? How much did I learn?

### Where Have We Seen This Before?

**Question:** Where does this sound familiar - doing something that changes the KL divergence?

**Answer:** It's **decision trees**! Again, instead of labeling features (picking the best feature like in ID3), here I'm picking the best **points** to label.

---

## Formal KL Divergence Approach

More formally, we want the KL divergence between:

$$D_{KL}\left(P(Y \mid X, X', \theta_{X, X'}) \,\|\, P(Y \mid X, \theta_X)\right)$$

Where:
- $P(Y \mid X, X', \theta_{X, X'})$ is the probability of labels $Y$ given the training set $X$, a new point $X'$ I'm going to label, and the updated parameters
- $P(Y \mid X, \theta_X)$ is the probability with the old parameters

**Question:** Do you want this KL divergence big or small?

**Answer:** **Big**! Learning is changing the distribution, so it should be big.

### KL Divergence Asymmetry

Remember that KL divergence is **asymmetric**.

**Question:** Why is $X$ and $X'$ on the left-hand side rather than the right-hand side of the KL divergence?

KL divergence is between the truth and the approximation - or rather, the better model and the approximation.

$$D_{KL}(\text{truth} \| \text{approximation})$$

The truth (or better model) is on the **left**, and the approximation is on the **right**.

Remember that KL divergence says: "How well does this (right side) approximate that (left side)?" How well does the right one approximate the left one?

So the left side with more data is a **better model**. It's got more data, it's "truer."

---

## The Challenge: Computing Before Labeling

This feels circular. Can I estimate how much the KL divergence will be, how big it will be, **before** I pay for the label?

I've got two $X'$ candidates: this image (image A) and that image (image B).

**Can I tell which one is, in fact, probably better to label?** Which one will shift the KL divergence more?

**The Problem:** I need to pick whether to label A or B **before** I pay for the label. After I pay for the label, it's too late. After the fact, I could pay you to label A and pay you to label B and see which one caused more change, but that's not helpful.

### Solution: Expected Value

**Heuristic idea:** The $X$ that is far from the current distribution is a reasonable heuristic. But I would like to actually do something that gives me a number.

The way to think about this is: **everything is in expected value**.

What you want is to label the point that, **in expectation**, will give you the most change.

---

## Expected Information Gain

So I plug in image A. I've got some probability:

$$P(Y \mid X = A, \theta)$$

Or I've got a probability:

$$P(Y \mid X = B, \theta)$$

I can plug in each of these, and I can say: "How likely am I to get $Y = \text{elephant}$ or $Y = \text{rhinoceros}$ by plugging that into my model with my current $\theta$?"

I can do that **now**, and this will tell me how likely I am to get out an elephant or a rhinoceros **before** I pay the person to label it.

### Example

If my current model is:
- 90% elephant
- 10% rhinoceros

Then in expectation, I get:
- 0.9 times the change in KL divergence if I get the label "elephant"
- 0.1 times the change in KL divergence if I get the label "rhinoceros"

**The key:** The way to view everything in this sort of approach is **in expectation**. You don't know, before you label it, what label you're going to get. But you always have a current guess of what you're going to get because you have a current model.

---

## Review Connection

I am super intentionally reviewing stuff from the first two weeks of class because what I'm showing now is a unifying idea:

We had supervised learning, unsupervised learning, and reinforcement learning. Now we're saying the same modeling principles that apply to supervised learning also apply to **deciding what to label**.

---

## Computing Expected KL Divergence

**Question from student:** "We don't know about the actual label..."

**Answer:** We don't know the actual label. But we have a current model $P_\theta$. This is my current neural net or whatever model I have.

Given some new image (this is image A, this is image B), I can plug that in, and my neural net's going to give me:
- A probability of elephant
- A probability of rhinoceros

I don't know until I pay you to label it whether it's an elephant or a rhinoceros. But I **do** know that there's some estimated probability of elephant and probability of rhinoceros.

### The Expected Value Formula

The expected value over the possible $Y$ labels I'm going to get for that $X$ (which I'm plugging in) of the KL divergence of how different the distribution is:

$$\mathbb{E}_{Y}\left[D_{KL}\left(P(Y \mid X, X', \theta_{X,X'}) \,\|\, P(Y \mid X, \theta_X)\right)\right]$$

Let me write this more precisely. The $\theta$ on the right is before I've actually seen the $X'$, and the $\theta$ on the left is after I've gotten the label.

$$\mathbb{E}_{Y \sim P(\cdot \mid X', \theta_X)}\left[D_{KL}\left(P_{\theta_{X \cup (X', Y)}} \,\|\, P_{\theta_X}\right)\right]$$

This equals the sum over possible outcomes:

$$= P(Y = \text{elephant} \mid X', \theta_X) \cdot D_{KL}\left(P_{\theta_{X \cup (X', \text{elephant})}} \,\|\, P_{\theta_X}\right)$$
$$+ P(Y = \text{rhinoceros} \mid X', \theta_X) \cdot D_{KL}\left(P_{\theta_{X \cup (X', \text{rhinoceros})}} \,\|\, P_{\theta_X}\right)$$

The expected value is summing over the two possible outcomes:
1. We got an elephant
2. We got a rhinoceros

Under the case of elephant, we have a KL divergence of how different the model's predictions are given we updated the weights with the elephant label. Same for rhinoceros.

---

## Practical Computation

**Assumption:** Computing is cheap, and labeling is expensive. That's the world we're in here.

### The Process

What I can do is:

1. Take my model and add in a hypothetical label: "This new image is an elephant"
2. Update the weights → Get a new $\theta$
3. On a separate computer (conceptually), also take the same model and update the weights with: "Nope, it's a rhinoceros" → Get another new $\theta$
4. Plug in a big dataset of $X$'s I want predictions on
5. Under the case where it was an elephant label, here's how much I learned (compute KL divergence)
6. Under the case where it was a rhinoceros label, here's how much I learned

The **expected value of the learning** (which is the KL divergence) is:
- The probability that I'm going to see an elephant label times (KL divergence if elephant)
- Plus the probability that I'm going to see a rhinoceros label times (KL divergence if rhinoceros)

We don't know for sure until we've seen things how good it's going to be, but we can **estimate** how likely we are to see each label given our current model. And given a new label, we can retrain our model.

We're living in a stochastic gradient descent sort of world - we're going to update the model given the new piece.

---

## Scaling to Real-World

If this seems expensive, in the real world you do this:

1. You're going to get 10,000 things
2. Pick and label the 10,000 things (in batch)

But you will still have some probability for each of the 10,000 of getting each of the labels (which may be one of a thousand different labels). But it's still a probability of seeing that label.

You can say: "If I get these 10,000 labels, here's how much it'll change the model."

So it's computationally sort of expensive, but:
- If it's a big model and you're labeling a lot of data, it's still typically cheaper than getting the labels
- If it's a small model, it's cheap

Think about having a doctor label some breast image as being cancerous or not, or waiting for a biopsy result. I can give the model the label as if I'd gotten the label "elephant" and update the model, and as if I'd gotten the label "rhinoceros" and update the model.

### Why Not Just Check Which Update is Largest?

**Scenario:** Say I think it's a 99% chance of being an elephant and I think it's a 0.01 chance of being a rhinoceros, based on my model. I'm pretty sure it's an elephant.

**Analysis:**

If it's an elephant:
- I won't learn very much (it's not unlikely to be low learning because that was high probability already)

If it turns out to be a rhinoceros:
- I'll learn a whole bunch! I thought that was an elephant, but it's a rhinoceros
- But it's only a 1 in 100 chance

**In expectation, I'm not going to learn very much.**

The expected value is:
$$\text{Probability of getting the answer} \times \text{How much you learn if you get the answer}$$

---

## Summary of KL Divergence Approach

We spent a lot of time on this, but this is super important. This is making sure everybody's clear on:

1. **Expected values** and this notion that you can update the model and see what would happen
2. How much would my guesses change on some set of $X$'s if I were to see this answer?

**Final step:** You just pick:

$$\arg\max_{X'} \mathbb{E}_{Y}\left[D_{KL}\left(P_{\theta_{X \cup (X', Y)}} \,\|\, P_{\theta_X}\right)\right]$$

That's the best one to label.

In the real world, you're usually going to do some approximation because labeling things one at a time is too expensive. So you're doing thousands of them, but the same idea applies.

---

## Important Warning

One warning that matters if you're in the statistics world:

**The model has to be reasonably good.**

- If you've got a linear model and you're in a linear world, then two points are fine
- If the world is **nonlinear** and you fit a linear model, you may do terribly badly

Mostly in machine learning, we're using pretty **flexible models**, unlike the statisticians who often use linear models. But just realize these methods sort of assume that the models are reasonably accurate representations of the true underlying function.

---

# Optimal Experimental Design

Now the second topic: **Optimal Experimental Design**.

This is the statistician's version of active learning. Instead of doing things one point at a time, we're now going to label a **batch** at a time.

## Core Concept

In experimental design, we imagine that running each $X$ that you want to label means you have to conduct an **experiment**.

### Example Scenarios

You might have a bunch of possible drugs you could test. You could generate possible molecules, and for each molecule, you'd like to find the label for it, which may require running an experiment. Again, labels are expensive.

I'm going to run a **batch** of these. What we'll see is we want the points to be spread out, but we want to look at the math of how to define "spread out."

---

## Formal Setup

Given a model with some parameters $W$ (think of a linear regression), which queries should we choose?

**Note:** A **query** is an $X$ for which you're asking for a $Y$. You should know the jargon - the query is the $X$.

### The Goal

The standard version for experimental design is: **which queries make you most sure about the $W$'s (the weights)?**

There's always this tension:
- In computer science, we really care about the $Y$'s (predictions)
- We don't care so much about the weights

Think of a neural net - I don't care what the weights are; I care that the predictions are accurate.

But **statisticians tend to think more in terms of the weights**.

For linear regression, there's a one-to-one mapping, so it works out the same. But we're going to now be sitting in **weight space**.

### Key Insight

It turns out, magically, that if you're actually doing a linear regression, the optimal experimental design is **not a function of the actual weights you learn**!

This is really cool. For nonlinear models, things are messy, but usually you do a Taylor series and assume they're locally linear. And then everything is linear. If you zoom in, it's linear!

---

## Mathematical Framework

We are sitting back in the first week of the course where we have a linear regression:

$$Y = X^T \beta + \epsilon$$

where we're in statistics land, so instead of $W$, we're going to use $\beta$ (because statisticians like $\beta$).

### MLE Estimate

As you will recall, the MLE estimate (or the $L_2$ error estimate) is:

$$\hat{\beta} = (X^T X)^{-1} X^T Y$$

This was way back in class.

### Distribution of Estimates

One can show (I won't do the math, but it's not very hard) that:

If $Y$ comes from $X\beta + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2)$ (Gaussian with mean zero, variance $\sigma^2$), then the weights, if you do this a whole bunch of times (remember back in old statistics, you imagine you create infinite numbers of sets of $n$ data points and keep training over and over again), what you'll find is:

$$\hat{\beta} \sim \mathcal{N}\left(\beta, \sigma^2 (X^T X)^{-1}\right)$$

The weights are normally distributed with:
- **Mean**: $\beta$ (it's an **unbiased estimator** - the estimate you get is correct on average)
- **Variance**: $\sigma^2 (X^T X)^{-1}$

This is the new piece of knowledge I didn't show before.

### Interpretation

If you fit linear regression a whole bunch of times on some data points, what's going to happen is:
- You'll get an estimate which is unbiased
- On average, you will get out $\beta$, the true weights
- The uncertainty will look like $\sigma^2$ times the inverse covariance matrix

---

## Minimizing Uncertainty

**Question:** Can I control $\sigma^2$?

**Answer:** That's the noise in the data. Maybe you can control it if you're a roboticist - buy a better sensor. If you're a chemist, buy a better sensor. But often, if you're a data scientist, you're just given $\sigma^2$. That's the noise in the $Y$'s. Can't help you.

**Question:** So I'd like to make the variance on $W$ big or small?

**Answer:** **Small**! I want to make the variance on $W$ small because I'm going to, on average, be closer to the true weights.

So I want to make this small:

$$\sigma^2 (X^T X)^{-1}$$

And now that leads me to the question: **What does it mean to be small?**

---

## Matrix Norms

**First question:** What's the dimension of $(X^T X)^{-1}$?

**Answer:** It's a $p \times p$ matrix (where $p$ is the number of features, dimension squared).

It's **positive semi-definite** (could be zero), so we might do a ridge regression to put a $\lambda$ into it to make it truly invertible.

I want to make this small.

**Question:** How do we measure how big a matrix is?

**Answer:** We're going to take a **matrix norm**.

---

## Optimization Problem

What we want to do is find some $X$'s to label (this is our experimental design problem):

$$\text{Pick } X_1, X_2, \ldots, X_n \text{ such that they minimize some norm of } (X^T X)^{-1}$$

We need to make it small, but we need some norm - some measure of it - a scalar.

There's a whole bunch of ways to make a matrix small, and they all have funny names in statistics, which I don't care if you learn:
- **A-optimal**
- **D-optimal**  
- **E-optimal**

But what I do want you to realize is: there's a bunch of ways to measure how big a matrix is, or how small a matrix is. They are all norms, and they all, in some sense, relate to the **eigenvalues** of the matrix.

---

## Eigenvalues and Matrix Size

**Question:** What properties do the eigenvalues of this matrix have?

**Answer:** They are **non-negative**! It's a positive semi-definite matrix. It's the equivalent of a squared matrix.

### Different Norms

You can look at:

1. **Trace of the matrix** (called **A-optimal**)
   - Sum of eigenvalues: $\text{tr}(X^T X)^{-1} = \sum_i \lambda_i$

2. **Determinant of the matrix** (called **D-optimal**)
   - Product of eigenvalues: $\det(X^T X)^{-1} = \prod_i \lambda_i$
   - You can take the log if you prefer: $\log \det(X^T X)^{-1}$

3. **Largest eigenvalue** (called **E-optimal**)
   - Like an $L_\infty$ norm: $\lambda_{\max}$

You can use the **Frobenius norm** (the $L_2$ norm), or you can pick whatever norm you want. But the point is: if you've got a matrix and you want to make a matrix small, you've got to pick what definition of "small" you want.

This is typically going to be:
- The $L_1$ norm of the eigenvalues
- The $L_2$ norm of the eigenvalues  
- The $L_\infty$ norm of the eigenvalues

The eigenvalues are a nice $p$-dimensional vector. The **size of a matrix is a function of its eigenvalues**, and the same way there are different norms of vectors, there are different norms of matrices. They all correspond to the norms of the eigenvalues of this nice square symmetric matrix.

---

## The Solution

So that's it. Then we can pick whichever norm we want. Maybe we want the Frobenius norm (the $L_2$ norm), and now we've got something that says:

**We have a well-posed problem:**

We're going to pick our $X$'s (some set of $N$ different $X$'s to label), and we're going to label the $X$'s in such a way that they make some norm of $(X^T X)^{-1}$ as small as possible.

**And that is the best $X$'s to label.**

### Remarkable Property

Somewhat remarkably, **if the model is linear, it doesn't matter what the $Y$'s are going to be**.

I'm going to circle back: I started by saying if you had a one-dimensional problem, what you want to do in one dimension is spread out the two points as far as possible.

Now what we're doing, in a very formal, mathematical way, is saying:

**What I want to do if I'm labeling $n$ points in a $p$-dimensional space is spread them out as far as possible.**

Except it's a little harder to picture what it means to be "spread out as far as possible" with $n$ points in a $p$-dimensional space.

---

## Interpretation: Spreading Out Points

What it really means is:

**Take their covariance matrix** so that you're not double-counting things. If you have a bunch of features that are highly redundant (highly correlated), they're going to have a high covariance, and they're going to tend to drive the inverse covariance down. So don't worry so much about spreading out things that are repeated along correlated dimensions.

Do make this whole inverse covariance matrix small.

---

## Summary of Experimental Design

That's the core of experimental design. It's:

**On the one hand:** A little formal and mathy

**On the other hand:** Not very hard

It just says:
1. We know that for linear regression, we know what the distribution of $\hat{W}$ looks like
2. Make the variance term $(X^T X)^{-1}$ small
3. Now there's an optimization problem, but I'm not going to worry about the details because there's plenty of code that will do it

You tell the code:
- Which norm of the matrix you want to use
- Something about the $X$'s

And it will then pick a set of $X$'s that minimizes that norm.

---

## Practical Considerations

**Question:** Is this too expensive to compute?

**Answer:** In general, if you're trying to train something where you have billions of images, this is **insanely, hopelessly expensive**.

But if you have something where you're going to take 20 amino acids and try and test each of them in the lab, the cost of running 20 experiments in a lab is **infinitely higher** than the cost of any computation you could do.

### When This Applies

We're sitting now in a space where we're saying:
- Experiments (getting labels) are **really expensive**
- Therefore, computing is **super cheap** in comparison

People have taken this over the decades and applied it to bigger settings like neural nets. In that case, there's a bunch of approximations you can do.

You're not going to do a search over all possible combinations of $X$'s to go into this optimization. You've got a bunch of $X$'s, you're picking $N$ out of a big set - it's an enormously complicated combinatorial computation.

---

## Extensions and Approximations

I don't want to get lost in the many, many hacks to make this computationally cheap.

What I do want to point out is that:

1. **All the hacks involve starting by thinking about what it is you're trying to optimize**, and then trying to find efficient ways to do it

2. **Almost all the places where people do this for neural nets**, they locally **linearize the neural net** around the place where you have the data, and then they do some approximation that tries to find good points

---

## General Principle

In general, the idea is:

**Try to spread the $X$'s you're labeling out - find ones that are far away from each other.**

This provides a **formal method** of saying what "far away from each other" means.

**Acknowledgment:** You're right that in the real world with a big dataset, this would be hopelessly expensive. But statisticians live with tiny datasets where this is very practical.

---

## What Does "Most Different" Mean?

**Question:** Is this equivalent to picking the ones that are most different from each other?

**Answer:** It is equivalent to picking the ones that are most different from each other, but it is a **formal definition** of what "most different" means.

"Most different" sounds like it means something, but actually: **Give me $n$ points in a $p$-dimensional space that are most different from each other** is not well-defined until you tell me what you mean by "most different."

### Distance Metrics

The first thing you have to tell me is:
- Are you in $L_2$ distance space?
- Are you in some other space?

This approach gives you a precise definition. I've shown up here 3 different definitions of points that are "most different from each other":
1. A-optimal (trace norm)
2. D-optimal (determinant)
3. E-optimal (max eigenvalue)

---

## Callback to Early Course Material

I'm cycling back to the very early part of the course. In the very beginning of the class, I said:

"How far away are two points from each other?"

The first response is: "Well, duh, compute the Euclidean distance - that's how far away they are from each other."

And I said: "Whoa, whoa, hang on. Why do you think Euclidean distance is the right metric? Often an $L_1$ distance is more sensible."

Now I'm saying: I've got $n$ points, and how far away are they? You've got to tell me what you mean by "far away."

### Multiple Notions of Distance

This says it really depends on whether you want something that's:
- Trying to make them, **on average**, far away from each other
- Or do you care about the ones that are **most extreme**, far away from each other?

$n$ points have a bunch of different "far-aways."

Even think about $L_2$ distance: If I want points that are far away in an $L_2$ sense, are they:
- Far away each one from every other one?
- Or each one from the other closest one?

---

## The Deeper Point

What we're trying to do is take something that sounds like it's math ("Give me $n$ points that are far away from each other") and say:

**No, no. That's not computable. I need an actual metric.**

What I want to point out is that if you're in matrix space, there are a bunch of definitions of how big a matrix is, the same way there are definitions of how big a vector is.

### Matrix Size via Eigenvalues

The basic measures of how big a matrix is are all tied to the **eigenvalues**:

- You can take the **biggest eigenvalue** - that's a measure of how big the matrix is (sort of an $L_\infty$-style measure)
- Maybe the nicest one would be to take the **$L_2$ norm of the eigenvalues** (Frobenius norm)
- But they're all legitimate sizes of matrices

---

## Active Learning vs. Experimental Design

**Key Differences:**

### Active Learning
- Pick the **next point** to label (one at a time)
- I said I'm happy with whatever model you have
- Sequential, greedy approach

### Experimental Design (Optimal Design)
- Pick **$n$ points** at a time (batch)
- I just derived this for a **linear model**
- The math on the screen is only going to work if life is linear or approximately linear

**So it's a very different world.**

You can try to merge them together, but people mostly don't. In active learning, you're labeling a bunch of things sort of one-ish at a time. Here we're saying: Give me $n$ at once because they're super expensive to go run the experiments.

We're assuming the model is roughly a linear regression model, so it's a very different set of assumptions.

---

## Greedy vs. Batch Selection

But in some sense, it's the same question: **I'm going to go about labeling $n$ points. How should I do it?**

### Sequential (Greedy) Approach

In the ideal world, if you can label them sequentially:
- You label this one
- The next one will be one this far away
- The next one will be one this far away

But note that that's **greedy**.

### Batch (Optimal) Approach

If I've got a whole bunch of points:

- If you only get to label **one** point, maybe I'll label one in the center (think that's the average)
- If I get to label **two**, maybe I'll pick two that are far away from each other
- If I'm labeling them **greedily**, then maybe here, then maybe here
- If I'm doing a **design all at once**, I can say: "Let me pick a set of them that are optimally spread around"

Try to label that set of them all at once - pick ones that **span the space**.

I was about to say "in some sense of spanning the space," but they span the space in the sense that they make $(X^T X)^{-1}$ small.

---

## A-Optimal Design Example

What happens if you do **A-optimal design**, which is the trace of $(X^T X)^{-1}$?

It turns out that minimizing the trace of $(X^T X)^{-1}$ is the same as minimizing the **Frobenius norm**.

**Jargon term you should know:** Frobenius is what I call $L_2$ (norm by Frobenius - it's named after a mathematician).

### Properties

A-optimal design is going to pick points that are closer to the boundary. If you've got something where you're labeling points that are sampled from some Gaussians, you're going to tend to get points to label that are as far away as possible.

It's easier to see what "far away" means in 2D space. If you're in a very high-dimensional space, then "far away" gets sort of messier to visualize.

---

# Response Surface Modeling

Finally, the third piece. I want to come to a very different kind of active learning called **Response Surface Modeling**.

## Key Difference

This is going to be the case where we're **not** trying to find a global model - not trying to label the points that give me the best model across all the points in some huge testing set.

**Instead:** We're trying to label the points that will give me the **best place** to run my robot or to run my power plant or to configure my system.

---

## Supervised Learning vs. Optimization

Think about classic **supervised learning**:

You want something that, in expectation over some $X$'s, is going to give you the **lowest error** (best predictions).

In **response surface modeling**, I'm going to have something where I want to find something that helps me, for a big space of $X$'s, to **find the maximum $Y$**.

**I want to maximize the function.**

**Question:** What kind of learning is that closest to? Supervised, unsupervised, or reinforcement learning?

---

## Formal Setup

Let me be a little formal about it. I've got some unknown function:

$$Y = f(X)$$

I don't know $f(X)$. All I'm going to get is $X$'s that I can sequentially label with $Y$'s.

**Goal:** Find

$$\arg\max_X f(X)$$

**But:** I don't know $f$.

Does that make sense as a well-posed problem?

---

## Real-World Examples

Hopefully it makes sense from a real-world perspective. There are a number of times:

### Factory Example
I'm running my factory, and $X$ is all the settings on the factory (temperature, pressure, flow rates, etc.), and $Y$ is how much money I'm going to make in the factory per hour.

I want to change the settings on the factory $X$ in a way that optimizes how much money I'm making per hour. The only problem is I don't have a model of the factory because, hey, I'm a machine learning person, and most people don't have good models of their factories.

### Biology Example
If you're a biologist, you've got a bunch of parameters:
- Temperatures
- Salinity  
- Sugar level
- Nutrients

You've got to pick the right $X$ vector of course (always a vector) that maximizes production. So $Y$ is how much you're growing of these organisms.

---

## What Type of Learning?

**Question:** What kind of learning is it closest to? You've got 3 choices: supervised, unsupervised, or reinforcement learning?

**Answer:** **Reinforcement learning!** It sounds like a reinforcement learning sort of problem.

In some sense:
- $X$ would be your control actions that you're taking
- $Y$ would be your reward you're trying to maximize

It's not quite RL, but it has some of that same flavor. It has an RL-ish flavor.

**Historical note:** This dates back - well, RL dates back before I went to school. They both date back forever. I went to school with one of the inventors of RL.

---

## The Problem Formally

The formal piece is: **I want to find $X$ to maximize $Y$ for some function $Y = f(X)$.**

I'm in a sort of **gradient ascent** world, but I don't actually know $f$, so I'm both:
1. **Learning** $f(X)$
2. While doing **gradient ascent**

---

## The Algorithm

What we're going to do:

1. **Start** with some $X$ and $Y$ (some training set)

2. **Fit a model** $\hat{f}(X)$ (or $\hat{f}_\theta(X)$)
   - The fancy name for this in response surface modeling is a "response surface," which is just $Y$ as a function of $X$
   - There are some weights $W$ or $\theta$ if I'm doing a parametric model

3. **Alternate** between two steps:
   
   **Step A: Optimize in $X$ space**
   - Given my current model, do a gradient ascent step
   - Move to a new $X$ that I think will give better $Y$
   
   **Step B: Update the model**
   - Run the system at that new $X$
   - Observe the actual $Y$ (which might not be exactly what the model predicted)
   - Refit the model with this new observation

---

## Visual Description

Let me draw a picture. I'm at some $X$, here's $Y$ as a function of $X$.

I've got my model (this is my estimated model, of course - I never know $f$; $f$ is the real world; only mathematicians know $f$).

So I'm here at some $X$ with my current model $\hat{f}(X)$.

### The Process

1. **Take the gradient** of my current model and do a gradient **ascent** step
   - This moves me up to some $X$ that I think is better

2. **Take the action** - run the plant at that new $X$
   - Run it for an hour
   - Observe the thing
   - Get a new $Y$

3. The $Y$ is close to what $\hat{f}(X)$ said, we hope, but **not exactly the same**
   - Maybe I actually get a $Y$ that's not quite as good as I thought it was going to be (bummer)

4. **Update my model**
   - I've got a new observation
   - Refit the model
   - The model now looks different

5. **Take another gradient ascent step** in the updated model
   - Move to a new $X$
   - Measure the new $Y$
   - Refit the model again

**I'm alternating between:**
- Doing a gradient ascent in $X$ to move toward a better $Y$ (higher output)
- Using whatever I observe to refit the model

---

## Connection to Reinforcement Learning

This should look sort of familiar back to some of the RL methods where there were cases where we were alternating between:
1. Learning some version of a model of the world (be it a $V$ or a $Q$ function)
2. Improving a policy

---

## Simplicity Compared to RL

This is **way simpler** than RL because it's just function approximation.

**Key question:** Do I care what this function is, how accurate it is, everywhere in the $X$ space?

**Answer:** Not at all!

I've drawn a one-dimensional space, but realize that we mostly live in a very high-dimensional space. For the vast amounts of most of the $X$ space, **you don't care what the model is**.

**I only want to know what the model is where I get really high performance** - where the $Y$ is big.

---

## Philosophy

It's a nice philosophy to say that for an awful lot of problems:
- I don't care about global modeling
- I'm not in classification land where I want to recognize elephants and rhinoceroses
- I'm in the land of **production** where I want to **maximize production**

**There's no point in modeling bad parts of the space.**

But of course, I need to have a model that's good enough in the regions I care about.

---

## How This Can Fail

**Question:** What's the downside? How could this fail?

There's some assumption about what kind of optimization this is.

**Student response:** Maybe convex, maybe not.

I didn't tell you anything about what the model looks like, so...

### If the Function is Convex

If this were a convex function, then you're in much better shape. You don't have to be convex, but if the world is convex and you do a reasonable job and don't take too big steps (and a bunch of other things), if the world's convex, you're pretty well off with this sort of approach.

### If the Function is Non-Convex

But the world does not have to be convex. The world could well look like it has **multiple peaks**!

**Problem:** Local optima.

---

## Concerns: Overfitting vs. Under-Exploration

**Student:** So it's overfitting in some funny sense?

**Response:** I have to regularize a little bit for any model, but I should get - if I'm operating around here, usually the world has enough noise, and usually I end up fitting something that's a fairly reasonable model in the local region.

So it's possible you'd overfit - that you get a noisy sample and one of the $X$'s looks really good, but in fact it's not actually good (you just happened to get lucky that time). So overfitting's always a concern, just because anytime you fit a model, and these are running real plants, so it's hard to collect lots of $X$'s to get the $Y$'s.

### The Bigger Problem: Under-Exploration

But even more than overfitting, suppose I've converged to a local maximum over here.

**Question:** What did I show my model? What does the real world look like?

If you don't do enough **exploration**, there may be parts of the world that are really good that you never find.

In some sense, it's not really overfitting in the classic ML sense from the first part of the course, but it's **under-exploring**.

---

## The Exploration Dilemma

**The bad news:** This is sort of an **unsolvable problem**.

You don't want to try the whole $X$ space because:
1. Most of it is expensive
2. You're producing real product
3. To run the system at a suboptimal setting is **losing money**

### Reality Check

Every time you go to your manager and say, "Hey, let's try taking the temperature and cranking it way up and seeing what happens," I can tell you for sure what your manager's going to say:

**"No way whatsoever."**

"We're working okay. We'll change it maybe half a degree, but we're not going to change it 10 degrees. You're crazy."

So they won't let you explore enough.

---

## Clarification Questions

**Question:** Why do we want to minimize the $Y$?

**Answer:** Two things are getting confused:

1. On the board, I've always talked about **maximizing** the $Y$ and doing **gradient ascent**
2. On the screen earlier, I wrote **minimize** the $Y$ with **gradient descent**

I don't care whether $Y$ is being minimized or maximized - it's **mathematically equivalent** (just up to a sign).

So yes, I'm sorry about the confusion there, but it's not important.

### Why Maximize or Minimize?

**Why do I want to maximize/minimize $Y$?**

Because $Y$ is some metric you care about:

#### Examples where you maximize:
- $Y$ is how much money you're making (most companies want to maximize profit)
- $Y$ is throughput (maximize how much product you're producing)
- $Y$ is GPU utilization (you want to maximize it)

#### Examples where you minimize:
- $Y$ is cost (minimize operating cost)
- $Y$ is idle time (minimize the time GPUs are sitting around waiting for work)

Think of this as picking settings for a factory, and $Y$ is your objective. They're one minus the other, so I don't care which one you do.

---

## The Core Idea of Response Surface Modeling

The important part is:

**The goal is there's something you want to make big or you want to make small.**

You're not really sure, as you change a bunch of things in your policy... $f$, in some sense, is like a policy. It's not quite a policy because it's not mapping to an action, it's giving you a reward.

Think of $Y$ as like a **reward** - it's how well are you doing. You're trying to maximize your reward by picking the $X$ that maximizes it.

---

## The Duality

Again, this duality, this back and forth:

**If you knew $f(X)$:**
- You could plug in a bunch of $X$'s and find the best $Y$

**But you don't know $f(X)$:**
- You need to actually try some $X$'s
- Get some $Y$ labels
- Fit the model

Now you can see you can do any sort of exploration you want with $X$'s to fit a good model, but you don't care about fitting the model in expectation over all $X$'s.

**You only care about fitting the model where the $Y$ is good.**

---

## Different Loss Function

This is **radically, radically different** from the typical loss function.

I don't care what $f(X)$ is everywhere. You can make it any function you want. These things are all easier if you can gradient descent them, but it's just some function with some parameters.

**The key difference:**

I want you to always be thinking about what the loss function is, and it's so different to care about:
- **"I care about finding the place that maximizes $Y$"**

versus

- **"I care about fitting the thing everywhere"**

---

## Visual Example

Here's a picture with minimizing an energy function.

I have $Y$ as a function of $X$, and I'd like to minimize it. I've got some data points (the red dots) that I've fit, and I've got some model I fit.

What I want to do is to **move down to the minimum** of the curve, but as I keep taking points, I'll keep re-estimating the curve and get a different estimate of where the minimum is.

This is exactly saying the thing I said before - alternating between optimizing and learning.

---

## Summary of Methods

So what did we cover?

### 1. Active Learning

We talked about:

- **Uncertainty sampling**: Label the $Y$ with probability most close to 0.5
- We did NOT talk about **query by committee** (won't be on the final)
  - Query by committee says: If you've got five different models (an ensemble), wherever the different models disagree, that's an interesting place to label
  - If all 5 models agree, don't label it
  - If 2 say yes and 3 say no, that's a great point to label
  - That's a different kind of uncertainty

- **Information gain**: Maximize information gain using KL divergence
  - That's certainly something to be thinking about

### 2. Optimal Experimental Design

Where we do something that in some form or another minimizes:

$$(X^T X)^{-1}$$

in some norm, so as to make $\hat{W}$ be low variance, which will give us weights close to the true value. Which is super nice.

### 3. Response Surface Modeling

Which said: There are a lot of cases where you're optimizing - where you're fitting the model and optimizing at the same time. That's another one of these back-and-forth algorithms.

---

# Model Interpretation and Explainable AI

Now I want to shift gears and start talking about something more fun: **model interpretation**.

We're going to start a little bit today, and then we'll finish on Wednesday. We'll get a little more detailed about it.

---

## Importance for Final Projects

What I want you to think about for your final projects as you build your models:

**What is it that you can tell me about what features are driving them?**

In general, it's just useful to **look at the data**.

---

## Basic Principle: Look at Your Data

I spend so much time telling computer scientists:

- If you're analyzing **texts**, read some of the texts
- If you're analyzing **images**, look at some of the images
- If they're breast cancer images and you don't know about breast cancer, talk to a doctor and have them walk you through them

You want to see what's going on both for building your model and because you're almost always doing this for someone else.

---

## Exploratory Data Analysis

Start with just simple raw statistics.

### Real-World Example: CDNow

In the year that Amazon was founded (it's a long time ago), I built a recommender system for a local startup called **CDNow** that was selling online CDs.

We looked at just simple things like **how many CDs each person had bought**:
- Almost everybody bought one CD from us
- A few people were buying hundreds of CDs

In the first month we launched, we tried to find out where they came from: **Japan**.

It turned out that retail was so expensive in Japan, you could mail order CDs from the U.S. and buy a CD cheaper than you could buy them in a standard supermarket or retail shop in Japan. So people were buying them and reselling them.

**Was this useful?** Sure! Make a different webpage for bulk buyers.

---

### Real-World Example: Supermarket

I was looking at supermarket checkout data. You check out, you get your receipt with all the groceries you bought.

There were some people who were buying **10,000 items a day**.

We were trying to do recommendation systems - today's task was what coupons to offer to get you to come back again:
- Here's a coupon for milk
- Here's a coupon for eggs

But if you're vegan, the egg coupons don't work. Giving you useful coupons is the goal.

But we wanted to throw out: **Who's buying 10,000 items a day?**

**Does anybody shop at a supermarket with a frequent shopper card?**

The **cashiers** typically have their own frequent shopper cards, and they'll swipe their own card for you to give you the discount.

But this is **not useful for recommender systems**. They're not normal people - they're a mixture of 100 people they bought stuff for.

---

## Simple Exploratory Data Analysis

**Look at the data:**
- Are there weird counts?
- Are there things that are extreme?
- What's going on there?

This is called **exploratory data analysis (EDA)**.

### For Images
- Look at the images
- Are they clear?
- Are they blurry?

### For Any Data
- How many things are missing?
- Just some simple statistics:
  - Means and standard deviations
  - If it's dollars or hours, what are the extremes?
  - What's the biggest number of hours?
  - What's the biggest number of dollars?
- Are they all negative? Are they all positive?
- Some sort of histogram

Look at the data as you do things.

---

## Feature Correlations

If you've got a small number of features (like 100), start with just **correlations**:
- How correlated are the features with the outputs?
- Are there strong relationships?

**Note:** For some things, you can't do that. If you're sitting in an embedding space, all you can do is reason about the embeddings differently.

But you could ask:
- How long are the texts?
- Are they 5 words? 50 words? Some 5,000 words?

Just look at what the data looks like.

---

## Visualization Examples

There are fun ways to visualize. These are images from U.S. yearbooks - average faces from different decades.

You can see what a guy in 1900 looks like compared to a guy in 2010. There are subtle shifts in the faces.

### Example: Women's Faces Over Time

Maybe it's a little more striking with women.

**What's happened between the 1910s and the 2010s?**

She looks really different than her great-grandmother:
- Different hairstyle (longer)
- Big smile

Now you all smile for the camera, but back in the teens and twenties, apparently people didn't (I don't know, before my time).

These visualizations let you see what's going on. It's fun - the faces were somehow rounder back then, now they're more elongated. I have no idea why, but you can see it.

---

## Examining Clusters

If you're doing clustering (clustering movies, clustering genes, clustering anything):

**Pick the ones that are close to the center** - what are they?

Look at the **centroid**:
- This is the "Star Wars cluster"
- This is the "particular small RNA cluster"

Look at them and understand what defines each cluster.

---

## Finding Representative Examples

The other thing that's super useful for any model: You can say for a given output, **what input maximizes some given output?**

We did this with neural nets, but let's think about it more generally.

### Two Perspectives

1. **Over all possible pixel combinations**, which one most maximizes my probability of "rhinoceros"?

2. **Of all my images in my dataset**, which images are most labeled as "rhinoceros"?

You can look at that also for **interior nodes** (hidden layer activations). Super nice.

---

## How to Search Over All Inputs

**Question:** How do you do a search over all possible images?

If I've got a neural net $f_\theta(X)$, and I want:

$$\arg\max_X P(\text{rhinoceros} \mid X, \theta)$$

Over all the images, which one maximizes my probability of rhinoceros?

**How do you maximize a neural net function?**

I mentioned this at one point in the middle of the semester.

---

## Gradient Descent in Input Space

Let me ask a different question: If I want:

$$\arg\max_\theta f(X, \theta)$$

How do I maximize that?

**Answer:** Gradient descent (or ascent), we hope!

---

## The Duality

Remember that neural nets (and all machine learning models) are a function both of:
1. The **inputs** ($X$)
2. The **weights** ($\theta$ or $W$) if it's parametric

If you want the argmax over the $X$'s, you can do a **gradient ascent in $X$ space**:

1. Start with a random image
2. Plug it in
3. Compute $\frac{\partial f}{\partial X}$
4. Do gradient ascent to walk up the $X$'s to find a maximum for the given weights in the model

**That's the $X$ that is most likely to be a rhinoceros.**

---

## Will This Find the Optimum?

**Question:** Is it going to find the global optimum?

**Answer:** No. This is a nonlinear function, it's non-convex. But it will find **an** optimum (a local maximum).

If you walk up with a reasonable gradient ascent schedule, it does a nice job.

---

## The Duality: Always Remember

It's worth realizing this duality. We go back and forth:

$$f(X; W) \quad \text{or} \quad f_W(X) \quad \text{or} \quad f(X, W)$$

However you want to write it - same idea.

We can optimize over $W$ (learning) or optimize over $X$ (finding representative inputs).

---

## Real Example: Twitter Profiles

This notion says: **What image most gives this label?**

I did this experiment. I grabbed a whole bunch of people's Twitter profiles. I estimated:
- How **extroverted** they were
- How **conscientious** they were

I picked the two highest people (whose permission I got - these are actually real people who consented to share them with me) over 100,000 Twitter users:

- **She was the most extroverted**
- **He was the most conscientious**

**What does a conscientious photo look like?**

It's not one where you've got a cat in front of your face (that's "open to experience").

You can look at them and sort of - I don't know what an extrovert looks like, but she sort of looks like it:
- Friendly and warm
- Likes talking to people
- Wants to have 10 of her closest friends over for dinner

---

## Takeaway for Model Interpretation

The takeaway from this is to think about your data and look at it. Are there concrete examples of what your model is doing that:

**A)** Sanity check it

**B)** Give you examples so you can show someone else

For example: "Yep, conscientious people wear a colored shirt when they're public on social media."

It's not even LinkedIn, it's Twitter. It's a style choice that correlates with personality.

---

## Conclusion

I'm going to stop there. We're going to come back on Wednesday, and we'll talk about **feature importance** again and again - diving deeper into interpretation methods.

### What You Should Be Doing

You should be working on the final projects and thinking about a very short presentation about what you're doing. I'll post more stuff on Ed.

Keep moving forward on your projects!

---

## Key Concepts Summary

### Active Learning
- Uncertainty sampling
- Information gain via KL divergence
- Sequential querying strategies

### Optimal Experimental Design
- Minimize $\text{norm}((X^T X)^{-1})$
- Different matrix norms (A-optimal, D-optimal, E-optimal)
- Spreading points in feature space
- Linear model assumption

### Response Surface Modeling
- Maximize $Y = f(X)$ while learning $f$
- Gradient ascent + model updating
- Exploration vs. exploitation tradeoff
- Application to optimization problems

### Model Interpretation
- Exploratory data analysis
- Looking at representative examples
- Gradient-based input visualization
- Understanding what drives predictions

---

# CIS 5200: Machine Learning - Lecture 26: Lecture on Feature Importance and Explainability

## Administrative Announcements

Okay… yep, we have an echo.

Okay, um…

### Final Project Presentations

Recitations this week, as noted, are presenting your final projects. I know they're not quite finished for many of you. You've got the weekend to work on it. But for the presentation, so people can see it, you should have seen on Ed the request to please upload your slides to the Google Drive for the recitations.

These are super short presentations. You'll have:
- 7 minutes to present
- 2 minutes for questions
- 1 minute to swap people around

So I don't want to be plugging in laptops or running around. And make sure if you put a link to your slides, they are globally readable. Because every time I do this, we go there and someone tries to click on the slides, and they can't see them.

Questions?

Are the questions… there… yeah, whatever the rubric says, and we will pass to the recitation instructors. The presentations are graded. The biggest grade is your final report, but the presentations are graded. You don't need to have finished everything, but you should be making a clear presentation:
- This is the problem
- This is the methodology
- This is the metric of how we're doing this
- This is the A-B test

You need to make sure you have a clean story, right? And a lot of machine learning is telling a clean story of:
- Why do I care?
- How did I test it?
- What did I find?
- What's the significance?

Good?

Cool. There's a request on Ed—the TAs couldn't find readings for the reinforcement learning section. Sorry, I will get those out, like, tonight, because we're missing readings for those.

## Introduction to Feature Importance

And for the last new content today, I wanted to talk about feature importance.

Last class, I ended by having a little bit of overview of explainability—look at examples, look at the data. And today I want to talk about a bunch of techniques for measuring feature/variable importance.

### Key Takeaways

What I want you to take away from here is probably 6 or 7 different jargon terms used in machine learning for understanding the kinds of feature importance, and in particular, to realize one simple piece here:

You fit some model, and if I ask questions like, "How important is the feature $X_1$?", what's a sensible way to measure it?

### Simple Example: Linear Model

Consider a simple linear model:

$$y = 1000 \cdot X_1 + 50 \cdot X_2 + 10 \cdot X_3$$

Does the coefficient of 1000 mean that $X_1$ is more important than the next two? **No.**

It depends upon the domain of $X_1$ and how much it varies. What you want for $X_1$ is: **How much does $X_1$ change?**

- If it goes from 0 to 1, then it's roughly 1000 units of importance
- If it goes from 0 to a million, it's roughly $1000 \times 1,000,000 = 1,000,000,000$ (a billion) units of importance

In some sense, and we'll get much more formal about this, how important something is is the product of the feature and its weight in a linear model, or it's how much the output changes if you change the input.

### Standardization

If you're a statistician, what you do is you take the $X$'s and standardize them:
- Subtract off the mean
- Divide by the standard deviation

$$X_{\text{standardized}} = \frac{X - \mu}{\sigma}$$

By the way, where is that most popular in machine learning?

**Deep learning**—it's common to standardize them. In theory, it shouldn't matter because it's a universal approximator, but it makes things converge better.

And there's one other place of the many techniques we've covered this semester where standardization is the default: **PCA (Principal Component Analysis)**.

Principal Component Analysis is not scale invariant. It says, what are the directions of maximal covariance in an $L_2$ sense? And so, depending upon what you're doing, often for PCA, you will standardize the $X$'s first. These are all jargon terms you should know and think about, because it gets you really wrong if you don't do them.

### Feature Removal and Retraining

You could also try and remove an $X$ and say, "Hey, how much effect does it have on the prediction if I get rid of an $X$?" And again, I'm going to be much more formal later on today.

But the way that most people don't do it is to remove the $X$ and retrain the model. That's one measure of importance: How much should I pay for this feature? Let me retrain the model without it.

### Feature Selection Methods

**Question:** What method did we use that statisticians like to decide which features to keep and which ones not to keep? We did feature selection such a long time ago.

What method do computer scientists like? What do scikit-learn people like for deciding which features to keep or not keep?

**Answer:** $L_1$ regularization would be the standard one that the scikit-learn people and computer scientists like.

And $L_0$ or **stepwise regression** is the one that statisticians like. Stepwise regression puts in some features and throws out other features.

#### Stepwise Regression Behavior

If you've got two features that are perfectly correlated, what does stepwise regression do with them?

**Remove one of them.** Which one? The first one you come to gets put in, and the second one gets dropped. The order of the features matters. In extremely stepwise regression, it's random, right? With stepwise, you try all of them, pick the best one—you can flip a coin.

So with stepwise regression, if you've got two good features, one's in and one's out.

#### Why Computer Scientists Don't Like Stepwise Regression

By the way, why does scikit-learn not like stepwise regression? Last I checked, they don't implement it. They only do $L_1$.

Why do computer scientists not like $L_0$ regression? What's the one mathematical property of $L_1$ that's…

**$L_0$ is non-convex!**

$L_1$ is convex. It means there's a global unique optimum. It means you just do gradient descent to find it. $L_0$ is an ad hoc, crappy search method that you can't—that provably, you're not going to be guaranteed to get the right thing unless you try too many combinations.

### Feature Importance Without Retraining

So you could try removing the features and retraining the model, which is rare except in statistics because it's too expensive. Or you could try: here's a model, I'm given the model, and now we're gonna take the feature and replace it always with something—a reference value like 0.

And we'll see a bunch of those. So, mostly feature importance says: **Given a model, we're not going to retrain it. Tell me how important every feature is.**

And mostly, we're not going to retrain the model with different features because it's too expensive, unless you're a statistician with 10 features, in which case you do stepwise.

## Correlation Among Features

The key thing to note is that real-world features are always highly correlated.

### Example: Correlated Features

If you have a model that says:

$$y = X_1 + X_5$$

And I have 4 copies of $X_1$. How could you get 4 copies of $X_1$ in real data?

**Example:** The item price, the tax, the item price plus the tax, the amount you were charged—those are all perfectly collinear (in some odd world where all taxes are the same).

So in real data, there's lots of correlation across the features. And in some sense, any feature interpretation model needs to deal with that.

### Principal Component Analysis for Correlation

Now, what's one way to deal with the correlation? We talked about PCA.

For the model above with $X_1, X_2, X_3, X_4$ (all copies of $X_1$) and $X_5$, how many principal components should this model have?

**Two.** There are two independent components here. Features 1, 2, 3, and 4 should be one component (average them), and feature 5 should be a different component.

So if the world is nice, you often do principal components where they now are likely to be fairly independent—they're orthogonal. But sometimes you do that, sometimes you don't. If you're in a neural net or a decision tree, then you've got all your features there.

### The Central Question

Now the question we're going to wrestle with for the next hour is: **How do you assign importance to $X_1, X_2, X_3$, and $X_4$ when they're all highly correlated?**

I'm gonna spend almost a whole hour talking about how to do that.

### Example Solution with Ridge Regression

One way would be to say, "Hey, I will give them each a weight of $\frac{1}{4}$." That's a perfect model:

$$y = \frac{1}{4}X_1 + \frac{1}{4}X_2 + \frac{1}{4}X_3 + \frac{1}{4}X_4 + X_5$$

What style of regression is going to give me a model that looks like that?

#### OLS Problem with Correlated Features

By the way, if I do OLS (Ordinary Least Squares) on a dataset generated by this, what happens?

Going back to day one—second week, right? The weights are:

$$\hat{\beta} = (X^T X)^{-1} X^T y$$

If you've got 4 columns of $X$ that are linearly dependent, what is $(X^T X)^{-1}$?

**Undefined!** There is no inverse. It's not invertible. If you try and solve OLS here, you're gonna get nothing out—you're gonna get a divide-by-zero error, typically.

#### Ridge Regression Solution

So what's our standard hack? **Ridge regression:**

$$\hat{\beta}_{\text{ridge}} = (X^T X + \lambda I)^{-1} X^T y$$

Now it's always invertible, guaranteed, nice math.

**Question:** Under what conditions does ridge regression give me that equal-weight solution?

**Answer:** When $\lambda$ is really small. If $\lambda$ goes to zero but does not hit 0—so what is $\lambda$? $\lambda$ says shrink everything somewhat. Shrink the bigger things more. And in the limit—and this is a formal math statement—in the limit as $\lambda$ tends towards zero (not setting $\lambda = 0$, which is not defined), you will in fact get out something that, in this case, forces all of those small weights to be equal.

#### L1 vs L2 Behavior

Will $L_1$ regression do that? What does $L_1$ do now? $L_1$ doesn't care, because if you have $\frac{1}{2}$ and 0, or $\frac{1}{4}$ and $\frac{1}{4}$, they have the same $L_1$ size.

$$\left|\frac{1}{2}\right| + |0| = \left|\frac{1}{4}\right| + \left|\frac{1}{4}\right| = \frac{1}{2}$$

Whereas $\frac{1}{2}$, 0 and $\frac{1}{4}$, $\frac{1}{4}$ as $L_2$ are not the same:

$$\sqrt{\left(\frac{1}{2}\right)^2 + 0^2} = \frac{1}{2} \neq \sqrt{\left(\frac{1}{4}\right)^2 + \left(\frac{1}{4}\right)^2} = \frac{1}{2\sqrt{2}}$$

If you take the sum of the squares and square root it, you really want to make them all small. Whereas if you take the absolute value of them, it's underspecified.

So in general, if you have things that are highly correlated, a little bit of ridge gives you something that's sort of sensible in terms of a stable solution and nice mathematical properties.

### The Feature Importance Dilemma

On the other hand, how important is $X_1$? Should it be $\frac{1}{4}$, or should it be 1? (I'm gonna assume $X$ is standardized.)

Imagine you take $X_1$ and you change it from 0 to 1. How much should $y$ change by?

Some measure of feature importance is just sensitivity: change the input, see how much the output changes.

The coefficient vector should have total weight 1 across all the correlated features, but I can't make the coefficient for all of them be 1, otherwise now I'm going to get 4. It's just 4 copies of the same thing, so you could say it should be [1, 0, 0, 0, 1], but that's a terrible solution. And [1, 1, 1, 1, 1] does not work because now you've got: every time you change $X_1, X_2, X_3, X_4$, you've changed it once (but the effect is multiplied by 4).

### Two Types of Feature Importance Questions

So note this sort of weird piece here: feature importance in a univariate sense. I could ask, "How much does $X_1$ correlate with $y$?" That's a well-posed question. Find the Pearson correlation. That's one measure of importance.

I could ask, "If I change $X_1$, how much does $y$ change?" And now there are two different questions:

1. **Off-manifold (Interventional):** How much does $y$ change if I change $X_1$ holding everything else constant? Which, in some sense, makes no sense because these things are all the same thing.

2. **On-manifold (Conditional):** How much does $y$ change if I change $X_1$ preserving the correlation matrix of how the $X$'s are correlated?

### The Manifold Concept

I'm gonna come back and cover this later again today, but I want to introduce one piece of jargon. One notion is that the $X$'s sit on some sort of a **manifold**.

A manifold—think of it like a subspace—is such that if you take all the points in $X_1$ through $X_5$ here in a five-dimensional space, most points don't exist.

In this space where $X_1 = X_2 = X_3 = X_4$, all these points are together. This five-dimensional space lives in, in fact, a **two-dimensional manifold/subspace**.

I only need to specify two numbers that tell me everything about that five-dimensional space.

#### Example: Face Images

If you think about images of faces, there's a big space! If it's a $50 \times 50 \times 3$ low-res image, it's a $50 \times 50 \times 3$-dimensional space. But all the face images sit in a sub-manifold. Almost all possible pixel combinations are not faces.

So if I ask, "How important is this pixel in classifying my face?", I have to ask: is it something where I'm changing all the other pixels keeping the correlation structure?

Most people either have hair or don't have hair. Most face images live on a very low-dimensional manifold compared to the original high-dimensional space.

If I think about changing a feature, if I remove half the glasses, probably the other half of the glasses should go too.

### On-Manifold vs Off-Manifold Changes

So in terms of learning, we can talk about changes that are either:

**On-manifold:** Change $X_1$ and make sure that I change $X_2, X_3, X_4$, keeping the correlation structure that I've observed in the world.

**Off-manifold:** Change $X_1$ and just keep all the others fixed.

They're neither right nor wrong, but they have different sorts of interpretations. And mostly, we'd like to have causal interpretations. If I go in and change $X_1$, frankly, often you can't change $X_1$ and leave $X_2$ the same.

**Example:** If I change the price of an article, I'm going to change how much you pay when I bill your credit card. It doesn't make any sense to say I'm going to change the price and not change how much you pay. That's a weird counterfactual move that doesn't make any sense.

Is this clear? This is the first of many jargon pieces that will show up again with different names later on.

## Methods for Measuring Feature Importance

### Replacing with Average Value

We talked about dropping out one of the features and setting it to its average value—sort of remove it and see what happens there. If I replace $X_1$ by its average and don't let it vary, what degradation in the model do I get?

If my model is $y = X_1 + X_5$, and I replace $X_1$ by its average, I've lost some information.

### Permutation Test (Random Forests)

That's not my favorite method, but it's often reported if you do random forests. One version is:

1. You train up your model
2. You then test it on a test set
3. You've got some error on the test set

Then you take that trained model (we're not going to retrain any models again for the rest of today). You permute that column of $X$. You've got your $X$'s: $X_1, X_2, X_3, X_4$. I take the $X_2$ column and I just shuffle it—a permutation test. I shuffle all the values here, and now I plug these $X$'s into my model.

It'll change the $\hat{y}$'s, probably. And I see how much it affects the output. I can measure how different the error is after versus the error before.

Often you normalize it by some uncertainty on the error, some standard error. If the error is 5 ± 2, and it goes from 5 to 6, the normalized change is:

$$\text{Normalized change} = \frac{6 - 5}{2} = 0.5$$

You shuffle the values within a column. If the feature's not in the model (this is a random forest—it could have selected the feature out), shuffling the feature will have **zero effect**. If the feature is highly predictive, it'll have a big effect.

#### Why Permute Instead of Remove?

**Question:** Why do we not just remove the column?

Well, first of all, I've now removed the column. Can I plug this into my random forest? No, because the model expects all the features. I could set them all to zero. That's done sometimes, but less commonly.

#### Reference Values: Zero vs Mean

So I could plug the model always into 0. But what's a better number I could plug in than zero?

**The mean.** I could plug in $\bar{X}_2$ all the way through. That preserves the mean but destroys the variance.

That's not a bad idea—that's okay. So in some sense, we're thinking about putting in some $X_2$ reference value (often called a **reference value**). You could plug the reference value of 0 in, but that's usually a bad idea because you can shift the whole thing up or down.

If you're a statistician and you've standardized the $X$'s, then 0 and the average are the same thing (because the mean is zero after standardization).

But if you're plugging stuff into a neural net, you've plugged in your pixels. For the neural net or a fancy model, it's much nicer to plug in the average.

#### Linear vs Nonlinear Models

For a linear model, you don't care about the standard deviation. But if it's nonlinear—if you've got a neural net or a random forest—arguably it's nicer to permute it because only the average model matters for a linear model, but for a nonlinear model, permutation's a little bit better.

So if you look at a typical machine learning paper in a medical journal where you're using a permutation test, if it's nonlinear, they use permutation. If it's logistic regression, who cares? Plug in the mean.

**Question clarification:** How does it perform without that column versus putting in a reference value?

**Answer:** What I'm trying to ask is: How important is this feature? And you say, "How well does it do without that column?"

Well, first of all, my model takes in all $X$'s—you can't delete the column. That's just a type error. You've got $p$ features; you can't plug in $p-1$. You'll get an error in your function call. So you gotta put something there. You could put zero, but zero may give you something very different from what happens if you put the average in.

It's not really telling you what you care about, which is: how much information content is there, and how do these things vary?

If I knew you were going to put zeros in, I would have put a constant term in the regression, and it would have captured it. I don't want to retrain the model because I'm often given someone else's model and don't have access to retrain it. Maybe it's a big neural net that somebody gave me, and I don't even have access to it.

So I want something to say what goes on by just changing the inputs. And it's nicer to say: what's the effect of removing the information but keeping the average information? Because that's something that should have been in the constant term in the regression.

## Univariate Correlations

In general, when I'm trying to interpret things, I mostly start by not looking at coefficients or changing a permutation test, but by looking at **univariate correlations**.

### Pearson Correlation

In general, if you take each feature and you have some $y$ (if $y$ is a real number), you can ask the question—and we'll do this first for a simple version and then later for fancier ones: As I change $X$, how does $y$ change?

Maybe here's the actual data, and here's my best line fit of how well—what's the change in $y$ with $X$? The **Pearson correlation** is the slope of this line if you standardize $X$ and $y$.

The Pearson correlation, or correlation as we call it, essentially assumes the relationship is linear. Think about what it is: subtract off the means, divide by the standard deviations—it's a linear model.

### Spearman Correlation

The other piece that you should know about is a **Spearman correlation**, which is a **rank-order correlation**.

For example, if $y$ always goes down monotonically with $X$, the Spearman correlation between $X$ and $y$ is exactly $-1$.

Rank order means I'm going to take all the $X$'s and sort them from smallest $X$ to biggest $X$, and I'm going to throw away the metric.

**Pearson correlation** is basically fitting linear regression. It assumes you're sitting in an $L_2$ space with distances.

**Spearman correlation** says: all I care about is the order. I can rank order all the $X$'s from smallest to biggest. I can rank order all of the $y$'s from biggest to smallest. And I can see how well those rank orders correlate.

#### Example: Income vs Age

For some things, you could ask: How does income vary with age? If you fit a linear regression of income versus age, you get something that's a really stupid model. Why?

You're fitting a couple of incredibly high incomes—the Elon Musks sitting out there—and they're not incredibly old. He's sorta old, but he's not that old. He's not a billion years old. So in a linear model, you're giving enormous weight to the Elon Musks of the world.

Whereas in rank order, fine, he's number one. But he's the same distance from number 1 to number 2 as it is from number 3 million to number 3 million and 1.

I'm not saying one's right or wrong, but they're very different inductive biases or assumptions about the world.

**Note:** If you're a statistician, what do you do with income, always? **Take the log of it.**

Log income versus age is closer to linear, but it's still not perfectly linear, so I still might want to use rank correlation.

Oh, and the Spearman correlation between log income and age is exactly the same as the Spearman correlation between income and age, because log is a monotone transformation—it doesn't shift the ranking.

### Non-Parametric Methods

We haven't talked much about non-parametric correlations or non-parametric measures in this course. But once you're in this sort of non-parametric rank order, you say: I don't care about the actual number, I just care where it sits in the ranking. And you've thrown away the assumptions about the model form.

We don't do this much in machine learning. Mostly we use fancy models like random forests or deep learning. But it's worth noting that there is a whole branch of statistics that says: all I care about is the rank order, and the actual values don't matter.

### Non-Linear Relationships

The other piece you could ask is: what's the correlation between $X$ and $y$ in this plot?

In this plot with a U-shaped relationship, what's the correlation between $X$ and $y$?

**Zero, basically.** We're going to fit the best linear regression line, which is about a line straight across. We look at the slope of that line. The correlation is zero.

On average, looking at $X$, is $y$ higher or lower as $X$ gets bigger or smaller? On average, there's no effect. It's just as likely to go up as to go down.

**Correlation assumes that the relationship is linear.** It fits the best line. Again, you can think of it as: standardize the $X$, standardize the $y$, fit linear regression, take the slope. That's the correlation.

So realize that correlation is often a nice measure of feature importance if the world is linear. If the world is massively non-linear, that's a problem.

#### Why Might the World Be Massively Nonlinear?

**Example:** Cookie quality versus temperature. I'm baking my cookies. I've got the temperature, and I've got some measure of how much I like them. What does cookie quality look like as a function of temperature?

For many people, it looks like an inverted U-shape (though my daughter thinks it looks monotonically decreasing—she likes them raw).

For many processes in the world, there's an optimum. Change the price of an object you're selling on Amazon—what maximizes the profit? If the price is too high, you don't sell any. If the price is too low, you lose money. Somewhere in the middle is the optimum.

#### Amazon's Active Learning on Prices

If you're Amazon, does Amazon do active learning on prices? They fit a model to try and maximize profitability. Are they actively changing prices on products?

**Yes.** Put a bunch of stuff in your shopping cart and watch them. The prices are jumping around—it's $5.17, it's $5.13, it's $5.29$. They're bouncing the price around, testing here, here, here, and seeing what happens with the prices, and they're trying to fit things around the optimum.

**Question:** What jargon term did I call that kind of active learning last class?

**Response surface modeling.** Amazon doesn't care how much money it loses down at low prices or how much money it makes over at high prices. Amazon cares about fitting the model where, in fact, you're making the most profit. That's the only place I care about modeling.

But I have to do some exploration, because if I always keep the things at $5.17, I never know whether I would have made more at $5.19 or not. Plus, the outside world changes—competitors change their prices, there are sales.

So companies are always dithering (technical term: **dither**)—making small changes in their prices, adding $\epsilon$, subtracting $\epsilon$, seeing what happens. They're trying to fit a response surface that says: what's a local shift in profitability as a function of the price?

#### Correlation at the Optimum

In general, if you look at correlation of price with profitability, what are you gonna find?

**No correlation!** Because you're just as likely to be too high as too low, and you fit the best regression line, and it's flat.

Does it mean price doesn't affect profitability? **No!** Price affects profitability. But locally around the optimum, on average, the correlation is zero.

### Important Note

**A correlation of zero does not mean that the feature is not important.** The correlation could be zero because:
1. The feature doesn't make any difference at all, OR
2. You're sitting very close to a local maximum or local minimum, and going either direction has the same effect

They both give a correlation of 0, but they mean something entirely different in the real world.

## Interlude: Facebook Data Analysis

I thought just for fun, to relax for 5 minutes before I actually go and do the second half of today's lecture, I'd share something interesting.

I collected a whole bunch of Facebook posts from tens of thousands of people and found the words most correlated with being female (which is a little gender embarrassing, but the males are just as bad, so don't get too upset yet).

First of all, note in a typical feature analysis, my definition of a word is very generous. I assume in this audience everyone knows what a "less than 3" (❤) is? I talk to older guys—they've never seen a heart, they don't know what's going on.

"Love you" is a word, or "so happy." These are pairs of words that are next to each other that have high pointwise mutual information. They tend to correlate with each other, show up together a lot. So it's a way of grouping together words.

### Female-Correlated Words

It's a little bit stereotypical, but it's sort of fun about these—the most cliché… Women's most correlated words were:
- "so" with three O's
- "yummy"
- "loves her"

### Male-Correlated Words

The male ones are a little bit more embarrassing:
- Beards show up there
- Government shows up there
- Xbox shows up there
- PS3

And again, this is statistical, not deterministic.

The other thing I should point out is I've displayed two things:
- The **size** is the correlation
- The **color** is the frequency

So it's good to know: the F-word is both highly correlated and frequent. "Wishes he" is highly correlated but not very frequent.

Going back, there are subtle differences. Women talk about their boyfriends and their husbands and "hubby" and best friend. And the boys talk about "my girlfriend."

Got little differences. I don't know, it's sort of funny. It's so cliché.

**Note:** This is very old—this is like a decade old. You can see there was a World Cup happening that year. You can see the date on it.

### Personality Traits

People who are neurotic (who take a survey and score high in neurotic):
- "sick of"
- "hating things"
- "lonely"
- "pissed"

People don't say "depressed" very much, or "depression," but if they are depressed, it's bad.

If you're well-adjusted, it's sort of cool actually. These are Americans. What do well-adjusted happy Americans talk about?

Interestingly, lots of sports:
- "success"
- "blessings"
- Religion (religious Americans are more well-adjusted and more agreeable, happier than non-religious ones)

But you also get:
- Soccer
- The beach
- Basketball
- Somewhere the Celtics and the Lakers
- Snowboarding

Sort of cool. So being into sports is maybe protective.

I talked to a guy who's trying to help males bond more. He says a thing to do for men is: you need more friends—watch the game with someone else, don't watch it alone. I did this a decade ago. He's now running experiments and trying to get guys (women are better at making friends; guys are not so good at it statistically) to go out there and watch the game together and bond.

He's trying to build community. (He's not trying to sell religion, which is another effective way of finding community.)

### Summary of This Section

Okay, so I'm digressing, but let me wrap up this section. We started at the end of last class saying: **look at the data.**

- Which things are super frequent?
- Which ones are rare?
- What's the highest income?
- What's the lowest income?

Often, I collect data from a bunch of countries, and there are people in my dataset claiming they're filling out my survey and claiming they're making over a million dollars a year for their household annual income. **Unlikely.** There's some sort of data error. Who knows—they were doing the local currency in Nigeria rather than US dollars, they were reporting monthly instead of yearly.

**Look at the data. Look at the big numbers, look at the bad ones.**

Anytime you're fitting something regression-like, if you've got one Elon Musk out there, it messes up your whole dataset. Or one person who puts an extra zero when they're filling out their income, or something got messed up.

**So look at the data, see things there, try clustering them.**

Often build a model and ask: which of the inputs maximizes the output? If you have profitability on items, which item is your most profitable item? What are your least profitable items? Oh, you're losing money on them! That's interesting. How many are you losing money on? Lots of them, okay! Maybe I should stop selling those.

So just sort things and look at them, and think about this for your final projects:
- Are there ways to look at what are the things that are giving you the highest probability, the lowest probability?
- Do they make sense?
- Are they mislabeled?

**Every dataset I've seen has got errors in the labeling in some sense.** Things get wrong.

Often look at correlations. For language, it's word clouds, but think about what's the way to visualize your data.

And remember that if you're fitting a neural net or a random forest and trying to measure feature importance, you're getting something that's a little bit confusing, and we're gonna spend the rest of the class talking about that.

**Often, just doing a univariate correlation**—which of your features going into your model are most predictive?

Now, if you're doing an embedding, that's not so useful because embedding is not coordinate-wise. The individual components of your vector embedding don't tell you much. So you're back to asking: which of your inputs maximizes your output? Look at them. Is there some feature that you can detect that tells you what things are causing you to say this is a nice statement or a mean statement, or friendly or hostile, or whatever your label is?

**Just look at the data.**

## Two Cultures: Machine Learning vs Statistics

Switching gears only very slightly to today's main lecture…

Leo Breiman, a late statistician, talked about **two cultures**.

### The Machine Learning Culture

In his world, this is the algorithmic modeling culture:
- Take a $y$, learn a model
- Minimize the out-of-sample predictive error
- We don't care too much about the $X$'s
- We don't care at all about the features in the model

Now, that's sort of changing because explainable AI is very hot right now. There are probably 7 people out of 10 on the faculty who do explainable AI as one of their big pieces. So explainability is very big.

But in some sense, it's still very different from statistics.

### The Statistics Culture (Beta Hat Culture)

What Leo calls the **beta hat culture**. In his world, you fit statistics, or you fit a simple model where you can look at the coefficients in the model, and you can say: How big is $\hat{\beta}$?

Of course, he has standardized the $X$'s because he's a statistician, and he standardized the $y$, so the $\hat{\beta}$ means something. It's on absolute terms because the $X$'s are all standardized (mean zero, standard deviation 1).

### Where We Stand

We mostly sit somewhere on the ML side, but these days people are really concerned about explainable AI—typically where you can't look and find the beta hats.

Look inside a random forest—there are no beta hats. It's not a parametric model.

Look inside a deep learning model—there's a word for that. What do people call looking at the weights?

**Mechanistic interpretability**, or just **interpretability**.

Very big right now is: look at not so much the weights in a neural net (which are confusing), but look at the activations, the output of the hidden neurons, and look at which neurons get activated for which inputs. That's **mechanistic interpretability**.

So that's a thing that's popular. This is sort of a different view of: is your model simple enough you can look inside it (you're a statistician), or is it something complicated where we're mostly doing input-output mappings?

## Correlation vs Causation

The other thing to note is that almost all of this course has been **correlational, not causal**.

Almost everything in machine learning is correlation.

**Question:** What is the one subsection of the course where we had causal models as opposed to correlational?

**Answer:** Reinforcement learning!

If you're doing supervised or unsupervised learning, there's no causality. You're finding the correlations between things. In RL, you're taking actions and seeing the effect of the actions. **You're learning a causal model of the world.**

That's really cool! If you're someone who's actually doing stuff—which is almost anybody who's gonna pay you, because they have to be making money—you care about causality.

So although many of you will have jobs where you spend your time analyzing correlations and building neural net models, most of what you're trying to do is understand the causality of the world and control it so you can help someone make more money (or make the world better).

But super important: RL—you take actions, you see the effect of the action, you're running experiments.

### Belief Networks and Causality

**Question about belief models:**

Belief models are interesting. Some of them do encode causality, and some of them don't. Many of them do encode causality.

A standard way (which is not a machine learning way so much) would be to write down a causal model of the world in the belief net and then estimate the probabilities. So you estimate all of the conditional probability tables.

Belief nets don't have to be causal, but many of the belief nets that people build are causal. Some are inverse causal—those are diagnostic ones.

You can build a belief net that takes diseases and then generates symptoms downstream of them. Most people build them the other way around—they build symptoms up top going down to diseases, so they're diagnostic rather than causal. The causal model now has to invert the belief net using Bayes' rule.

But yeah, belief nets are a good example of something that is often causal.

## Why Do We Want Explainability?

Why do you want to do feature interpretation?

1. **Debug your model** and see why it doesn't work
2. **Know if it generalizes**
3. **Know if it's worth collecting these features**
4. **Explain the world** – Most of the models I build are for researchers—people in the medical school, psychology department, business school. They want to understand what's going on, so they really want causality much more than they want predictive accuracy.
5. **Trust and transparency** – Sometimes you want to know not just what's the answer, but why did you give me that answer? Should I trust it?

**Example:** Tell me what features led you to say this person was likely to commit suicide and therefore should be referred to a psychiatrist. I don't trust you, machine learning model. I could ask ChatGPT, "Is this person suicidal?" But I'm not sure I trust the answer. I could ask ChatGPT why, and I don't trust that answer either.

## The Problem of Correlated Features

As I said before, but I want to say it again: **explanation is tricky**.

You build a model that takes in:
- Your age
- Your height
- Your weight
- Your body mass index (BMI)
- A bunch of other scores (glaucoma scores, other risk scores you get)

If I try and measure how important each of these are in some prediction, what's the problem with this model in terms of how important are each of these?

**They're correlated.**

Are they very correlated or sort of correlated? Well, BMI is what? **It's an analytic function of height and weight.**

$$\text{BMI} = \frac{\text{weight (kg)}}{\text{height (m)}^2}$$

You can tell me height, weight, or BMI—given any two of them, I can compute the third one. They're not linearly correlated; it's a non-linear relationship. But they're all correlated.

### Which of These Are Causal?

**Age:** For sure, although not very useful. You know what? What one number would you like to change to live longer? Age! If I could be 6 years old, I'd live much longer than being my age. But that's not very useful.

**Height:** Okay, actually, shorter people live a little bit longer than very tall people. But if I get a leg extension surgery to be shorter, it doesn't cause me to live longer—causality's not really there in a useful way.

**Weight:** GLP-1 drugs, Ozempic—you live longer if you're obese and you take a good GLP-1 drug. **Causal**, and the mechanism… What's more predictive, probably your BMI?

Why is BMI better than weight? If I'm 5'5" or 6'5", is the weight the same? Weight relative to height is some measure of obesity. You like it or not—I mean, there are better models, but the point is that many of these things are correlated.

**Is age and height correlated?** Well, if I've got a lot of pediatric data, I can tell you the one-year-olds and the 16-year-olds are very different heights and weights. It is also the case for adults that, in general, Americans who are 60 are heavier than Americans who are 20. There's a correlation structure there.

So all these things are correlated, and now if you're asking feature importance, you have to ask: Why do I care? And what does it mean to change one without changing the other?

I could change age without changing height. But to change height without changing weight and BMI is pretty bizarre. That's **off-manifold**. I've broken the correlation structure. In this case, the correlation structure is an equation which we computed:

$$\text{BMI} = \frac{\text{weight (kg)}}{\text{height (m)}^2}$$

Or in other units: centimeters over kilogram squared, or whatever.

**We want the causality, and so you gotta be careful about choosing these variables.**

## More Precise Definitions

Let's now be a little bit more precise and define a few more pieces.

We talked about:
- Univariate correlation (like Pearson correlation)
- Removing a feature and retraining the model
- Setting a feature to its average
- Permuting features

Usually you don't retrain because it's too expensive. And we're going to talk later about Shapley values.

## LIME (Local Interpretable Model-agnostic Explanations)

I want to mention a couple of methods that are used. One that was very popular a couple years ago and is still used a bit in machine learning is called **LIME**.

I think it gives a good example of a slightly ad hoc feature importance method that's popular, and it demonstrates something that, in fact, will turn out to be related to a Shapley value in a few minutes.

### Example: Tree Frog Image Classification

I've got some image up there. And I've built (or someone else has built) a predictive model that says: what's the probability of this image being each of a thousand different items? In particular, it says that the probability it's a tree frog is 0.54.

So far, so good? Now we'd like to know: **Why does the model think it's a tree frog?**

### Why This Is Important

This is important because people have done the same thing with cancer images, and when you use this method for cancer images, sometimes what you find is: **the little watermark that tells you which machine it was read on predicts whether it was cancer or not.**

Is that correlationally reasonable? If you collect half a million cancer images (cancer, non-cancer), label them from the doctor, train a neural net to predict cancer and non-cancer, and you apply a method like LIME, sometimes LIME says: "I think this person has cancer," and the region it thinks is this one right here—and you look at it, **it's not even on the person. It's a watermark.**

A little thing that comes on the image when you take it. Often the image has some information about the machine it was taken on (like your camera could put in "taken by my iPhone" on this date, in this location).

**Is this a machine learning error or not?**

**No, it's a totally reasonable correlation.** People on some machines are more likely to have cancer than people on other machines. Why? Partly because if I have an image either taken from the hospital at University of Pennsylvania on our very expensive high-resolution machines or taken at Bryn Mawr Hospital on their cheaper, less good machines, who's more likely to have cancer? **Bad cancer—the HUP one on the expensive machine.**

It's predictive. I'm not sure I want to use that in the model or not (that's an engineering question), but it's statistically correct.

But I would like to know, as a doctor looking at it: does the machine think she has cancer because it was taken at HUP with the expensive machine, or because of this region of the breast? They're very different pieces to direct the doctor who's trying to double-check these things.

### The LIME Process

With that background, let's look at LIME. Here's the process:

1. **Segment the image:** We're going to take the image and run a standard segmentation algorithm over it (I like SAM from Meta, but pick something that breaks the image into a bunch of little segments). This is a clustering algorithm, but we'll take an off-the-shelf one. So now we have, instead of pixels, clusters—you can sort of picture how big the clusters are by looking at it.

2. **Gray out random subsets:** Now what I'm gonna do is I'm going to gray out a random subset of the little regions. Just replace them with gray.

3. **Feed into the model:** I'm gonna feed them into my same pre-trained model. I'm gonna see what the prediction is.

In the top example, you can see they've grayed out a bunch of stuff, and you still see the frog's head and eyes and the little red part of it. That's 0.85 probability to be a frog. That's more likely to be a tree frog than the original one.

You can see another one with a bunch of random pieces grayed out. The middle one, where it's almost all grayed out, is not a tree frog (it's something else, because this is a softmax, but it's certainly not a tree frog).

Something where a lot is grayed out except the left hand is 0.5 probability tree frog—it's really close.

4. **Analyze which regions matter:** So now I've got a bunch of different images with different pieces grayed out. Now I can ask the question: which of these pixels (or in this case, which of these regions—because I'm not doing a single pixel, because neighboring pixels are highly correlated and we've segmented them, so each little segment is all pretty much the same color)—which of these regions are most predictive of it being a tree frog?

It's like a super-pixel. Now we could ask: if I take different pieces here, what I want to know is which of these regions (there are a bunch of regions in here—you can see what the regions look like, a bunch of these little regions) are most predictive of it being a tree frog? And the ones that are most predictive are the ones I care about.

5. **Locally weighted regression:** One version to do that: each of these is sort of a feature, and plug them into a **locally weighted regression** and try and find the ones that have the highest weights and the most importance.

6. **Results:** What I get out here is that these are the regions (there's one little one up here which is a mistake) and mostly it's the face here and the eyes and the mouth (it's missing half the eye, but these are the pieces it's most relying on).

### What We've Done

We've done something that says we're sort of like zeroing out some features. The way we zero out each feature: here's the little region, we zero out the region by setting it to gray. We see how much effect it has. And we see in combination which of these are most predictive.

### Questions About Reconstruction

**Question:** Would trying to reconstruct the original image be equally good? What I could do is take each of these masked items and try and reconstruct the original image (a self-supervised or unsupervised learning approach). Equally good?

**Answer:** **No.** I don't care about reconstructing the rest of the image. I care about predicting the label.

Most of the image—there's a lot of background stuff—the background is truly not important. Now, when you look at these, I've seen cases where you try and predict "is it a zebra or not," and what it mostly looks at is: is the grass green or gray, or is it cement? Because zebras apparently mostly live in this training set in more yellow grass rather than green grass or cement.

So sometimes it looks at the background. **I don't want to reconstruct the savannah or the trees around it. I want to know: is it a zebra?**

**Follow-up question:** With the reconstruction, are you talking about the label?

**Answer:** Not really, because the problem is: for most of the image, if I take a picture of a zebra with a bunch of elephants around in the savannah with nice little fever tree acacias and stuff around there, there's a whole bunch of stuff in the image. And the fact is, sure, elephants are more likely to be there than cows in this environment, but I don't care about that.

I'm asking: you told me there's a zebra in the image. Why do you think there's a zebra in the image? **I want black and white stripes**, which is what I look for when I look for zebras. (Whereas my British friends have zebra crossings with black and white stripes. In America, we don't have zebra crossings.)

I think it's very different. **Reconstruction is unsupervised/semi-supervised. It cares about everything. Feature importance says: I'm predicting a $y$.**

And note that I had some particular $y$: Why do you think this is a tree frog? This is 0.54 probability tree frog. There's 0.46 probability distributed over all the other stuff.

Why do you think this is a ceramic doll? And maybe that's looking at the tummy. Or why do you think it's not a ceramic doll?

**Note that feature interpretation as we do it is almost always about: How much does this feature determine the label?**

**Further clarification:** Isn't reconstruction unsupervised?

**Answer:** Yes. Reconstruction sits in the unsupervised camp. It says you only have an $X$. One part of the image is telling me about the rest of the image. Reconstruction sits only in $X$ space—there's no $y$ in reconstruction. Think of PCA and then reconstruct from the PCA, or think of embedding.

So reconstruction sits only in $X$ space. There's no $y$ in reconstruction. **That's a fundamentally different problem** than feature interpretation as I and pretty much everybody in ML uses it, which is: How much does this $X$ predict the $y$ label?

I really think it's different. We have fundamentally:
- Supervised learning
- Unsupervised learning
- RL

They're just different problems. Reconstruction is unsupervised—there's no $y$.

**Comment about diagnosis and causality:**

Diagnosis and causality flip the direction—instead of flipping the $X$ and the $y$, you can go from disease to symptom or from symptom to disease, but those both have an asymmetry. They both have a feature and a label. And that's a really different problem: which features predict the label?

You can do either way. You can say: give it a symptom, predict which disease you'll have. You can say: give it a disease, predict which symptoms you'll have. You could do a regression that way. It's a flipped one, but it's still supervised learning.

**Key point:** It's super important to understand that often, given something like an image, you'd like to know: What are the parts of the image that most caused you to give it this label?

Typically, by showing it to a clinician, these are used a lot for doctors. It says: "Hey, I've circled the part that I thought was important. Here I've circled the tree frog." And that one little piece in the corner—this is why I think it's a tree frog.

Note the process, which was randomly zeroing out a bunch of features (in this case, segments), and then saying: for each of these features that I zeroed out, when I zeroed it out, how much effect did it have on $y$?

If you zero out the stuff in the bottom left or in the upper right-hand corner, zeroing that out has no effect on your prediction. Replace it with gray, plug it in the neural net, no change. Whereas if you take his smiling or frowning mouth here and zero that out, those segments have a big effect on the $y$. **So these are the most important features.**

### Locally Weighted Regression Details

**Question about how the locally weighted regression works:**

I'm trying to avoid spending too much effort here, but I think the way to think about this is: every segment here is a feature. I would like to build some sort of a model that predicts: is it a tree frog or not as a function of those features?

Instead of actually doing a global linear regression across everything possible, in some sense, what I want is to do a regression that puts lots of weight on image pieces that are close to the one you're looking at.

I don't care about a global regression. Remember back when we did RBF (radial basis functions) and locally weighted regressions? We said: if you do a regression, one way to do it is the farther the observation is away from the one you care about, the less weight it has.

So the way that these guys are doing it is saying: I don't want to assume linearity and fit a global model of how important every feature is. I'm looking around some particular piece—around the image that I care about, around that piece there—I want to have a regression that gives more weight to the things that are close.

I'm also gonna say: don't worry about it, I didn't really cover it in detail, so I waved my hands.

## Types of Explanations

There are several kinds of explanations, and it's interesting to think about them more carefully.

### Interventional vs Conditional

**Interventional:** You change one feature, leaving the other features fixed, which tells you about the model. If I change this segment of the image leaving everything else fixed, does it change my prediction? That's a description of something about your algorithm.

In some sense, it allows you to violate the correlational structure.

**Conditional:** I'm going to change this one piece of the image, and I'll shift all the other pieces in the same correlation structure. So if I change this piece of the frog image from green to red, I'll shift the rest of the frog red as well, effectively, because that's the correlation structure.

I gave these two things a totally different name before:
- **Interventional = Off-manifold**
- **Conditional = On-manifold**

Intervention says: I'm going to change the feature, breaking the correlation structure, leaving everything else fixed. **That never happens in the world. You've broken the correlation structure.**

Machine learning people mostly do **off-manifold interventional** because it's clean and simple, and it explains the model. But it's a little weird because it says: "Hey, I've made this person taller, but I haven't changed their weight or their BMI or their age." But that's weird.

If I make you 10 centimeters taller, I really should change—well, first of all, your weight statistically if you're on-manifold, and for sure your BMI.

So the standard feature change that you get with LIME and most things is **interventional**, or if you will, **off-manifold**. I'm making the person 10 centimeters taller, but I'm not changing their BMI—that violates the physics of the world.

### Model-Based vs Model-Agnostic

The other jargon you should know (second of three jargons on this slide):

**Model-based:** That would be the beta hat culture. You look inside the model and see what's there.

**Model-agnostic:** Except for mechanistic interpretability, most machine learning is model-agnostic. You change the $X$, see how the $y$ changes. You don't look inside the model at all.

### Local vs Global

Finally, you can have:

**Local feature importance:** For a single observation.

**Global feature importance:** Average feature importance across all observations.

When I did the LIME example, was that local or global? **That was local.** For this image, tell me why you think it's a tree frog.

When I looked at the early example today, $y = $ a linear function of $X_1, X_2, X_3, X_4, X_5$—local or global? **Global.** Averaged over all the data of the training set. This is how much it matters on average.

They're both legitimate. Sometimes you want to look at one particular patient and say: "Hey, why do you think she has cancer? Circle the region, computer." And sometimes you want to say: "Hey, here's a model. Tell me what features this model is using on average to try and predict housing prices, or whether you get a mortgage, or whether you get hired or not."

That's a property of the model. It's **global**. It's not for any particular person, not "why did I not get the job?" but "what features do you look at in your model when you're trying to give someone a job?"

People worry about them. How much does it matter whether it's a male name or a female name? That's a common test people run.

By the way, with males and females, now it's actually slightly reversed. Last I checked, GPT, given the identical resume with a male name and a female name, is more likely to recommend the male candidate to be hired. Sorry, girls/women.

But this is a feature importance test one would like to run if you're a company. Good companies like Amazon famously built a resume screener for checking who to hire, and they tested it, and it turned out it was biased. It was using features it shouldn't have, and they never launched it because they said: "We don't… there are lots of good people we'd like to hire, and we don't like the fact that our machine learning model is paying attention to the wrong features."

So it's important for safety reasons—but safety could include: am I unintentionally discriminating against women? That would be a safety issue. Check to see what features are driving the decision, either for an individual person or globally across your whole model.

### Causality

Finally, we often want to have **causality**. Often the question is: What can I change? And if I changed it, what effect would it have on the outcome?

Some things are changeable and some are not. And some change as a function of other ones. That's life.

#### Two Different Situations

Consider:
1. An insurance company trying to decide how much to charge
2. A doctor trying to decide what to diagnose

**Only one of those cares about causality**: The insurance company doesn't care. A correlational model, a machine learning model, is fine. They want to know how much they're going to have to pay out in expectation next year (the expected value, negative expected value). They don't care about the causality, mostly.

Whereas the doctor wants to know: What should I tell you to do to change your outcome, to make that $P(\text{Death})$ lower?

So, you know, they're both useful.

## Feature Clustering Example

When you look at individual predictions—and here I grabbed a bunch of data from Children's Hospital here and took all the features I had—often you have to do a bunch of work.

We have:
- The triage category, which is highly predictive of dying because you get triaged when you come to the ER
- A bunch of heart rate measures: maximum heart rate, minimum heart rate, average heart rate (but they're all highly correlated)

You throw them into a random forest—they all show up. You put them in ridge logistic regression—they all show up. So you see a bunch of things like maximum heart rate and minimum heart rate and another triage category, and where you came from, and another triage category, and your worst pain score. You see all these things that are highly correlated.

**Individual features are often fairly meaningless.** So what you have to do is cluster them together.

Here I've taken just the correlation across all the people in the hospital and then done an **agglomerative clustering** (which we haven't covered in this course, but you could have run a k-means instead).

You see that lots of the heart rates tend to cluster together along with respiratory rate. Lots of the GCS scores tend to correlate along with where you came from. There are lots of things that correlate.

You could do a PCA, you could do a clustering, but often you're taking lots of features before that are individual and grouping them together. For LIME, we took the segments—we clustered the pixels using a segmentation algorithm. **If you've got a bunch of features that are moving together, typically before you explain, you cluster the features.**

Now you can ask the feature clusters: How important are all the triage categories? How important are all the heart rates? **Sets of features.**

## Summary of Methods

We've covered a bunch of these, which I'm going to skip most of in the interest of time:
- Correlation
- Removing features
- Permuting features
- Retraining

### Partial Dependence Plots

The one we didn't talk about (the last substantive thing I will cover) is **partial dependence plots**, which are a cool way to look at something.

The idea of a partial dependence plot is: I want to look at the effect of one feature, say $X_1$, marginalizing over all of the other values.

Marginalizing means effectively taking the expectation, summing over all the other ones.

So if I want to know how important $X_1$ is, I can say: What's the effect of changing $X_1$ averaging over all the other combinations of $X_2$ and $X_3$ that I have in my dataset?

You can take each of my points because the world could be nonlinear. Your actual output $y$ depends—in this case, linearly on $X_1, X_2, X_3$:

$$y = \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3$$

But in general, it could be that I want to look: for different $X_2$ and $X_3$, what happens to $y$ as I change $X_1$?

We're going to skip Shapley values, which we're not going to cover.

#### Example: Age Effect on Mortality

What I might get for something which says: if I look for age in my dataset, how does age affect my average effect on my outcome (in this case, mortality)?

What I see is: if I take the effect of age at each different age and then I average out over all the other features I have for these people (I marginalize over, I take the expected value over all the other features), you can see that on average (these are all relative to some mean): younger people are going to, in this case, be healthier, live longer. Older people, less good.

## Conclusion and Key Takeaways

I'm running out of time, but I think the key takeaway to note is that there are a bunch of methods, and some of them have nice mathematical properties.

We didn't cover Shapley values, but LIME would be one example where you say:
- Replace this feature with a reference value (the average)
- Take all the other features and permute them all, or marginalize them, or do something with them
- See what the effect is for the one feature
- Or take all the other possible values of this feature and do partial dependence plots

Change this one feature and say: how does it change things? **There's a bunch of tools out there.**

But they all have this question that you're implicitly or explicitly assuming: **Is it interventional (off-manifold)?** Usually yes.

Or is it something that says: if I change this, change the other things to preserve the correlation structure? That's very rare, but in some sense is closer to causality.

### Important Considerations

The thing to worry about is: **Is the method assuming the world is linear?** Which a lot of them are, like correlations (at least Pearson correlation—I talked about Spearman correlation).

Or is it something that actually captures nonlinearity, that shows the nonlinearity?

As you look at these methods:
- Is it local or is it global?
- Is it interventional or conditional?
- Is it model-based or model-agnostic?
- Does it assume linearity or handle nonlinearity?

Think about these features. I've got about 6 or 7 terms I've used. Go back and look at them.

You're not responsible for the Shapley values, which I didn't cover. There's also a bunch of slides at the end which I'll remove eventually.

## Course Wrap-Up

At this point, **we're done with all the material**.

- Presentations
- Final project
- Yay, yay, go for it!

Monday will be just a review, and then the midterm a week from Friday.

Hang in there—the end's almost here. Remember to **sleep occasionally**, and I'll see you all Monday for the last class.

Thank you.

---
